---
slug: the-strength-in-weak-data-part-3-prepping-the-model-dataset
title: "The Strength in Weak Data Part 3: Prepping the Model Dataset"
authors: [kristin]
tags: [crop, yield, corn, prediction, model]
unlisted: true
# image: https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/social_jennings.png
hide_table_of_contents: false
keywords: [crop, yield, corn, prediction, model]
---

Hello friends, thanks for following my journey so far. To catch you up, I'm trying to solve the problem of farmers and traders relying on weak and untimely predictions of corn yield. Weak because they are at the national level and untimely because the predictions come once a month.

So here's the deal: farmers and traders have been relying on national-level corn yield predictions that are not only **weak** but also **painfully slow**, arriving just **once a month**. Imagine making critical decisions based on a single data point each month. Not ideal, right?

Exactly.

{/* truncate */}



Now, consider this: the **$21 billion corn commodities market** is being traded on this scant information. Yikes! It's time we find some efficiencies using advanced geospatial modeling and the latest research.

Let's find some efficiencies using advanced geospatial modeling and the latest research‚Ä¶

Based on my previous articles, we know the Solar Induced Flourescence is a direct measurement of photosynthesis and therefore a better indicator of crop yield than NVDI indices.

But there's a catch‚Äîthis data is weak because it's at a **25 km¬≤ resolution**, which any geospatial machine learning scientist would find, well, less than ideal. To make matters worse, our actual yield data (our target variable) is at the **county level**. Double yikes!

However! this problem can be solved!

Thanks to the scaling capabilities for geospatial modeling in FUSED I am able to solve this data by increasing the amount of data and therefore the statistical power of it.

## Diving into the Data Details

Let's get into the details of how much data that actually is. If you remember from the previous post, I was able to change 1 data point per county to 20k by



<div style={{textAlign: 'center'}}>
<img src={'https://img.notionusercontent.com/s3/prod-files-secure%2Fbf8160c8-8a03-4e5b-9fa0-c290118bf66b%2F18684f6e-34b5-43fd-b667-d03794cc1905%2FScreenshot_2024-12-04_at_11.36.00_AM.png/size/w=1420?exp=1733407479&sig=cr4WqZn9WJrAUcLFf9OZLdvKX8vn-FvAEhg-J7a4U74'} alt="File"/>
</div>


Okay let's say I want to do this for this area of interest:

<div style={{textAlign: 'center'}}>
<img src={'https://img.notionusercontent.com/s3/prod-files-secure%2Fbf8160c8-8a03-4e5b-9fa0-c290118bf66b%2F26beedb6-63e0-4a93-a431-e54e2cbebec3%2Fb6bc9fdc-9655-45c3-a4bc-f75fbef07e1b.png/size/w=2000?exp=1733407522&sig=osr4Wfby_TEVIe2lmxX_X1dU-otMGceVmNV2f1ahgbE'} alt="File"/>
</div>

Within this area, we have **1,333 counties**. Assuming each is similar in size to my home county of Lyon County, we can calculate:

1,333 counties √ó 20,000 data points = **26 million data points**

That's **20,000 times** the statistical power! üéâ

Let me say that louder for the people in the back: **26,000,000 vs. 1,333 data points**

And that's just for one time period. If we run a model on 2‚Äì4 time periods, we're looking at nearly **100 million data points**. Now, building a model on 100 million data points isn't trivial, but with the compute technology at Fused, this process becomes a breeze.

I'm aiming to predict corn yield based on my SIF readings from early May, late May, early June, and late June. So, we need to build out a table like this:

## Let me say that louder 26,000,000 vs. 13,000

| Date  | State | County | GEOID | SIF_Corn | Percent_Corn | Number of Acres/Pixels | Yield  (bu/acre) (function of ‚Äúdate‚Äù and ‚ÄúGeoID‚Äù |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 201506a | 19 | 119 | 19119 | .56 | 45% | 50,000 | 183 |  |


I want to be able to quickly validate this table against a map, so I will build out my map in Fused in Python and use SQL to query the table. In the past, working with these two languages, would have taken 2 different programs, I would have to store in a data warehouse, and roughly 5 hours to this program. With fused, I can simply call the python object and query in SQL all within the same UDF-taking 5 seconds. Here is what it looks like:

<div style={{textAlign: 'center'}}>
<img src={'https://img.notionusercontent.com/s3/prod-files-secure%2Fbf8160c8-8a03-4e5b-9fa0-c290118bf66b%2F1717d76a-aad4-4e8b-8e1f-27a16720b589%2FPHOTO-2024-12-03-19-14-50.jpg/size/w=1420?exp=1733407559&sig=0748G0x0n13OOILwH82-m3vLJ8Cvp25enOoWUqhMtPU'} alt="File"/>
</div>


State ansi codes: ["38", "46", "27", "55", "19", "31", "20", "29", "17", "18", "26", "39", "21", "47", "08"]

Next, we'll need to split the data carefully to avoid bias. (Stay tuned for more details on our data splitting strategy!)

Here's what our training dataset looks like:

<div style={{textAlign: 'center'}}>
<img src={'https://img.notionusercontent.com/s3/prod-files-secure%2Fbf8160c8-8a03-4e5b-9fa0-c290118bf66b%2F33cc434d-48f7-46ee-aaa3-107cd43786f5%2Fimage.png/size/w=1420?exp=1733407586&sig=K0c8bsJgSUiuIAC96FFL-nReNNwE1xA9UXnzLi7oQ1o'} alt="File"/>
</div>


Viola!

Now the really fun part. I get to run all sorts of different models. Auto Regressive model because it predicts the value of a variable based on its own past values, which I think will work well for crop yields. But for that‚Ä¶.you will have to wait on the next blog post. See you next ime!
