---
title: Batch jobs
sidebar_label: Batch jobs
unlisted: true
---

import LinkButtons from "@site/src/components/LinkButtons.jsx";
import CellOutput from "@site/src/components/CellOutput.jsx";
import {BokehFigure, PlotlyFigure} from "@site/src/components/Plotting.jsx";
import Tag from '@site/src/components/Tag'

## <Tag color="#D1E550" fontColor="#141414" >ðŸš§ Under Construction</Tag>


Reference:
[Discord community request](https://discord.com/channels/1199097729243152434/1267578417411526788/1267578417411526788).

This guide shows how to execute a batch job with [fused-py](/python-sdk/).

Running long processes can be computationally expensive or challenging due to potential interruptions like browser **closures** or network issues. For these cases, `fused-py` can run batch jobs on an EC2 instance.

As an example, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

The following sample UDF downloads a zipped file from S3 file specified with the `source_s3_path` parameter, unzips it, then uploads it to the S3 path specified by `destination_s3_path`. A few notes to keep in mind:



## 1. Define UDF

This UDF first downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench [file explorer](/workbench/file-explorer/). Otherwise, you might need to adjust the permissions of the target bucket.

```python
import fused

@fused.udf
def udf(
    source_s3_path = '',
    destination_s3_path = ''
):
    import zipfile
    import s3fs
    import os
    import pandas as pd

    file_path = fused.download(source_s3_path, source_s3_path.split('/')[-1])

    # Create a temporary directory to extract the files
    s3 = s3fs.S3FileSystem()
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/unzipped_files')

    # Upload each file to the S3 bucket
    for root, dirs, files in os.walk('/tmp/unzipped_files'):
        for file in files:
            file_path = os.path.join(root, file)
            print('file_path', file_path, destination_s3_path)
            s3.put(file_path, destination_s3_path)

    return pd.DataFrame({'status': ['success']}) # UDFs must return a table or raster

```

## 2. Run UDF as a batch job

Run the UDF with `engine='batch'`. Ensure your Fused account has batch-mode enabled - get in touch Fused to enable this.

```python
job_id = fused.run(udf, engine='batch', source_s3_path = 'https://datadownload-production.s3.amazonaws.com/WCMC_carbon_tonnes_per_ha.zip', destination_s3_path = 's3://fused-users/fused/plinio/dswid/WCMC_carbon_tonnes_per_ha_10gb/')
job_id
```

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` to get the job logs as it runs.

```python
fused.api.job_tail_logs("df335890-4406-4832-bf93-6a3b092e496d")
```

<CellOutput>
{
  `Logs for: df335890-4406-4832-bf93-6a3b092e496d
Configuring packages and waiting for logs...`
}
</CellOutput>
