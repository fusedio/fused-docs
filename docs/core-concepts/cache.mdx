---
id: cache
title: Caching
tags: [cache]
sidebar_position: 5
---

{/* TODO: 
Intro concept of caching in Fused: Goal of Fused is to make developing & running faster. Caching can be a smart way to make recurring requests more responsive
Fused has 2 main cache types:
- `@fused.cache` decorator
    - Demonstrate (probably in notebook)
    - Show how to use
    - Explain *when* someone would want to use it (it's not always helpful)
        - Incredibly helpful for making workbench more responsive: query data 1 time and then make operations on it
        - Can be used as "1 time ingestion" for non partioned files
    - Mention limits
        - 24h limit
        - Changes on each code or input change, so quite brittle
- UDF caching 
    - Demonstrate (also in notebook probably: call same UDF 1 first time, then make same call again -> Should be faster)
    - Explain when this is helpful
        - Show in workbench toggle to turn this on: UDF Builder -> Settings -> Share (on by default)
        - In code somewhere?
- Caching best practices
    - When NOT to use cache
        - Ingesting data instead, prevents from having to cache things all time -> Useful when data is used many times in different places
        - When data is really too big to even be loaded in a single UDF
        Caching is really at it's best to speed up tasks that would take a few seconds or tens of seconds
*/}

{/* Caching stores the result of slow function calls so they only need to run once. This persists objects across reruns and makes UDFs faster. */}


# Caching

_This pages explains how caching makes Fused more responsive & some best practices for making the best use of it_

## Caching Basics

The goal of Fused is to make developing & running code faster for data scientists. This is done by using [efficient file formats](/core-concepts/data_ingestion/file-formats/) and making [UDFs simple to run](/core-concepts/run-udfs/). On top of those, Fused relies heavily on caching to make recurring calls much faster.

At a high level, caching is storing the output of a function run with some input so we can directly access the result next time that function is called with the same input, rather than re-computing it to save time & processing cost.

import FunctionRun from '@site/static/img/core-concepts/caching/function_input_run_cache.png';

<div style={{textAlign: 'center'}}>
<img src={FunctionRun} alt="Function + Input run" style={{width: 800,}} />
</div>

_The first run of a [Function + Input] is processed, but the next time that same combination is called, the result is retrieved much faster_

As soon as either the function or the inputs change however, the output needs to be processed (as the result of this new combination has not been computed before)

import DifferentFct from '@site/static/img/core-concepts/caching/running_different_function.png';

<div style={{textAlign: 'center'}}>
<img src={DifferentFct} alt="Different Function + Input run" style={{width: 800,}} />
</div>

Fused uses a few different types of cache, but they all work in this same manner


## Caching a function inside a UDF: [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache)

Any function inside a UDF can cached using [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) decorator around it:

```python showLineNumbers
@fused.udf
def udf():
    import pandas as pd

    @fused.cache
    def load_data(i):
        return pd.DataFrame({'id': [i]})

    df_first = load_data(i=1)
    df_second = load_data(i=2)
    return pd.concat([df_first, df_second])
```

The first time Fused sees the function code and parameters, Fused runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code, Fused skips running the function and returns the cached value.


## Advanced

### Caching & [`bbox`](/core-concepts/filetile/#the-bbox-object)

Pass [`bbox`](/core-concepts/filetile/#the-bbox-object) to make the output unique to each [Tile](/core-concepts/filetile/#tile).

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF=None):

    @fused.cache
    def fn(bbox):
        return bbox

    return fn(bbox)
```

Note that this means that if you're running your Tile UDF in Workbench, every time you pan around on the [map](/workbench/udf-builder/map/) you will cache a new file

For this reason, it's recommend to keep cache for tasks that _aren't_ dependent on your `bbox` when possible, for example:

```python {5} showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF):

    @fused.cache
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    # Loading of our slow data does not depend on bbox so can be cached even if we pan around
    gdf = loading_slow_geodataframe()
    gdf_in_bbox = gdf[gdf.geometry.within(bbox.iloc[0].geometry)]

    return gdf_in_bbox
```

### Defining a `ttl`: How long to keep your cache for

You can define how long to keep your cache data for with `ttl`:

```python showLineNumbers
@fused.udf
def udf():

    @fused.cache(
        ttl=24 # Your cache will stay available for 24h
    )
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    return gdf
```

::: tip
    Read more about this in the [Python SDK page on `@fused.cache`](/python-sdk/top-level-functions/#fusedcache)
:::

### Saving on `local` or `mount`

Your cache is by default saved to `mount/`. This means if you or a team mate runs the same function with the same input they can also leverage your previously cached functions

You can however decide to only have this available to each individual instance by passing `storage="local"`:

```python {5} showLineNumbers
@fused.udf
def udf():

    @fused.cache(
        storage="local"
    )
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    return gdf
```

::: tip
    Read more about this in the [Python SDK page on `@fused.cache`](/python-sdk/top-level-functions/#fusedcache)
:::

## Best practices

Using `@fused.cache` is mostly helpful to cache functions that have long, repetitive calls like loading data from slow file formats for example.

Here are 2 simple UDFs to demonstrate the impact:
- `without_cache_loading_udf` -> Doesn't use cache
- `with_cache_loading_udf` -> Caches the loading of a CSV

```python {6} showLineNumbers
@fused.udf
def without_cache_loading_udf( 
    ship_length_meters: int = 100, 
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):    
    # @fused.cache
    def load_ais_data(ais_path: str):
        import pandas as pd
        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

and the same:
```python {6} showLineNumbers
@fused.udf
def with_cache_loading_udf(
    ship_length_meters: int = 100, 
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    @fused.cache
    def load_ais_data(ais_path: str):
        import pandas as pd
        return pd.read_csv(ais_path)
    
    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

Comparing the 2:

import BenchmarkCache from '@site/static/img/core-concepts/caching/caching_benchmark.png';

<div style={{textAlign: 'center'}}>
<img src={BenchmarkCache} alt="Function + Input run" style={{width: 800,}} />
</div>


{/* 
TODO:
Works best when caching repetitive queries where function AND input don't change so:
- Want to cache loading data function, especially for slow data (pd.read_csv() for example or `rio.open()`)
    -> Allows to load data 1 time, but do different operations on it as you explore your data
- Repetitive operations that can take long amount of processing
- But not the best to rely for large data, at this point ingest might make more sense
 */}

## Caching in UDFs run with a [token](/core-concepts/run-udfs/run-small-udfs/#token)

While [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) allows you to cache functions _inside_ UDFs, UDFs ran with [tokens](/core-concepts/run-udfs/run-small-udfs/#token) are cached by default.

This is enabled by default when sharing a token from Workbench:

import WorkbenchToken from '@site/static/img/core-concepts/caching/workbench_token_caching.png';

<div style={{textAlign: 'center'}}>
<img src={WorkbenchToken} alt="Workbench token saving" style={{width: 600,}} />
</div>

We can demonstrate this with this UDF, that has a `time.sleep(5)` in it. In Workbench we can get the Python token and run it 2 times:

import TokenCache from '@site/static/img/core-concepts/caching/cached_token_udf.png';

<div style={{textAlign: 'center'}}>
<img src={TokenCache} alt="Token fused.run() caching" style={{width: 800,}} />
</div>

This means that UDFs that are repeatably called with `fused.run(udf)` become much more responsive (if they have the same inputs)

{/* 
TODO:
- UDFs are directly cached by default, so calling `fused.run(my_udf)` again without any change will not change anything
    - especially true if this is a File UDF (and inputs don't change that much)
    - For tile -> does change frequently as every `bbox` is different
- In workbench can turn this off (show screenshot). On by default
 */}

