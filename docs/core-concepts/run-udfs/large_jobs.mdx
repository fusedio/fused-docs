---
id: run_large
title: Large UDF run
tags: [write, endpoints, api, http, file, tile]
sidebar_position: 2
toc_min_heading_level: 2
toc_max_heading_level: 4
---

Some jobs require more resources than a few Gb of RAM or take more than a few seconds to run. This section will show how to run such larger jobs
{/* TODO: This should link to a "Best Practices" section about how big should the data processed by a single UDF be: 1 big large jobs, or many small jobs? */}

### Defining "Large" jobs

Large jobs are jobs to run UDFs that are:
- Longer than 120s to run
- Require large resources (more than a few Gb of RAM)
{/* TODO: Do these also include GPU instances with `run_remote`? */}

To run these we use higher-latency instances than for small jobs, but with the ability to specific RAM, CPU count & storage depending on the needs. 

This is useful for example when running large ingestion jobs which can require a lot of RAM & storage
{/* TODO: Link to ingestion page once we have it up */}

### A Simple UDF to demonstrate

We'll use the same UDF as in the [running multiple small jobs section](/core-concepts/run-udfs/run-small-udfs/#running-multiple-small-fusedrun-jobs-in-parallel):

```python showLineNumbers
@fused.udf
def udf(val):
    import pandas as pd
    return pd.DataFrame({'val':[val]})
```

As mentioned in the [Small UDF job section](/core-concepts/run-udfs/run-small-udfs/#fusedrun), to call it 1 time we can use `fused.run()`:

```python showLineNumbers
fused.run(udf, val=1)
```

{/* We then have 2 options to run this multiple times: */}


{/* ## Using "offline" instances (`run_remote()`)

_When to use: This is for high-latency (anything more than a few seconds) parallel processing of longer or more complex jobs_ */}


## Running a large job job: `job.run_remote()`

{/* 
Topics:
- Need to define a job: job = udf(arg_list=range(10))
- Can be used for parallel processing 
- Or for specifying running with larger instance
 */}

Running a UDF over a large instance is done in 2 steps:
1. Creating a job with the UDF to run and [passing input parameters](/core-concepts/run-udfs/run_large/#passing-udf-parameters-with-arg_list) to run the UDF over
2. Send the job to a remote instance and (optionally) [defining the instance arguments](/core-concepts/run-udfs/run_large/#run_remote-instance-arguments)

```python showLineNumbers
# We'll run this UDF 5 times with 5 different inputs
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

### Passing UDF parameters with `arg_list`

**Single Parameter**

As mentioned [above](/core-concepts/run-udfs/run_large/#running-a-large-job-job-jobrun_remote) to pass UDF arguments to a remote job, use `arg_list` to specify a `list` of inputs to run your job over:

```python showLineNumbers
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

**Multiple Parameters**

Currently `arg_list` only supports giving 1 input variables to each UDF. We can work around this by aggregating multiple variables into a `dict` and having a UDF take a `dict` as input:

```python showLineNumbers
@fused.udf
def udf(variables: dict = {'val1':1, 'val2':2}):
    import pandas as pd
    import fused

    # Retrieving each variables from the dictionary
    val1 = variables['val1']
    val2 = variables['val2']

    # Some simple boilerplate logic
    output_value = int(val1)*int(val2)

    df = pd.DataFrame(data={'output':[output_value]})
    print(f"Saving df with {output_value=}")
    
    # Saving output to shared file location to access results later
    # `/mnt/cache` is the shared file location for all small & large jobs
    df.to_csv(f"/mnt/cache/demo_multiple_arg_list_run/output_{str(output_value)}.csv")

    return df
```

:::note
  You do need to type the Python type of your input variable for this to work. If we had defined:
  ```python showLineNumbers
  @fused.udf
  def udf(variables = {}):
    # Notice the lack of `variables: dict = {}`
    ...
    return df
  ```
  without typing `variables` as a `dict` this would fail as Fused server has no way of knowing what to expect from `variables`
:::

We can then call this UDF as a remote job by passing a list of dictionaries to `arg_list`:
```python showLineNumbers
job = udf(arg_list=[{"val1": 5, "val2": 2}, {"val1": 3, "val2": 4}])
job.run_remote()
```

We can confirm this worked by viewing our results by browsing [File Explorer](/workbench/file-explorer/):

import ImgRunRemoteMultiArgList from '@site/docs/core-concepts/run-udfs/efs_results_run_remote_multiple_args.png';

<div style={{textAlign: 'center'}}>
  <img src={ImgRunRemoteMultiArgList} alt="Multiple arg list run remote output on File Explorer" style={{width: 800}} />
</div>

### `run_remote` instance arguments

With `job.run_remote()` you also have access to a few other arguments to make your remote job fit your needs:

- `instance_type`: Decide which type of machine to run your job on (see below for which ones we support)
- `disk_size_gb`: The amount of disk space in Gb your instance requires (up to `999` Gb)

For example if you want a job with 16 vCPUs, 64Gb of RAM and 100Gb of storage you can call:
```python showLineNumbers
job.run_remote(instance_type="m5.4xlarge", disk_size_gb=100)
```

<details>
  <summary>Currently supported `instance_type`</summary>

  Fused `run_remote()` `instance_type` are based around [AWS General Purpose instance types](https://aws.amazon.com/ec2/instance-types/). We do support the following:
  
  ```python showLineNumbers
  supported_instance_types = [
    "m5.large",
    "m5.xlarge",
    "m5.2xlarge",
    "m5.4xlarge",
    "m5.8xlarge",
    "m5.12xlarge",
    "m5.16xlarge",
    "r5.large",
    "r5.xlarge",
    "r5.2xlarge",
    "r5.4xlarge",
    "r5.8xlarge",
    "r5.12xlarge",
    "r5.16xlarge",
  ]
  ```

</details>

## Accessing job logs

You can view the logs of all your on-going and past `run_remote()` jobs either:

import ReactPlayer from 'react-player';
import ImgRunRemoteLogs from '@site/docs/core-concepts/run-udfs/run_remote_output.png';
import ImgRunRemoteEmail from '@site/docs/core-concepts/run-udfs/run-remote-email.png';

<details>
  <summary>In a notebook</summary>

    Running `job.run_remote()` in a notebook gives you a clickable link:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteLogs} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>

<details>
  <summary>In Fused Workbench</summary>

    Under the "Jobs" tab, on the bottom left of Workbench:

    <ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/core-concepts/run-udfs/run-remote-job-logs.mp4" width="100%" />

</details>

<details>
  <summary>By email (you'll receive 1 email for each job)</summary>

    Each job leads to an email summary with logs upon completion:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteEmail} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>


## Getting results

To get data back from your `run_remote()` jobs is a bit more complicated than for ["real-time"](/core-concepts/run-udfs/run-small-udfs/#getting-real-time-results). Our recommendation is to have your UDF write data directly to disk or cloud storage and access it after

<details>
  <summary>Example job: saving to disk</summary>

    A common use case for offline jobs is as a "pre-ingestion" process. You can find a real-life example of this in our [dark vessel detection example](/user-guide/examples/dark-vessel-detection/#32---writing-a-udf-to-open-each-ais-dataset)

    Here all we're returning is a status information in a pandas dataframe, but the our data in unzipped, read and saved to S3:

    ```python showLineNumbers
    import fused

    @fused.udf()
    def read_ais_from_noaa_udf(datestr='2023_03_29'):
        import os
        import requests
        import io
        import zipfile
        import pandas as pd

        url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'
        # This is our local mount file path, 
        path=f'/mnt/cache/AIS_demo/{datestr[:7]}/'
        daily_ais_parquet = f'{path}/{datestr[-2:]}.parquet'

        # Download ZIP file to mounted disk
        r=requests.get(url)
        if r.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
                with z.open(f'AIS_{datestr}.csv') as f:
                    df = pd.read_csv(f)

                    # highlight-next-line
                    df.to_parquet(daily_ais_parquet)      
            return pd.DataFrame({'status':['Done']})
        else:
            return pd.DataFrame({'status':[f'read_error_{r.status_code}']})
    ```

    Since our data is written to cloud storage, it can now be accessed anywhere else, through another UDF or any other application with access to cloud storage.

    :::tip
      You can use [File Explorer](/workbench/file-explorer/) to easily see your outputs! In this case of the above example typing `efs://AIS_{datestr}.csv` (and replacing `datestr` with your date) will show the results directly in File Explorer!
    :::

</details>

{/* TODO: Need example here */}
{/* TODO: Show output on File Explorer */}


### Large jobs trade-offs

- Takes a few seconds to startup machine
- Can run as long as needed


