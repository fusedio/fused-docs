---
id: run_multiple
title: Multiple UDFs
tags: [write, endpoints, api, http, file, tile]
sidebar_position: 2
toc_min_heading_level: 2
toc_max_heading_level: 4
---

UDF work best when handling small amounts of data at time, but often datasets are large. This is where running UDFs in bulk becomes needed
{/* TODO: This should link to a "Best Practices" section about how big should the data processed by a single UDF be: 1 big large jobs, or many small jobs? */}

There are 2 main approaches to running multiple UDFs:
- **Using "offline" instances**
    - For high latency, large volumes of data
- **Using "real-time" instances**
    - For small requests to finish quick

### A Simple UDF

Here's a simple UDF:

```python showLineNumbers
@fused.udf
def udf(val):
    import pandas as pd
    return pd.DataFrame({'val':[val]})
```

As mentioned in the [Single UDF section](/core-concepts/run-udfs/run-single-udfs/), to call it 1 time we can do:

```python showLineNumbers
fused.run(udf, val=1)
```

Which returns:

```bash
  | val
------
0 | 1
```

We then have 2 options to run this multiple times:


## Using "offline" instances (`run_remote()`)

_When to use: This is for high-latency (anything more than a few seconds) parallel processing of longer or more complex jobs_


#### Running "offline" job
Using the same UDF as above we can run this UDF 10 times on an "offline" instance:

```python showLineNumbers
# We'll run this UDF 10 times
job = udf(arg_list=range(10))
job.run_remote()
```

This pushes a job onto a persistent machine on Fused server.

<details>
  <summary>`run_remote` additional arguments</summary>

    With `job.run_remote()` you also have access to a few other arguments to make your remote job fit your need:

    - `instance_type`: Decide which type of machine to run your job on (based on the [AWS General Purpose instance types](https://aws.amazon.com/ec2/instance-types/))
    - `disk_size_gb`: The amount of disk space in Gb your instance requires

    For example if you want a job with 16 vCPUs, 64Gb of RAM and 100Gb of storage you can call:
    ```python showLineNumbers
    job.run_remote(instance_type="m5.4xlarge", disk_size_gb=100)
    ```

</details>

{/* This is missing a section on how to get the output from the job directly, similar to runner.get_concat() in the other section */}

#### Accessing "offline" job logs

You can view the logs of all your on-going and past `run_remote()` jobs either:

import ReactPlayer from 'react-player';
import ImgRunRemoteLogs from '@site/docs/core-concepts/run-udfs/run_remote_output.png';
import ImgRunRemoteEmail from '@site/docs/core-concepts/run-udfs/run-remote-email.png';

<details>
  <summary>In a notebook</summary>

    Running `job.run_remote()` in a notebook gives you a clickable link:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteLogs} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>

<details>
  <summary>In Fused Workbench</summary>

    Under the "Jobs" tab, on the bottom left of Workbench:

    <ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/core-concepts/run-udfs/run-remote-job-logs.mp4" width="100%" />

</details>

<details>
  <summary>By email (you'll receive 1 email for each job)</summary>

    Each job leads to an email summary with logs upon completion:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteEmail} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>


#### Getting "offline" results

To get data back from your "offline" run is a bit more complicated than for ["real-time"](/core-concepts/run-udfs/run_multiple/#getting-real-time-results). Our recommendation is to have your UDF write data directly to disk or cloud storage and access it after

<details>
  <summary>Example job: saving to disk</summary>

    A common use case for offline jobs is as a "pre-ingestion" process. You can find a real-life example of this in our [dark vessel detection example](/user-guide/examples/dark-vessel-detection/#32---writing-a-udf-to-open-each-ais-dataset)

    Here all we're returning is a status information in a pandas dataframe, but the our data in unzipped, read and saved to S3:

    ```python showLineNumbers
    import fused

    @fused.udf()
    def read_ais_from_noaa_udf(datestr='2023_03_29'):
        import os
        import requests
        import io
        import zipfile
        import pandas as pd

        url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'
        # This is our local mount file path, 
        path=fused.file_path(f'/AIS/{datestr[:7]}/')
        daily_ais_parquet = f'{path}/{datestr[-2:]}.parquet'

        # Download ZIP file to mounted disk
        r=requests.get(url)
        if r.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
                with z.open(f'AIS_{datestr}.csv') as f:
                    df = pd.read_csv(f)

                    # highlight-next-line
                    df.to_parquet(daily_ais_parquet)      
            return pd.DataFrame({'status':['Done']})
        else:
            return pd.DataFrame({'status':[f'read_error_{r.status_code}']})
    ```

    Since our data is written to cloud storage, it can now be accessed anywhere else, through another UDF or any other application with access to cloud storage.

</details>

{/* Need example here */}


Tradeoffs:
- Takes a few seconds to startup machine
- Can run as long as needed

## [Experimental] Using "real-time" instances (`run_pool` & `PoolRunner`)

_When to use: This is for quick jobs that can finish in less than 120s_

:::note
    This is not a feature directly implemented in `fused`. Instead we're going to use a code from the `fused.public.common` utils module, that might change over time
    
    You can learn more about import utils from UDFs [here](/core-concepts/write/#import-utils-from-other-udfs)
:::

If you want to quickly run a UDF a few times over a dataset, you can use `PoolRunner`. Using a `lambda` function you can map the function to pass to `PoolRunner`:

```python showLineNumbers
runner = fused.utils.common.PoolRunner(lambda val: fused.run(udf, val=val), range(10))
runner.get_result_all()
```

In a notebook `runner.get_result_all()` prints the progress status over time until the job is done:

import ImgPoolRunner from '@site/docs/core-concepts/run-udfs/poolrunner-get_results_all.png';

<div style={{textAlign: 'center'}}>
    <img src={ImgPoolRunner} alt="Pool Runner" style={{width: 600}} />
</div>

#### Getting "real-time" results

You can then get all your results by concatinating them:

```python showLineNumbers
# In this example udf() returns a Pandas DataFrame so `.get_concat()` 
result = runner.get_concat()
```

with `type(output)` being a `pandas.core.frame.DataFrame`

Tradeoffs:
- No startup time
- Will timeout after 120s


