---
id: cloud-storage
title: Cloud Storage
sidebar_label: Cloud Storage
sidebar_position: 2
---

import Tag from '@site/src/components/Tag'
import Details from '@theme/MDXComponents/Details';

# Cloud Storage

Fused supports multiple cloud storage options for reading and writing data.

## Supported Storage Paths

| Provider | Format | Example |
|----------|--------|---------|
| Fused managed | `fd://` | `fd://my-data/file.parquet` |
| AWS S3 | `s3://` | `s3://bucket-name/path/file.parquet` |
| Google Cloud | `gs://` or `gcs://` | `gs://bucket-name/path/file.parquet` |
| HTTP(S) | `https://` | `https://example.com/file.csv` |

:::tip Fused-managed storage
For details on using `fd://` paths and the `/mnt/cache` disk, see [File System](/guide/advanced-setup/file-system).
:::

## `/mnt/cache` Disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and `fused.download` write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system:

```python showLineNumbers
# Write to mount
df.to_parquet("/mnt/cache/data.parquet")

# List files
@fused.udf
def udf():
    import os
    for each in os.listdir('/mnt/cache/'):
        print(each)
```

:::note
If you encounter `Error: No such file or directory: '/mnt/cache/'`, contact the Fused team to enable it for your environment.
:::

## Connect Your Own Bucket

<Tag color="#3399ff">Enterprise</Tag> _This feature is accessible to organizations with a Fused Enterprise subscription._

Connect S3 or GCS buckets to access their files interactively from within the File Explorer UI and programmatically from within UDFs.

Contact Fused to set an S3 or GCS bucket on the File Explorer for all users in your organization. Alternatively, set a bucket as a "favorite" so it appears in the File Explorer for your account only.

### Amazon S3

Set the policy below on your bucket, replacing `YOUR_BUCKET_NAME` with its name. Fused will provide `YOUR_ENV_NAME`.

<Details open={false}>

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Allow object access by Fused fused account",
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::926411091187:role/rt-production-YOUR_ENV_NAME",
                    "arn:aws:iam::926411091187:role/ec2_job_task_role-v2-production-YOUR_ENV_NAME",
                ]
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObjectAttributes",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::YOUR_BUCKET_NAME/*",
                "arn:aws:s3:::YOUR_BUCKET_NAME"
            ]
        }
    ]
}
```

</Details>

Alternatively, use this [Fused app](https://www.fused.io/workbench#app/s/i/fa_2yQFVcbSYR1vW4Aa8zL1HC) to automatically structure the policy for you.

The bucket must enable the following CORS settings to allow uploading files from Fused:

<Details open={false}>

```json
[
    {
        "AllowedHeaders": [
            "range",
            "content-type",
            "content-length"
        ],
        "AllowedMethods": [
            "GET",
            "HEAD",
            "PUT",
            "POST"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": [
            "content-range"
        ],
        "MaxAgeSeconds": 0
    }
]
```
</Details>

#### Encrypted S3 Buckets

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket.

Configure KMS policy:

```json
{
  "Sid": "AllowCrossAccountUseOfKMS",
  "Effect": "Allow",
  "Principal": {
    "AWS": "arn:aws:iam::<FUSED_ACCOUNT>:role/<FUSED_ROLE_NAME>"
  },
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

### Google Cloud Storage (GCS)

To connect a Google Cloud Storage bucket to your Fused environment:

**1. Create a Service Account in GCS**

Set up a Google Cloud service account with permissions to read, write, and list from the GCS bucket. See the Google Cloud documentation for instructions to:
- [Create a Service Account](https://cloud.google.com/iam/docs/service-accounts-create)
- [Set permissions for the Service Account](https://cloud.google.com/iam/docs/manage-access-service-accounts)

**2. Download the JSON Key File**

Download the JSON Key file associated with the Service Account. This file contains credentials that Fused will use to access the GCS bucket.

**3. Set the JSON Key as a Secret**

Set the JSON Key as a secret in the [secrets management UI](/reference/workbench/preferences#secrets-management). The secret must be named `gcs_fused`.

You then need to write these credentials to a JSON file and pass them to Google:

```python
@fused.udf
def udf():
    from google.cloud import storage

    # get GCP secrets
    with open("/tmp/gcs_key.json", "w") as f:
        f.write(fused.secrets["gcs_fused"])
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcs_key.json"

    # your code here
```

## Read & Write Examples

### Reading from S3

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):
    import pandas as pd
    return pd.read_csv(path)
```

### Writing to S3

```python
df.to_parquet("s3://my-bucket/data.parquet")
```

### Writing to GCS

```python
df.to_parquet("gcs://my-bucket/data.parquet")
```

### Download to Fused mount

```python
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    return str(out_path)
```

Files will be written to `/mnt/cache/`, where any other UDF can then access them.

## Working with Compressed Files

For datasets that aren't cloud-native (e.g., from [Zenodo](https://zenodo.org/), [Humanitarian Data Exchange](https://data.humdata.org/)), you may need to download and extract compressed archives.

<img src="/img/tutorials/engineering-etl/download_uncompress_rar.png" alt="Download and uncompress RAR" style={{maxWidth: 600, width: "100%", borderRadius: 8, marginBottom: 16}} />

### Download Large Files

For files that take longer than 120s to download, run as a [batch job](/guide/working-with-udfs/execution/batch-jobs):

```python showLineNumbers
@fused.udf(
    instance_type='c2-standard-4',  # Small instance - download uses little resources
    disk_size_gb=999                # Large disk for the file
)
def udf():
    import s3fs
    import requests
    import os, tempfile
    
    url = "https://zenodo.org/records/4395621/files/Correlation_Merged1998_nc2.rar?download=1"
    s3_path = f"s3://fused-asset/data/my-files/{url.split('/')[-1]}"
    if s3fs.S3FileSystem().exists(s3_path):
        return f'File exists: {s3_path}'

    temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(url.split('/')[-1])[1]).name
    resp = requests.get(url, stream=True)
    resp.raise_for_status() 
    total_size = int(resp.headers.get('Content-Length', 0))
    size_mb = total_size / (1024 * 1024)

    with open(temp_path, 'wb') as f:
        for i, chunk in enumerate(resp.iter_content(chunk_size=total_size//100)):
            print(f'{i}% | {round(size_mb*i/100,1)}/{round(size_mb)} MB')
            f.write(chunk)

    s3 = s3fs.S3FileSystem()
    s3.put(temp_path, s3_path)
    print(f"Uploaded to: {s3_path}")
```

### Extract ZIP Files

**List files in archive:**

```python showLineNumbers
@fused.cache
def get_zip_file_info(url):
    import pandas as pd
    import zipfile
    import s3fs
    
    s3 = s3fs.S3FileSystem()
    with s3.open(url, "rb") as f:
        with zipfile.ZipFile(f) as zip_ref:
            file_info = []
            for filename in zip_ref.namelist():
                info = zip_ref.getinfo(filename)
                file_info.append({
                    "filename": filename,
                    "compressed_size_mb": round(info.compress_size / (1024 * 1024), 2),
                    "uncompressed_size_mb": round(info.file_size / (1024 * 1024), 2),
                })
    return pd.DataFrame(file_info)
```

**Extract specific files:**

```python showLineNumbers
@fused.cache
def extract_file_from_zip(url, filename, output_path):
    import zipfile, tempfile, os
    import s3fs

    s3 = s3fs.S3FileSystem()
    with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=os.path.splitext(filename)[1]) as output_file:
        temp_path = output_file.name
        CHUNK_SIZE = 100 * 1024 * 1024  # 100MB chunks
        
        with s3.open(url, "rb") as f:
            with zipfile.ZipFile(f) as zip_ref:
                with zip_ref.open(filename) as file:
                    while chunk := file.read(CHUNK_SIZE):
                        output_file.write(chunk)
    
    s3.put(temp_path, output_path)
    return output_path
```

### Extract RAR Files

```python showLineNumbers
@fused.cache
def get_rar_file_info(url):
    import pandas as pd
    import rarfile
    import s3fs
    
    s3 = s3fs.S3FileSystem()
    with s3.open(url, "rb") as f:
        with rarfile.RarFile(f) as rar_ref:
            file_info = []
            for filename in rar_ref.namelist():
                info = rar_ref.getinfo(filename)
                file_info.append({
                    "filename": filename,
                    "compressed_size_mb": round(info.compress_size / (1024 * 1024), 2),
                    "uncompressed_size_mb": round(info.file_size / (1024 * 1024), 2),
                })
    return pd.DataFrame(file_info)
```

Use [`fused.submit()`](/guide/working-with-udfs/execution/realtime#running-jobs-in-parallel-fusedsubmit) to extract multiple files in parallel.
