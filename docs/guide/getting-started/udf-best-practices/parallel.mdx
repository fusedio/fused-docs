---
id: parallel
title: How to run in parallel
sidebar_label: How to run in parallel
sidebar_position: 2
---

# Running Jobs in Parallel: `fused.submit()`

Run a UDF over a list of inputs in parallel with [`fused.submit()`](/python-sdk/top-level-functions/#fusedsubmit).

```python showLineNumbers
@fused.udf
def udf(val):
    import pandas as pd
    return pd.DataFrame({'val': [val]})
```

```python showLineNumbers
inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
results = fused.submit(udf, inputs)
```

## Passing multiple parameters

For UDFs with multiple parameters, pass a DataFrame where each column matches a parameter name:

```python showLineNumbers
@fused.udf
def udf(a: str = "a", b: str = "b"):
    import pandas as pd
    return pd.DataFrame({"result": [a + b]})
```

```python showLineNumbers
import pandas as pd

inputs = pd.DataFrame({
    'a': ['a', 'A', 'x'],
    'b': ['b', 'B', 'y']
})

results = fused.submit(udf, inputs)
```

Each row becomes a separate job with the corresponding parameter values.

## Execution modes

| Approach | What happens | Best for |
|----------|--------------|----------|
| [Realtime workers](#realtime-workers-default) (default) | Spawns many UDFs in parallel | Many quick jobs (under 120s, under 4GB each) |
| [Batch instance](#batch-instance) | Runs all jobs on one large machine | Jobs needing more time or RAM |

### Realtime workers (default)

Each input spawns a separate [realtime UDF](/guide/getting-started/udf-best-practices/realtime):

```python showLineNumbers
results = fused.submit(udf, inputs)
```

**Characteristics:**
- Each job has [realtime limits](/guide/getting-started/udf-best-practices/realtime#realtime-limits) (120s, ~4GB RAM)
- Default: **32 workers** in parallel, can scale up to **1000+** with `max_workers`:

```python showLineNumbers {5}
# Limit to 50 concurrent workers
results = fused.submit(
    udf, 
    inputs, 
    max_workers=50
)
```

:::tip Job length rule of thumb: 30-45s
Aim for jobs that take 30-45s each. This gives a safety margin before the 120s timeout.
:::

### Batch instance

All jobs run on a single dedicated machine:

```python showLineNumbers {4-5}
results = fused.submit(
    udf, 
    inputs, 
    instance_type="large", 
    collect=False
    )
```

**Characteristics:**
- Jobs queue and run as cores become available across the instance resources
- **No time limit**, high RAM per job
- Resources depend on `instance_type` chosen; see [Batch Jobs](/guide/getting-started/udf-best-practices/batch-jobs#instance-types) for options

:::warning Best practice: save to S3, return file paths
For batch jobs, write results to [cloud storage](/guide/data-input-outputs/import-connection/cloud-storage) rather than returning large data:

```python showLineNumbers
@fused.udf
def batch_udf(input_path: str):
    import pandas as pd
    
    df = pd.read_parquet(input_path)
    result = process(df)
    
    # Save to S3 and return path
    output_path = f"s3://my-bucket/results/{input_path.split('/')[-1]}"
    result.to_parquet(output_path)
    return output_path  # NOT return result
```

Use [non-blocking execution](/guide/getting-started/udf-best-practices/parallel#non-blocking-execution) with `collect=False` and gather file paths after:

```python showLineNumbers
job = fused.submit(batch_udf, input_paths, instance_type="large", collect=False)
job.wait()
output_paths = job.collect()
```
:::

## Tips

- **Test first** with `debug_mode=True`:

```python showLineNumbers {5}
# Runs only the first input to verify setup
result = fused.submit(
    udf, 
    inputs, 
    debug_mode=True
)
```

- **Start small**, then scale:

```python showLineNumbers {4}
# Test with first 5 inputs
results = fused.submit(
    udf, 
    inputs[:5]
)
```

- **Check timing** of each job:

```python showLineNumbers
results = fused.submit(
    udf, 
    inputs[:5], 
    collect=False
)
print(results.times())
```

## Advanced options

### Non-blocking execution

Use `collect=False` to run jobs in background:

```python showLineNumbers
job = fused.submit(udf, inputs, collect=False)

# Check progress
job.wait()  # Shows progress bar

# Get results when ready
results = job.collect()
```

### Monitoring methods

```python showLineNumbers
job.wait()          # Progress bar
job.total_time()    # Total wall time
job.times()         # Time per job
job.first_error()   # First error encountered
job.collect()       # Get results
```

### Execution parameters

| Parameter | Description |
|-----------|-------------|
| `max_workers` | Number of parallel workers (realtime only) |
| `engine` | `local` or `remote` (see [Execution engines](/guide/getting-started/udf-best-practices/realtime#execution-engines)) |
| `max_retry` | Max retries per failed job |
| `ignore_exceptions` | Ignore exceptions when collecting results |

See the [Python SDK docs](/python-sdk/top-level-functions/#fusedsubmit) for more details.

## Wall time vs CPU time

**Wall time**: Total elapsed time for all jobs:

```python showLineNumbers
print(job.total_time())
```

**CPU time**: Sum of individual job times (this counts toward billing):

```python showLineNumbers
total_cpu = sum(t.total_seconds() for t in job.times())
```

## Example use cases

- üìà [Making a Climate Dashboard](/examples/climate-dashboard) ‚Äî Processing 20TB of data in minutes
- ‚õ¥Ô∏è [Dark Vessel Detection](/examples/dark-vessel-detection#33---run-this-udf-over-a-month-of-ais-data) ‚Äî Retrieving 30 days of AIS data
- üõ∞Ô∏è [Satellite Imagery](/examples/satellite-imagery#preparing-fusedsubmit-to-run-in-parallel) ‚Äî Processing Maxar's Open Data STAC Catalogs
