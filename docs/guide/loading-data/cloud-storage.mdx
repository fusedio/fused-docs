---
title: Cloud Storage
sidebar_label: Cloud Storage
sidebar_position: 3
description: Read and write to S3, GCS, and Azure Blob Storage
---

# Cloud Storage

Fused UDFs can read and write directly to any cloud storage provider. It's just Python.

```python
import pandas as pd
df = pd.read_parquet("s3://bucket/file.parquet")
```

---

## Supported Providers

| Provider | URL Prefix | Example |
|----------|------------|---------|
| [Amazon S3](/reference/data-loading/cloud#amazon-s3) | `s3://` | `s3://my-bucket/data.parquet` |
| [Google Cloud Storage](/reference/data-loading/cloud#google-cloud-storage) | `gs://` | `gs://my-bucket/data.parquet` |
| [Azure Blob Storage](/reference/data-loading/cloud#azure-blob-storage) | `az://` | `az://container/data.parquet` |
| [HTTP/HTTPS](/reference/data-loading/cloud#httphttps) | `https://` | `https://example.com/data.csv` |
| [Fused Storage](/reference/data-loading/cloud#fused-storage-fd) | `fd://` | `fd://my-data/file.parquet` |

---

## Private Bucket Access

For private buckets, set up credentials in **Workbench → Preferences → Secrets**.

Then access normally - Fused handles authentication:

```python
df = pd.read_parquet("s3://my-private-bucket/data.parquet")
```

---

## Using Your Own S3 Bucket

You can configure Fused to write to your own S3 bucket by setting up an IAM policy.

**Steps:**
1. Create an IAM policy granting Fused access
2. Add your bucket credentials in Workbench Preferences
3. Use your bucket path in UDFs

For detailed setup instructions, see [Ingest to Your Own S3 Bucket](/guide/loading-data/geospatial-data-ingestion/ingest-your-data#ingest-to-your-own-s3-bucket).

---

## Code Examples

- [S3 snippets](/reference/data-loading/cloud#amazon-s3)
- [GCS snippets](/reference/data-loading/cloud#google-cloud-storage)
- [Azure snippets](/reference/data-loading/cloud#azure-blob-storage)
- [HTTP snippets](/reference/data-loading/cloud#httphttps)
