---
title: Batch Jobs
sidebar_label: Batch Jobs
sidebar_position: 2
description: Run long or resource-intensive UDFs with run_batch()
---

# Batch Jobs

For UDFs that need more time or resources than realtime allows.

**When to use:**
- Execution >120 seconds
- Memory >8GB required
- Large dataset processing
- Data ingestion jobs

---

## Quick Start

```python
@fused.udf
def udf(val):
    import pandas as pd
    # Heavy processing here
    return pd.DataFrame({'result': [val]})

# Create and run batch job
job = udf(arg_list=[1, 2, 3, 4, 5])
job.run_batch()
```

---

## Instance Types

Specify resources with `instance_type`:

```python
job.run_batch(instance_type="m5.4xlarge", disk_size_gb=100)
```

| Instance | vCPUs | Memory |
|----------|-------|--------|
| `small` | 2 | 2 GB |
| `medium` | 16 | 64 GB |
| `large` | 64 | 512 GB |

See [Instance Types](/core-concepts/run-udfs/run_large#currently-supported-instance_type) for full list.

---

## Multiple Parameters

Pass dictionaries for multiple inputs:

```python
@fused.udf
def udf(config: dict = {'a': 1, 'b': 2}):
    return process(config['a'], config['b'])

job = udf(arg_list=[
    {"a": 1, "b": 2},
    {"a": 3, "b": 4}
])
job.run_batch()
```

---

## Monitoring Jobs

```python
# Check status
print(job.status)

# Follow logs
job.tail_logs()

# Cancel if needed
job.cancel()
```

Jobs also send email notifications on completion.

---

## Saving Results

Batch jobs should write results to storage:

```python
@fused.udf
def udf(date: str = "2024-01-01"):
    import pandas as pd
    
    # Process data
    df = heavy_processing(date)
    
    # Save to shared storage
    df.to_parquet(f"/mnt/cache/results/{date}.parquet")
    
    return "Done"
```

Access results via [File Explorer](/reference/workbench/file-explorer) or in subsequent UDFs.

---

## Next Steps

- [Parallel Processing](/guide/running-udfs/parallel-processing) - Many small tasks with `fused.submit()`
- [run_batch() details](/core-concepts/run-udfs/run_large) - Full API details

