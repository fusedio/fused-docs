---
id: udf-best-practices
title: Making the most out of UDFs
sidebar_label: Building & Running UDFs
sidebar_position: 0
---

# Build & Running UDFs

_An opinionated guide to making the most out of Fused UDFs_

[Fused UDFs](/guide/running-udfs/writing-udfs) are Python functions that run on serverless compute and can be called from anywhere with [`fused.run(udf)`](/reference/python-sdk/top-level-functions#fusedrun). This guide is a resource meant to help you on your way to making the most out of UDFs.

## A short reminder: The anatomy of a UDF

```python showLineNumbers
@fused.udf
def udf(my_awesome_input: int = 1):
    import pandas as pd

    return pd.DataFrame({"Look at my output: ": [my_awesome_input]})
```

Each UDF has a few specific elements:
- The [`@fused.udf` decorator](/guide/running-udfs/writing-udfs#the-fusedudf-decorator)
- Arguments -ideally typed-
- Imports _inside_ the function
- Some logic
- A [supported `return` object](/guide/running-udfs/writing-udfs#return-types)

:::note
All of this is explained in the ["Write UDF"](/guide/running-udfs/writing-udfs) section in much more details.
:::

You can then run UDFs from _anywhere_ with [`fused.run(udf)`](/reference/python-sdk/top-level-functions#fusedrun). These are still Python functions, giving you a lot of flexibility on what you can do, but we have some recommendations for keeping them fast & efficient.

## Writing efficient UDFs

### Keep things small

The main benefit of Fused UDFs is how responsive they are. They achieve this by running on Python serverless compute. They can [time out](/guide/running-udfs/realtime#timeout-errors), so the best way to keep workflows fast is to keep them small:

- Break pipelines into single-task UDFs
- Leverage [`fused.run()`](/reference/python-sdk/top-level-functions#fusedrun) to chain UDFs together
- Or [run small tasks in parallel](#run-tasks-in-parallel-fusedsubmit)

<details>
    <summary>Example: Breaking down a complex pipeline into smaller UDFs</summary>

    ❌ Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):
        import pandas as pd

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ✅ Instead, break it down:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):
        import pandas as pd
        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):
        import pandas as pd

        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ```python showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):
        import pandas as pd

        df = fused.run(load_data_udf, data_path=data_path)
        processed_df = fused.run(process_data_udf, df=df)

        return processed_df
    ```
</details>

### Run often, Iterate quickly

Just like writing short cells when developing in a Jupyter Notebook, we recommend you keep your UDFs short & fast to execute

⚡️ Aim for **UDFs that take up to 30-45s to run**

UDFs run with [`fused.run()`](/reference/python-sdk/top-level-functions#fusedrun) [time out after 120s](/guide/running-udfs/realtime#timeout-errors) so we recommend you keep a buffer in case your UDF takes a bit longer to execute

<details>
    <summary>Visual: UDF timing guideline</summary>

    This is a breakdown of what happens when you run a UDF with `fused.run()` and why we recommend you keep your UDFs at the 30s-45min mark:

    ![UDF Design Guidelines](/img/user-guide/best-practices/udf_design_timing.png)

</details>

### Run tasks in parallel: [`fused.submit()`](/guide/running-udfs/parallel-processing)

[`fused.submit()`](/guide/running-udfs/parallel-processing) lets you run a UDF over a set of inputs in parallel. This is helpful when wanting to scale a small operation over a large number of inputs for example: 
- converting a list of files from 1 format to another
- computing a simple operation over a large time series

Best Practices for using [`fused.submit()`](/guide/running-udfs/parallel-processing):
- Run UDFs that take between 30-45s to run (leaving overhead for any UDF in the chain to take a bit longer)
- Return small dataframes (a few rows / columns) OR save to a file and return a file path. This makes it easier to aggregate all the data together in a small dataframe

<details>
    <summary>Example: Parallelizing a UDF with [`fused.submit()`](/guide/running-udfs/parallel-processing)</summary>

    Let's imagine we have a `fetch_single_data` that loads data from an API for a large amount of inputs `input_data=[0,1,2,3,4,5,7,8,9]`:

    ```python showLineNumbers
    @fused.udf
    def fetch_single_data(single_input: int):
        import pandas as pd
        import time

        # Considering this as our API call, sleeping to simulate the time it takes to get the data
        time.sleep(3)

        # This is a small dataframe, but we could also save to a file and return the file path
        return pd.DataFrame({"data": [f"processed_{str(single_input)}"]})
    ```
    If we were to run this UDF in [Workbench](/reference/workbench/overview) we would only be able to run it for 1 input at a time, so we could edit our UDF to loop over the inputs:
    ```python showLineNumbers
    @fused.udf
    def fetch_data(inputs: list):
        import pandas as pd
        import time

        fetched_data = []
        for i in inputs:
            # Considering this as our API call
            time.sleep(3)
            fetched_data.append(f"processed_{str(i)}")

        return pd.DataFrame({"data": fetched_data})
    ```
    However, running this UDF with `fused.run(fetch_data, inputs=input_data)` will take longer as we add inputs, we could even quickly go over [the 120s limit](/guide/running-udfs/realtime#timeout-errors). We still do want to fetch data across all our inputs which is where [`fused.submit()`](/guide/running-udfs/parallel-processing) comes in:

    Going back to our original UDF, we can now run it with `fused.submit()` to run it in parallel:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(input_data):

        results = fused.submit(
            fetch_single_data,
            input_data,
            engine='local', # This ensures the UDF is run in our local server rather than spinning up new instances.
        )
        fetched_data = results.collect_df()

        return fetched_data
    ```

    This is of course a simplified example, but it shows how you can use `fused.submit()` to run a UDF in parallel for each input.

    This now runs a lot faster by running the `fetch_single_data` UDF in parallel for each input.

    :::note
    The example here blocks the main thread until all the `fused.submit()` calls have finished. This means you might have to wait longer in [Workbench](/reference/workbench/overview) for the results to show up.
    :::

    Comparison of both approaches in Workbench:

    Running with `fused.run()`, 30.89s:
    ![10 inputs fused.run](/img/user-guide/best-practices/10_udfs_fused_run.png)

    Running with `fused.submit()`, 5.3s:
    ![10 inputs fused.submit](/img/user-guide/best-practices/10_udfs_fused_submit.png)

</details>

### Use [offline instances](/guide/running-udfs/batch-jobs) for your large jobs

You might not be able to run your UDF within the 120s limit of the default [`fused.run(udf)`](/reference/python-sdk/top-level-functions#fusedrun) call or break it down into smaller UDFs and parallelize it with [`fused.submit()`](/guide/running-udfs/parallel-processing). In that case you can [use offline instances](/guide/running-udfs/batch-jobs). These also allow you to:
- [Choose instances](/guide/running-udfs/batch-jobs#run_batch-instance-arguments) with more memory & CPU
- Run your UDF for longer than 120s (at the expense of slower startup time and [requiring to save your data to storage](/guide/running-udfs/batch-jobs#getting-results) to retrieve it)

You can find an example of using offline instances in [one of our end to end examples](/guide/use-cases/dark-vessel-detection#34---ingest-1-month-of-ais-data-into-a-geo-partitioned-format) when ingesting data into portioned cloud native formats

### Cache as much as you can

Fused relies heavily on [caching](/guide/scaling-up/cache) repetitive tasks to make recurring calls much faster (and more compute efficient)

✅ You want to use caching for functions with inputs that are recurring:
- Loading a dataset
- Computing a recurring operation with default variables
- Intermediate results you'll reuse soon

❌ When not to use caching:
- In most cases, for functions taking `bounds` as an argument -> your function + input cache would get re-generated for each new `bounds` (which changes each time you pan around in [Workbench Map](/reference/workbench/udf-builder/map) view for example)
- Data you want others in your team or external to Fused to use. You're better off writing your data to cloud storage like `s3` or `gcs`

<details>
    <summary>Example: Caching a repetitive task</summary>

    Re-using the example from [keeping things small](#keep-things-small):

    ❌ Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):
        import pandas as pd

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ✅ Instead, break it down AND cache the calls:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):
        import pandas as pd
        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):
        import pandas as pd
        # Some complicated processing logic to create df_processed
        # ...
        return processed_df
    ```

    ```python {5-7,9-11} showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):
        import pandas as pd

        @fused.cache
        def load_data(data_path):
            return fused.run(load_data_udf, data_path=data_path)

        @fused.cache
        def process_data(df):
            return fused.run(process_data_udf, df=df)

        df = load_data(data_path)
        processed_df = process_data(df)

        return processed_df
    ```
</details>

:::tip
Read more about the caching details:
- [in the dedicated section](/guide/scaling-up/cache)
- How you can use cache to [speed up exploration of slow to read datasets](/guide/geospatial-ingestion/why-ingestion#using-cache-as-a-single-use-ingester)
:::

### Prepare your large datasets

Fused works at its best with data that is fast to read and can be read in tiles or chunks. We know that most of the data out there isn't in the most [efficient file formats](/guide/geospatial-ingestion/geospatial-file-formats) which is why we provide tools to [ingest your own data](/guide/geospatial-ingestion/ingest-your-data) into cloud-optimized, partitioned formats.

We have a [dedicated page](/guide/geospatial-ingestion/why-ingestion#when-is-ingestion-needed) for when you should consider ingesting your own data. As a rule of thumb you want to consider ingesting your data when:
- Files are read multiple times and >100MB
- Files that are slow or require some processing to open (`.zip` for example)

### Don't start from scratch: [UDF Explorer](/reference/workbench/udf-explorer)

Just like using libraries in Python to leverage existing tools, you don't need to start from scratch in Fused. We have a [Catalog of existing UDFs](/reference/workbench/udf-explorer) built & maintained by us and the community.

You can find a host of different UDFs that can serve as a starting point or as inspiration to create your own UDFs:
- Open datasets from common open repository [like STAC catalogs](https://www.fused.io/workbench/catalog/Sentinel_Tile_Example_2-ddb4c495-40e3-48ba-849c-256487f8a9cb)
- Run [on the fly ML prediction](https://www.fused.io/workbench/catalog/Dl4eo_Airplane_Detection_Global-91f81c0c-08f6-4a0c-82f9-bd2e0322b4e7) on satellite images
- Compute a [spatial join](https://www.fused.io/workbench/catalog/Overture_Nsi-dd89972c-ce30-4544-ba0f-81fc09f5bbef) between 2 datasets

![UDF Explorer](/img/user-guide/best-practices/udf_catalog.png)

You can also [contribute your own UDFs](/reference/workbench/udf-explorer#contribute-to-fused) to the community!

## Debugging UDFs

The reality of writing code is that stuff breaks, often and sometimes in mysterious ways. Here's some of our recommendations for how to debug your UDFs

### Use `print()`

UDFs return `stdout` either in [Workbench Code Editor](/reference/workbench/udf-builder/code-editor) or locally when running `fused.run(udf)` so the easiest way to get info about your UDFs is to use good old `print`:

```python showLineNumbers
@fused.udf
def udf(n: int = 1):
    print(f"{n=}")
    return
```

Since Python 3.8 you can use [f-string debugging](https://docs.python.org/3/whatsnew/3.8.html#f-strings-support-for-self-documenting-expressions-and-debugging) which is what we recommend you use:
```python showLineNumbers
print(f"{my_fancy_variable=}")
```

This allows you to print many variables without getting lost with what is what

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs className="unique-tabs">
  <TabItem value="workbench" label="Workbench" default>
    ![UDF Explorer](/img/user-guide/best-practices/stdout.png)
  </TabItem>
  <TabItem value="notebook" label="notebook" default>
    ![UDF Explorer](/img/user-guide/best-practices/notebook_print_stdout.png)
  </TabItem>
</Tabs>

### Type all your inputs

We strongly recommend you [type](https://docs.python.org/3/library/typing.html) all your inputs with the appropriate type:

```python {3} showLineNumbers
@fused.udf
def udf(
    bounds:fused.types.Bounds=None, n:int=1
):
    ...
    return
```

This has 2 effects:
- It makes your code more readable to others
- Fused only supports a [few types](/guide/running-udfs/writing-udfs#parameter-types) at the moment. Any non-typed or unsupported types will be passed as `str`

### Use the line by line execution debugger

Is your UDF taking unexpectedly long to run? You can check which lines take the longest to run by using the line by line execution debugger in [Workbench Code Editor](/reference/workbench/udf-builder/code-editor).

import LazyReactPlayer from '@site/src/components/LazyReactPlayer'

<LazyReactPlayer 
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/geospatial/line_by_line_qa.mp4"
    playing={true}
    muted={true}
    loop={true}
    controls={true}
    width="80%"
    height="auto"
    style={{
        maxWidth: "800px",
        margin: "0 auto 2rem auto",
        display: "block",
        aspectRatio: "16/9"
    }}
/>

### Use `time.time()`

Sometimes you're not sure what's taking so long. The simplest way to figure this out is to use [`time.time()`](https://docs.python.org/3/library/time.html#time.time):

<details>
    <summary>Example: finding a slow process</summary>

    ```python {4,9-11,17-19} showLineNumbers
    @fused.udf
    def udf():
        import time
        beginning_time = time.time()

        # long processing step #1
        time.sleep(5)
        end_process_1 = time.time()
        process_time_1 = round(
            end_process_1 - beginning_time, 2
        )
        print(f"{process_time_1=}")


        # short processing step
        time.sleep(0.2)
        process_time_2 = round(
            time.time() - end_process_1, 2
        )
        print(f"{process_time_2=}")

        return
    ```

    Would give us:

    ```
    >>> process_time_1=5.0
    >>> process_time_2=0.2
    ```
</details>

### Test out your UDFs before running them in parallel

When using `fused.submit()` to run a UDF in parallel you can use the `debug_mode` to run the 1st argument directly to test if your UDF is working as expected:

```python showLineNumbers
job = fused.submit(udf, inputs, debug_mode=True)
```

This runs `fused.run(udf, inputs[0])` and returns the results. It allows you to quickly test out `udf` before running it on a large number of inputs.

:::tip
Since [UDFs are cached by default](/guide/scaling-up/cache#caching-a-udf), using `fused.submit(udf, inputs, debug_mode=True)` means Fused won't run the 1st input again, as you'll have just run it! (unless `udf` is set with `cache_max_age=0`)
:::


### Join the [Discord](https://discord.com/invite/BxS5wMzdRk) for support

We host & run a Discord server where you can ask any questions! We or the community will do our best to help you out!

![Discord](/img/user-guide/best-practices/discord_server.png)

