---
title: Caching
sidebar_label: Caching
sidebar_position: 2
description: Speed up UDFs by caching expensive operations
---

# Caching

Cache expensive operations to speed up your UDFs.

## Basic Usage

Wrap any function with `@fused.cache`:

```python
@fused.cache
def load_data(path):
    import pandas as pd
    return pd.read_parquet(path)

@fused.udf
def udf(path: str = "s3://bucket/large_file.parquet"):
    df = load_data(path)  # Cached after first call
    return df.head(100)
```

## How It Works

1. First call: Function runs normally, result is cached
2. Subsequent calls: Returns cached result instantly
3. Cache key: Based on function name + arguments

## Cache Expiry

Set how long cached results are valid:

```python
@fused.cache(cache_max_age="24h")
def fetch_api_data(endpoint):
    import requests
    return requests.get(endpoint).json()
```

Supported units: `s` (seconds), `m` (minutes), `h` (hours), `d` (days)

## Force Refresh

Ignore cache and recompute:

```python
@fused.cache(cache_reset=True)
def load_data(path):
    return pd.read_parquet(path)
```

## Exclude Parameters from Cache Key

Useful for parameters that don't affect the result:

```python
@fused.cache(cache_key_exclude=["conn"])
def query_db(query, conn):
    return conn.execute(query).df()
```

## Common Patterns

### Cache Data Loading

```python
@fused.cache
def load_model():
    import joblib
    return joblib.load("s3://bucket/model.pkl")
```

### Cache API Calls

```python
@fused.cache(cache_max_age="1h")
def fetch_weather(city):
    import requests
    return requests.get(f"https://api.weather.com/{city}").json()
```

### Cache Heavy Computations

```python
@fused.cache
def train_model(data_path, params):
    df = pd.read_parquet(data_path)
    model = fit_model(df, **params)
    return model
```

## Cache Storage

| Location | Description |
|----------|-------------|
| `"auto"` | Default, uses mount if available |
| `"mount"` | Shared across all UDF runs |
| `"local"` | Only within current execution |

```python
@fused.cache(cache_storage="mount")
def load_data(path):
    return pd.read_parquet(path)
```

## Next Steps

- [Batch Jobs](/core-concepts/run-udfs/run_large/) - Process large datasets
- [fused.submit](/reference/python-sdk/top-level-functions#fusedsubmit) - Parallel execution

> ðŸ“– **Reference:** See [Cache API](/core-concepts/cache/) for full documentation.

