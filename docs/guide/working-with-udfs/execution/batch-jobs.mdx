---
id: batch-jobs
title: Batch Jobs
sidebar_label: Batch Jobs
sidebar_position: 3
---

import LazyReactPlayer from '@site/src/components/LazyReactPlayer'

Batch jobs are for UDFs that need more time or resources than [realtime execution](/guide/working-with-udfs/execution/realtime).

## When to use batch

Use batch when your UDF:
- Takes longer than **120s** to run
- Needs more than **~4GB RAM**

**Tradeoffs:**
- Slower startup (machine needs to spin up)
- No execution time limit
- Higher resource availability

## Quick comparison

| Method | Use case |
|--------|----------|
| `fused.run(..., instance_type='small')` | Single job needing more time/resources |
| `fused.submit(..., instance_type='large')` | Multiple jobs, each needing more resources |
| `job.run_batch()` | [Advanced] Multiple jobs with `arg_list` |

## Option 1: `fused.run()` with `instance_type`

The simplest way to run a batch jobâ€”just add `instance_type`:

```python showLineNumbers
fused.run("my_udf", instance_type="small")
```

This runs your UDF on a dedicated instance instead of the realtime pool.

```python showLineNumbers
# From another UDF
@fused.udf
def udf():
    # Kicks off a batch job for the upstream UDF
    result = fused.run("heavy_processing_udf", instance_type="small")
    return result
```

## Option 2: `fused.submit()` with `instance_type`

Run multiple inputs on a dedicated batch instance:

```python showLineNumbers
results = fused.submit(udf, inputs, instance_type="large")
```

This runs all jobs on a single large machine (e.g., 64 cores), where each job gets access to the full instance resources. Useful when:
- Individual jobs need more than 120s
- Individual jobs need more RAM
- You want to leverage a large machine's parallelism

## Instance types

For convenience, use these aliases:

| Alias | Maps to | vCPUs | RAM |
|-------|---------|-------|-----|
| `small` | `t3.small` | 2 | 2 GB |
| `medium` | `m5.4xlarge` | 16 | 64 GB |
| `large` | `r5.16xlarge` | 64 | 512 GB |

<details>
<summary>All AWS instance types</summary>

| Instance Type | vCPUs | Memory (GB) |
|---------------|-------|-------------|
| `m5.large` | 2 | 8 |
| `m5.xlarge` | 4 | 16 |
| `m5.2xlarge` | 8 | 32 |
| `m5.4xlarge` | 16 | 64 |
| `m5.8xlarge` | 32 | 128 |
| `m5.12xlarge` | 48 | 192 |
| `m5.16xlarge` | 64 | 256 |
| `r5.large` | 2 | 16 |
| `r5.xlarge` | 4 | 32 |
| `r5.2xlarge` | 8 | 64 |
| `r5.4xlarge` | 16 | 128 |
| `r5.8xlarge` | 32 | 256 |
| `r5.12xlarge` | 48 | 384 |
| `r5.16xlarge` | 64 | 512 |
| `t3.small` | 2 | 2 |
| `t3.medium` | 2 | 4 |
| `t3.large` | 2 | 8 |
| `t3.xlarge` | 4 | 16 |
| `t3.2xlarge` | 8 | 32 |

More details on [AWS instance types](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html).

</details>

<details>
<summary>All GCP instance types</summary>

| Instance Type | vCPUs | Memory (GB) |
|---------------|-------|-------------|
| `c2-standard-4` | 4 | 16 |
| `c2-standard-8` | 8 | 32 |
| `c2-standard-16` | 16 | 64 |
| `c2-standard-30` | 30 | 120 |
| `c2-standard-60` | 60 | 240 |
| `m3-ultramem-32` | 32 | 976 |
| `m3-ultramem-64` | 64 | 1,952 |

More details on [GCP instance types](https://cloud.google.com/compute/docs/machine-types).

</details>

## Running from Workbench

Add `instance_type` to the `@fused.udf` decorator:

```python showLineNumbers
@fused.udf(instance_type='small')
def udf(name: str = "world"):
    import pandas as pd
    return pd.DataFrame({"hello": [name]})
```

![Running a job from workbench](batch_job_workbench.png)

:::note
UDFs with `instance_type` require manual execution (`Shift+Enter`) and show a confirmation modal.
:::

## Important behaviors

### Nested UDF calls

Batch context does **not** propagate to nested `fused.run()` calls:

```python showLineNumbers
@fused.udf(instance_type='small')
def batch_udf():
    # This runs as REALTIME, not batch!
    result = fused.run("another_udf")
    
    # To run as batch, explicitly pass instance_type
    result_batch = fused.run("another_udf", instance_type="small")
    return result
```

### Cache keying

`instance_type` is part of the cache key. Running the same UDF with different `instance_type` values creates separate cache entries:

```python showLineNumbers
# These are cached separately
fused.run("my_udf")                           # realtime cache
fused.run("my_udf", instance_type="small")    # batch cache
```

### No confirmation programmatically

:::warning
Using `fused.run(..., instance_type='small')` from code kicks off batch jobs **without confirmation**. This is different from Workbench which shows a confirmation modal. Be careful not to accidentally trigger many batch jobs in loops.
:::

### No execution time limit

Batch jobs have no time limit. You can manually cancel jobs from the [Jobs page](/workbench/jobs/) in Workbench.

## Getting results

Batch jobs should write results to cloud storage (S3, GCS, etc.) rather than returning large data:

```python showLineNumbers
@fused.udf
def batch_job(input_path: str):
    import pandas as pd
    
    # Process data
    df = pd.read_parquet(input_path)
    result = heavy_processing(df)
    
    # Write to S3
    output_path = f"s3://my-bucket/results/{input_path.split('/')[-1]}"
    result.to_parquet(output_path)
    
    return pd.DataFrame({"status": ["done"], "output": [output_path]})
```

## Monitoring jobs

```python showLineNumbers
# View job status
print(job.status)

# Follow logs in real-time
print(job.tail_logs())

# Get all logs
print(job.print_logs())

# Cancel a job
job.cancel()
```

Jobs can also be monitored in [Workbench](/workbench/jobs/) under the "Jobs" tab.

## Advanced: `run_batch()` with `arg_list`

:::note
This is an advanced pattern. For most use cases, prefer `fused.run()` or `fused.submit()` with `instance_type`.
:::

For running a UDF over multiple inputs as a batch job:

```python showLineNumbers
@fused.udf
def udf(val: int = 0):
    import pandas as pd
    return pd.DataFrame({'val': [val]})

job = udf(arg_list=[0, 1, 2, 3, 4])
job.run_batch(instance_type="m5.4xlarge", disk_size_gb=100)
```

<details>
<summary>Multiple parameters with arg_list</summary>

`arg_list` only supports one parameter. Work around this with a dict:

```python showLineNumbers
@fused.udf
def udf(params: dict = {'val1': 1, 'val2': 2}):
    import pandas as pd
    val1 = params['val1']
    val2 = params['val2']
    return pd.DataFrame({'result': [val1 * val2]})

job = udf(arg_list=[{"val1": 5, "val2": 2}, {"val1": 3, "val2": 4}])
job.run_batch()
```

</details>

## Example use cases

- [Scaling an ingestion job](/examples/dark-vessel-detection#34---ingest-1-month-of-ais-data-into-a-geo-partitioned-format) of AIS point data
- [Ingesting cropland data](/examples/zonal-stats#2-bring-in-your-data) for zonal statistics
