---
id: fused-submit
title: Run UDFs in Parallel
sidebar_label: Run UDFs in Parallel
sidebar_position: 4
---

Run a UDF over multiple inputs in parallel.

## Signature

```python
fused.submit(
    udf,
    arg_list,
    engine='remote',
    instance_type='realtime',
    max_workers=32,
    collect=True,
    cache_max_age=None,
)
```

## Parameters

### `arg_list` â€” Input formats

| Format | Example | Use case |
|--------|---------|----------|
| List | `[0, 1, 2, 3]` | Single parameter |
| List of dicts | `[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]` | Multiple parameters |
| DataFrame | `pd.DataFrame({'a': [1, 3], 'b': [2, 4]})` | Multiple parameters |

Each item/row becomes a separate job.

```python
@fused.udf
def udf():
    results = fused.submit(
        single_job_udf, 
        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]
    )
    return results

@fused.udf
def single_job_udf(a: str, b: str):
    import pandas as pd
    return pd.DataFrame({"result": [a + b]})
```

### `engine`

Same as [fused.run](/guide/working-with-udfs/fused-run#engine). Default: `remote`

### `instance_type`

Same as [fused.run](/guide/working-with-udfs/fused-run#instance_type). Default: `realtime`

| Mode | Best for |
|------|----------|
| `realtime` | Many quick jobs |
| `small`/`medium`/`large` | Long/heavy jobs |

### `max_workers`

Number of realtime instances to spin up in parallel. Default: `32`, Max: `1000`.

```python
# Spin up 100 realtime instances in parallel
fused.submit(udf, inputs, max_workers=100)
```

{/* TODO: Add this back in once we have proper API for it*/}
{/* ### `n_processes_per_worker` ðŸš§

| Instance type | Default | Effect |
|---------------|---------|--------|
| `realtime` | 1 | Each arg = 1 lambda |
| Batch | # of cores | Parallel within instance |

Set >1 for realtime to chunk args within each lambda, reducing total invocations. */}

### `collect`

| Value | Behavior | Returns |
|-------|----------|---------|
| `True` (default) | Blocking, waits for all jobs | `DataFrame` |
| `False` | Non-blocking | [`JobPool`](#jobpool-methods) |

### `debug_mode`

When `True`, runs only the first input via `fused.run()`. Use for testing before scaling.

```python
# Test with first input only
fused.submit(udf, inputs, debug_mode=True)
```

### `max_retry`

Max retries per failed job. Default: `2`

### `ignore_exceptions`

When `True`, failed runs are silently skipped in results. Default: `False`

{/* ### `flatten`

| Value | Result format |
|-------|---------------|
| `True` (default) | Flat DataFrame |
| `False` | Results nested in `results` column | */}

### `cache_max_age`

{/* Based on the Python SDK page, so leaving this as is for now */}
Same as [fused.run](/guide/working-with-udfs/fused-run#cache_max_age). Additionally, when `collect=True`, collected results are cached locally for `cache_max_age` or 12h by default.

## JobPool methods

When using `collect=False`, you get a `JobPool` object:

```python
job = fused.submit(udf, inputs, collect=False)

job.wait()           # Show progress bar
job.total_time()     # Total wall time
job.times()          # Time per job
job.first_error()    # First error encountered
job.collect()        # Get results as DataFrame
```

## Tips

**Test first:**
```python
fused.submit(udf, inputs, debug_mode=True)
```

**Start small:**
```python
fused.submit(udf, inputs[:5])
```

**Aim for 30-45s per job** â€” gives safety margin before 120s timeout.

**For batch jobs, save to S3:**
```python
@fused.udf
def batch_udf(input_path: str):
    result = process(input_path)
    output_path = f"s3://bucket/results/{...}"
    result.to_parquet(output_path)
    return output_path  # Return path, not data
```

## See also

- [fused.run](/guide/working-with-udfs/fused-run) â€” single UDF execution
- [How to run in parallel](/guide/working-with-udfs/udf-best-practices/parallel) â€” walkthrough with examples
