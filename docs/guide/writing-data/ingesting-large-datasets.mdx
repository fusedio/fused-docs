---
title: Ingesting Large Datasets
sidebar_label: Large Dataset Ingestion
sidebar_position: 4
description: Use fused.ingest() to partition and optimize large geospatial datasets
---

# Ingesting Large Datasets

For large geospatial datasets, use `fused.ingest()` to partition and optimize data for fast spatial queries.

---

## Why Ingest?

| Problem | Solution |
|---------|----------|
| Large file, slow to query | Geo-partitioned for fast bbox queries |
| Inefficient format | Converted to optimized GeoParquet |
| Single file bottleneck | Parallel chunks for fast access |

---

## When to Use

- Datasets larger than ~100MB
- Geospatial data you'll query frequently
- Data that needs spatial filtering

---

## Basic Usage

```python
import fused

user = fused.api.whoami()['handle']

job = fused.ingest(
    input="https://example.com/large_dataset.zip",
    output=f"fd://{user}/data/my_dataset/",
)

job.run_batch()
```

---

## Monitor Progress

```python
job.tail_logs()
```

---

## Reading Ingested Data

After ingestion, spatial queries are fast:

```python
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    import geopandas as gpd
    
    # Fused automatically filters by bounds
    return gpd.read_parquet("fd://user/data/my_dataset/", bbox=bounds)
```

---

## Learn More

- [Why Ingestion?](/guide/loading-data/geospatial-data-ingestion/why-ingestion)
- [File Formats Guide](/guide/loading-data/file-formats)
