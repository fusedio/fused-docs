---
sidebar_label: fused.h3
title: fused.h3
toc_max_heading_level: 5
sidebar_position: 3
---

The `fused.h3` module provides functions for working with H3-indexed datasets, including ingestion, reading, and metadata management.

```python
import fused

# Ingest raster data to H3
fused.h3.run_ingest_raster_to_h3(...)

# Read H3-indexed data
table = fused.h3.read_hex_table(...)
```

---

## persist_hex_table_metadata

```python
persist_hex_table_metadata(
    dataset_path: str,
    output_path: str | None = None,
    metadata_path: str | None = None,
    verbose: bool = False,
    row_group_size: int = 500,
    indexed_columns: list[str] | None = None,
    pool_size: int = 10,
    metadata_compression_level: int | None = None,
) -> str
```

Persist metadata for a hex table to enable serverless queries.

Scans all parquet files in the dataset, extracts metadata needed for fast reconstruction, and writes one `.metadata.parquet` file per source file.

Each metadata file contains:
- Row group metadata (offsets, H3 ranges, etc.)
- Full metadata_json stored as custom metadata in the parquet schema

Requires a `hex` column (or variant like `h3`, `h3_index`) in all files.

**Parameters:**

- **dataset_path** (`str`) – Path to the dataset directory (e.g., `"s3://bucket/dataset/"`)
- **output_path** (`str | None`) – Deprecated - use `metadata_path` instead.
- **metadata_path** (`str | None`) – Directory path where metadata files should be written. If None, writes to `{source_dir}/.fused/{source_filename}.metadata.parquet` for each source file.
- **verbose** (`bool`) – If True, print timing/progress information.
- **row_group_size** (`int`) – Number of rows per row group in output parquet file. Default is 500.
- **indexed_columns** (`list[str] | None`) – List of column names to index. If None (default), indexes only the first identified indexed column. If empty list, indexes all detected indexed columns.
- **pool_size** (`int`) – Number of parallel workers to use for processing files. Default is 10.
- **metadata_compression_level** (`int | None`) – Optional zstd compression level for metadata parquet files.

**Returns:**

- `str` – Path to metadata directory (if metadata_path provided) or first metadata file path.

**Example:**

```python
fused.h3.persist_hex_table_metadata("s3://my-bucket/my-dataset/")
# Returns: 's3://my-bucket/my-dataset/.fused/file1.metadata.parquet'

fused.h3.persist_hex_table_metadata(
    "s3://my-bucket/my-dataset/",
    metadata_path="s3://my-bucket/metadata/"
)
# Returns: 's3://my-bucket/metadata/'
```

---

## read_hex_table

```python
read_hex_table(
    dataset_path: str,
    hex_ranges_list: list[list[int]],
    columns: list[str] | None = None,
    base_url: str | None = None,
    verbose: bool = False,
    return_timing_info: bool = False,
    batch_size: int | None = None,
    max_concurrent_downloads: int | None = None,
) -> pa.Table | tuple[pa.Table, dict[str, Any]]
```

Read data from an H3-indexed dataset by querying hex ranges.

This is an optimized version that assumes the server always provides full metadata (start_offset, end_offset, metadata, and row_group_bytes) for all row groups. This function eliminates all metadata API calls by using prefetched metadata.

**Parameters:**

- **dataset_path** (`str`) – Path to the H3-indexed dataset (e.g., `"s3://bucket/dataset/"`)
- **hex_ranges_list** (`list[list[int]]`) – List of [min_hex, max_hex] pairs as integers. Example: `[[622236719905341439, 622246719905341439]]`
- **columns** (`list[str] | None`) – Optional list of column names to read. If None, reads all columns.
- **base_url** (`str | None`) – Base URL for API. If None, uses current environment.
- **verbose** (`bool`) – If True, print progress information. Default is False.
- **return_timing_info** (`bool`) – If True, return a tuple of (table, timing_info) instead of just the table.
- **batch_size** (`int | None`) – Target size in bytes for combining row groups. If None, uses `fused.options.row_group_batch_size` (default: 32KB).
- **max_concurrent_downloads** (`int | None`) – Maximum number of simultaneous download operations.

**Returns:**

- `pa.Table` – PyArrow Table containing the concatenated data from all matching row groups. If `return_timing_info` is True, returns a tuple of (table, timing_info dict).

**Example:**

```python
import fused

# Read data for a specific H3 hex range
table = fused.h3.read_hex_table(
    dataset_path="s3://my-bucket/my-h3-dataset/",
    hex_ranges_list=[[622236719905341439, 622246719905341439]]
)
df = table.to_pandas()
```

---

## read_hex_table_slow

```python
read_hex_table_slow(
    dataset_path: str,
    hex_ranges_list: list[list[int]],
    columns: list[str] | None = None,
    base_url: str | None = None,
    verbose: bool = False,
    return_timing_info: bool = False,
    metadata_batch_size: int = 50,
) -> pa.Table | tuple[pa.Table, dict[str, Any]]
```

Read data from an H3-indexed dataset by querying hex ranges (slower version with metadata API calls).

This function queries the dataset index to find matching row groups, then fetches metadata and downloads data. Adjacent row groups from the same file are combined into single downloads for better S3 performance.

**Parameters:**

- **dataset_path** (`str`) – Path to the H3-indexed dataset (e.g., `"s3://bucket/dataset/"`)
- **hex_ranges_list** (`list[list[int]]`) – List of [min_hex, max_hex] pairs as integers.
- **columns** (`list[str] | None`) – Optional list of column names to read. If None, reads all columns.
- **base_url** (`str | None`) – Base URL for API. If None, uses current environment.
- **verbose** (`bool`) – If True, print progress information. Default is False.
- **return_timing_info** (`bool`) – If True, return a tuple of (table, timing_info) instead of just the table.
- **metadata_batch_size** (`int`) – Maximum number of row group metadata requests to batch together in a single API call. Default is 50.

**Returns:**

- `pa.Table` – PyArrow Table containing the concatenated data from all matching row groups.

**Example:**

```python
import fused

table = fused.h3.read_hex_table_slow(
    dataset_path="s3://my-bucket/my-h3-dataset/",
    hex_ranges_list=[[622236719905341439, 622246719905341439]]
)
df = table.to_pandas()
```

---

## read_hex_table_with_persisted_metadata

```python
read_hex_table_with_persisted_metadata(
    dataset_path: str,
    hex_ranges_list: list[list[int]],
    columns: list[str] | None = None,
    metadata_path: str | None = None,
    verbose: bool = False,
    return_timing_info: bool = False,
    batch_size: int | None = None,
    max_concurrent_downloads: int | None = None,
    max_concurrent_metadata_reads: int | None = None,
) -> pa.Table | tuple[pa.Table, dict[str, Any]]
```

Read data from an H3-indexed dataset using persisted metadata parquet files.

This function reads from per-file metadata parquet files instead of querying a server. Each source parquet file has a corresponding `.metadata.parquet` file stored at `{source_dir}/.fused/{source_filename}.metadata.parquet`.

Supports subdirectory queries - if dataset_path points to a subdirectory, the function will look for metadata files for files in that subdirectory.

**Parameters:**

- **dataset_path** (`str`) – Path to the H3-indexed dataset (e.g., `"s3://bucket/dataset/"`). Can also be a subdirectory path for filtering.
- **hex_ranges_list** (`list[list[int]]`) – List of [min_hex, max_hex] pairs as integers.
- **columns** (`list[str] | None`) – Optional list of column names to read. If None, reads all columns.
- **metadata_path** (`str | None`) – Directory path where metadata files are stored. If None, looks for metadata at `{source_dir}/.fused/` for each source file.
- **verbose** (`bool`) – If True, print progress information. Default is False.
- **return_timing_info** (`bool`) – If True, return a tuple of (table, timing_info) instead of just the table.
- **batch_size** (`int | None`) – Target size in bytes for combining row groups. If None, uses `fused.options.row_group_batch_size`.
- **max_concurrent_downloads** (`int | None`) – Maximum number of simultaneous download operations.
- **max_concurrent_metadata_reads** (`int | None`) – Maximum number of simultaneous metadata file reads.

**Returns:**

- `pa.Table` – PyArrow Table containing the concatenated data from all matching row groups.

**Example:**

```python
import fused

# First, persist metadata
fused.h3.persist_hex_table_metadata("s3://my-bucket/my-dataset/")

# Then read using persisted metadata (no server needed)
table = fused.h3.read_hex_table_with_persisted_metadata(
    dataset_path="s3://my-bucket/my-dataset/",
    hex_ranges_list=[[622236719905341439, 622246719905341439]]
)
```

---

## run_ingest_raster_to_h3

```python
run_ingest_raster_to_h3(
    src_path: str | list[str],
    output_path: str,
    metrics: str | list[str] = "cnt",
    res: int | None = None,
    k_ring: int = 1,
    res_offset: int = 1,
    chunk_res: int | None = None,
    file_res: int | None = None,
    overview_res: list[int] | None = None,
    overview_chunk_res: int | list[int] | None = None,
    max_rows_per_chunk: int = 0,
    include_source_url: bool = True,
    target_chunk_size: int | None = None,
    debug_mode: bool = False,
    remove_tmp_files: bool = True,
    tmp_path: str | None = None,
    overwrite: bool = False,
    steps: list[str] | None = None,
    extract_kwargs: dict | None = {},
    partition_kwargs: dict | None = {},
    overview_kwargs: dict | None = {},
    **kwargs
)
```

Run the raster to H3 ingestion process.

This process involves multiple steps:

- Extract pixel values and assign to H3 cells in chunks (extract step)
- Combine the chunks per partition (file) and prepare metadata (partition step)
- Create the metadata `_sample` file and overviews files

**Parameters:**

- **src_path** (`str | list[str]`) – Path(s) to the input raster data. When a single path, the file is chunked for processing based on `target_chunk_size`. When a list, each file is processed as one chunk.
- **output_path** (`str`) – Path for the resulting Parquet dataset.
- **metrics** (`str | list[str]`) – The metrics to compute per H3 cell. Supported metrics are either `"cnt"` or a list containing any of `"avg"`, `"min"`, `"max"`, `"stddev"`, `"mode"`, and `"sum"`.
- **res** (`int | None`) – The resolution of the H3 cells in the output data. By default, inferred based on the resolution of the input data.
- **k_ring** (`int`) – The k-ring distance at resolution `res + res_offset` to which the pixel value is assigned. Defaults to 1.
- **res_offset** (`int`) – Offset to child resolution (relative to `res`) at which to assign the raw pixel values to H3 cells.
- **file_res** (`int | None`) – The H3 resolution to chunk the resulting files of the Parquet dataset. Specify `file_res=-1` to have a single output file.
- **chunk_res** (`int | None`) – The H3 resolution to chunk the row groups within each file.
- **overview_res** (`list[int] | None`) – The H3 resolutions for which to create overview files. By default, resolutions 3-7.
- **overview_chunk_res** (`int | list[int] | None`) – The H3 resolution(s) to chunk the row groups within each overview file.
- **max_rows_per_chunk** (`int`) – Maximum number of rows per chunk in the resulting data and overview files. If 0 (default), `chunk_res` and `overview_chunk_res` are used.
- **include_source_url** (`bool`) – If True, include a `"source_url"` column in the output dataset. Defaults to True.
- **target_chunk_size** (`int | None`) – Approximate number of pixel values to process per chunk in the extract step. Defaults to 10,000,000.
- **debug_mode** (`bool`) – If True, run only the first two chunks for debugging. Defaults to False.
- **remove_tmp_files** (`bool`) – If True, remove temporary files after ingestion. Defaults to True.
- **tmp_path** (`str | None`) – Optional path for temporary files. If specified, the extract step is skipped.
- **overwrite** (`bool`) – If True, overwrite the output path if it already exists. Defaults to False.
- **steps** (`list[str] | None`) – Processing steps to run. Can include `"extract"`, `"partition"`, `"metadata"`, and `"overview"`. By default, all steps run.
- **extract_kwargs** (`dict | None`) – Additional keyword arguments for [`fused.submit`](/python-sdk/api-reference/fused-submit) for the extract step.
- **partition_kwargs** (`dict | None`) – Additional keyword arguments for `fused.submit` for the partition step.
- **overview_kwargs** (`dict | None`) – Additional keyword arguments for `fused.submit` for the overview step.

The extract, partition and overview steps are run in parallel using [`fused.submit()`](/python-sdk/api-reference/fused-submit). By default, the function will first attempt to run this using "realtime" instances, and retry any failed runs using "large" instances.

**Example:**

```python
fused.h3.run_ingest_raster_to_h3(
    src_path="s3://my-bucket/raster.tif",
    output_path="s3://my-bucket/output/",
    metrics=["avg", "min", "max"],
    res=10,
)
```

To run everything locally:

```python
fused.h3.run_ingest_raster_to_h3(..., engine="local")
```

To customize instance types per step:

```python
fused.h3.run_ingest_raster_to_h3(
    ...,
    extract_kwargs={"instance_type": "realtime", "max_workers": 256},
    partition_kwargs={"instance_type": "medium", "max_workers": 5},
)
```

---

## run_partition_to_h3

```python
run_partition_to_h3(
    input_path: str | list[str],
    output_path: str,
    metrics: str | list[str] = "cnt",
    groupby_cols: list[str] = ["hex", "data"],
    window_cols: list[str] = ["hex"],
    additional_cols: list[str] = [],
    res: int | None = None,
    chunk_res: int | None = None,
    file_res: int | None = None,
    overview_res: list[int] | None = None,
    overview_chunk_res: int | list[int] | None = None,
    max_rows_per_chunk: int = 0,
    remove_tmp_files: bool = True,
    overwrite: bool = False,
    extract_kwargs: dict = {},
    partition_kwargs: dict = {},
    overview_kwargs: dict = {},
    **kwargs
)
```

Run the H3 partitioning process.

This pipeline assumes that the input data already has a `"hex"` column with H3 cell IDs at the target resolution. It will then repartition the data spatially and add overview files.

**Parameters:**

- **input_path** (`str | list[str]`) – Path(s) to the input data. Can be a path to file or directory, or list of file paths.
- **output_path** (`str`) – Path for the resulting Parquet dataset.
- **metrics** (`str | list[str]`) – The metrics to compute per H3 cell. Supported: `"cnt"` or a list containing `"avg"`, `"min"`, `"max"`, `"stddev"`, and `"sum"`.
- **groupby_cols** (`list[str]`) – The columns indicating the groups for which aggregated metrics are calculated. Default is `["hex", "data"]`. Must always include `"hex"`. Only applies when `"cnt"` metric is specified.
- **window_cols** (`list[str]`) – The columns for which to calculate total counts over using a window function. Default is `["hex"]`. Only applies when `"cnt"` metric is specified.
- **additional_cols** (`list[str]`) – Additional columns from the input data to include in the output dataset. Default is empty.
- **res** (`int | None`) – The resolution at which to assign the pixel values to H3 cells.
- **file_res** (`int | None`) – The H3 resolution to chunk the resulting files. Specify `file_res=-1` for a single output file.
- **chunk_res** (`int | None`) – The H3 resolution to chunk the row groups within each file.
- **overview_res** (`list[int] | None`) – The H3 resolutions for which to create overview files.
- **overview_chunk_res** (`int | list[int] | None`) – The H3 resolution(s) to chunk the row groups within each overview file.
- **max_rows_per_chunk** (`int`) – Maximum number of rows per chunk. If 0 (default), `chunk_res` and `overview_chunk_res` are used.
- **remove_tmp_files** (`bool`) – If True, remove temporary files after ingestion. Defaults to True.
- **overwrite** (`bool`) – If True, overwrite the output path if it already exists. Defaults to False.
- **extract_kwargs** (`dict`) – Additional keyword arguments for the extract step.
- **partition_kwargs** (`dict`) – Additional keyword arguments for the partition step.
- **overview_kwargs** (`dict`) – Additional keyword arguments for the overview step.

**Example:**

```python
# Partition data that already has H3 hex column
fused.h3.run_partition_to_h3(
    input_path="s3://my-bucket/data-with-hex/",
    output_path="s3://my-bucket/partitioned-output/",
    metrics=["avg", "sum"],
    res=10,
)
```

---
