---
sidebar_label: Job (deleteme)
title: fused.models.api.job
toc_max_heading_level: 5
unlisted: true
---


## JobStepConfig Objects (udf parent)

```python
class JobStepConfig(FusedBaseModel)
```

### metadata

User defined metadata. Any metadata values must be JSON serializable.

### ignore\_chunk\_error

If `True`, continue processing even if some computations throw errors.

### run\_remote

```python
def run_remote(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Execute this operation

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

### set\_output

```python
def set_output(table_or_url: Optional[str] = None,
               *,
               table: Optional[str] = None,
               url: Optional[str] = None,
               inplace: bool = False,
               overwrite: Optional[bool] = None) -> JobStepConfig
```

Update output tables on this operation

**Arguments**:

- `table_or_url` - Automatically set either `table` or `url` depending on whether this is a URL.


**Arguments**:

- `table` - The name of the table to use for output. This table name must be unique. Defaults to None.
- `url` - If set, the URL to write the table to. Overrides `table` and `base_path`.
- `inplace` - If True, modify and return this object. If False, modify and return a copy. Defaults to False.
- `overwrite` - If True, overwrite the output dataset if it already exists. Defaults to None to not update.


**Returns**:

  _description_


## GeospatialPartitionJobStepConfig Objects ðŸ‘‰ (ingest)

```python
class GeospatialPartitionJobStepConfig(PartitionJobStepConfig)
```

### drop\_out\_of\_bounds

Whether to drop points that are outside of the WGS84 valid bounds.

### lonlat\_cols

Names of longitude, latitude columns to construct point geometries from.

This currently applies only to loading Parquet files.

If the original files are in a format such as CSV, pass the names of the longitude
and latitude columns in the GDALOpenConfig. If you pass those to GDALOpenConfig, do
not also pass names to lonlat_columns here.

### partitioning\_method

The method used for deciding how to group geometries.

### subdivide\_start

Geometries with greater area than this (in WGS84 degrees) will be subdivided.
Start area should be greater than or equal to stop area.

### subdivide\_stop

This is the area that will stop continued subdivision of a geometry.
Stop area should be less than or equal to start area. Additionally stop area cannot
be zero, as that would cause infinite subdivision.

### split\_identical\_centroids

Whether to split a partition that has identical centroids (such as if all geometries
in the partition are the same) if there are more such rows than defined in
"partitioning_maximum_per_file" and "partitioning_maximum_per_chunk".

### target\_num\_chunks

The target for the number of chunks if partitioning_maximum_per_file is None.

### gdal\_config

Options to pass to GDAL for opening files.

### run\_remote

```python
def run_remote(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Execute this operation

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.

- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## UdfJobStepConfig Objects ðŸ‘‰ (udf)

```python
class UdfJobStepConfig(JobStepConfig)
```

A job step of running a UDF.

### set\_input

```python
def set_input(input: Optional[List[Any]],
              inplace: bool = False) -> UdfJobStepConfig
```

Set the input datasets on this operation

**Arguments**:

- `input` - A list of JSON-serializable objects to pass as input to the UDF, or None to run once with no arguments.
- `inplace` - If True, modify and return this object. If False, modify and return a copy. Defaults to False.

### set\_udf

```python
def set_udf(udf: AnyBaseUdf | dict | str,
            parameters: Optional[Dict[str, Any]] = None,
            replace_parameters: bool = False,
            inplace: bool = False) -> UdfJobStepConfig
```

Set a UDF on this operation.

**Arguments**:

- `udf` - the representation of this UDF
- `parameters` - Parameters to set on the UDF. Defaults to None to not set parameters.
- `replace_parameters` - If True, unset any parameters not passed in parameters. Defaults to False.
- `inplace` - If True, modify and return this object. If False, modify and return a copy. Defaults to False.

### run\_local

```python
def run_local(sample: Any | None = ...,
              validate_output: bool = False,
              validate_imports: Optional[bool] = None,
              **kwargs) -> UdfEvaluationResult
```

Run a UDF locally on sample data.

**Arguments**:

- `sample` - The sample input to pass to the UDF. Defaults to None.
- `validate_output` - If True, the output of the UDF is validated and schema is updated. If False,
  the output is returned as-is. Defaults to False.
- `**kwargs` - Additional keyword arguments to be passed to the UDF.


**Returns**:

  The output of the UDF applied to the input data.


**Raises**:

  Any exceptions raised by the UDF during its execution.



## RunResponse Objects

```python
class RunResponse(BaseModel)
```

### job\_id

The identifier of this job.

### status

The status of the instance running this job.

### get\_status

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

### get\_logs

```python
def get_logs(since_ms: Optional[int] = None) -> List[Any]
```

Fetch logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.


**Returns**:

  Log messages for the given job.

### print\_logs

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.


**Returns**:

  None

### get\_exec\_time

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

### tail\_logs

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1) -> None
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

### wait\_for\_job

```python
def wait_for_job(poll_interval_seconds: float = 5,
                 timeout: Optional[float] = None) -> RunResponse
```

Block the Python kernel until this job has finished

**Arguments**:

- `poll_interval_seconds` - How often (in seconds) to poll for status updates. Defaults to 5.
- `timeout` - The length of time in seconds to wait for the job. Defaults to None.


**Raises**:

- `TimeoutError` - if waiting for the job timed out.


**Returns**:

  The status of the given job.

### cancel

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.

### refresh\_status

```python
def refresh_status() -> RunResponse
```

Refresh the status of this job.

### from\_job\_id

```python
@classmethod
def from_job_id(cls, job: Union[str, RunResponse]) -> RunResponse
```

Creates a RunResponse object from either a job ID or a RunResponse.

**Arguments**:

- `job` - Either a job ID string, or a RunResponse.


**Returns**:

  A RunResponse object.

## JobResponse Objects

```python
class JobResponse(BaseModel)
```

### id

The identifier of this job.

### creation\_date

When this job was started.

### job\_status

The status of this job.

### job\_status\_date

When the job_status was last updated

### config

```python
def config() -> JobStepConfig
```

Fetch the job step configuration

**Returns**:

  The configuration for the job step in this job.

### steps\_config

```python
def steps_config() -> JobConfig
```

Fetch the job configuration

**Returns**:

  The configuration for all job steps in this job.

### get\_status

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

### get\_logs

```python
def get_logs(since_ms: Optional[int] = None) -> List[Any]
```

Fetch logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.


**Returns**:

  Log messages for the given job.

### print\_logs

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.


**Returns**:

  None

### get\_exec\_time

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

### tail\_logs

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1)
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

### wait\_for\_job

```python
def wait_for_job(poll_interval_seconds: float = 5,
                 timeout: Optional[float] = None) -> RunResponse
```

Block the Python kernel until this job has finished

**Arguments**:

- `poll_interval_seconds` - How often (in seconds) to poll for status updates. Defaults to 5.
- `timeout` - The length of time in seconds to wait for the job. Defaults to None.


**Raises**:

- `TimeoutError` - if waiting for the job timed out.


**Returns**:

  The status of the given job.

### cancel

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.
