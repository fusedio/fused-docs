---
title: Quickstart for Data Engineers
sidebar_label: Data Engineer
sidebar_position: 2
description: Get started with Fused for data engineering workflows
---

# Quickstart for Data Engineers

Build APIs, run batch jobs, and create data pipelines.

## Connect to Cloud Storage or Data Warehouses

```python
@fused.udf
def sync_to_snowflake():
    import snowflake.connector
    
    conn = snowflake.connector.connect(
        user=fused.secret('SNOWFLAKE_USER'),
        password=fused.secret('SNOWFLAKE_PASSWORD'),
        account='your_account'
    )
    # Load and sync data...
```

Read more about bringing data in & out of Fused:
- [Loading Data](/guide/loading-data/)
    - [Files](/guide/loading-data/local-files)
    - [Cloud](/guide/loading-data/cloud-storage)
    - [Databases](/guide/loading-data/databases)
    - [APIs & STAC Catalogs](/guide/loading-data/apis)
- [Writing Data](/guide/writing-data/)
    - [To Cloud Storage](/guide/writing-data/to-cloud-storage)
    - [To Databases](/guide/writing-data/to-databases)
    - [Ingesting Large Datasets](/guide/writing-data/ingesting-large-datasets)
    - [Turn Data into an API](/guide/writing-data/turn-your-data-into-an-api)

## Turn any data into an API

```python
@fused.udf
def udf(table: str = "sales", limit: int = 100, start_date: str = "2024-01-01"):
    import duckdb
    
    conn = duckdb.connect()
    df = conn.execute(f"""
        SELECT * FROM 's3://bucket/{table}.parquet' 
        WHERE date >= '{start_date}'
        LIMIT {limit}
    """).df()
    
    return df
```

Save in Workbench â†’ Get HTTP endpoint:

```
https://udf.ai/fsh_[shared_token].parquet?table=orders&limit=50&start_date=2024-06-01
```

This can be used to:
- Create a G[eospatial Map Tile Server](/guide/running-udfs/realtime#tiling) for large data
- [Allow an LLM to talk to your data through MCP](/guide/building-apps/let-anyone-talk-to-your-data)

{/* ## Run Batch Jobs

```python
@fused.udf
def process_file(file_path: str):
    import pandas as pd
    
    df = pd.read_parquet(file_path)
    df['processed_at'] = pd.Timestamp.now()
    df.to_parquet(file_path.replace('raw', 'processed'))
    
    return {"status": "success", "rows": len(df)}

# Run over many files in parallel
files = ["s3://bucket/raw/file1.parquet", "s3://bucket/raw/file2.parquet"]
results = fused.submit(process_file, [{"file_path": f} for f in files])
```

## Ingest Large Datasets

```python
job = fused.ingest(
    input="s3://bucket/raw_data/",
    output="s3://bucket/processed/",
    partitioning_method="rows",
)
job.run_batch(instance_type="m5.4xlarge")
``` */}

## Parameterized Endpoints for UDFs

```python
@fused.udf
def udf(
    start_date: str = "2024-01-01",
    end_date: str = "2024-12-31",
    region: str = "US",
    format: str = "summary"
):
    # Query based on parameters
    df = query_data(start_date, end_date, region)
    
    if format == "summary":
        return df.groupby('category').sum()
    return df
```

You can then call this UDF endpoint anywhere with query parameters:
```
https://udf.ai/fsh_[shared_token].parquet?start_date=2024-01-01&end_date=2024-12-31&region=US
```

## Chose Any Common Output Formats

```
https://udf.ai/fsh_[shared_token].parquet  # Parquet file
https://udf.ai/fsh_[shared_token].csv      # CSV file
https://udf.ai/fsh_[shared_token].json     # JSON
https://udf.ai/fsh_[shared_token].html     # HTML (Creates a standard HTML table or image)
```

## Next Steps

- [Writing Data](/guide/writing-data/) - Export to S3
- [Scaling Up](/guide/scaling-up/) - Batch jobs, performance
- [Python SDK Reference](/reference/python-sdk/) - Full API docs
