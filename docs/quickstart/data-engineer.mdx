---
title: Quickstart for Data Engineers
sidebar_label: Data Engineer
sidebar_position: 2
description: Get started with Fused for data engineering workflows
---

# Quickstart for Data Engineers

Build APIs, run batch jobs, and create data pipelines.

## Setup

```bash
pip install fused
```

```python
import fused
from fused.api import NotebookCredentials
credentials = NotebookCredentials()
```

## Create an API Endpoint

```python
@fused.udf
def udf(table: str = "sales", limit: int = 100, start_date: str = "2024-01-01"):
    import duckdb
    
    conn = duckdb.connect()
    df = conn.execute(f"""
        SELECT * FROM 's3://bucket/{table}.parquet' 
        WHERE date >= '{start_date}'
        LIMIT {limit}
    """).df()
    
    return df
```

Save in Workbench â†’ Get HTTP endpoint:

```
https://fused.io/.../run/file?table=orders&limit=50&start_date=2024-06-01
```

## Run Batch Jobs

```python
@fused.udf
def process_file(file_path: str):
    import pandas as pd
    
    df = pd.read_parquet(file_path)
    df['processed_at'] = pd.Timestamp.now()
    df.to_parquet(file_path.replace('raw', 'processed'))
    
    return {"status": "success", "rows": len(df)}

# Run over many files in parallel
files = ["s3://bucket/raw/file1.parquet", "s3://bucket/raw/file2.parquet"]
results = fused.submit(process_file, [{"file_path": f} for f in files])
```

## Ingest Large Datasets

```python
job = fused.ingest(
    input="s3://bucket/raw_data/",
    output="s3://bucket/processed/",
    partitioning_method="rows",
)
job.run_batch(instance_type="m5.4xlarge")
```

## Key Patterns

### Parameterized Endpoints

```python
@fused.udf
def udf(
    start_date: str = "2024-01-01",
    end_date: str = "2024-12-31",
    region: str = "US",
    format: str = "summary"
):
    # Query based on parameters
    df = query_data(start_date, end_date, region)
    
    if format == "summary":
        return df.groupby('category').sum()
    return df
```

### Scheduled Runs

Call your UDF endpoint from any scheduler (Airflow, cron, etc.):

```bash
# In Airflow DAG or cron job
curl "https://fused.io/.../run/file?format=parquet" > daily_export.parquet
```

### Output Formats

```
?format=parquet  # Parquet file
?format=csv      # CSV file
?format=json     # JSON
```

### Connect to Data Warehouses

```python
@fused.udf
def sync_to_snowflake():
    import snowflake.connector
    
    conn = snowflake.connector.connect(
        user=fused.secret('SNOWFLAKE_USER'),
        password=fused.secret('SNOWFLAKE_PASSWORD'),
        account='your_account'
    )
    # Load and sync data...
```

## Next Steps

- [Writing Data](/guide/writing-data/) - Export to S3
- [Scaling Up](/guide/scaling-up/) - Batch jobs, performance
- [Python SDK Reference](/reference/python-sdk/) - Full API docs
