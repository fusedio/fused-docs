---
slug: ai-for-web-scraping
title: "Scraping structured data from web pages"
---

This tutorial demonstrates how to use Fused User Defined Functions ([UDFs](/core-concepts/write/)) to scrape web pages and extract structured data. We'll walk through building a dataset of public schools from a specific geographic area using three different scraping approaches.


## Introduction

Imagine you need to compile a comprehensive list of public schools for a specific area. A quick Google search leads us to the [Top Ranked Public Schools](https://www.publicschoolreview.com/top-ranked-public-schools) website. This site contains multiple subpages for different locations like [Connecticut](https://www.publicschoolreview.com/top-ranked-public-schools/connecticut) or [New York](https://www.publicschoolreview.com/top-ranked-public-schools/new-york).

Each page displays only a limited number of schools initially, requiring users to click "see more" to load additional results. Our goal is to programmatically extract data from these pages and generate CSV files for any location without manual navigation.

import SeeMoreImg from '@site/static/img/tutorials/data-science-ai/see_more.png';

<div style={{textAlign: 'center'}}>
<img src={SeeMoreImg} alt="File" style={{}} />
</div>

## Method 1: Individual Page Scraping

When you know the exact page URL and whether it contains paginated content, use the [scrapegraph web scraper UDF](https://www.fused.io/workbench/udf/catalog/scrapegraph_web_scraper-0a121078-1b03-4c62-ba23-274ffb4c7568) to extract structured data from single pages.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The target webpage URL to scrape |
| **query** | string | Natural language description of data to extract |
| **output_schema** | dict (optional) | Expected structure of the output data |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_3A1QcdR5kJEwmDSkYxc934",
    url="https://www.publicschoolreview.com/top-ranked-public-schools/connecticut/tab/all/num/1",
    query="Extract school names, ranks, and addresses",
    pagination_pages=2
)
df.head()
```

**Output:**

import DfOutput from '@site/static/img/tutorials/data-science-ai/df_out_1.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput} alt="File" style={{}} />
</div>

## Method 2: Batch Scraping Multiple Pages

When you need to scrape data from multiple known URLs simultaneously, use the [scrapegraph multi scraper UDF](https://www.fused.io/workbench/udf/catalog/scrapegraph_multi_scraper-8aaafa63-3888-4f12-8d31-6e3f9098b2ae). This UDF creates a unified output schema across all pages and processes them in parallel using [fused.submit](/python-sdk/top-level-functions/#fusedsubmit) calls.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **urls** | list | List of webpage URLs to scrape in batch |
| **query** | string | Natural language description of data to extract |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape per URL |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_5WETmX04oWgWtSCxwv1ZNr",
    urls=[
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/1",
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/3",
    ],
    query="Extract school names, grade ranges, and addresses for all NYC schools"
)
df.head()
```

**Output:**

import DfOutput2 from '@site/static/img/tutorials/data-science-ai/df_out_2.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput2} alt="File" style={{}} />
</div>

## Method 3: Intelligent Crawling and Scraping

When you only know the top-level domain but not the specific page URLs, use the [firecrawl search UDF](https://www.fused.io/workbench/udf/catalog/firecrawl_search-43935d03-7e82-4173-baa8-2f0e63d0c473) to automatically discover and scrape relevant pages. This UDF crawls the website, identifies the most relevant pages based on your search criteria, and extracts the requested data.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The base URL to crawl and search |
| **search_prompt** | string | Natural language description of content to find |
| **extraction_prompt** | string | Natural language description of data to extract |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_6mpu2dqoEBc1W80GjhZLSM",
    url="https://www.publicschoolreview.com",
    search_prompt="best public schools in connecticut",
    extraction_prompt="Extract school names, grade ranges, and addresses"
)
df.head()
```

**Output:**

import DfOutput3 from '@site/static/img/tutorials/data-science-ai/df_out_3.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput3} alt="File" style={{}} />
</div>


## Using HTTPS API Endpoints

For integration into external systems or automated workflows, you can [generate shared HTTPS endpoints](/core-concepts/run-udfs/run-small-udfs/#https-requests) for these UDFs. This allows you to retrieve data in CSV format using simple HTTPS requests.

### Available Endpoints

**Individual Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_3A1QcdR5kJEwmDSkYxc934/run/file?format=csv"
```

**Multi-Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_5WETmX04oWgWtSCxwv1ZNr/run/file?format=csv"
```

**Intelligent Crawler:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_6mpu2dqoEBc1W80GjhZLSM/run/file?format=csv"
```

## Making it Your Own

These are community UDFs that you can fork and customize for your needs. Simply click on the "Make a copy to modify" in the Fused Workbench to create your own copy.

**API Setup Required**: You'll need to configure your own API keys for third party services as [fused secrets](/workbench/preferences/#secrets-management)

## See Also

- Turning [any UDF into an API](/tutorials/engineering-etl/#turn-your-data-into-an-api)
- Letting anyone talk to your data [through MCP Servers](/tutorials/Analytics%20&%20Dashboard/let-anyone-talk-to-your-data/)
- [Running jobs in parallel](/core-concepts/run-udfs/run-small-udfs/#running-jobs-in-parallel-fusedsubmit) with `fused.submit()`

*Note: These web scraping features are currently in active development and you may encounter occasional issues.*