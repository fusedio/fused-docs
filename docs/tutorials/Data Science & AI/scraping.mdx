---
slug: ai-for-web-scraping
title: "Scraping structured data from web pages"
---

This tutorial demonstrates how to use Fused User Defined Functions ([UDFs](/core-concepts/write/)) to scrape web pages and extract structured data. We'll walk through building a dataset of public schools from a specific geographic area using three different scraping approaches.


## Introduction

Imagine you need to compile a comprehensive list of public schools for a specific area. A quick Google search leads us to the [Top Ranked Public Schools](https://www.publicschoolreview.com/top-ranked-public-schools) website. This site contains multiple subpages for different locations like [Connecticut](https://www.publicschoolreview.com/top-ranked-public-schools/connecticut) or [New York](https://www.publicschoolreview.com/top-ranked-public-schools/new-york).

Each page displays only a limited number of schools initially, requiring users to click "see more" to load additional results. Our goal is to programmatically extract data from these pages and generate CSV files for any location without manual navigation.

import SeeMoreImg from '@site/static/img/tutorials/data-science-ai/see_more.png';

<div style={{textAlign: 'center'}}>
<img src={SeeMoreImg} alt="File" style={{}} />
</div>

## Method 1: Individual Page Scraping

When you know the exact page URL and whether it contains paginated content, use the [scrapegraph web scraper UDF](https://www.fused.io/workbench/udf/catalog/scrapegraph_web_scraper-0a121078-1b03-4c62-ba23-274ffb4c7568) to extract structured data from single pages.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The target webpage URL to scrape |
| **query** | string | Natural language description of data to extract |
| **output_schema** | dict (optional) | Expected structure of the output data |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_3A1QcdR5kJEwmDSkYxc934",
    url="https://www.publicschoolreview.com/top-ranked-public-schools/connecticut/tab/all/num/1",
    query="Extract school names, ranks, and addresses",
    pagination_pages=2
)
df.head()
```

**Output:**

import DfOutput from '@site/static/img/tutorials/data-science-ai/df_out_1.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput} alt="File" style={{}} />
</div>

## Method 2: Batch Scraping Multiple Pages

When you need to scrape data from multiple known URLs simultaneously, use the [scrapegraph multi scraper UDF](https://www.fused.io/workbench/udf/catalog/scrapegraph_multi_scraper-8aaafa63-3888-4f12-8d31-6e3f9098b2ae). This UDF creates a unified output schema across all pages and processes them in parallel using [fused.submit](/python-sdk/top-level-functions/#fusedsubmit) calls.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **urls** | list | List of webpage URLs to scrape in batch |
| **query** | string | Natural language description of data to extract |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape per URL |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_5WETmX04oWgWtSCxwv1ZNr",
    urls=[
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/1",
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/3",
    ],
    query="Extract school names, grade ranges, and addresses for all NYC schools"
)
df.head()
```

**Output:**

import DfOutput2 from '@site/static/img/tutorials/data-science-ai/df_out_2.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput2} alt="File" style={{}} />
</div>

## Method 3: Intelligent Crawling and Scraping

When you only know the top-level domain but not the specific page URLs, use the [firecrawl search UDF](https://www.fused.io/workbench/udf/catalog/firecrawl_search-43935d03-7e82-4173-baa8-2f0e63d0c473) to automatically discover and scrape relevant pages. This UDF crawls the website, identifies the most relevant pages based on your search criteria, and extracts the requested data.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The base URL to crawl and search |
| **search_prompt** | string | Natural language description of content to find |
| **extraction_prompt** | string | Natural language description of data to extract |

### Example Usage

**Using the Python SDK:**

```python
import fused

df = fused.run(
    "fsh_6mpu2dqoEBc1W80GjhZLSM",
    url="https://www.publicschoolreview.com",
    search_prompt="best public schools in connecticut",
    extraction_prompt="Extract school names, grade ranges, and addresses"
)
df.head()
```

**Output:**

import DfOutput3 from '@site/static/img/tutorials/data-science-ai/df_out_3.png';

<div style={{textAlign: 'center'}}>
<img src={DfOutput3} alt="File" style={{}} />
</div>


## Using HTTP API Endpoints

For integration into external systems or automated workflows, you can [generate shared HTTP endpoints](/core-concepts/run-udfs/run-small-udfs/#https-requests) for these UDFs. This allows you to retrieve data in CSV format using simple HTTP requests.

### Available Endpoints

**Individual Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_3A1QcdR5kJEwmDSkYxc934/run/file?dtype_out_vector=csv"
```

**Multi-Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_5WETmX04oWgWtSCxwv1ZNr/run/file?dtype_out_vector=csv"
```

**Intelligent Crawler:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_6mpu2dqoEBc1W80GjhZLSM/run/file?dtype_out_vector=csv"
```

## Summary

This tutorial covered three distinct approaches for web scraping with Fused:

1. **Individual Page Scraping**: Best for scraping single pages with known URLs
2. **Batch Multi-Page Scraping**: Ideal when you have multiple specific URLs to process
3. **Intelligent Crawling**: Perfect when you need to discover relevant pages automatically

Choose the method that best fits your use case and data requirements. All approaches return structured data that can be easily integrated into your data pipelines and analysis workflows.

*Note: These web scraping features are currently in active development and you may encounter occasional issues.*