---
id: Climate Dashboard
title: Climate Dashboard
sidebar_label: Climate Dashboard
sidebar_position: 0
---

# Building a Climate dashboard

We're going to build an interactive dashboard of global temperature data, after processing 1TB of data in a few minutes!

### Install `fused`

```python
pip install "fused[all]"
```

Read more about installing Fused [here](/python-sdk/#python-install).

<details>
<summary>Authenticate in Fused</summary>

In a notebook:

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the link to authenticate.

Read more about [authenticating in Fused](/python-sdk/authentication/).

</details>

{/* TODO:  Remove this to clean up. Not needed anymore*/}
{/* ### Your first User Defined Function

Fused is built around the concept of [User Defined Functions (UDFs)](/core-concepts/why/). These are serverless Python functions:

```python
@fused.udf
def udf(path: str = 's3://fused-asset/data/era5/t2m/datestr=2024-01-01/0.parquet'):
    import pandas as pd
    df = pd.read_parquet(path, columns=['daily_mean'])

    return df.head()
```

Call your UDF to execute it on Fused server:

```python
fused.run(udf)
```

You get your results back in a couple of seconds:

```shell
>>> 	daily_mean
hex	
644014746717455876	268.686016
644014747002884118	268.747975
644014747398193161	268.623347
``` */}

### Processing 1 month

{/* TODO: Have link to ingestion pipeline later on */}
[ERA5](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5) global weather data was ingested using Fused ingestion pipeline.

```python
import fused
```

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs className="unique-tabs">
  <TabItem value="DuckDB">
    ```python
    @fused.udf
    def udf(
        month: str = "2024-01",
    ):
        import duckdb
        result = duckdb.sql(f"""
          SELECT 
              datestr::VARCHAR as datestr,
              ROUND(AVG(daily_mean), 2) as daily_mean_temp
          FROM 's3://fused-asset/data/era5/t2m/datestr={month}-*/*.parquet'
          GROUP BY datestr
          ORDER BY datestr
        """).df()

        output_fp = fused.file_path(f"monthly_climate/{month}.pq")
        result.to_parquet(output_fp)
        
        return result
    ```
  </TabItem>
  <TabItem value="Pandas">

    NOTE: `pandas` approach is a bit slower than DuckDB.

    ```python
    @fused.udf
    def udf(month: str = "2024-01",):
        import pandas as pd

        files = fused.api.list(f"s3://fused-asset/data/era5/t2m/datestr={month}-")

        dfs = [pd.read_parquet(file, columns=['daily_mean']).assign(datestr=file.split('datestr=')[1].split('/')[0]) for file in files]
        result = pd.concat(dfs).groupby('datestr')['daily_mean'].mean().round(2).reset_index()

        output_fp = fused.file_path(f"monthly_climate/{month}.pq")
        result.to_parquet(output_fp)
        
        return result
    ```

  </TabItem>
</Tabs>

```python
fused.run(udf)
```

```shell
>>>   datestr     daily_mean_temp
3  2024-01-04            277.36
4  2024-01-05            277.26
5  2024-01-06            277.17
```

### 20 years of data (1TB in < 1min!)

Explore the available data for yourself in [File Explorer](https://www.fused.io/workbench/files?path=s3%3A%2F%2Ffused-asset%2Fdata%2Fera5%2Ft2m%2F)

We'll process 20 years of data:

```python
data_until = 2005

available_days = fused.api.list('s3://fused-asset/data/era5/t2m/')
recent_months = list(set([
   path.split('datestr=')[1][:7] for path in available_days 
   if int(path.split('datestr=')[1][:4]) >= data_until
]))
```

This corresponds to ~1TB of data!

<details>
<summary>Size of data quick calculation</summary>

Each file being about 140MB a quick back of the envelope calculation gives us:
```python
recent_days = [day for day in available_days if day.split('datestr=')[1][:7] in recent_months]
len(recent_days) * 140 / 1000 # size in GB of files we'll process
```

```bash
1005.62
```
</details>

Fused allows us to run a [UDF in parallel](/run-udfs/run-small-udfs/#running-multiple-jobs-in-parallel). So we'll process 1 month of data across hundreds of jobs:

```python
results = fused.submit(
  udf, 
  recent_months, 
  max_workers=250, 
  collect=False
)
```

See a progress bar of jobs running:

```python
results.wait()
```

See how long all the jobs took:

```python
results.total_time()
```

```bash
>>> datetime.timedelta(seconds=40, ...)
```

**We just processed 20 years of worldwide global data, over 1TB in 40s!!**

All we need to do now is aggregate the data by month:

```python
@fused.udf(cache_max_age='0s')
def udf():
    import duckdb
    
    monthlys = fused.api.list(fused.file_path(f"monthly_climate/"))
    file_list = "', '".join(monthlys)
    
    result = duckdb.sql(f"""
       SELECT 
           LEFT(datestr, 7) as month,
           ROUND(AVG(daily_mean_temp), 2) as monthly_mean_temp
       FROM read_parquet(['{file_list}'])
       GROUP BY month
       ORDER BY month
    """).df()
    print(result)
    return result
```

Instead of running this locally, we'll open it in [Workbench](/workbench/), Fused's web-based IDE:

```python
# Save to Fused
udf.to_fused("monthly_mean_temp")

# Load again to get the Workbench URL
loaded_udf = fused.load("monthly_mean_temp")
loaded_udf.catalog_url()
```

Click on the link to open the UDF in Workbench. Click "+ Add to UDF Builder" 

{/* TODO: Add a screenshot of Table view in Wb */}

### Interactive canvas

{/* TODO */}

### Next Steps

{/* TODO */}
