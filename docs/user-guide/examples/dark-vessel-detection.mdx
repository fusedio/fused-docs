---
sidebar_label: "Dark Vessel Detection"
title: "Dark Vessel Detection"
tags: ['example', 'sentinel 1', 'raster', 'vector', 'ingestion']
sidebar_custom_props:
    name: "Dark Vessel Detection"
    image: 'https://fused-magic.s3.us-west-2.amazonaws.com/thumbnails/udfs-staging/Fused_Logo.png'
    url: "/user-guide/examples/dark-vessel-detection"
    urlTS: "/user-guide/examples/dark-vessel-detection"
    description: 'Using Sentinel 1 radar images + AIS ship data to detect potential illegal ship activity'
---

_A complete example show casing how to use Fused to ingest data into a geo-partitioned, cloud friendly format, process images & vectors and use UDFs to produce an analysis_

:::note
    This tutorial assumes you have [access to Fused](/user-guide/login/) & [installed the `fused` Python package](/python-sdk/#install) locally
:::

## 1. The problem: Detecting illegal boats

{/* Start by showing what the end result looks like: "this is what we're going to do in this example"*/}

Monitoring what happens at sea isn't the easiest task. Shores are outfitted with radars and each ship has a transponder to publicly broadcast their location (using [Automatic Identification System, AIS](https://en.wikipedia.org/wiki/Automatic_identification_system)), but ships sometimes want to hide their location when taking part in illegal activities. 

Global Fishing Watch [has reported on "dark vessels"](https://globalfishingwatch.org/research-project-dark-vessels/) comparing Sentinel 1 radar images to public AIS data and matching the two to compare where boats report being versus where they _actually_ are. 

In this example, we're going to showcase a basic implementation of a similar analysis to identify _potential_ dark vessels, all in Fused.

import ImageWorkflow from '@site/docs/user-guide/examples/dark_vessel_methodology.png';

<div style={{textAlign: 'center'}}>
<img src={ImageWorkflow} alt="Dark Vessel Detection workflow" style={{width: 1000}} />
</div>

Here's the result of our analysis, running in real time in Fused:

import ReactPlayer from 'react-player'

{/* <ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/output_analysis_S1_ais.mp4" width="100%" /> */}
<ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/analysis_walkthrough_dark_vessel_detection.mp4" width="100%" />

{/* Need more context around how the analysis is done */}

Here are the steps we'll produce:
- Getting Sentinel 1 radar images over our Area of Interest and within our Time of Interest
- Run a simple algorithm to detect bright spots in radar images -> Return boats outline
- Fetch AID data over the same Area & Time of Interest
- Merge the 2 datasets together
- Keep boats that appear in Sentinel 1 but not in AIS -> Those are our potential Dark Vessels.

import ImageDataPipeline from '@site/docs/user-guide/examples/dark_vessel_data_breakdown.png';

<div style={{textAlign: 'center'}}>
<img src={ImageDataPipeline} alt="Dark Vessel Detection data pipeline" style={{width: 1000}} />
</div>


:::note
    This is crude analysis, mostly meant to demonstrate some of the major components of Fused, not to expose any dark vessels. The analysis has some major limitations and itself should be taken with a grain of salt. 
    
    That being said, we encourage you to use it as a starting point for your own work!
:::



## 2. Data for our analysis

We need 2 main datasets:
- [Sentinel 1 radar images](https://sentinel.esa.int/web/sentinel/copernicus/sentinel-1)
- [AIS data](https://en.wikipedia.org/wiki/Automatic_identification_system)

Both of which are free & open data sources. Since we want our analysis to take only a few seconds at most to run our data needs to be **fast to read** & **scalable to access**.

{/* Need a link towards cloud native so people can read more about this somewhere else */}
This is why we want both of our datasets to in a Cloud Native format. At the highest level and in practice this means that our data is:
- On a cloud storage (i.e. on S3 buckets)
- In a format that's fast to read and allows loading only small areas at a time ([Cloud Optimized GeoTiff](https://cogeo.org/) or [GeoParquet](https://geoparquet.org/))

{/* Need a blogpost / docs page about "why cloud native" or something along those lines */}

After looking around at different options we're going to use:
- Sentinel 1 data from the [Microsoft Planetary Computer Data Catalog](https://planetarycomputer.microsoft.com/dataset/sentinel-1-grd)

    For the nerds out there, we're using the Ground Range Detected product, not the [Radiometrically Terrain Corrected](https://planetarycomputer.microsoft.com/dataset/sentinel-1-rtc) because we're looking at boats in the middle of the ocean, so terrain shouldn't be any issue.
    - This dataset is available as Cloud Optimized GeoTiff through a [STAC Catalog](https://stacspec.org/en/), meaning we can directly use this data as is.

- AIS data from the [NOAA Digital Coast](https://www.coast.noaa.gov/digitalcoast/tools/ais.html). We have data all around the continental US [per day as individual `.zip` files](https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/index.html)
    - This dataset is not in a cloud native format. If we were to use this directly, every time we were to make a change to our analysis or look at a new area we'd have to find the correct `.zip` file, decompress it, read the entire AIS data for any given day then query only around our area of interest. This is possible, but brings out iteration speed from seconds to minutes.

import ImgAISNoaa from '@site/docs/user-guide/examples/AIS_noaa_coast_portal.png';

<div style={{textAlign: 'center'}}>
<img src={ImgAISNoaa} alt="Dark Vessel Detection data pipeline" style={{width: 600}} />
</div>

## 3. Ingesting AIS data

{/* 
Topics:
- Sentinel 1 already in COG + STAC -> No need to do ingest it ourselves
- AIS is in zip files on a server somewhere -> we're going to re-ingest it ourselves to make it cloud native (i.e. on cloud on S3 bucket) + geo-partitioned (i.e. in a format that's fast to read - parquet + chunked accordingly) 
 */}

Since our AIS dataset is not in a geo-partitioned, cloud native format, our first step is to ingest it into a tiled format and put it on a cloud bucket. Thankfully we can do all of this in Fused.

By the end of this ingestion process we'll have:
- AIS data for 2024 on a cloud storage (AWS S3 bucket)
- In a GeoParquet format so it's fast to read
- Tiled & Chunked so we can read only a small portion at a time, making it even faster

To get this done we'll:
1. Get a location to store our partitioned AIS data on (this can be any AWS S3 bucket, we'll use one managed by Fused to make this simpler)
{/* Link to UDF docs */}
2. Write a simple UDF to unzip each AIS data over a given date range, read it and save it as a parquet file on S3
{/* Link to batch job doc */}
3. Run this UDF as a batch job to run this across all of the 2024 AIS archive
4. Ingest all of the monthly AIS `.parquet` files into geo-partitioned files to speed up their read time over small areas

### 3.1 - Deciding on where to store out partitioned data

We first need to unzip & read our AIS data before ingesting in as a geo-partitioned cloud native format. One simple way to do this is to write a UDF that reads a zip file and saves it as `.parquet` file on a mounted disk.

:::note
    Fused UDFs by default run on serverless instances, so their local storage changes at every run. To keep data persistent across runs we use shared mounted storage across all the instances of your team
:::

`fused.file_path()` returns the mount path of any file we'd like to create

```python showLineNumbers
import fused

datestr='2024_09_01'

url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'
path=fused.file_path(f'/AIS/{datestr[:7]}/{datestr[-2:]}')
```

{/* 
NOTE: This might be confusing: `fused.file_path()` returns different values when run locally or when run inside a UDF
TODO: Decide whether or not it's worth it to go on a tangent explaining this to people?
*/}
{/* We would then get:

```bash
'/tmp/fused/AIS/2023_03/29'
``` */}

AIS datapoint from NOAA's platform are available per day but we'll aggregate them per month to make it simpler to manage:

```
- /tmp/fused/AIS/2024_08/
    - 01.parquet
    - 02.parquet
    ...
- /tmp/fused/AIS/2024_09/
    - 01.parquet
    - 02.parquet
...
```

### 3.2 - Writing a UDF to open each AIS dataset

The rest of the logic is to open each file, read it as a CSV and write it to parquet.

Read more about how to [write UDFs here](/core-concepts/write/)

```python showLineNumbers
@fused.udf()
def read_ais_from_noaa_udf(datestr='2023_03_29', overwrite=False):
    import os
    import requests
    import io
    import zipfile
    import pandas as pd
    import s3fs
    # This is the specific URL where daily AIS data is available
    url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'
    # This is our local mount file path, 
    path=fused.file_path(f'/AIS/{datestr[:7]}/')
    daily_ais_parquet = f'{path}/{datestr[-2:]}.parquet'

    # Skipping any existing files
    if os.path.exists(daily_ais_parquet) and not overwrite:
        print('exist')
        return pd.DataFrame({'status':['exist']})

    # Download ZIP file to mounted disk
    r=requests.get(url)
    if r.status_code == 200:
        with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
            with z.open(f'AIS_{datestr}.csv') as f:
                df = pd.read_csv(f)
                # MMSI is the unique identifier of each boat. This is a simple clean up for demonstration
                df['MMSI'] = df['MMSI'].astype(str)
                df.to_parquet(daily_ais_parquet)
                print(f"Saved {daily_ais_parquet}")
        return pd.DataFrame({'status':['Done'], "file_path": [daily_ais_parquet]})
    else:
        return pd.DataFrame({'status':[f'read_error_{r.status_code}']})
```

{/* Running this UDF 1 time to try it out */}

We can run this UDF a single time to make sure it works:

```python showLineNumbers
single_ais_month = fused.run(read_ais_from_noaa_udf, datestr="2024_09_01")
```

Which returns:
```bash
Saved /mnt/cache/AIS/2024_09/01.parquet
```

To recap what we've done so far:
- Build a UDF that takes a date, fetches a `.zip` file from NOAA's AISDataHandler portal and saves it to our UDFs' mount (so other UDFs can access it)
- Run this UDF 1 time for a specific date

### 3.3 - Run this UDF over a year of AIS data

Next step: Run this for a whole period!

:::note
    You can read more on how to run UDFs multiple time in the dedicated section
:::

Since each UDF takes a few seconds to run per date, we're going to use the experimental `PoolRunner` to call a large pool of UDFs all at once with a large date range

With a bit of Python gymnastics we can create a list of all the dates we'd like to process. Preparing to get all of September 2023 would look like this:

```python showLineNumbers
import pandas as pd
range_of_ais_dates = [str(i)[:10].replace('-','_') for i in pd.date_range('2024-09','2024-10')[:-1]]
```

Giving us a list of date ranges:

```bash
['2024_09_01', '2024_09_02', '2024_09_03', '2024_09_04', '2024_09_05', '2024_09_06', '2024_09_07', '2024_09_08', '2024_09_09', '2024_09_10', '2024_09_11', '2024_09_12', '2024_09_13', '2024_09_14', '2024_09_15', '2024_09_16', '2024_09_17', '2024_09_18', '2024_09_19', '2024_09_20', '2024_09_21', '2024_09_22', '2024_09_23', '2024_09_24', '2024_09_25', '2024_09_26', '2024_09_27', '2024_09_28', '2024_09_29', '2024_09_30']
```

{/* TODO: Again link to PoolRunner docs */}
We can create a simple `lamda` function that takes each value and sends it to a `PoolRunner` job:

```python showLineNumbers
runs = fused.utils.common.PoolRunner(lambda datestr: fused.run(read_ais_from_noaa_udf(datestr=datestr, overwrite=False)), range_of_ais_dates)
```

Since `PoolRunner` is running on a "real-time" instance, we can also query the status in a notebook as the job is executing:

```python showLineNumbers
runs.get_result_all()
```

import ImgGetResultsAll from '@site/docs/user-guide/examples/run_get_results_all_notebook_output.png';

<div style={{textAlign: 'center'}}>
<img src={ImgGetResultsAll} alt="Pool Runner live results" style={{width: 400}} />
</div>

We've now unzipped, opened & saved 30 days of data!

One handy way to make sure our data is in the right place is to check it in [File Explorer](/workbench/file-explorer/) in [Workbench](/workbench/). In the search bar type: `efs://AIS/2024_09/`:

import ImgEFSWorkbench from '@site/docs/user-guide/examples/fused_workbench_file_explorer_efs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgEFSWorkbench} alt="Pool Runner live results" style={{width: 600}} />
</div>

You'll see all our daily files! Notice how each file is a few 100 Mb. These files are still big individual files, i.e. would take a little while to read.

### 3.4 - Ingest 1 month of AIS data into a geo-partitioned format

{/* TODO: This section (and the docs in general) are missing info about number of chunks*/}

These individual parquet files are now store on our mount disk. We could save them directly onto cloud storage but before that we can geo-partition them to make them even faster to read

:::tip
    You can read more about why Cloud Native formats are so interesting on [the Cloud Native Geo website](https://guide.cloudnativegeo.org/)
:::

Fused provides a simple way to do this with the [ingestion process](/core-concepts/content-management/Ingest/). Our ingestion pipeline will then slice the dataset in order to make 

import ImgGeoParquet from '@site/docs/user-guide/examples/geoparquet_overview.png';

<div style={{textAlign: 'center'}}>
<img src={ImgGeoParquet} alt="A simple overview of Geoparquet benefits" style={{width: 800}} />
</div>

_Image credit from the [Cloud Native Geo slideshow](https://guide.cloudnativegeo.org/overview.html#/geoparquet)_

To do this we need a few things:
- **Our input dataset**: in this case our month of AIS data.
    - We need to refer to the files directly so we'll use the 
- **A target cloud bucket**: We're going to create a bucket to store our month of geo-partitioned AIS data in parquet files
- A target number of chunks to partition our data in. For now we're going to keep it at 500
- Latitude / Longitude columns to determine the location of each point

```python showLineNumbers
ais_daily_parquets = [f'file:///mnt/cache/AIS/{day[:-3]}/{day[-2:]}.parquet' for day in range_of_ais_dates]
```

```python showLineNumbers
job = fused.ingest(
    ais_daily_parquets,
    's3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09',
    target_num_chunks=500,
    lonlat_cols=('LON','LAT')
)
```

{/* TODO: Link to offline run remote */}
To run this job we're going to use the "offline" `job.run_remote()` as we latency doesn't matter much (we can wait a few extra seconds) and we'd rather have a larger machine & a larger storage:

```python showLineNumbers
j.run_remote(
    instance_type='r5.8xlarge', # We want why big beafy machine to do the partitioning in parallel, so large amounts of CPUs
    disk_size_gb=999 # need a large amount of disk because this job will open each output parquet file to calculate centroid 
)
```

Running this in a notebook gives us a link to logs so we can follow the progress of the job on the offline machine:

import ImgRunRemoteLogs from '@site/docs/user-guide/examples/ingest_run_remote_job_logs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgRunRemoteLogs} alt="Notebook run remote print" style={{width: 800}} />
</div>

Following the link shows us the live logs of what our job is doing:

import ImgWorkbenchRunRemoteLogs from '@site/docs/user-guide/examples/workbench_run_remote_logs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgWorkbenchRunRemoteLogs} alt="Workbench run remote logs" style={{width: 800}} />
</div>

We can once again check that our geo-partitioned images are available using [File Explorer](/workbench/file-explorer/). This time because our files are a lot faster to read we can even see the preview in the map view:

<ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/geopartioned_AIS_file_explorer.mp4" width="100%" />


Our AIS data is ready to be use for the entire month of September 2024. To narrow down our search, we now need to get a Sentinel 1 image. Since these images are taken every 6 to 12 days depending on the region, we'll find a Sentinel 1 image and then narrow our AIS data to just a few minutes before and after the acquisition time of the Sentinel 1 image

{/* 
TODO:
- Show in Workbench / File Explorer that we can see all the data for the month
*/}
## 4. Retrieving Sentinel 1 images

_ðŸš§ Under construction_

## 5. Simple boat detection in Sentinel 1 radar images

_ðŸš§ Under construction_

## 6. Retrieving AIS data for our time of Interest

_ðŸš§ Under construction_

## 7. Merging the 2 datasets together

_ðŸš§ Under construction_

## Limitations & Next steps

{/* 
Talk about how this analysis is quite simple, but a good starting point 
Next steps:
- 
*/}

_ðŸš§ Under construction_