# Fused Documentation - Complete Reference

> Fused is an end-to-end cloud platform for data analytics, built around User Defined Functions (UDFs): Python functions that can be run via HTTPS requests from anywhere, without any install required.

This comprehensive reference contains the complete text of all Fused documentation, including all API methods, examples, tutorials, and guides.

================================================================================

# GUIDE

## Dependencies
Path: guide/advanced-setup/dependencies.mdx
URL: https://docs.fused.io/guide/advanced-setup/dependencies

# Dependencies

To keep things simple, Fused maintains a single runtime image. This means that any UDF you run will be executed with these dependencies by default

## UDF Dependencies

The Python packages are listed below and can also be found in this public `.txt` file.

Get in touch to have a package added to the list of dependencies or to learn about private runtime images for your organization.

## [BETA] Install your own dependencies

The simplest way to add your own library is to run the dedicated Package Management Fused app. This app allows you to create a different environment and add any module you'd like

You need to make sure you have access to Fused Apps to be able to run this.

[Image: Beta package management app]

You'll then need to import the environment path in your UDF:

```python showLineNumbers
@fused.udf
def udf():

  sys.path.append(f"/mount/envs/demo_env/lib/python3.11/site-packages/")

  # Logic using your dedicated package
  return
```

================================================================================

## Secrets management
Path: guide/advanced-setup/environment-variables.mdx
URL: https://docs.fused.io/guide/advanced-setup/secrets-management

Save constants to an `.env` file to make them available to UDFs as environment variables. You should use the secrets manager for sensitive information like API keys.

First, run a File UDF that sets variables in an `.env` file in the `/mnt/cache/` directory.

```py
@fused.udf
def udf():
    env_vars = """
    MY_ENV_VAR=123
    """

    # Path to .env file in disk file system
    env_file_path = '/mnt/cache/.env'

    # Write the environment variables to the .env file
    with open(env_file_path, 'w') as file:
        file.write(env_vars)
```

Now, any UDF can load the values from `.env` as environment variables with the `load_dotenv` and access them with os.getenv.

```py
@fused.udf
def udf():
    from dotenv import load_dotenv

    # Load environment variable
    env_file_path = '/mnt/cache/.env'
    load_dotenv(env_file_path, override=True)

    # Access environment variable
    print(f"Updated MY_ENV_VAR: ")
```

================================================================================

## File System
Path: guide/advanced-setup/file-system.mdx
URL: https://docs.fused.io/guide/advanced-setup/file-system

Fused provides two file systems to make files accessible to all UDFs: an S3 bucket and a disk. Access is scoped at the organization level.

## `fd://` S3 bucket

Fused provisions a private S3 bucket namespace for your organization. It's ideal for large-scale, cloud-native, or globally accessible datasets, such as ingested tables, GeoTIFFs, and files that need to be read outside of Fused.

Use the File explorer to browse the bucket and see its full path.

[Image: file explorer]

Fused utility functions may reference it with the `fd://` alias.

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="fd://census/ca_bg_2022/",
).run_batch()
```

## `/mnt/cache` disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and `fused.download` write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system. For example, to list files in the directory:

```python showLineNumbers
@fused.udf
def udf():

    for each in os.listdir('/mnt/cache/'):
        print(each)
```

### Troubleshooting

If you encounter the following error, it means `/mnt/cache` is not yet configured for your environment. To resolve this issue, please contact the Fused team to enable it.

```text
Error: No such file or directory: '/mnt/cache/'
```

================================================================================

## Git Integration
Path: guide/advanced-setup/git-integration.mdx
URL: https://docs.fused.io/guide/advanced-setup/git-integration

Github integration allows you to:
- Commit your code to Fused public & community repositories for anyone to use!
- Saving & Versioning of UDFs
- Collaboration across teams (for Professional users)

## Connecting your own repositories

<Tag color="#3399ff">Professional and Enterprise</Tag> _Integrating private repositories is accessible to organizations with a Fused Professional & Enterprise subscription (see Plans)._

Fused Github integration plugs directly into your own repositories.

### Configuring Github integration

1. Create a new Github repository. There's no enforced repo structure because Fused scans the entire repo for UDFs, although the Public UDFs repo may serve as guideline.

2. Install the Fused GitHub app for your organization. Navigate to this GitHub URL, click "Configure", and select the GitHub organization that contains the repository.

[Image: install fused github]

3. Scope the app to the target repository. It's recommended to select only the specific repo.

[Image: install fused github 2]

4. Once the above is complete, please reach out to the Fused team to finish setup (`info@fused.io`):
- Provide Fused team with your Github path & name: `some-organization/some-repo`. (Example: `fusedio/udfs`)

5. Confirm the integration is enabled by checking that repo UDFs appear under the "Team UDFs" tab in the UDF Explorer.

You can see all the repositories you have access to in the Versions Tab:

[Image: Github Versions Tab]

## Next Steps

- Version Control — how to version control your UDFs

================================================================================

## Python installation
Path: guide/advanced-setup/local-installation.mdx
URL: https://docs.fused.io/guide/advanced-setup/local-installation

# Python installation

Install and set up the Fused Python SDK on your local machine.

This section is only for local environments outside of Fused. If you're working in Workbench, `fused` is already installed for you.

> The latest version of `fused` is <FusedVersionLive />.

Installing `fused` is required if you're running Fused locally or in a development environment. If you're working in Workbench, `fused` is already installed for you.

## Install

**Python Version:** 3.10+

```bash
pip install "fused[all]"
```

1. Set up a Python environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install the `fused` package:

```bash
# Base package only
pip install fused

# For raster data processing
pip install "fused[raster]"

# For vector data processing
pip install "fused[vector]"

# All optional dependencies (recommended)
pip install "fused[all]"
```

</Tabs>

## Log out

Log out the current user. This deletes the credentials saved to disk.

```python

fused.api.logout()
```

## Get Bearer token

Get your account's Bearer (Access) token for authenticating API requests.

```python

fused.api.access_token()
```

Do not share your Bearer token. It allows impersonation of your account.

## Quick test

Verify your installation works:

```python

@fused.udf
def udf():
    return "Hello from Fused!"

fused.run(udf)
```

================================================================================

## On-Prem Setup
Path: guide/advanced-setup/on-prem-setup.mdx
URL: https://docs.fused.io/guide/advanced-setup/on-prem-setup

Fused offers an on-prem version of the application in a Docker container. The container runs in your computing environment (such as AWS, GCP, or Azure) and your data stays under your control.

The container image is currently distributed via a private release. Email `info@fused.io` for access.

## Fused On-Prem Docker Installation Guide

[Image: On prem]

_Diagram of the System Architecture_

### 1. Install Docker

Follow these steps to install Docker on a bare-metal environment:

Step 1: Update System Packages

Ensure your system is up-to-date:
```bash
sudo apt update && sudo apt upgrade -y
```

Step 2: Start & Enable Docker
```bash
sudo apt install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable docker
sudo systemctl start docker
```

Step 3: Add Docker Permission to local user (after this command is run, the shell session must be restarted)
```bash
sudo usermod -G docker $(whoami)
```

Step 4: Configure Artifact Registry
```bash
gcloud auth configure-docker us-west1-docker.pkg.dev
```

### 2. Install Dependencies and Create Virtual Environment

Step 1: Install pip
```bash
sudo apt install python3-pip python3.11-venv
```

Step 2: Create virtual environment
```bash
python3 -m venv venv
```

Step 3: Activate virtual environment
```bash
source venv/bin/activate
```

Step 4: Install Fused and dependencies
```bash
pip install pandas ipython https://fused-magic.s3.us-west-2.amazonaws.com/fused-1.14.1.dev2%2B2c8d59a-py3-none-any.whl
```

### 3. Configure Fused on the Docker container

Run the following in a Python environment within the container to configure the on-prem profile. The Fused team will provide values specific to your account via secure communication.

```python showLineNumbers
# Run this locally - not in Workbench

fused.options.base_url = "***"
fused.options.auth.client_id = "***"
fused.options.auth.client_secret = "***"
fused.options.auth.audience = "***"
fused.options.auth.oauth_token_url = "***"
fused.options.auth.authorize_url = "***"

fused.options.save()
```

The code above only needs to be run once. After this is complete, Fused will use the local configuration for future batch jobs.

If Fused has already been configured for batch jobs, you may need to remove the local `~/.fused` directory before running the above code.

### 4. Authenticate an individual Fused user account

Step 1: Start a Python shell
```bash
python
```
Step 2: Obtain credentials URL

```python showLineNumbers
# Run this locally - not in Workbench

credentials = fused.api.NotebookCredentials()
credentials.url
```

Step 3:
Go to the credentials URL from the prior step in a web browser. Copy the code that is generated and paste into Python.
```python showLineNumbers
# Run this locally - not in Workbench
credentials.finalize(code="xxxxxxxxxxxxxxx")
```

### 5. Create Google Cloud service account key and add to Fused

Step 1:
In Google Cloud Console, go to `IAM & Admin > Service Accounts`. Select the service account you want to use, click on the three dots on the right, and select `Manage Keys`. Choose JSON and download the key.

Step 2:
Login to the Fused workbench environment settings. Click `Add new secret`. For name use `gcs_fused` and for value paste the contents of the JSON key file.

### 6. Run Fused API: Test UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

```python showLineNumbers
# Run this locally in notebook - not in Workbench
@fused.udf
def udf(datestr=0):

  loguru.logger.info(f'hello world ')
```

Step 2: Rename this UDF to "hello_world_udf" & Save

[Image: Hello World UDF]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF from Python

```python showLineNumbers
# Run this locally - not in Workbench

fused.api.FusedAPI()

my_udf = fused.load("hello_world_udf") # Make sure this is the same name as the UDF you saved
job = my_udf(arg_list=[1, 2])
fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1"
  ]
)

job_status = job.run_batch()
job_status.run_and_tail_output()
```

Optionally, to mount a filestore volume to the node that runs the job, add the following to the `additional_docker_args`. This assumes that filestore is mounted at `/mnt/cache` on the host machine.
```python showLineNumbers
# Run this locally - not in Workbench
additional_docker_args=["-v", "/mnt/cache:/mnt/cache"]
```

### 7. Run Fused API: Example with ETL Ingest UDF

Now that we've tested a simple UDF we can move to a more useful UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

You'll need a GCS Bucket to save this to, pass it to `bucket_name` in the UDF definition for now

```python  showLineNumbers
@fused.udf
def udf(datestr: str='2001-01-03', res:int=15, var='t2m', row_group_size:int=20_000, bucket_name:str):

  path_in=f'https://storage.googleapis.com/gcp-public-data-arco-era5/raw/date-variable-single_level//2m_temperature/surface.nc'
  path_out=f"gs:///data/era5/t2m/datestr=/0.parquet"

  if len(fused.api.list(path_out))>0:
    df = pd.DataFrame([])
    print("Already exists")
    return None

  def get_data(path_in, path_out):
    path = fused.download(path_in, path_in)
    xds = xarray.open_dataset(path)
    df = xds[var].to_dataframe().unstack(0)
    df.columns = df.columns.droplevel(0)
    df['hex'] = df.index.map(lambda x:h3.api.basic_int.latlng_to_cell(x[0],x[1],res))
    df = df.set_index('hex').sort_index()
    df.columns=[f'hour' for hr in range(24)]
    df['daily_min'] = df.iloc[:,:24].values.min(axis=1)
    df['daily_max'] = df.iloc[:,:24].values.max(axis=1)
    df['daily_mean'] = df.iloc[:,:24].values.mean(axis=1)
    return df

  df = get_data(path_in, path_out)

  memory_buffer = io.BytesIO()
  table = pa.Table.from_pandas(df)
  pq.write_table(table, memory_buffer, row_group_size=row_group_size, compression='zstd', write_statistics=True)
  memory_buffer.seek(0)

  gcs = gcsfs.GCSFileSystem(token=json.loads(fused.secrets['gcs_fused']))
  with gcs.open(path_out, "wb") as f:
    f.write(memory_buffer.getvalue())

  print(df.shape)
  return None
```

Step 2: Rename this UDF to "ETL_Ingest"

[Image: Ingest ETL in workbench]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF

```python showLineNumbers
# Run this locally - not in Workbench

fused.api.FusedAPI()

udf = fused.load("ETL_ingest")
start_datestr='2020-02-01'; end_datestr='2020-03-01';
arg_list = pd.date_range(start=start_datestr, end=end_datestr).strftime('%Y-%m-%d').tolist()
job = udf(arg_list=arg_list)

fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1", "-v", "./.fused:/root/.fused"
  ]
)

job_status = job.run_batch()
job_status.run_and_tail_output()
```

## Commands

### `run-config`

`run-config` runs the user's jobs. The job configuration can be specified either on the command line, as a local file path, or as an S3/GCS path. In all cases the job configuration is loaded as JSON.

```
Options:
  --config-from-gcs FILE_NAME   Job step configuration, as a GCS path
  --config-from-s3 FILE_NAME    Job step configuration, as a S3 path
  --config-from-file FILE_NAME  Job step configuration, as a file name the
                                application can load (i.e. mounted within the
                                container)
  -c, --config JSON             Job configuration to run, as JSON
  --help                        Show this message and exit.
```

### `version`

Prints the container version and exits.

## Environment Variables

The on-prem container can be configured with the followin environment variables.

- `FUSED_AUTH_TOKEN`: Fused token for the licensed user or team. When using the FusedDockerAPI, this token is automatically retrieved.
- `FUSED_DATA_DIRECTORY`: The path to an existing directory to be used for storing temporary files. This can be the location a larger volume is mounted inside the container. Defaults to Python's temporary directory.
- `FUSED_GCP`: If "true", enable GCP specific features. Defaults to false.
- `FUSED_AWS`: If "true", enable AWS specific features. Defaults to false.
- `FUSED_AWS_REGION`: The current AWS region.
- `FUSED_LOG_MIN_LEVEL`: Only logs with this level of severity or higher will be emitted. Defaults to "DEBUG".
- `FUSED_LOG_SERIALIZE`: If "true", logs will be written in serialized, JSON form. Defaults to false.
- `FUSED_LOG_AWS_LOG_GROUP_NAME`: The CloudWatch Log Group to emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_LOG_AWS_LOG_STREAM_NAME`: The CloudWatch Log Stream to create and emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_PROCESS_CONCURRENCY`: The level of process concurrency to use. Defaults to the number of CPU cores.
- `FUSED_CREDENTIAL_PROVIDER`: Where to obtain AWS credentials from. One of "default" (default to ec2 on AWS, or none otherwise), "none", "ec2" (use the EC2 instance metadata), or "earthdata" (use EarthData credentials in `FUSED_EARTHDATALOGIN_USERNAME` and `FUSED_EARTHDATALOGIN_PASSWORD`).
- `FUSED_EARTHDATALOGIN_USERNAME`: Username when using earthdata credential provider, above.
- `FUSED_EARTHDATALOGIN_PASSWORD`: Password when using earthdata credential provider, above.
- `FUSED_IGNORE_ERRORS`: If "true", continue processing even if some computations throw errors. Defaults to false.
- `FUSED_DISK_SPACE_GB`: Maximum disk space available to the job, e.g. for temporary files on disk, in gigabytes.

## Connecting an encrypted S3 bucket

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket. The following steps are required to connect an encrypted S3 bucket:

- Configure KMS policy
```json
,
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

- Configure S3 bucket policy
```json
,
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3        "arn:aws:s3      ]
    }
  ]
}
```

================================================================================

## Data Formats
Path: guide/data-input-outputs/data-formats-snippets.mdx
URL: https://docs.fused.io/guide/data-input-outputs/data-formats-snippets

# Data Formats Quick Reference

Quick code snippets for reading and writing common data formats in Fused.

---

## Tables

### CSV

```python

# Read
df = pd.read_csv("s3://bucket/data.csv")

# Write
df.to_csv("fd://my-data/output.csv", index=False)
```

### Parquet

```python

# Read
df = pd.read_parquet("s3://bucket/data.parquet")

# Write
df.to_parquet("fd://my-data/output.parquet")
```

### Excel

```python

# Read
df = pd.read_excel("s3://bucket/data.xlsx")
```

### JSON

```python

# Read
df = pd.read_json("s3://bucket/data.json")

# Write
df.to_json("fd://my-data/output.json")
```

---

## Geospatial Tables

### GeoParquet

```python

# Read
gdf = gpd.read_parquet("s3://bucket/data.parquet")

# Write
gdf.to_parquet("fd://my-data/output.parquet")
```

### GeoJSON

```python

# Read
gdf = gpd.read_file("https://example.com/data.geojson")

# Write
gdf.to_file("fd://my-data/output.geojson", driver="GeoJSON")
```

### Shapefile (zipped)

```python

# Read from URL
gdf = gpd.read_file("https://example.com/data.zip")

# Read from S3
gdf = gpd.read_file("s3://bucket/data.zip")
```

---

## Raster Images

### GeoTIFF / Cloud Optimized GeoTIFF (COG)

```python
# Using Fused common utils
common = fused.load("https://github.com/fusedio/udfs/tree/main/public/common/")
arr = common.read_tiff(tile, "s3://bucket/image.tif")
```

```python
# Using rasterio directly

with rasterio.open("s3://bucket/image.tif") as src:
    arr = src.read()
```

### PNG/JPEG

```python
from PIL import Image

from io import BytesIO

response = requests.get("https://example.com/image.png")
img = Image.open(BytesIO(response.content))
```

---

## Cloud Storage Paths

| Provider | Format | Example |
|----------|--------|---------|
| Fused managed | `fd://` | `fd://my-data/file.parquet` |
| AWS S3 | `s3://` | `s3://bucket-name/path/file.parquet` |
| Google Cloud | `gs://` or `gcs://` | `gs://bucket-name/path/file.parquet` |
| Azure | `az://` | `az://container/path/file.parquet` |
| HTTP(S) | `https://` | `https://example.com/file.csv` |

---

## STAC Catalogs

```python

# Connect to STAC catalog
catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

# Search for items
items = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=[-122.5, 37.5, -122.0, 38.0],
    datetime="2023-01-01/2023-12-31"
).item_collection()

# Load with odc.stac
ds = odc.stac.load(items, bands=["red", "green", "blue"])
```

---

## DuckDB Queries

```python
common = fused.load("https://github.com/fusedio/udfs/tree/main/public/common/")
con = common.duckdb_connect()

# Query S3 directly
df = con.sql("""
    SELECT * FROM read_parquet('s3://bucket/data.parquet')
    WHERE value > 100
    LIMIT 1000
""").df()
```

---

## Recommended Formats

| Data Type | Recommended Format | Why |
|-----------|-------------------|-----|
| Tables | **Parquet** | Columnar, compressed, fast |
| Geospatial tables | **GeoParquet** | Spatial indexing, cloud-native |
| Raster images | **Cloud Optimized GeoTIFF** | Tiled, overviews, partial reads |
| Large datasets | **Partitioned GeoParquet** | Use `fused.ingest()` |

For large datasets (&gt;1GB), use `fused.ingest()` to create optimized, partitioned files.

================================================================================

## Download
Path: guide/data-input-outputs/export-api/download.mdx
URL: https://docs.fused.io/guide/data-input-outputs/export-api/download

# Download

Download remote files to the local system to make them available to UDFs across runs. Files are written to a disk shared across all UDFs in an organization.

## `fused.download`

Download any file to: `/mount/tmp/` which any other UDF can then access.

```python showLineNumbers
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    print(out_path)
```

The `download` function sets a lock to ensure the download happens only once, in case the UDF is called concurrently.

================================================================================

## Geospatial Export
Path: guide/data-input-outputs/export-api/geospatial.mdx
URL: https://docs.fused.io/guide/data-input-outputs/export-api/geospatial-export

# Geospatial Export

Integrate Fused UDFs with popular mapping and visualization tools.

## DeckGL

DeckGL is a highly performant framework to create interactive map visualizations that handle large datasets.

This guide shows how to load data from Fused into DeckGL maps created from a single standalone HTML page.
### Setup

1. First create a UDF and generate an HTTPS endpoint.

2. Create an `.html` file following this template. This code creates a DeckGL map then introduces a layer that renders data from the specified Fused endpoint.

```html

```

### H3HexagonLayer

Create an `H3HexagonLayer`.

```js
new H3HexagonLayer(),
```

### Vector Tile Layer

Vector Tile layers are created by placing a `GeoJsonLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a vector layer.

The layer in the sample map comes from Overture Buildings UDF.

```js
new TileLayer(//?format=geojson",
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new GeoJsonLayer(props, );
  },
});
```

### Raster Tile Layer

Raster Tile layers are created by placing a `BitmapLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a raster layer. The sample layer below was created from the NAIP Tile UDF.

```js
new TileLayer(//?format=png`,
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new BitmapLayer(props, );
  },
  pickable: true,
});
```

Learn more about DeckGL

## Felt

Felt is a collaborative mapping platform for creating interactive maps. Load Fused data directly via URLs.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `format=png`
   - Replace path with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=png
```

3. In Felt, click "Upload from URL" and paste the modified URL

### Vector Data

1. Create a UDF that returns vector data
2. Generate a shared URL and modify it:
   - Set `format=csv` or `format=parquet`
   - Add UDF parameters as needed

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?format=csv&param1=value1
```

3. In Felt, click "Upload from URL" and paste the URL

Learn more about Felt

## Kepler

Kepler is an open source tool for visualizing large geospatial datasets. The Fused UDF Builder provides direct integration with Kepler.

### Usage

1. Create a UDF that returns vector data
2. In the UDF Builder, click "Open in Kepler.gl" on the top-right menu
3. Wait for data transfer and click "Open in Kepler.gl" in the bottom-right

This opens your data directly in Kepler for advanced visualization and analysis.

Learn more about Kepler

//?format=png",
    tile_size=512,
    zoom_offset=-1,
)
m.add_layer(tile_layer)
m
```

### Vector Tiles

```python

m = ipyleaflet.Map(center=(37.7749, -122.4194), zoom=17)

# Add vector tile layer
vector_layer = ipyleaflet.VectorTileLayer(
    url="https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=mvt"
)
m.add_layer(vector_layer)
m
```

Learn more about Leaflet */}

## Mapbox

Mapbox GL JS creates interactive web maps. Load Fused data using tile sources.

### Basic Setup

- Generate a Mapbox token

```html

```

### Vector Tiles

```html

```

### Raster Tiles

```html

```

Learn more about Mapbox GL JS

## QGIS

QGIS is an open source desktop GIS platform. Load Fused data as raster tiles, vector tiles, or vector files.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `format=png`
   - Replace with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=png
```

3. In QGIS: Right-click "XYZ Tiles" → "New Connection"
4. Paste the URL and configure the layer

### Vector Tiles

1. Create a UDF that returns vector tiles
2. Generate a shared URL with `format=mvt`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=mvt
```

3. In QGIS: Right-click "Vector Tiles" → "New Connection"
4. Paste the URL and configure the layer

### Vector Files

1. Create a UDF that returns vector data
2. Generate a shared URL with `format=geojson`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?format=geojson
```

3. In QGIS: Layer → Add Layer → Add Vector Layer
4. Paste the URL as the data source

Learn more about QGIS

## Related 

- Generate HTTPS endpoints for your UDFs
- Check the Fused catalog for ready-to-use UDFs

================================================================================

## MCP Servers
Path: guide/data-input-outputs/export-api/mcp-servers.mdx
URL: https://docs.fused.io/guide/data-input-outputs/export-api/mcp-servers

# Connect any UDF to an MCP Server

Make your data available through MCP servers in 1 minute. Let anyone talk to your data directly in plain English; no setup required!

### Try it out

See the demo canvas below with all the live demos:

Or try this one out, asking any questions about Population & Income from Census data:

> "What is the population of Brooklyn, NY in 2024?"

</div>

### Turn your data into an MCP Server

For any UDF you have:

1. Go to AI chat
2. Click the 3 dots in the bottom right corner
3. Click "Generate MCP Config"

The AI should give you a JSON object back with your MCP Server details. For example:

```json

}
```

================================================================================

## Tokens & Endpoints
Path: guide/data-input-outputs/export-api/tokens-endpoints.mdx
URL: https://docs.fused.io/guide/data-input-outputs/export-api/tokens-endpoints

# Turn your data into an API

1. In Workbench create a new UDF that returns data, for example:

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
    housing['price_per_area'] = round(housing['price'] / housing['area'], 2)
    
    return housing[['price', 'price_per_area']]
```

</Tabs>

---

## HTTPS Requests

You can call saved UDFs via HTTPS calls, effectively turning your data into an API!

### Shared Token

When you save a UDF for the first time, it by default creates a shared token for you, something like:

```
fsh_**********h4t
```

This is a unique token that is used to identify your UDF and call it via HTTPS requests.

Here is what a UDF endpoint looks like:

```
https://udf.ai/YOUR_TOKEN.png
```

Manage your account's shared tokens here.

### Creating a Token in Python

```python showLineNumbers
token = my_udf.create_access_token()
print(token)
```

This returns something like: `'fsh_**********q6X'` (You can recognise this to be a shared token because it starts with `fsh_`)

### Passing Parameters

You can pass parameters to your UDF via the URL by adding query parameters (`&param=value`) to the URL.

For example a UDF that takes `lat` and `lon` as parameters:

```python showLineNumbers
@fused.udf
def udf(lat, lon):
    return pd.DataFrame()
```

Could be called with:

```
https://udf.ai/YOUR_TOKEN?lat=37.7749&lon=-121.4194
```

See Calling UDFs as API for more details.

### Tiling 

You can integrate your UDF as a vector or raster tile server by adding `tiles` path parameter, followed by templated `///` path parameters.

You need to make sure:
- Your UDF is set to Tile mode
- You properly pass `///` instead of default value
- Depending on your server type, you might need to add the `format` parameter

```
https://udf.ai/YOUR_TOKEN/run/tiles///?=png
```

### Private Token

Calling UDFs with Bearer authentication requires an account's private token.

```bash
curl -XGET "https://udf.ai/YOUR_TOKEN.png" -H "Authorization: Bearer $ACCESS_TOKEN"
```

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

### Serialization Format

See the full list of supported formats in Calling UDFs as API Output Formats.

---

## Calling UDFs without Python SDK

You can run a UDF directly via HTTP request without needing Python when making the call.

### 1. Get your Bearer token

First retrieve your bearer token (this requires Python once):

```python showLineNumbers
from fused._auth import AUTHORIZATION
AUTHORIZATION.credentials.access_token
```

Returns something like: `'eyJ4K9pL2Mx...'`

### 2. Find your client ID

**Basic Tier (no environment):** Use `basic-tier` as your client_id.

**With environment:** In Workbench, go to "Preferences" and note the "Kernel" name displayed.

[Image: Finding your kernel name in Workbench]

### 3. Make the POST request

```bash
curl -X 'POST' \
  'https://udf.ai/run/udf' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer <BEARER_TOKEN_GOES_HERE>' \
  -H 'Content-Type: application/json' \
  -d ',\"metadata\":,\"code\":\"@fused.udf\\ndef udf(name: str = \\\"world\\\"):\\n    import pandas as pd\\n    return pd.DataFrame()\\n\",\"headers\":[]}}","dtype_in":"json","format":"geojson,png","cache":true}'
```

For subscribed accounts, replace `basic-tier` with your environment name:

```
https://www.fused.io/server/v1/realtime/<your-client-id>/api/v1/run/udf
```

---

## Caching Responses

If a UDF's cache is enabled, its endpoints cache outputs for each combination of code and parameters. The first call runs and caches the UDF, and subsequent calls return cached data.

Control cache behavior with the `cache_max_age` parameter:

```
https://udf.ai/YOUR_TOKEN.json?cache_max_age=1d
```

See Caching for more details.

================================================================================

## Cloud storage
Path: guide/data-input-outputs/import-connection/cloud-storage.mdx
URL: https://docs.fused.io/guide/data-input-outputs/import-connection/cloud-storage

# Cloud storage

Fused supports multiple cloud storage options for reading and writing data.

## Supported storage paths

| Provider | Format | Example |
|----------|--------|---------|
| Fused managed | `fd://` | `fd://my-data/file.parquet` |
| AWS S3 | `s3://` | `s3://bucket-name/path/file.parquet` |
| Google Cloud | `gs://` or `gcs://` | `gs://bucket-name/path/file.parquet` |
| HTTP(S) | `https://` | `https://example.com/file.csv` |

For details on using `fd://` paths and the `/mnt/cache` disk, see File System.

## Connect your own bucket

Connect S3 or GCS buckets to access their files interactively from within the File Explorer UI and programmatically from within UDFs.

Contact Fused to set an S3 or GCS bucket on the File Explorer for all users in your organization. Alternatively, set a bucket as a "favorite" so it appears in the File Explorer for your account only.

### Amazon S3

Set the policy below on your bucket, replacing `YOUR_BUCKET_NAME` with its name. Fused will provide `YOUR_ENV_NAME`.

Alternatively, use this Fused app to automatically structure the policy for you.

The bucket must enable the following CORS settings to allow uploading files from Fused:

#### Encrypted S3 Buckets

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket.

Configure KMS policy:

```json
,
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

### Google Cloud Storage (GCS)

To connect a Google Cloud Storage bucket to your Fused environment:

**1. Create a Service Account in GCS**

Set up a Google Cloud service account with permissions to read, write, and list from the GCS bucket. See the Google Cloud documentation for instructions to:
- Create a Service Account
- Set permissions for the Service Account

**2. Download the JSON Key File**

Download the JSON Key file associated with the Service Account. This file contains credentials that Fused will use to access the GCS bucket.

**3. Set the JSON Key as a Secret**

Set the JSON Key as a secret in the secrets management UI. The secret must be named `gcs_fused`.

You then need to write these credentials to a JSON file and pass them to Google:

```python
@fused.udf
def udf():
    from google.cloud import storage

    # get GCP secrets
    with open("/tmp/gcs_key.json", "w") as f:
        f.write(fused.secrets["gcs_fused"])
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcs_key.json"

    # your code here
```

## Read & write examples

### Reading from S3

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):

    return pd.read_csv(path)
```

### Writing to S3

```python
df.to_parquet("s3://my-bucket/data.parquet")
```

### Writing to GCS

```python
df.to_parquet("gcs://my-bucket/data.parquet")
```

### Download to Fused mount

```python
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    return str(out_path)
```

Files will be written to `/mnt/cache/`, where any other UDF can then access them.

## Downloading large remote files

For datasets from external sources like Zenodo or Humanitarian Data Exchange that take longer than 120s to download, run as a batch job:

```python showLineNumbers
@fused.udf(
    instance_type='c2-standard-4',  # Small instance - download uses little resources
    disk_size_gb=999                # Large disk for the file
)
def udf():

    url = "https://zenodo.org/records/4395621/files/my_large_file.zip"
    s3_path = f"s3://fused-asset/data/my-files/"
    
    # Skip if already downloaded
    if s3fs.S3FileSystem().exists(s3_path):
        return f'File exists: '

    # Download with progress
    temp_path = tempfile.NamedTemporaryFile(delete=False).name
    resp = requests.get(url, stream=True)
    resp.raise_for_status()
    
    with open(temp_path, 'wb') as f:
        for chunk in resp.iter_content(chunk_size=8192):
            f.write(chunk)

    # Upload to S3
    s3fs.S3FileSystem().put(temp_path, s3_path)
    return f"Uploaded to: "
```

Once downloaded, use the Reading Data guide for extracting compressed files (ZIP/RAR).

## `/mnt/cache` disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and `fused.download` write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system:

```python showLineNumbers
# Write to mount
df.to_parquet("/mnt/cache/data.parquet")

# List files
@fused.udf
def udf():

    for each in os.listdir('/mnt/cache/'):
        print(each)
```

If you encounter `Error: No such file or directory: '/mnt/cache/'`, contact the Fused team to enable it for your environment.

================================================================================

## Databases
Path: guide/data-input-outputs/import-connection/databases.mdx
URL: https://docs.fused.io/guide/data-input-outputs/import-connection/databases

# Databases

Connect Fused to external databases like Snowflake and BigQuery.

## Snowflake

Set `user` and `password` in the Fused secrets management UI first.

```python showLineNumbers
@fused.udf
def udf(query: str = 'SELECT CURRENT_VERSION()'):

    try:
        conn = snowflake.connector.connect(
            user=fused.secret('SNOWFLAKE_USER'),
            password=fused.secret('SNOWFLAKE_PASSWORD'), 
            account='your_account_identifier',
            warehouse='your_warehouse',
            database='your_database',
            schema='your_schema'
        )
        
        # Execute query and return as DataFrame
        cursor = conn.cursor()
        cursor.execute(query)
        
        # Use pandas to read directly from cursor
        df = cursor.fetch_pandas_all()
        
        cursor.close()
        conn.close()
        
        return df
        
    except Exception as e:
        print(f"Snowflake connection failed: ")
        raise
```

Read more about Snowflake's authentication.

---

## BigQuery - Option 1: Credentials file

Fused integrates with Google BigQuery with the Python `bigquery` library.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/bq_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from BigQuery

Create a UDF to perform a query on a BigQuery dataset and return the results as a DataFrame or GeoDataFrame. Authenticate by passing the key file path to `service_account.Credentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, geography_column=None):
    from google.cloud import bigquery
    from google.oauth2 import service_account

    # This UDF will only work on runtime with mounted EFS
    key_path = "/mnt/cache/bq_creds.json"

    # Authenticate BigQuery
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )

    # Create a BigQuery client
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # Structure spatial query
    query = f"""
        SELECT * FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`
        LIMIT 10
    """

    if geography_column:
        return client.query(query).to_geodataframe(geography_column=geography_column)
    else:
        return client.query(query).to_dataframe()
```

## Big Query - Option 2: Secrets

If you already have a `gcs_secret` in Fused secrets, you can use it to access your GCP secrets. Otherwise you can simply create new secrets in the Fused secrets manager with:
- `GS_ACCESS_KEY_ID`
- `GS_SECRET_ACCESS_KEY`

You can for example use this to access the Github Activity Data

```python showLineNumbers
@fused.udf
def udf(repo_name: str = "athasdev/athas"):

    # This is not required if your account already has the `gcs_secret` in Fused secrets
    os.environ['GS_ACCESS_KEY_ID'] = fused.secrets["GS_ACCESS_KEY_ID"]
    os.environ['GS_SECRET_ACCESS_KEY'] = fused.secrets["GS_SECRET_ACCESS_KEY"]
    
    from google.cloud import bigquery
    # Initialize BigQuery client
    client = bigquery.Client()
    
    # Get total stars 
    total_query = f"""
        SELECT 
            repo.name as repository,
            COUNT(*) as total_stars
        FROM `githubarchive.day.202508*`
        WHERE type = 'WatchEvent' 
 --           AND repo.name = ''
        GROUP BY repository
    """
    
    total_query = f""" SELECT  * FROM `githubarchive.day.202508*` limit 10"""
    
    # Run the query
    query_job = client.query(total_query)
    
    # Convert to pandas DataFrame
    total_df = query_job.to_dataframe()
     
    return total_df
```

================================================================================

## Google Earth Engine
Path: guide/data-input-outputs/import-connection/geospatial/gee.mdx
URL: https://docs.fused.io/guide/data-input-outputs/import-connection/geospatial/gee

# Google Earth Engine

Fused interfaces Google Earth Engine with the Python `earthengine-api` library. This example shows how to load data from GEE datasets into Fused UDFs and read it with xarray.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/gee_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from Google Earth Engine

Create a UDF to load data from a GEE ImageCollection and open it with xarray. Authenticate by passing the key file path to `ee.ServiceAccountCredentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, n=10):

    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")

    # Authenticate GEE
    key_path = '/mnt/cache/gee_creds.json'
    credentials = ee.ServiceAccountCredentials("fused-account@fused-gee.iam.gserviceaccount.com", key_path)
    ee.Initialize(opt_url="https://earthengine-highvolume.googleapis.com", credentials=credentials)

    # Generate GEE bounding box for spatial filter
    geom = ee.Geometry.Rectangle(*bounds.total_bounds)
    scale = 1 / 2 ** max(0, bounds.z[0])  # A larger scale will increase your resolution per z but slow down the loading

    # Load data from a GEE ImageCollection
    ic = ee.ImageCollection("MODIS/061/MOD13A2").filter(
        ee.Filter.date("2023-01-01", "2023-06-01")
    )

    # Open with xarray (the `xee` package must be present for engine="ee" to work)
    ds = xarray.open_dataset(ic, engine="ee", geometry=geom, scale=scale).isel(time=0)

    # Transform image color with a utility function
    arr = common.arr_to_plasma(ds["NDVI"].values.squeeze().T, min_max=(0, 8000))
    return arr

```

================================================================================

## STAC Catalogs
Path: guide/data-input-outputs/import-connection/geospatial/stac.mdx
URL: https://docs.fused.io/guide/data-input-outputs/import-connection/geospatial/stac

# STAC Catalogs

Access STAC (SpatioTemporal Asset Catalog) catalogs with pystac and odc.stac.

## Earth on AWS

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-77.083, 38.804, -76.969, 38.983],
):

    odc.stac.configure_s3_access(aws_unsigned=True)
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

    # Loading Elevation model
    items = catalog.search(
        collections=["cop-dem-glo-30"], 
        bbox=bounds
    ).item_collection()

    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)

    return xarray_dataset["data"], bounds
```

## Microsoft Planetary Computer

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-122.463,37.755,-122.376,37.803],
):

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        modifier=planetary_computer.sign_inplace,
    )

    # Loading Elevation model
    items = catalog.search(collections=["cop-dem-glo-30"],bbox=bounds).item_collection()
    
    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)
    
    return xarray_dataset["data"], bounds
```

================================================================================

## Local Files
Path: guide/data-input-outputs/import-connection/local-files.mdx
URL: https://docs.fused.io/guide/data-input-outputs/import-connection/local-files

# Local Files

Quickly bring any local data into Fused.

## Drag & Drop

Drop files directly into the File Explorer:

[Image: Drag and drop files directly into Workbench]

## Upload with Python

Install `fused`, authenticate & run:

```python
fused.api.upload("my_local_file.csv", "fd://my_data/file.csv")
```

`fd://` is the Fused provisioned private S3 path for your team.

## Optimize Data Loading

**For files &lt; 1GB:** Use function caching to speed up loading:

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):

    @fused.cache
    def load_data(path):
        return pd.read_csv(path)

    return load_data(path)
```

This is especially useful for slow formats (CSV, Excel, etc.).

**For files &gt; 1GB:** Use Geospatial Ingestion to create cloud-optimized, partitioned files:

```python
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output="fd://census/dc_tract/",
)

job.run_batch()
```

================================================================================

## Geospatial Ingestion
Path: guide/data-input-outputs/read-write/geospatial/ingestion.mdx
URL: https://docs.fused.io/guide/data-input-outputs/read-write/geospatial/ingestion

# Geospatial Data Ingestion

_This page gives you tools to make your data fast to read so your UDFs are more responsive._

## Why Ingestion?

The whole purpose of Fused is to speed up data science pipelines. To make this happen we need data to be responsive, regardless of size. The ideal solution is to have all data sitting in RAM right next to compute, but in real-world applications:

- Datasets (especially geospatial data) can be in the Tb or Pb range which rarely fit in storage, let alone RAM
- Compute needs to be scaled up and down depending on workloads

One solution is **Cloud Optimized formats**: Data lives in the cloud but also leverages file formats that are fast to access. Just putting a `.zip` file that needs to be uncompressed at every read on an S3 bucket is still very slow. Ingested data should be:

- **On the cloud** so dataset size doesn't matter (AWS S3, Google Cloud Storage, etc.)
- **Partitioned** (broken down into smaller pieces that are fast to retrieve so we can load only sections of the dataset we need)

This makes it fast to read for any UDF, so developing in Workbench UDF Builder & running UDFs is a lot faster & responsive!

### When is ingestion needed?

You don't _always_ need to ingest your file into a cloud, geo-partitioned format:
- Small files (< 100Mb) that are fast to open (already in `.parquet`) that you only read once

**Example of data you should ingest:** 1Gb `.zip` of shapefile
- `.zip` means you need to unzip your file each time you open it. This slows down working with the data _every time_.
- shapefile contains multiple files, it isn't the fastest to read

**Example of data you don't need to ingest:** 50Mb `.parquet`
- Even if the data isn't geo-partitioned, loading this data should be fast enough

---

## Ingest Geospatial Table Data

To run an ingestion job on vector data we need:
1. **Input data** - This could be CSV files, a `.zip` containing shapefiles or any other non-partitioned data
2. **A cloud directory** - Where we save ingested data and later access it through UDFs

Our ingestion process:
1. Uploads the `input`
2. Creates geo-partitions of the input data

This is defined with `fused.ingest()`:

```python showLineNumbers
# Run this locally - not in Workbench
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract/",
)
```

Ingestion jobs often take more than a few seconds and require a lot of RAM, making this a large run. Use `run_batch()` so your ingestion job can take as long as needed.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = job.run_batch()
```

Refer to the dedicated documentation page for `fused.ingest()` for more details on all parameters

## Reading Ingested Data

Ingested tables can easily be read with the Fused utility function `table_to_tile`, which spatially filters the dataset and reads only the chunks within a specified polygon.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, 
    table="s3://fused-asset/infra/building_msft_us/"
):
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    return common.table_to_tile(bounds, table)
```

## Common Ingestion Patterns

### Ingest a table from a URL

Ingests a table from a URL and writes it to an S3 bucket specified with `fd://`.

```python showLineNumbers

job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract/",
).run_batch()
```

### Ingest multiple files

```python showLineNumbers

job_id = fused.ingest(
    input=["s3://my-bucket/file1.parquet", "s3://my-bucket/file2.parquet"],
    output=f"fd://census/dc_tract",
).run_batch()
```

To ingest multiple local files, first upload them to S3 with fused.api.upload then specify an array of their S3 paths as the input to ingest.

### Row-based ingestion

Standard ingestion is row-based, where the user sets the maximum number of rows per chunk and file.

```python showLineNumbers   
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    explode_geometries=True,
    partitioning_method="rows",
    partitioning_maximum_per_file=100,
    partitioning_maximum_per_chunk=10,
).run_batch()
```

### Area-based ingestion

Fused also supports area-based ingestion, where the number of rows in each partition is determined by the sum of their area.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_area",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
).run_batch()
```

### Ingest GeoDataFrame

Ingest a GeoDataFrame directly.

```python showLineNumbers
job_id = fused.ingest(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).run_batch()
```

### Ingest Shapefile

We recommend you `.zip` your shapefile and ingest it as a single file:

```python showLineNumbers
job_id = fused.ingest(
    input="s3://my_bucket/my_shapefile.zip",
    output="s3://sample-bucket/file.parquet",
).run_batch()
```

## Ingest to your own S3 bucket

`fused.ingest` supports writing output to any S3 bucket as long as you have appropriate permissions:

1. Open the `S3 Policy` tab in the profile page of Workbench and enter your S3 bucket name. This returns a JSON object.
2. Copy this IAM policy and paste it in your AWS S3 Policy tab.
3. Write the output to your S3 bucket using the `output` parameter:

```python showLineNumbers
# Assuming s3://my-bucket/ is your own managed S3 bucket
job_id = fused.ingest(
    input="s3://my-bucket/file.parquet",
    output="s3://my-bucket/ingested/", 
).run_batch()
```

## Troubleshooting

If you encounter the following error message, please contact the Fused team to request an increase:

```text
Error: `Quota limit: Number of running instances`
```

================================================================================

## Reading geospatial
Path: guide/data-input-outputs/read-write/geospatial/reading.mdx
URL: https://docs.fused.io/guide/data-input-outputs/read-write/geospatial/geospatial-reading

# Reading geospatial

Common examples for reading geospatial data in Fused.

## Python Packages

### `geopandas`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/subway_stations.geojson"):

    return gpd.read_file(path)
```

### `shapely`

```python
@fused.udf
def udf():

    from shapely.geometry import Point, Polygon
    
    # Create geometries with shapely
    points = [Point(-122.4, 37.8), Point(-122.3, 37.7)]
    polygon = Polygon([(-122.5, 37.7), (-122.3, 37.7), (-122.3, 37.9), (-122.5, 37.9)])
    
    gdf = gpd.GeoDataFrame(
        ,
        geometry=points + [polygon],
        crs=4326
    )
    
    return gdf
```

### `rioxarray`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/elevation.tif"):

    # Read raster data with rioxarray
    raster = rxr.open_rasterio(path)
    
    # Convert to DataFrame for display
    df = raster.to_dataframe().reset_index()
    
    return df.head(1000)
```

### `xarray`

```python
@fused.udf
def udf():

    # Download NetCDF data to mount disk for proper reading
    path = fused.download('s3://fused-sample/demo_data/2025_01_01_ERA5_surface.nc','2025_01_01_ERA5_surface.nc')
    ds = xr.open_dataset(path)
    
    # Convert to DataFrame
    df = ds.to_dataframe().reset_index()
    
    return df.head(1000)
```

## Vector Formats

### GeoJSON (.geojson, .json)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.geojson"):

    return gpd.read_file(path)
```

### Shapefile (.shp + .shx, .dbf, .prj)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_shapefile.shp"):

    return gpd.read_file(path)
```

### GeoPackage (.gpkg)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_geopackage.gpkg"):

    return gpd.read_file(path)
```

### KML/KMZ (.kml, .kmz)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.kml"):

    return gpd.read_file(path)
```

### GeoParquet (.parquet)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/buildings.parquet"):

    return gpd.read_parquet(path)
```

### CSV with coordinates (.csv)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.csv"):

    from shapely.geometry import Point
    
    # Read CSV
    df = pd.read_csv(path)
    
    # Convert to GeoDataFrame
    gdf = gpd.GeoDataFrame(
        df, 
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs=4326
    )
    
    return gdf
```

### Excel with coordinates (.xlsx)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.xlsx"):

    from shapely.geometry import Point
    
    # Read Excel file
    df = pd.read_excel(path)
    
    # Convert to GeoDataFrame if coordinates exist
    if 'longitude' in df.columns and 'latitude' in df.columns:
        gdf = gpd.GeoDataFrame(
            df,
            geometry=gpd.points_from_xy(df.longitude, df.latitude),
            crs=4326
        )
        return gdf
    
    return df
```

## Raster Formats

### GeoTIFF (.tif, .tiff)

```python
@fused.udf
def udf(
    path: str = 's3://fused-sample/demo_data/satellite_imagery/wildfires.tiff'
):

    with rasterio.open(path) as src:
        data = src.read()
        bounds = src.bounds

    return data, bounds
```

### NetCDF (.nc)

```python
@fused.udf
def udf():

    # Download to mount disk for proper NetCDF reading
    path = fused.download('s3://fused-sample/demo_data/climate_data.nc', 'climate_data.nc')
    
    # Open NetCDF dataset
    ds = xr.open_dataset(path)
    
    return ds.to_dataframe().reset_index().head(1000)
```

For working with STAC catalogs (Earth on AWS, Microsoft Planetary Computer), see STAC.

================================================================================

## Writing geospatial
Path: guide/data-input-outputs/read-write/geospatial/writing.mdx
URL: https://docs.fused.io/guide/data-input-outputs/read-write/geospatial/geospatial-writing

# Writing geospatial

When working with geospatial data in Fused we recommend saving files in these formats:
- **Vector data**: GeoParquet
- **Raster data**: Cloud Optimized GeoTIFF (COG)

## Vector: GeoParquet

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/subway_stations.geojson"):

    gdf = gpd.read_file(path)
    
    # Process data...
    
    # Save to your Fused bucket
    username = fused.api.whoami()['handle']
    output_path = f"fd:///subway_stations.parquet"
    gdf.to_parquet(output_path)

    return f"File saved to "
```

## Raster: Cloud Optimized GeoTIFF (COG)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/satellite_imagery/wildfires.tiff"):

    # Read the raster data
    with rasterio.open(path) as src:
        data = src.read()
        profile = src.profile
    
    # Process the data
    processed_data = np.where(data > np.percentile(data, 80), 255, 0).astype(np.uint8)
    
    # Update profile for writing
    profile.update()
    
    # Write to Fused's shared disk (accessible to all UDFs in org)
    username = fused.api.whoami()['handle']
    output_path = f"/mnt/cache/wildfires_processed_.tif"
    
    with rasterio.open(output_path, 'w', **profile) as dst:
        dst.write(processed_data)
    
    return f"File saved to shared disk at "
```

## Large Datasets: `fused.ingest()`

For large geospatial datasets, use `fused.ingest()` to create optimized, geo-partitioned files. This enables efficient spatial queries on datasets of any size.

```python
# Get your user handle 
user = fused.api.whoami()['handle']

# Ingest Washington DC Census data
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd:///data/census/partitioned/",
)

job.run_batch()
```

You can tail logs to see how the job is progressing:

```python
fused.api.job_tail_logs("your-job-id")
```

Learn more about geospatial data ingestion.

================================================================================

## Reading files
Path: guide/data-input-outputs/read-write/reading.mdx
URL: https://docs.fused.io/guide/data-input-outputs/read-write/reading

# Reading files

Common examples for reading tabular data in Fused.

For geospatial formats (GeoJSON, Shapefile, GeoTIFF, etc.), see Reading Geospatial Data.

## CSV

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):

    return pd.read_csv(path)
```

## Parquet

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.parquet"):

    return pd.read_parquet(path)
```

## JSON

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/config.json"):

    return pd.read_json(path)
```

## Excel

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/report.xlsx"):

    return pd.read_excel(path)
```

## DuckDB (SQL Queries)

Query files directly with SQL using DuckDB:

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.parquet"):

    conn = duckdb.connect()
    result = conn.execute(f"""
        SELECT * 
        FROM ''
        WHERE latitude IS NOT NULL
        LIMIT 1000
    """).df()
    
    return result
```

Using Fused's common utilities for authenticated S3 access:

```python
@fused.udf
def udf():
    common = fused.load("https://github.com/fusedio/udfs/tree/main/public/common/")
    con = common.duckdb_connect()
    
    df = con.sql("""
        SELECT * FROM read_parquet('s3://bucket/data.parquet')
        WHERE value > 100
        LIMIT 1000
    """).df()
    
    return df
```

## Compressed Files (ZIP/RAR)

### ZIP Files

**List files in archive:**

```python
@fused.cache
def get_zip_file_info(url):

    s3 = s3fs.S3FileSystem()
    with s3.open(url, "rb") as f:
        with zipfile.ZipFile(f) as zip_ref:
            file_info = []
            for filename in zip_ref.namelist():
                info = zip_ref.getinfo(filename)
                file_info.append()
    return pd.DataFrame(file_info)
```

**Extract specific files:**

```python
@fused.cache
def extract_file_from_zip(url, filename, output_path):

    s3 = s3fs.S3FileSystem()
    with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=os.path.splitext(filename)[1]) as output_file:
        temp_path = output_file.name
        CHUNK_SIZE = 100 * 1024 * 1024  # 100MB chunks
        
        with s3.open(url, "rb") as f:
            with zipfile.ZipFile(f) as zip_ref:
                with zip_ref.open(filename) as file:
                    while chunk := file.read(CHUNK_SIZE):
                        output_file.write(chunk)
    
    s3.put(temp_path, output_path)
    return output_path
```

### RAR Files

**List files in archive:**

```python
@fused.cache
def get_rar_file_info(url):

    s3 = s3fs.S3FileSystem()
    with s3.open(url, "rb") as f:
        with rarfile.RarFile(f) as rar_ref:
            file_info = []
            for filename in rar_ref.namelist():
                info = rar_ref.getinfo(filename)
                file_info.append()
    return pd.DataFrame(file_info)
```

**Extract specific files:**

```python
@fused.cache
def extract_file_from_rar(url, filename, output_path):

    s3 = s3fs.S3FileSystem()
    with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=os.path.splitext(filename)[1]) as output_file:
        temp_path = output_file.name
        CHUNK_SIZE = 100 * 1024 * 1024  # 100MB chunks
        
        with s3.open(url, "rb") as f:
            with rarfile.RarFile(f) as rar_ref:
                with rar_ref.open(filename) as file:
                    while chunk := file.read(CHUNK_SIZE):
                        output_file.write(chunk)
    
    s3.put(temp_path, output_path)
    return output_path
```

Use `fused.submit()` to extract multiple files in parallel.

## Recommended Formats

| Data Type | Recommended Format | Why |
|-----------|-------------------|-----|
| Tables | **Parquet** | Columnar, compressed, fast |
| Large tables | **Partitioned Parquet** | Efficient queries on subsets |

For large datasets (>1GB), consider partitioning your data or using geospatial ingestion for spatial data.

================================================================================

## Writing files
Path: guide/data-input-outputs/read-write/writing.mdx
URL: https://docs.fused.io/guide/data-input-outputs/read-write/writing

# Writing files

Common examples for writing tabular data in Fused.

For geospatial formats (GeoParquet, GeoTIFF, etc.), see Writing Geospatial Data.

## CSV

```python
@fused.udf
def udf():

    df = pd.DataFrame()
    
    # Write to Fused managed storage
    df.to_csv("fd://my-data/output.csv", index=False)
    
    return "Saved!"
```

## Parquet

Parquet is the recommended format for tabular data - it's columnar, compressed, and fast.

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):

    df = pd.read_csv(path)
    df['price_per_area'] = round(df['price'] / df['area'], 2)
    
    # Save to your Fused bucket
    username = fused.api.whoami()['handle']
    output_path = f"fd:///housing_processed.parquet"
    df.to_parquet(output_path)

    return f"File saved to "
```

## JSON

```python
@fused.udf
def udf():

    df = pd.DataFrame()
    
    df.to_json("fd://my-data/output.json")
    
    return "Saved!"
```

## Where to Save Files

| Storage | Path Format | Best For |
|---------|-------------|----------|
| Fused S3 bucket | `fd://path/file.parquet` | Persistent storage, sharing |
| Mounted disk | `/mnt/cache/file.parquet` | Temporary files, caching |
| Your own S3 | `s3://bucket/file.parquet` | Enterprise integration |

See Cloud Storage for details on connecting your own buckets.

================================================================================

## First UDF & Basics
Path: guide/getting-started/first-udf-basics.mdx
URL: https://docs.fused.io/guide/getting-started/first-udf-basics

# First UDF & Basics

## Fused

Fused makes it easy to run Python at scale. You write Python code, and Fused handles execution, scaling, and API generation automatically.

If it runs locally, it runs on Fused — at scale.

It decouples *what you write* from *how it runs* and *where it goes*.

## UDFs

UDFs (User Defined Functions) are Python functions with a `@fused.udf` decorator that return data. Each UDF becomes its own API endpoint with a unique URL.

Try it in Workbench:

```python
@fused.udf
def udf(name: str = "Fused"):

    return pd.DataFrame(!']})
```

A UDF needs:
- The `@fused.udf` decorator
- A function (called `udf`)
- Parameters with **type hints** and **default values**

UDFs can return tables (`DataFrame`, `GeoDataFrame`), arrays (`numpy`, `xarray`), or simple Python objects. Learn more about writing UDFs.

## Data

Bring your own data into Fused:

- **Drag & drop** — Drop local files directly into Workbench
- **Cloud storage** — Read from S3, GCS, or Azure (public or connect your own bucket)
- **Databases** — Read from Snowflake, BigQuery, or other databases
- **APIs** — Fetch from any HTTP endpoint

```python
@fused.udf
def udf():

    return pd.read_csv("s3://your-bucket/your-data.csv")
```

Need credentials for private data? See secrets management.

## Formats

Every UDF is a dynamic file system:

[Image: Dynamic File System Overview]

Same UDF, multiple output formats. Change the URL extension to get JSON, CSV, Parquet, PNG, TIFF, or MVT — no code changes needed.

```
https://udf.ai/YOUR_TOKEN.json
https://udf.ai/YOUR_TOKEN.csv
https://udf.ai/YOUR_TOKEN.parquet
```

Pass parameters via URL to change outputs dynamically:

```
https://udf.ai/YOUR_TOKEN.json?name=World
```

Learn more about Calling UDFs as API.

## Caching

First request runs your code. Subsequent requests (same code, same parameters) return cached results in under a second. UDFs are cached for 90 days by default.

Try it yourself:

```python
@fused.udf
def udf():

    time.sleep(5)
    return 
```

Run once → 5 seconds. Run again → ~1 second (cached).

Set any cache duration you need, including `0` to force a fresh run every time:

```python
@fused.udf(cache_max_age="0s")
def udf():

    time.sleep(5)
    return 
```

Learn more about caching.

## Pipelines

UDFs can call other UDFs with `fused.run()` to build data pipelines. Each stays standalone with its own endpoint.

```python
# UDF 1: load_cities
@fused.udf
def udf(continent: str = "Europe"):

    cities = pd.DataFrame()
    return cities[cities.continent == continent]
```

```python
# UDF 2: big_cities
@fused.udf
def udf(continent: str = "Europe", min_pop: int = 3):
    cities = fused.run('load_cities', continent=continent)
    return cities[cities.population >= min_pop]
```

`big_cities` calls `load_cities` and passes through the `continent` param. Both UDFs have their own API endpoint — call either one directly:

```
https://udf.ai/YOUR_TOKEN.json?continent=Asia&min_pop=5
```

Organise UDFs visually in Canvas — connect them, build workflows, create dashboards.

================================================================================

## Using AI
Path: guide/getting-started/using-ai.mdx
URL: https://docs.fused.io/guide/getting-started/using-ai

# Using AI

AI is built into Workbench to help you write, edit, and debug UDFs. There are 2 main ways to use AI in Fused:

- **AI Tab** for large changes
- **Inline Edit** for small quick edits

For more details, tips, and video tutorials, see the AI Assistant reference.

## AI Tab

Open the AI Tab to make large changes to your UDF or start from scratch.

[Image: The AI Tab is associated with the currently selected UDF, not the whole canvas.]

**Good for:**
- Rewriting or restructuring entire UDFs (for example, joining datasets with complex queries)
- Customizing templates (like standalone maps or charts)
- Getting feedback on your code & asking questions about your data

## Inline Edit

Use inline edit for quick, targeted changes to specific code sections.

[Image: inline edit]

**Good for:**
- Small edits when you know exactly what needs to change (filtering, simple joins, etc.)
- When you know exactly what needs to change, but don't want to rewrite the entire UDF

## Handling Changes

AI changes are shown as a green/red diff. Changes are applied by default, but you control what to accept or reject.

[Image: inline edit]

## Error Summaries

When a UDF errors, Fused provides an AI-generated summary explaining the issue.

For **realtime UDFs**, you also get two fix options:
- **Deep Fix** — Opens AI Tab for larger changes
- **Quick Fix** — Inline edit on the lines around the error

For **batch jobs**, only the error summary is shown.

================================================================================

## Workbench Intro
Path: guide/getting-started/workbench-intro.mdx
URL: https://docs.fused.io/guide/getting-started/workbench-intro

# Workbench Intro 

Workbench is Fused's browser based IDE. 

[Image: Workbench Preview]

It allows you to:
- Write Python UDFs directly in the browser — no setup needed
- Build workflows and shareable dashboards in Canvas
- Browse cloud storage and preview files in File Explorer

## Canvas

Freeform canvas where each UDF is a draggable card. Connect UDFs with the (+) icon to build workflows, annotate with text elements, and create interactive dashboards shareable via link.

[Image: Canvas Preview]

_An example of a workflow in Canvas: Change the crop type to see the changes in the dataframe and map. Try it out →_

**Good for:**
- Chaining UDFs together into repeatable workflows
- Annotating work with text elements next to code
- Creating interactive dashboards shareable with a link

Learn more about Canvas →

## File Explorer

Browse public or private cloud storage buckets. Preview large files directly in the browser, then double-click to open in a UDF and start working.

[Image: File Explorer]

_Exploring the Overture Buildings dataset. Try it out →_

**Good for:**
- Browsing files across different buckets (S3, GCS)
- Previewing large files without writing code
- Quickly opening files in a UDF template

Learn more about File Explorer →

## UDF Builder (Advanced)

A GIS-style layer view for advanced geospatial exploration. Toggle UDFs on/off like layers in QGIS/ArcGIS, with automatic tiling for large datasets based on the viewport.

[Image: UDF Builder]

**Good for:**
- Advanced exploration of large geospatial datasets
- Working across multiple layers (each UDF is a layer)
- Automatic viewport-based tiling for performance

Learn more about UDF Builder →

================================================================================

## Aggregations
Path: guide/h3-analytics/aggregations.mdx
URL: https://docs.fused.io/guide/h3-analytics/aggregations

# Aggregate

After ingesting a dataset to H3 hexagons it's possible to visualize it in Fused but we can also directly work with the data.

This page shows how to aggregate data at different resolutions or create derivative layers.

[Image: Aggregating H3 Data]

Example aggregating elevation data across the US at different H3 resolutions:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-74.556, 40.400, -73.374, 41.029],  # Default to full NYC
):
    path = "s3://fused-asset/hex/copernicus-dem-90m/"
    hex_reader = fused.load("https://github.com/fusedio/udfs/tree/8024b5c/community/joris/Read_H3_dataset/")

    df = hex_reader.read_h3_dataset(path, bounds, res=None) # res=None lets the hex_reader determine the resolution based on the current bounds

    return df
```

You can already use the h3_reader.read_h3_dataset and specify:
- `res` at a specific resolution
- `value` to filter on a specific value (especially for categorical data)

This reader uses DuckDB under the hood to read the data meaning you can write additional queries _after_ reading the data as shown in creating derivative layers.

## Aggregate at lower resolutions

H3 hexagons allow for aggregation at different resolutions. The ingestion process already creates overviews at different H3 resolutions - see Resolution Guide for details. 

### Numerical data

Example numerical values:
- Temperature
- Elevation
- Income
- Population

Aggregating means:
- Summing values (i.e. how many people live in a H3 cell in total)
- Taking mean / max / min / stddev (i.e. what is the average temperature in a H3 cell)

Example: Aggregating elevation across the US at different H3 levels

```python showLineNumbers 
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-74.556, 40.400, -73.374, 41.029],  # Default to full NYC
    res: int = 4, # Default to H3 resolution 4
):
    path = "s3://fused-asset/hex/copernicus-dem-90m/"
    hex_reader = fused.load("https://github.com/fusedio/udfs/tree/dd40354/community/joris/Read_H3_dataset/")
    df = hex_reader.read_h3_dataset(
        path, 
        bounds, 
        res=res
    )
    return df
```

### Categorical data

Example categorical data:
- Land Use
- Crop Type
- Zone

Aggregating means:
- Counting values (i.e. how many different crop types are there in a H3 cell)
- Taking mode (i.e. what is the most common crop type in a H3 cell)

Example: Aggregating Corn yields across the US at different H3 levels

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-127.54220803198237,10.667151173068717,-66.93703570835524,55.22298640160706],
    res: int = None,  # if left to None, hex_reader will determine resolution itself
    data_value: int = 1,  # 1=Corn in CDL for this example
    year: int = 2024,
):

    path = f"s3://fused-asset/hex/cdls_v8/year=/"

    common = fused.load("https://github.com/fusedio/udfs/tree/6dd2c4e/public/common/")
    hex_reader = fused.load("https://github.com/fusedio/udfs/tree/f2b3909/community/joris/Read_H3_dataset/")

    # Read all hexagons where the CDL value matches the requested data class (e.g. Corn)
    df = hex_reader.read_h3_dataset(path, bounds, res=res, value=data_value)
    print(df.T)
    
    if 'pct' not in df.columns:
        # Dynamically compute the % of each hex covered by the specified crop type
        data_res = h3.get_resolution(df["hex"].iloc[0])
        print(f"")

        con = common.duckdb_connect()
        df = con.query(f"""
            SELECT 
                hex,
                SUM(area) as total_area,
                ANY_VALUE(data) as data,
                h3_get_hexagon_area_avg(, 'm^2') as hex_area,
                (SUM(area) / h3_get_hexagon_area_avg(, 'm^2')) * 100 as pct
            FROM df 
            WHERE data == 
            GROUP BY hex
        """).to_df()

    print(df.shape)
    if df.shape[0] > 0:
        return df
    else:
        return None
```

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-74.556, 40.400, -73.374, 41.029],  # Default to full NYC
):
    path = "s3://fused-asset/hex/copernicus-dem-90m/"
    
    hex_reader = fused.load("https://github.com/fusedio/udfs/tree/8024b5c/community/joris/Read_H3_dataset/")
    df = hex_reader.read_h3_dataset(path, bounds, res=None)

    # Slope calculation using h3_grid_ring
    common = fused.load("https://github.com/fusedio/udfs/tree/5b11e17/public/common/")
    con = common.duckdb_connect()
    qr = f"""
    WITH hex_neighbors AS (
        SELECT 
            h1.hex AS hex,
            h1.data_avg AS elevation,
            UNNEST(h3_grid_ring(h1.hex, 1)) AS neighbor_hex
        FROM df h1       -- Notice that DuckDB allows you to directly call the dataframe by its name
    ),
    neighbor_data AS (
        SELECT
            n.hex,
            n.elevation,
            n.neighbor_hex,
            df2.data_avg AS neighbor_elevation,
            h3_get_hexagon_edge_length_avg(h3_get_resolution(n.hex), 'm') AS edge_length_m
        FROM hex_neighbors n
        JOIN df df2 ON n.neighbor_hex = df2.hex
    ),
    calculations AS (
        SELECT
            hex,
            elevation,
            DEGREES(ATAN(ABS(elevation - neighbor_elevation) / edge_length_m)) AS slope_deg
        FROM neighbor_data
    ),
    slope_aggregated AS (
        SELECT
            hex,
            elevation,
            AVG(slope_deg) * 100 AS avg_slope_deg,
            MAX(slope_deg) * 100 AS max_slope_deg
        FROM calculations
        GROUP BY hex, elevation
    )
    SELECT
        s.hex,
        ROUND(s.elevation, 2) AS elevation,
        ROUND(s.avg_slope_deg, 2) AS avg_slope_deg,
        ROUND(s.max_slope_deg, 2) AS max_slope_deg
    FROM slope_aggregated s;
    """
    processed_df = con.execute(qr).df()

    print(processed_df.describe())

    return processed_df
```

</details>

================================================================================

## Converting to H3
Path: guide/h3-analytics/converting.mdx
URL: https://docs.fused.io/guide/h3-analytics/converting

# Converting Data to H3

This page covers different methods to convert your data into H3 hexagons.

---

## File to H3

Turning a single small dataset into a grid of H3 hexagons.

### Point Count to Hex 

The following example uses a simple CSV of 311 calls in the New York City area, showing a heatmap of calls per hex 9 cell

```python showLineNumbers 
@fused.udf
def udf(
    noise_311_link: str = "https://gist.githubusercontent.com/kashuk/670a350ea1f9fc543c3f6916ab392f62/raw/4c5ced45cc94d5b00e3699dd211ad7125ee6c4d3/NYC311_noise.csv",
    res: int = 9
):
    # Load common utilities (includes duckdb helper)
    common = fused.load("https://github.com/fusedio/udfs/tree/b7637ee/public/common/")
    con = common.duckdb_connect()

    # Keep latitude and longitude (averaged per hex) alongside the hex count
    qr = f"""
    SELECT
      h3_latlng_to_cell(lat, lng, ) AS hex,
      COUNT(*) AS cnt,
      AVG(lat) AS lat,
      AVG(lng) AS lng
    FROM read_csv_auto('')
    WHERE lat IS NOT NULL AND lng IS NOT NULL
    GROUP BY 1
    """

    df = con.sql(qr).df()

    # Debugging: print the resulting DataFrame schema
    print(df.T)

    return df
```

Link to UDF in Fused Catalog

```python showLineNumbers 
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-125.0, 24.0, -66.9, 49.0],
    path: str = "s3://fused-asset/demos/catchment_analysis/simplified_acs_bg_ca_2022.parquet",
    min_hex_cell_res: int= 11, # Increase this if working with high res local data
    max_hex_cell_res: int= 4,
):

    common = fused.load("https://github.com/fusedio/udfs/tree/f430c25/public/common/")
    
    # Dynamic H3 resolution
    def dynamic_h3_res(b):
        z = common.estimate_zoom(b)
        return max(min(int(2 + z / 1.5),min_hex_cell_res), max_hex_cell_res)
    
    parent_res = max(dynamic_h3_res(bounds) - 1, 0)
    
    # Load and clip data
    gdf = gpd.read_parquet(path)
    tile = common.get_tiles(bounds, clip=True)
    gdf = gdf.to_crs(4326).clip(tile)
    
    # Early exit if empty
    if len(gdf) == 0:
        return pd.DataFrame(columns=["hex", "POP", "pct"])
    
    # Hexify
    con = common.duckdb_connect()
    df_hex = common.gdf_to_hex(gdf, res=parent_res, add_latlng_cols=None)
    con.register("df_hex", df_hex)
    
    # Aggregate to parent hexagons and calculate percentages
    # In this case we're aggregating by sum
    query = f"""
    WITH agg AS (
        SELECT 
            h3_cell_to_parent(hex, ) AS hex, 
            SUM(POP) AS POP
        FROM df_hex
        GROUP BY hex
    )
    SELECT
        hex,
        POP,
        POP * 100.0 / SUM(POP) OVER () AS pct
    FROM agg
    ORDER BY POP DESC
    """
    
    return con.sql(query).df()
```

Link to UDF in Fused Catalog

---

## Ingesting Dataset to H3

For large datasets, pre-compute and ingest data to H3 format using `fused.h3.run_ingest_raster_to_h3()`.

### Raster to H3

#### Basic Example

```python showLineNumbers 
@fused.udf
def udf():
    src_path = "s3://fused-asset/data/nyc_dem.tif"
    output_path = "s3://fused-users/fused/joris/nyc_dem_h3/"  # <-- update this path

    result_extract, result_partition = fused.h3.run_ingest_raster_to_h3(
        src_path, 
        output_path, 
        metrics=["avg"],
    )

    # verify ingestion succeeded
    if not result_extract.all_succeeded():
        print(result_extract.errors())
    if result_partition is not None and not result_partition.all_succeeded():
        print(result_partition.errors())
```

This produces H3 hexagon data like:

**Required parameters:**
- `src_path`: Raster file path(s) on S3 (TIFF or any GDAL-readable format)
- `output_path`: Writable S3 location for output
- `metrics`: Aggregation method per H3 cell (`"avg"`, `"sum"`, `"cnt"`, `"min"`, `"max"`, `"stddev"`)

#### How It Works

The function runs multiple UDFs in parallel under the hood. The orchestrating UDF doesn't need much resources, but can exceed the 2-min realtime limit. For larger data, use a batch instance:

```python showLineNumbers 
@fused.udf(instance_type="small")
def udf():
    src_path = "s3://fused-asset/data/nyc_dem.tif"
    ...
```

#### Output Structure

The ingestion creates:
- **Parquet data files** (e.g., `577234808489377791.parquet`) - each row is an H3 cell with `hex` ID + computed values
- **`_sample` metadata file** - chunk/file bounding boxes for fast spatial queries
- **`/overview/` directory** - pre-aggregated files at lower resolutions (`hex3.parquet`, `hex4.parquet`, etc.)

[Image: Files created by the ingestion process]

**Overview file** (`/overview/hex7.parquet`):

---

### Metrics

Choose metrics based on your raster data type:

| Metric | Use Case | Output Columns |
|--------|----------|----------------|
| `"cnt"` | Categorical data (land use, crop types) | `data`, `cnt`, `cnt_total` |
| `"avg"` | Continuous averages (temperature, elevation, density) | `data_avg` |
| `"sum"` | Totals (population counts) | `data_sum` |
| `"min"`, `"max"`, `"stddev"` | Additional statistics | `data_min`, `data_max`, `data_stddev` |

`"cnt"` cannot be combined with other metrics. Other metrics can be combined: `metrics=["avg", "min", "max"]`

#### Counting (categorical data)

For discrete/categorical rasters like land use, `"cnt"` counts occurrences per category:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-73.983, 40.763, -73.969, 40.773],
    res: int = None,
):
    # Cropland Data Layer ingested with "cnt" metric
    path = "s3://fused-asset/hex/cdls_v8/year=2024/"
    utils = fused.load("https://github.com/fusedio/udfs/tree/79f8203/community/joris/Read_H3_dataset")
    df = utils.read_h3_dataset(path, bounds, res=res)
    return df
```

```
                  hex  data  cnt  cnt_total
0  626740321835323391   122   12         14
1  626740321835323391   123    1         14
2  626740321835323391   121    1         14
```

Each H3 cell can have multiple rows (one per category).

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-73.983, 40.763, -73.969, 40.773],
    res: int = None,
):
    path = "s3://fused-asset/hex/nyc_dem/"
    utils = fused.load("https://github.com/fusedio/udfs/tree/79f8203/community/joris/Read_H3_dataset")
    df = utils.read_h3_dataset(path, bounds, res=res)
    return df
```

```
                  hex   data_avg
0  617733122581069823  78.822576
1  617733122610954239  78.225562
```

```python showLineNumbers 
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-74.556, 40.400, -73.374, 41.029],
    res: int = 9,
):
    path = "s3://fused-users/fused/joris/nyc_dem_h3/"
    utils = fused.load("https://github.com/fusedio/udfs/tree/dd40354/community/joris/Read_H3_dataset/")
    df = utils.read_h3_dataset(path, bounds, res=res)

    map_utils = fused.load("https://github.com/fusedio/udfs/tree/dd40354/community/milind/map_utils")
    config = 
        }
    }
    return map_utils.deckgl_hex(df, config=config)
```

</details>

For more on standalone maps, see Standalone Maps. For Workbench styling, see H3 Visualization.

---

### Vector to H3

For vector data, use `fused.submit()` to parallelize hexagonification:

<img 
  alt="Ingesting Vector Data to H3 Example"
  src=
  style=}
/>

This method is for datasets < 100k vectors. Run in Single (viewport) mode, not Tiled.

```python showLineNumbers 
@fused.udf
def udf(
    bounds: fused.types.Bounds = [8.4452104984018,41.76046948393174,8.903258920921276,42.053137175457145]
):
    common = fused.load("https://github.com/fusedio/udfs/tree/208c30d/public/common/")
    res = common.bounds_to_res(bounds, offset=0)
    res = max(9, res)
     
    gdf = get_data()

    if gdf.shape[0] > 100_000:
        print("Dataset too large. Contact info@fused.io for scaling.")
        return

    vector_chunks = common.split_gdf(gdf[["geometry"]], n=32, return_type="file")
    df = fused.submit(hexagonify_udf, vector_chunks, res=res, engine="remote").reset_index(drop=True)
    df = df.groupby('hex').sum(['cnt','area']).sort_values('hex').reset_index()[['hex', 'cnt', 'area']]
    df['area'] = df['area'].astype(int)

    return df 

@fused.udf
def hexagonify_udf(geometry, res: int = 12):
    common = fused.load("https://github.com/fusedio/udfs/tree/208c30d/public/common/")
    gdf = common.to_gdf(geometry)
    gdf = common.gdf_to_hex(gdf[['geometry']], res=15)
    con = common.duckdb_connect()
    df = con.sql(f"""
        SELECT h3_cell_to_parent(hex, ) AS hex, 
               COUNT(1) AS cnt, 
               SUM(h3_cell_area(hex, 'm^2')) AS area
        FROM gdf
        GROUP BY 1
        ORDER BY 1
    """).df()
    return df

@fused.cache
def get_data():
    gdf = fused.get_chunk_from_table(
        "s3://us-west-2.opendata.source.coop/fused/overture/2025-12-17-0/theme=buildings/type=building/part=3", 10, 0
    )
    return gdf
```

Open in Catalog →

Reach out to our team at info@fused.io

================================================================================

## Joining
Path: guide/h3-analytics/joining.mdx
URL: https://docs.fused.io/guide/h3-analytics/joining

# Join

This page shows how to join two H3 hexagon datasets. This allows merging different datasets together to simplify analysis as each cell contains all the data. 

This is particularly helpful when working with:
- Datasets in different spatial projections
- Sparse datasets like location data
- Datasets of different resolutions or types

[Image: Joining H3 Datasets]

### Assumptions

We assume both datasets:
- Use `hex` as the H3 hexagon column name (as would be the case for any ingested H3 dataset through Fused)
- Are accessible at the same hex resolutions

### Joining logic

We use DuckDB to join the datasets together. 

Using the example from the previous "Aggregate" section, we can join the elevation & crop data layer together.

Elevation dataset (UDF catalog link):

|         hex         |   data_avg   |
|---------------------|--------------|
| 599718693687615487  | 14.60947     |
| 599718693687877631  | 31.12178     |
| ...                 | ...          |
| 599718693688401919  | 23.30812     |
| 599718693688664063  | 45.28209     |

Crop Data Layer dataset (filtering to only show corn, i.e. `data_value=1` UDF Catalog Link):

| hex               | data | area      |
|-------------------|------|-----------|
| 600179630763671551| 1    | 507124    |
| 600179630763671551| 1    | 3537      |
| ...               | ...  | ...       |
| 600181356234760191| 1    | 172089    |
| 600191310987132927| 1    | 801       |

We can join both tables together on the `hex` column:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=[-74.556, 40.4, -73.374, 41.029],
    res: int = 5,
):
    # These are the two datasets coming from the previous "Aggregate" section
    elevation = fused.run('copdem_elevation', bounds=bounds, res=res)
    cdl = fused.run("reading_cdl_2024_hex_simplified", bounds=bounds, res=res, data_value=1)

    common = fused.load("https://github.com/fusedio/udfs/tree/9a3aae2/public/common/")
    con = common.duckdb_connect()

    qr = f"""
    SELECT 
        e.hex,
        e.data_avg as elevation_data,
        c.data as cdl_data,
        c.total_area as cdl_total_area
    FROM elevation as e
    LEFT JOIN cdl as c
    ON e.hex = c.hex
    """

    return con.execute(qr).df()
```

Returns the following table:

| hex                 | elevation_data | cdl_data | cdl_total_area |
|---------------------|---------------|----------|---------------|
| 599650080954204159  | 289.9432      | 1.0      | 74533290      |
| 599650080954466303  | 257.9233      | 1.0      | 90340230      |
| ...                 | ...           | ...      | ...           |
| 599671053654220799  | 203.5272      | 1.0      | 97844060      |
| 599671261723484159  | 306.1185      | 1.0      | 121949200     |

### Example

- Environmental Variable Exploration canvas merging all these variables into a single dataset:
    - Crop Data Layer
    - Land Irrigation
    - Soil Type
    - Temperature
    - Precipitation

================================================================================

## H3 Analytics Overview
Path: guide/h3-analytics/overview.mdx
URL: https://docs.fused.io/guide/h3-analytics/h3-overview

# H3 Analytics Overview

## When to use H3

Consider using H3 tiling when:
- Creating heatmaps of events (populations, counts, etc.)
- Wanting to compare datasets across resolutions & scale
- Working with sparse datasets

Example of buildings heatmap based on the Overture Buildings dataset:

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/blob/main/public/common/")
@fused.udf
def udf(bounds: fused.types.Bounds = [-122.71963771127753,36.53196328805067,-120.70395948802646,38.082911654639275]):
    res = bounds_to_res(bounds)
    print(res)
    releases = ['2024-02-15-alpha-0', '2024-03-12-alpha-0', '2024-08-20-0', '2024-09-18-0', '2024-10-23-0', '2024-11-13-0', '2024-12-18-0', '2025-01-22-0', '2025-03-19-1', '2025-04-23-0', '2025-05-21-0']
    release1 = releases[-1]
    df1 = common.read_hexfile(bounds, f"s3://fused-users/fused/sina/overture_overview//hex.parquet", clip=True)

    return df1

@fused.cache
def bounds_to_res(bounds, res_offset=0, max_res=14, min_res=3):
    z = common.estimate_zoom(bounds)
    return max(min(int(3 + max(0, z - 3) / 1.7 + res_offset), max_res), min_res)
```

</details>

## Converting Datasets to H3 tiles

We've provided a few options for converting datasets to H3 tiles:

| Type | Data Size | Hexagonification Type | H3 Output | Consideration |
|------|-----------|----------------------|-----------|---------------|
| File to H3 | Small (< 100MB) | On the fly | Static Resolution | Easy to adjust. Can easily change field to hexagonify |
| Dynamic Tile to H3 | Small or Large | On the fly | Dynamic Resolution | Easy to adjust. Can easily change field to hexagonify |
| Ingesting Dataset to H3 | Large (> 100MB) | Pre-computed | Dynamic Resolution | Requires deciding on which variable to hexagonify ahead of time |

For choosing the right resolution, see Resolution Guide.

## Example UDFs

- Ookla Download Speed Heatmap: Visualize download speeds across the world.
- US Crop Viewer: Explore crops around the US
- Overture Release Difference Heatmap: Visually identify differences between 2 Overture Building dataset releases.

================================================================================

## Resolution Guide
Path: guide/h3-analytics/resolution-guide.mdx
URL: https://docs.fused.io/guide/h3-analytics/resolution-guide

# H3 Resolution Guide

Cheat sheet for choosing the right H3 resolution.

## Resolution Table

| H3 Resolution (`res`) | Average Hex Area (m²) | Average Hex Edge Length (m) | Pixel Size Equivalent (m, √ avg hex area) | Common Raster Dataset Example |
|-----------------------|-----------------------|-----------------------------|-----------------------------------|----------------------------------------|
| 0 | 4,357,449,416,078 | 1,281,256 | 2,087,465 | |
| 1 | 609,788,441,794 | 483,057 | 780,889 | |
| 2 | 86,801,780,399 | 182,513 | 294,621 | |
| 3 | 12,393,434,655 | 68,979 | 111,325 | |
| 4 | 1,770,347,654 | 26,072 | 42,075 | ERA 5 (0.25deg ~ 27km at Equator)|
| 5 | 252,903,858 | 9,854 | 15,903 | |
| 6 | 36,129,062 | 3,725 | 6,011 | |
| 7 | 5,161,293 | 1,406 | 2,272 | |
| 8 | 737,328 | 531 | 859 | |
| 9 | 105,333 | 201 | 325 | Modis Vegetation Index (250m)|
| 10 | 15,048 | 76 | 123 | |
| 11 | 2,150 | 29 | 46 | Landsat Collection 2 (30m) |
| 12 | 307.1 | 10.8 | 17.5 | Sentinel-2 (10m) |
| 13 | 43.9 | 4.1 | 6.6 | |
| 14 | 6.3 | 1.5 | 2.5 | USGS LiDAR DEM (1m) |
| 15 | 0.9 | 0.6 | 0.9 | |

**Notes:**
- Values are approximate (area and edge length slightly vary by latitude/longitude).
- "Pixel Size Equivalent" is the square root of the average hex area, to aid raster/pixel comparison.

For more details see the H3 Cell Counts Stats page.

## Quick Reference

| Resolution | Avg Edge Length | Use Case |
|------------|-----------------|----------|
| 0 | 1107.71 km | Continental |
| 3 | 59.81 km | Regional |
| 5 | 8.54 km | City |
| 7 | 1.22 km | Neighborhood |
| 9 | 174.38 m | Block |
| 11 | 24.91 m | Building |
| 15 | 0.51 m | Sub-meter |

## Choosing Resolution

By default, when ingesting raster data to H3, the resolution is inferred from the raster data. Using the resolution of the raster input, it chooses an H3 resolution with a cell size that is as close as possible to but larger than the pixel size (at the center of the raster). 

For example:
- Resolution 11 for a raster with pixel size of 30x30m
- Resolution 10 for a raster with pixel size of 90x90m

================================================================================

## Visualization
Path: guide/h3-analytics/visualization.mdx
URL: https://docs.fused.io/guide/h3-analytics/visualization

# H3 Visualization

## In Workbench Map Viewer

Quick styling options for H3 data in Workbench:
- Vector `H3HexagonLayer`
- Vector `H3HexagonLayer` with Tiles

---

## Interactive Visualizations in Canvas

In Fused Canvas you can create interactive visualizations by combining UDFs together. 

We can create a more advanced version of these visualizations using by creating UDFs that leverage postMessages to talk to each other. 

### Connecting Histogram & Map

**Launch the interactive Crop Exploration Dashboard →**

We provide these UDFs for now as-is. You can duplicate them and modify them to work with your own data:

  The Range Histogram UDF allows you to visualize the distribution of a continuous variable (like elevation or temperature) as an interactive histogram. You can select a range to filter the data on the map.

  ```python

@fused.udf
def udf(
    # Replace with your own data URL 
    data_url: str = "https://staging.udf.ai/UDF_join_era5_cdl_elevation/run?dtype_out_raster=png&dtype_out_vector=parquet",
    theme: str = "workbench",
    auto_fetch: bool = True,
    dataset: str = "all",              # IMPORTANT: match the map’s DATASET or use "all"
    num_bins: int = 50,
    value_field: str = "elevation",
    title: str = "Elevation",
    channel: str = "fused-elevation-temp"         # IMPORTANT: match the map’s CHANNEL
):
    common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
    _ = fused.load("join_era5_cdl_elevation")
    themes = 
    }
    selected = themes.get(theme, themes['workbench'])
 
    safe_title = (
        title.replace("&", "&amp;")
             .replace("<", "&lt;")
             .replace(">", "&gt;")
             .replace('"', "&quot;")
             .replace("'", "&#39;")
    )

    html_content = """

    """

    return common.html_to_obj(html_content)
    ```

  The Categorical Bar Chart UDF is used to visualize counts of each category (such as crop types). Clicking a bar will filter the map to display only the selected category.

  ```python
  @fused.udf
def udf(
    # Replace with your own data URL
    data_url: str = "https://staging.udf.ai/UDF_join_era5_cdl_elevation/run?dtype_out_raster=png&dtype_out_vector=parquet",
    theme: str = "workbench",
    auto_fetch: bool = True,
    dataset: str = "all",
    value_field: str = "crop_rank_1",
    title: str = "Top 10 Crop Types",
    channel: str = "fused-elevation-temp"
):
    common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
    _ = fused.load("join_era5_cdl_elevation")
    
    # Load crop name mapping
    crop_df = fused.run("cdl_crop_types_indexing") 
    crop_mapping = dict(zip(crop_df['ID'], crop_df['Name']))
     
    # Convert to JavaScript object string
    crop_mapping_js = ": '',\n"
    crop_mapping_js += "            }"
    
    themes = 
    }
    selected = themes.get(theme, themes['workbench'])

    safe_title = (
        title.replace("&", "&amp;")
             .replace("<", "&lt;")
             .replace(">", "&gt;")
             .replace('"', "&quot;")
             .replace("'", "&#39;")
    )

    html_content = """

    """

    return common.html_to_obj(html_content)
  ```

  The Map UDF renders the geospatial hexagons and supports interactions with the histogram or bar chart to update map highlights in real time.

  ```python
  common = fused.load("https://github.com/fusedio/udfs/tree/f430c25/public/common/")

    DEFAULT_CONFIG = r""",
    "hexLayer": 
    } 
    }"""
    
    DEFAULT_STYLE_URL = "https://basemaps.cartocdn.com/gl/dark-matter-gl-style/style.json"

    @fused.udf
    def udf( 
        # replace with your own data URL
        data_url: str = "https://staging.udf.ai/UDF_join_era5_cdl_elevation/run?dtype_out_raster=png&dtype_out_vector=parquet",
        config_json: str = DEFAULT_CONFIG,
        mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
        center_lng: float = -119.4179,
        center_lat: float = 36.7783,
        zoom: float = 4,
        tooltip_columns: list = ["crop_rank_1", "elevation", "daily_mean"],
        default_query: str = "SELECT * FROM data",
        map_style_url: str = DEFAULT_STYLE_URL,
        channel: str = "fused-elevation-temp",
        dataset: str = "all",
    ):
        from jinja2 import Template
    
        style_url = map_style_url or DEFAULT_STYLE_URL
        _ = fused.load("join_era5_cdl_elevation")

        html = Template(r"""

    """).render(
            data_url=data_url,
            config_json=config_json,
            mapbox_token=mapbox_token,
            map_style_url=style_url,
            default_style_url=DEFAULT_STYLE_URL,
            center_lng=center_lng,
            center_lat=center_lat,
            zoom=zoom,
            tooltip_columns=tooltip_columns,
            default_query=default_query,
            channel=channel,
            dataset=dataset,
        )

        return common.html_to_obj(html)
  ```

</details>

Feel free to directly reach out to us at `info@fused.io` if you have any questions on implementing this yourself

================================================================================

## UDFs as API
Path: guide/working-with-udfs/run-udfs-as-api.mdx
URL: https://docs.fused.io/guide/working-with-udfs/run-udfs-as-api

# UDFs as API

Every UDF can be used as a file system:

[Image: Dynamic File System Overview]

## Output formats

UDFs can return data in various formats via the `format` query parameter:

### Vector data formats

| Format | Extension | Description |
|--------|-----------|-------------|
| `parquet` | `.parquet` | Efficient columnar storage format |
| `geojson` | `.geojson` | Standard GeoJSON format |
| `json` | `.json` | JSON objects and GeoJSON |
| `csv` | `.csv` | Comma-separated values |
| `mvt` | `.mvt` | Mapbox Vector Tiles |
| `feather` | `.feather` | Apache Arrow IPC format |
| `excel` | `.xlsx` | Excel spreadsheet |
| `xml` | `.xml` | XML data |
| `html` | `.html` | HTML document |

### Raster data formats

| Format | Extension | Description |
|--------|-----------|-------------|
| `png` | `.png` | Lossless raster images |
| `jpeg` | `.jpeg` / `.jpg` | Compressed raster images |
| `gif` | `.gif` | Animated or static images |
| `webp` | `.webp` | Modern efficient image format |
| `apng` | `.apng` | Animated PNG images |
| `svg` | `.svg` | Scalable vector graphics |
| `tiff` | `.tiff` / `.tif` | GeoTIFF and TIFF rasters |
| `npy` | `.npy` | NumPy array binary format |

## Example usage

### Returning as JSON

```bash
https://udf.ai/YOUR_TOKEN.json
```

### Returning as CSV

```bash
https://udf.ai/YOUR_TOKEN.csv
```

### Returning as HTML

```bash
https://udf.ai/YOUR_TOKEN.html
```

## Tile endpoints

For map tile servers, use the `tiles` path with `//` template:

### Vector tiles (MVT)

```bash
https://udf.ai/YOUR_TOKEN/run/tiles///?=mvt
```

### Raster tiles (PNG)

```bash
https://udf.ai/YOUR_TOKEN/run/tiles///?=png
```

## Integrations

These dynamic endpoints work with:
- DeckGL - Interactive maps
- Mapbox - Web maps
- QGIS - Desktop GIS
- Felt - Collaborative mapping
- Google Sheets - Spreadsheet import

================================================================================

## Parallel execution
Path: guide/working-with-udfs/run-udfs-in-parallel.mdx
URL: https://docs.fused.io/guide/working-with-udfs/fused-submit

Run a UDF over multiple inputs in parallel.

## Signature

```python
fused.submit(
    udf,
    arg_list,
    engine='remote',
    instance_type='realtime',
    max_workers=32,
    collect=True,
    cache_max_age=None,
)
```

## Parameters

### `arg_list` — Input formats

| Format | Example | Use case |
|--------|---------|----------|
| List | `[0, 1, 2, 3]` | Single parameter |
| List of dicts | `[, ]` | Multiple parameters |
| DataFrame | `pd.DataFrame()` | Multiple parameters |

Each item/row becomes a separate job.

```python
@fused.udf
def udf():
    results = fused.submit(
        single_job_udf, 
        [, ]
    )
    return results

@fused.udf
def single_job_udf(a: str, b: str):

    return pd.DataFrame()
```

### `engine`

Same as fused.run. Default: `remote`

### `instance_type`

Same as fused.run. Default: `realtime`

| Mode | Best for |
|------|----------|
| `realtime` | Many quick jobs |
| `small`/`medium`/`large` | Long/heavy jobs |

### `max_workers`

Number of realtime instances to spin up in parallel. Default: `32`, Max: `1000`.

```python
# Spin up 100 realtime instances in parallel
fused.submit(udf, inputs, max_workers=100)
```

### `collect`

| Value | Behavior | Returns |
|-------|----------|---------|
| `True` (default) | Blocking, waits for all jobs | `DataFrame` |
| `False` | Non-blocking | `JobPool` |

### `debug_mode`

When `True`, runs only the first input via `fused.run()`. Use for testing before scaling.

```python
# Test with first input only
fused.submit(udf, inputs, debug_mode=True)
```

### `max_retry`

Max retries per failed job. Default: `2`

### `ignore_exceptions`

When `True`, failed runs are silently skipped in results. Default: `False`

### `cache_max_age`

Same as fused.run. Additionally, when `collect=True`, collected results are cached locally for `cache_max_age` or 12h by default.

## JobPool methods

When using `collect=False`, you get a `JobPool` object:

```python
job = fused.submit(udf, inputs, collect=False)

job.wait()           # Show progress bar
job.total_time()     # Total wall time
job.times()          # Time per job
job.first_error()    # First error encountered
job.collect()        # Get results as DataFrame
```

## Tips

**Test first:**
```python
fused.submit(udf, inputs, debug_mode=True)
```

**Start small:**
```python
fused.submit(udf, inputs[:5])
```

**Aim for 30-45s per job** — gives safety margin before 120s timeout.

**For batch jobs, save to S3:**
```python
@fused.udf
def batch_udf(input_path: str):
    result = process(input_path)
    output_path = f"s3://bucket/results/"
    result.to_parquet(output_path)
    return output_path  # Return path, not data
```

## See also

- Run UDFs in python — single UDF execution
- Scaling out UDFs — best practices for parallel and batch execution

================================================================================

## Running UDFs
Path: guide/working-with-udfs/run-udfs-in-python.mdx
URL: https://docs.fused.io/guide/working-with-udfs/fused-run

Run a UDF and get results back.

## Signature

```python
fused.run(
    udf,
    engine='remote',
    instance_type='realtime',
    cache_max_age=None,
    max_retry=0,
)
```

## Parameters

### `udf` — Ways to reference a UDF

| Method | Syntax | Use case |
|--------|--------|----------|
| Your UDF | `fused.run("my_udf")` | UDFs you created |
| Teammate's UDF | `fused.run("teammate@fused.io/my_udf")` | UDFs from your team |
| Team UDF | `fused.run("team/my_udf")` | Shared team UDFs |
| Public UDF | `fused.run("UDF_Name")` | Public UDFs (free) |
| Token | `fused.run("fsh_***")` | Share UDF without exposing code |
| Git commit | `fused.run("github.com/.../tree//")` | Production stability |

```python
commit_hash = "bdfb4d0"
udf = fused.load(f"https://github.com/fusedio/udfs/tree//public/My_UDF/")
fused.run(udf)
```
Avoid pointing to `main` branch—your UDF will change when others push to it.

### `engine`

| Engine | Where it runs | Use case |
|--------|---------------|----------|
| `remote` (default) | Spins up new serverless instance | Standard usage |
| `local` | Current process | Run in existing compute |

**`local` contexts:**

| Context | What happens |
|---------|--------------|
| Inside a UDF | Shares that UDF's compute (120s, ~4GB) |
| Inside a batch job | Shares the batch instance resources |
| On your laptop | Runs on local machine |

### `instance_type`

| Type | RAM | Time limit | Startup |
|------|-----|------------|---------|
| `realtime` (default) | ~4GB | 120s | ~5s |
| `small` | 2 GB | None | ~30s |
| `medium` | 64 GB | None | ~30s |
| `large` | 512 GB | None | ~30s |

Set a default instance type when defining a UDF:

```python
@fused.udf(instance_type="large")
def udf():
    ...
```

In Workbench, UDFs with a non-realtime instance type will prompt for confirmation before running as a batch job.

| Instance Type | vCPUs | Memory (GB) |
|---------------|-------|-------------|
| `m5.large` | 2 | 8 |
| `m5.xlarge` | 4 | 16 |
| `m5.2xlarge` | 8 | 32 |
| `m5.4xlarge` | 16 | 64 |
| `m5.8xlarge` | 32 | 128 |
| `m5.12xlarge` | 48 | 192 |
| `m5.16xlarge` | 64 | 256 |
| `r5.large` | 2 | 16 |
| `r5.xlarge` | 4 | 32 |
| `r5.2xlarge` | 8 | 64 |
| `r5.4xlarge` | 16 | 128 |
| `r5.8xlarge` | 32 | 256 |
| `r5.12xlarge` | 48 | 384 |
| `r5.16xlarge` | 64 | 512 |
| `t3.small` | 2 | 2 |
| `t3.medium` | 2 | 4 |
| `t3.large` | 2 | 8 |
| `t3.xlarge` | 4 | 16 |
| `t3.2xlarge` | 8 | 32 |

| Instance Type | vCPUs | Memory (GB) |
|---------------|-------|-------------|
| `c2-standard-4` | 4 | 16 |
| `c2-standard-8` | 8 | 32 |
| `c2-standard-16` | 16 | 64 |
| `c2-standard-30` | 30 | 120 |
| `c2-standard-60` | 60 | 240 |
| `m3-ultramem-32` | 32 | 976 |
| `m3-ultramem-64` | 64 | 1,952 |

</details>

### `cache_max_age`

Control how long results are cached. UDFs are cached for **90 days by default**.

| Value | Meaning |
|-------|---------|
| `None` (default) | Follow `@fused.udf()` setting (90 days) |
| `"0s"` | No caching |
| `"10s"`, `"48h"`, `"1d"` | Cache for specified duration |

Set a default cache duration in the UDF decorator:
```python
@fused.udf(cache_max_age="1d")
def udf():
    ...
```

See Caching for more details on how caching works.

### `sync`

| Value | Behavior |
|-------|----------|
| `True` (default) | Blocking call, returns result |
| `False` | Returns coroutine for async execution |

```python
# Async example
async def run_parallel():
    tasks = [fused.run("my_udf", date=d, sync=False) for d in dates]
    return await asyncio.gather(*tasks)
```

`sync=False` only works with `engine='remote'` and saved UDFs.

### `max_retry`

Number of retries on failure. Default: `0`

## Passing arguments

Pass UDF parameters as keyword arguments:

```python
@fused.udf
def udf(name: str, count: int = 1):

    return pd.DataFrame()
```

```python
fused.run(udf, name="hello", count=3)
```

## Reserved parameters

These parameters control how Fused structures the `bounds` object for tile UDFs.

### With `x`, `y`, `z`

```python
fused.run("UDF_Overture_Maps_Example", x=5241, y=12662, z=15)
```

### With `bounds` as GeoDataFrame

```python

bounds = gpd.GeoDataFrame.from_features()
fused.run("UDF_Overture_Maps_Example", bounds=bounds)
```

### With `bounds` as bbox list

```python
# [min_x, min_y, max_x, max_y]
fused.run("UDF_Overture_Maps_Example", bounds=[-122.349, 37.781, -122.341, 37.818])
```

## See also

- How to run a realtime job — walkthrough with examples
- Run UDFs in parallel — run over multiple inputs in parallel
- Caching — how caching works

================================================================================

## Scaling Out UDFs
Path: guide/working-with-udfs/udf-best-practices/batch-jobs.mdx
URL: https://docs.fused.io/guide/working-with-udfs/udf-best-practices/scaling-out

When realtime isn't enough: run many jobs in parallel, or run on a dedicated machine with more resources.

## Choosing your approach

| Your situation | Use |
|----------------|-----|
| Many inputs (files, dates, regions) | Parallel execution |
| Single job needs >120s or >4GB RAM | Dedicated instance |
| Many heavy jobs | Combining both |

## Parallel execution

`fused.submit()` runs a UDF over multiple inputs in parallel, spinning up separate instances for each job.

```python
inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
results = fused.submit(udf, inputs)
```

For the full API reference, see fused.submit().

### Start small, then scale

Don't immediately spin up 1000 jobs. Test progressively:

```python
# First test with 5 inputs
results = fused.submit(udf, inputs[:5])

# Then 10, then 50, then scale up
results = fused.submit(udf, inputs[:50])
```

### Target 30-45s per job

Each parallel job has a **120s timeout**. Aim for 30-45s per job to leave safety margin for slower runs.

If your jobs consistently hit the timeout, either:
- Break them into smaller chunks
- Use `instance_type` for dedicated machines

### Test first with debug mode

Run only the first input to verify your setup works:

```python
result = fused.submit(udf, inputs, debug_mode=True)
```

### Check timing

Monitor how long each job takes:

```python
job = fused.submit(udf, inputs[:5], collect=False)
job.wait()
print(job.times())  # Time per job
```

### Error handling

**By default, errors aren't cached.** If a job fails (e.g., API timeout), it will retry fresh on the next run. See Caching for more on how caching works.

However, if you wrap errors in try/except and return a result, that result gets cached:

```python
@fused.udf
def udf(url: str):
    try:
        return fetch_data(url)
    except Exception as e:
        return   # This gets cached!
```

Use `ignore_exceptions=True` to skip failed jobs when collecting results:

```python
results = fused.submit(udf, inputs, ignore_exceptions=True)
```

---

## Dedicated instances

Batch jobs run on dedicated machines with more resources and no time limits. Use them when your UDF needs more than realtime can offer.

### When to use

Use a dedicated instance when your UDF exceeds realtime limits:
- Takes longer than **120s** to run
- Needs more than **~4GB RAM**

**Tradeoffs:**
- ~30s startup time (machine needs to spin up)
- Higher resource availability

### Starting a batch job

Add `instance_type` to run on a dedicated machine:

```python
@fused.udf(instance_type='small')
def udf():
    # This UDF runs on a batch instance
    ...
```

[Image: Running a job from workbench]

In Workbench, batch UDFs require manual execution (`Shift+Enter`) and show a confirmation modal.

Batch and realtime jobs have separate caches. Running the same UDF with and without `instance_type` will cache results independently.

### Write to disk, don't return data

Batch jobs should **write results to cloud storage or mount**, not return them. Data returned from batch jobs can be lost if the connection times out.

```python
@fused.udf(instance_type='small')
def batch_job(input_path: str):

    # Process data
    df = pd.read_parquet(input_path)
    result = heavy_processing(df)
    
    # Write to S3, not return
    output_path = f"s3://my-bucket/results/"
    result.to_parquet(output_path)
    
    return output_path  # Return the path, not the data
```

### Monitor your jobs

Batch jobs take time to start and run. Use the **Jobs page** in Workbench to monitor:
- **Status**: Running, Completed, Failed
- **CPU & Memory**: Resource usage over time
- **Disk**: Storage consumption
- **Runtime**: How long the job has been running
- **Logs**: Real-time output from your UDF

```python
# Programmatic monitoring
job.status          # Check status
job.tail_logs()     # Stream logs
job.cancel()        # Stop a job
```

### Expect startup delay

Batch machines take ~30s to spin up. Plan accordingly—batch jobs aren't for quick iterations.

Test your UDF on a small data sample using realtime execution. Once you're confident it works, switch to `instance_type` to scale up.

### Instance types

| Alias | vCPUs | RAM |
|-------|-------|-----|
| `small` | 2 | 2 GB |
| `medium` | 16 | 64 GB |
| `large` | 64 | 512 GB |

See the full list supported of AWS / GCP instance types

---

## Combining both

If your jobs need more than 120s or ~4GB RAM each, combine parallel execution with dedicated instances:

```python
results = fused.submit(
    udf, 
    inputs, 
    instance_type="large",
    collect=False
)
```

---

## Example use cases

- Climate Dashboard — Processing 20TB of data in minutes
- Dark Vessel Detection — Retrieving 30 days of AIS data
- Satellite Imagery — Processing Maxar's Open Data STAC Catalogs
- Ingesting cropland data for zonal statistics
- Data ingestion for large geospatial files

## See also

- Running UDFs — `fused.run()` details
- parallel execution —  `fused.submit()` details
- Caching — how caching works

================================================================================

## Efficient caching
Path: guide/working-with-udfs/udf-best-practices/caching.mdx
URL: https://docs.fused.io/guide/working-with-udfs/udf-best-practices/caching

Fused caches UDF results automatically to make repeated calls faster. Cached calls return instantly and **don't consume Fused Credit Units**—you only pay for compute once, then reuse the result for free.

## Two types of cache

| Type | When it applies | Storage | Default Duration | Speed |
|------|-----------------|---------|-------------|-------|
| UDF cache | `fused.run()` results | S3 | 90 days | Good |
| `@fused.cache` | Functions inside UDFs | mount | 12 hours | Fast |

Both work the same way: store the result of [function + inputs], return cached result on repeat calls. Change the function or inputs → cache miss → recompute.

## UDF cache

Every `fused.run()` call is cached automatically. No configuration needed.

```python
fused.run(my_udf)  # First call: runs UDF
fused.run(my_udf)  # Second call: returns cached result
```

**Disable caching** when you need fresh results:

```python
@fused.udf(cache_max_age=0)
def udf():
    ...

# Or at call time
fused.run(my_udf, cache_max_age=0)
```

**Reset cache** to force a fresh run once:

```python
fused.run(my_udf, cache_reset=True)
```

## @fused.cache

Use `@fused.cache` for expensive operations *inside* a UDF—loading slow file formats, heavy computations that repeat across runs.

```python
@fused.udf
def udf(ship_length: int = 100):
    
    @fused.cache
    def load_data(path):

        return pd.read_csv(path)  # Slow format, cache it
    
    df = load_data("s3://bucket/large_file.csv")
    return df[df.Length > ship_length]
```

The CSV loads once and caches. Changing `ship_length` doesn't reload the file—only changing the `path` would.

**When to use it:**
- Loading CSV, Shapefile, or other slow formats
- Expensive computations that don't depend on all UDF parameters
- API calls you don't want to repeat

**When NOT to use it:**
- Very large datasets (>10GB)—consider ingesting to cloud-native formats instead

## `cache_max_age` reference

Control how long cached results stay valid.

**Format:** `30s` (seconds), `10m` (minutes), `24h` (hours), `7d` (days)

**Where you can set it:**

| Context | Example |
|---------|---------|
| UDF definition | `@fused.udf(cache_max_age="24h")` |
| fused.run() | `fused.run(udf, cache_max_age="1h")` |
| @fused.cache | `@fused.cache(cache_max_age="30m")` |
| HTTPs endpoint | `udf.fused.io/token?cache_max_age=0` |

**Priority:** `fused.run()` > `@fused.udf()` > default (90 days)

## Common gotchas

### Parent/child UDF changes

When calling one UDF from another, the child won't automatically refresh when the parent changes:

```python
@fused.udf
def child_udf():
    data = fused.run("parent_udf")  # Won't re-fetch if parent changes
    return data
```

Fix: disable cache on the child so it always calls the parent:

```python
@fused.udf(cache_max_age=0)
def child_udf():
    data = fused.run("parent_udf")  # Always gets latest from parent
    return data
```

### Realtime vs batch have separate caches

```python
fused.run("my_udf")                         # realtime cache
fused.run("my_udf", instance_type="small")  # different cache (batch)
```

### Caching with `bounds` in Tile UDFs

When using Tile UDFs, panning the map triggers new UDF calls with different `bounds`. If you cache a function that takes `bounds` as input, each tile creates a separate cache entry.

**This can be useful:** Pan back to a previously viewed area and the cached tiles load instantly.

**Watch out for:** If you're iterating on code, you may accumulate many cache entries. Consider what actually needs to vary with `bounds`:

```python
# Caches per tile - good for expensive tile-specific operations
@fused.cache
def process_tile(bounds):
    ...

# Caches once - better when data doesn't depend on bounds
@fused.cache
def load_data(path):
    return gpd.read_file(path)

gdf = load_data(path)
return gdf[gdf.geometry.intersects(bounds_geom)]  # Filter after
```

## Monitor your cache usage

See how much caching is saving you in the **Account** page in Workbench. The usage dashboard shows cache hits vs actual compute across different time ranges.

[Image: Cache usage in Account page]

## See also

- Data ingestion — for large datasets, ingest to Parquet/GeoParquet instead of caching
- Run UDFs in python — full `fused.run()` reference

================================================================================

## Geospatial processing
Path: guide/working-with-udfs/udf-best-practices/geospatial-single-vs-tile.mdx
URL: https://docs.fused.io/guide/working-with-udfs/udf-best-practices/geospatial-single-vs-tile

# Geospatial processing

There are 2 main ways to work with spatial data in Fused:

## Single UDF

Single UDFs are the default behavior in Fused. The UDF is run 1 time with the current viewport bounds.

[Image: Single UDF Mode]

Characteristics:
- 1 Single UDF is called on the entire Map Viewport
- Panning Map does NOT re-run the UDF
- "Results" Tab shows the output of the unique UDF called

### Single HTTPS Call

A Single UDF is called like any other UDF via a HTTPS request:

```bash
https://www.fused.io/.../run/file?format=csv
```

Or via the Fused Python SDK:

```python showLineNumbers
fused.run("single_udf")
```

There are 2 types of Spatial Single UDFs:

### Single (Viewport)

Will prioritize the current MAP VIEWPORT bounds, regardless of the `bounds` parameter.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = [-74.38,40.32,-73.31,41.29]):
    print(bounds) # This will be the current MAP VIEWPORT bounds, regardless of the `bounds` parameter
    return bounds
```

### Single (Parameter)

Will use the `bounds` parameter, regardless of the current MAP VIEWPORT bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = [-74.38,40.32,-73.31,41.29]):
    print(bounds) # This will be the `bounds` parameter, regardless of the current MAP VIEWPORT bounds
    return bounds
```

### Example UDF

- Get Isochrone
    - Returns an isochrone (how far a person cal walk / drive) polygon for a given point

## Tile UDF

Tile UDFs tile the users viewport into a grid of smaller tiles and running a UDF for each.

[Image: Tile UDF Mode]

Characteristics:
- Multiple Tile UDFs are called to cover the current Map Viewport
- Panning Map re-runs the UDF for each tile
- "Results" Tab shows the output of the _latest_ tile called

### Tile HTTPS Call

A Tile UDF is called by passing Web mercator XYZ tiles through the HTTPS request

**Two ways to call the HTTPS endpoint:**

**1. Dynamic tile server (for map applications)**

```bash
https://www.fused.io/.../run/tiles///?format=png
```
Use this template URL in mapping libraries `//` gets automatically replaced with tile coordinates as users pan and zoom.

Example use cases:
- QGIS
- Felt
- DeckGL
- Mapbox

**2. Specific tile call**
```bash
# Calls a single tile - San Francisco center at zoom 13
https://www.fused.io/.../run/tiles/13/1316/3169?format=png
```
Use this to fetch data for one specific tile location.

Or via the Fused Python SDK:

```python showLineNumbers
# With bounds parameter
fused.run("tile_udf", bounds=bounds)
```

```python showLineNumbers
# With x, y, z parameters
fused.run("tile_udf", x=1, y=2, z=3)
```

### Example UDF

- Overture Maps Example
    - Manipulate _all_ the buildings from the Overture Maps dataset by leveraging Fused ingestion
- Landsat Tile Example
    - Returns NDVI tile computed on the fly from Landsat data

## Comparing Single and Tile UDFs

| Feature | Single UDF | Tile UDF |
|---------|------------|-----------|
| UDF Calls | 1 | Multiple |
| Parameters | `bounds` (Optional) | `bounds` or `x`, `y`, `z` |
| Map rendering | Static | Dynamic |
| HTTP Call | Single Call | Tile Server calls or Single Call (specific tile) |
| Runtime Tab display | Unique output of the UDF called | Output of the _latest_ tile called |
| Use Case | Small static data | Calling large dataset through tile server |

## `bounds`

`bounds` is simply a list of 4 coordinates representing the bounds of a geometry. The 4 coordinates represent `[xmin, ymin, xmax, ymax]` of the bounds.

In Fused it is defined with the `fused.types.Bounds` type:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    print(bounds)

>>> [-1.52244399, 48.62747869, -1.50004107, 48.64359255]
```

### `bounds` to `GeoDataFrame`

Converting `bounds` (list of 4 coordinates) to a `GeoDataFrame`:

</Tabs>

### Legacy types

Legacy types that might still appear in older UDFs.

#### [Legacy] `fused.types.Tile`

This is a geopandas.GeoDataFrame with `x`, `y`, `z`, and `geometry` columns.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Tile=None):
    print(bounds)

>>>      x    y   z                                           geometry
>>> 0  327  790  11  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `fused.types.TileGDF`

This behaves the same as `fused.types.Tile`.

#### [Legacy] `fused.types.ViewportGDF`

This is a geopandas.geodataframe.GeoDataFrame with a `geometry` column corresponding to the Polygon geometry of the current viewport in the Map.
```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.ViewportGDF=None):
    print(bbox)
    return bbox

>>>  geometry
>>>  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `bbox` object

UDFs defined using the legacy keyword `bbox` are automatically now mapped to `bounds`. Please update your code to use `bounds` directly as this alias will be removed in a future release.

================================================================================

## Run UDFs efficiently
Path: guide/working-with-udfs/udf-best-practices/realtime.mdx
URL: https://docs.fused.io/guide/working-with-udfs/udf-best-practices/realtime

Realtime is the default way to run UDFs—no configuration needed. Every UDF runs in realtime mode unless you explicitly request a batch instance.

This guide covers best practices for calling UDFs from other UDFs and building pipelines. For the full `fused.run()` reference, see Run UDFs in python.

## Limits

| Resource | Limit |
|----------|-------|
| Execution time | 120s |
| RAM | ~4GB |

Need more? See Scaling out UDFs.

## Calling another UDF

Use `fused.run()` to call one UDF from another:

```python
@fused.udf
def parent_udf():
    # Call child UDF
    result = fused.run("child_udf", name="hello")
    return result
```

[Image: Calling another UDF in workbench]

In Workbench, `fused.run("udf_name")` creates a visual link between parent and child UDFs, making pipelines easy to follow.

For geospatial UDFs, you can pass `bounds` as a bbox list, GeoDataFrame, or tile coordinates. See Reserved parameters.

## Best practices

### Keep results fresh

By default, UDF results are cached. To always get fresh data when calling another UDF, set `cache_max_age=0` on your UDF:

```python
@fused.udf(cache_max_age=0)
def udf():
    data = fused.run('parent_udf')
    return data
```

Setting `cache_max_age=0` means this UDF runs from scratch every time—no caching. Use this when your output depends on frequently changing data.

### Pin to commit hash for production

When calling UDFs from GitHub, pin to a specific commit:

```python
commit_hash = "bdfb4d0"
udf = fused.load(f"https://github.com/fusedio/udfs/tree//public/My_UDF/")
fused.run(udf)
```

Avoid pointing to `main` branch—your UDF will break when others push changes.

## Warm & cold starts

After inactivity, Fused needs to spin up an instance and load the environment. This **cold start** typically takes 10-15s.

Once warm, subsequent realtime calls execute within seconds. Instances stay warm with regular use. Fused does not charge for cold start time.

[Image: Cold start vs warm execution]

## When to scale up

| Need | Solution |
|------|----------|
| Run same UDF over multiple inputs | Parallel execution |
| More than 120s or ~4GB | Dedicated instances |

## See also

- Run UDFs in python — full `fused.run()` reference
- Tokens & endpoints — call UDFs via HTTP
- Caching — how caching works

================================================================================

## Working as a Team
Path: guide/working-with-udfs/udf-best-practices/version-control.mdx
URL: https://docs.fused.io/guide/working-with-udfs/udf-best-practices/version-control

# Working as a Team

Version control your UDFs with GitHub to share across your team and track changes over time.

**Prerequisite:** GitHub integration setup

## Committing a UDF

1. Click **Version Control** in the left panel
2. Select the UDF to commit
3. (Optional) Add a commit message or change the target repository
4. Click **Open PR**
5. Merge the PR on GitHub

[Image: Version Control Flow]

## Calling UDFs

**Saved UDF** — Personal, stored in Fused cloud. Appears in the **Saved** tab of the UDF explorer.
```python
fused.run("my_udf")
```

**Team UDF** — Version controlled, shared with team. Appears in the **Team** tab of the UDF explorer.
```python
fused.run("team/my_udf")  # tracks main
```

Pin to a commit to avoid breaking changes when `main` updates:
```python
# Point to specific commit, pinned version
✅ fused.run("https://github.com/org/repo/<COMMIT_SHA>/my_udf")

# Point to main, may break if main changes
❌ fused.run("team/my_udf")
❌ fused.run("https://github.com/org/repo/main/my_udf")
```

See all ways to reference a UDF.

## See also

- Calling UDFs — how to call UDFs from other UDFs
- Git Integration — how to integrate your own repositories with Fused

================================================================================

## Why Fused
Path: guide/working-with-udfs/why-fused.mdx
URL: https://docs.fused.io/guide/working-with-udfs/why-fused

# Why Fused

At Fused, our mission is to help get things done, fast. We want every team to be able to get from **Analytics to Action** as quickly as they can. 

We also deeply believe AI is changing the way we work. So here's Notebook LLM telling you all about Fused in 5min:

## Our core beliefs

We believe every Analytics team should have the tools to:
- 🌍 Answer the big picture problems first;
- 💡 Iterate on their analysis when new data & algorithms becomes available;
- 🏃 Ship a first version rather than getting it perfect;
- 📊 Visualize & report their work to anyone in their team.

## User Defined Functions

A lot of the tools Analytics teams have today slow the process down:
- Python dependency management gets in the way of getting work done.
- A lot of the scientific Python tooling focused more on getting the result down to 10 decimal places rather than answering the big picture

That's why we built Fused around User Defined Functions (UDFs). 

[Image: udf pipeline]

UDFs are the DNA of analytics. They are Python functions that can be called from anywhere:
- 🐍 No environment setup: Just start writing Python immediately.
- 🔗 Shareable as HTTPS endpoints in 2 clicks: Ship your work to the rest of the team
- 🔄 Iterable: Edit your code, Save, and see the results downstream immediately.
- 🚀 Scales with your hardware requirements: From running a subset of data to analyzing the entire world.

## UDFs are the DNA of analytics

Making every process of your Analytics a UDF makes it faster:

- **Data needs to be ingested constantly**: UDFs can be edited as datasets change & evolve. They get updated when you save them. 
- **New algorithms come and go**: UDFs allow you to iterate on existing data and swap out just what you need.
- **Reporting & Visualization evolve**: UDFs can take your analysis and render it in dynamic ways.

## From your laptop to the World

Look, we know that many Analytics projects start in a notebook on a laptop. 

- 💻 Start by running UDFs locally, then in 2 lines of code scale to datasets the size of the world
- 🌍 UDFs can be called from anywhere: From a notebook, a frontend application or integration platforms
- 🔀 Work locally or in Workbench, our browser based IDE interchangeably  

## Efficiently Scaling

Because Fused is built for scale:
- ☁️ Serverless computing: Only pay for the processing you actually use
- ⚡️ Caching makes recurring calls faster & cheaper

## Get started using UDFs right now

Check out:
- ⚡️ The Getting Started guide: Learn how to use UDFs in 5 minutes
- 📚 Writing UDFs: Everything you need to know about building UDFs
- 🎓 Our Examples: Real world examples of how to use UDFs

================================================================================

## Writing UDFs
Path: guide/working-with-udfs/writing-udfs.mdx
URL: https://docs.fused.io/guide/working-with-udfs/writing-udfs

[Image: udf anatomy]

Follow these steps to write a User Defined Function (UDF).

- Decorate a function with `@fused.udf`
- Declare the function logic
- Optionally cache parts of the function
- Set typed parameters to dynamically run based on inputs
- Return a vector table or raster
- Save the UDF

## `@fused.udf` decorator

First decorate a Python function with `@fused.udf` to tell Fused to treat it as a UDF.

## Function declaration

Next, structure the UDF's code. Declare import statements within the function body, express operations to load and transform data, and define a return statement. This UDF is called `udf` and returns a `pd.DataFrame` object.

```python showLineNumbers
@fused.udf # <- Fused decorator
# highlight-start
def udf(name: str = "Fused"): # <- Function declaration

    return pd.DataFrame(!']})
# highlight-end
```
The UDF Builder in Workbench imports the `fused` module automatically. To write UDFs outside of Workbench, install the Fused Python SDK with `pip install fused` and import it with `import fused`.

Placing import statements within a UDF function body (known as "local imports") is not a common Python practice, but there are specific reasons to do this when constructing UDFs. UDFs are distributed to servers as a self-contained units, and each unit needs to import all modules it needs for its execution. UDFs may be executed across many servers (10s, 100s, 1000s), and any time lost to importing unused modules will be multiplied.

An exception to this convention is for modules used for function annotation, which need to be imported outside of the function being annotated.

## `@fused.cache` decorator

Use the @fused.cache decorator to persist a function's output across runs so UDFs run faster.

```python showLineNumbers
@fused.udf # <- Fused decorator
def udf(bounds: fused.types.Bounds = None, name: str = "Fused"):

    # highlight-start
    @fused.cache # <- Cache decorator
    def structure_output(name):
        return pd.DataFrame(!']})
    # highlight-end

    df = structure_output(name)
    return df
```

## Typed parameters

UDFs resolve input parameters to the types specified in their function annotations.
This example shows the `bounds` parameter typed as `fused.types.Bounds`
and `name` as a string.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, # <- Typed parameters
    name: str = "Fused"
):
```

To write UDFs that run successfully as both `File` and `Tile`, set `bounds` as the first parameter, with `None` as its default value. This enables the UDF to be invoked successfully both as `File` (when `bounds` isn't passed) and as `Tile`. For example:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    ...
    return ...
```

### Supported types

Fused supports a wide range of parameter types for UDFs. Parameters without a specified type are handled as strings by default.

| Type | Description | Serialization Format |
|------|-------------|----------------------|
| `str` | String values | Any string value |
| `int` | Integer values | Numeric strings or integers |
| `float` | Floating point values | Numeric strings or floats |
| `bool` | Boolean values | Strings "true"/"false" (case-insensitive) |
| `list` | List of values | JSON-serialized list |
| `dict` | Dictionary of key-value pairs | JSON-serialized dict strings |
| `tuple` | Tuple of values | JSON-serialized list |
| `uuid.UUID` | UUID values | UUID strings |
| `pd.DataFrame` | Pandas DataFrame | JSON-serialized table strings |
| `gpd.GeoDataFrame` | GeoPandas GeoDataFrame | GeoJSON strings or bbox arrays |
| `shapely.Geometry` | Shapely geometry objects | WKT strings |
| `fused.types.Bounds` | Bounding box as `[minx, miny, maxx, maxy]` | Bbox array or GeoJSON |
| `fused.types.Bbox` | Bounding box as Shapely geometry | Bbox array or GeoJSON |
| `fused.types.TileXYZ` (Legacy) | Mercantile tile coordinates | Bbox array or GeoJSON |
| `fused.types.TileGDF` (Legacy) | GeoDataFrame with x/y/z tile columns | Bbox array or GeoJSON |
| `fused.types.ViewportGDF` (Legacy) | GeoDataFrame for viewport (no x/y/z) | Bbox array or GeoJSON |

The UDF Builder runs the UDF as a Map Tile if the first parameter is typed as `fused.types.Bounds`.

### `pd.DataFrame` as JSON

Pass tables and geometries as serialized UDF parameters in HTTPS calls. Serialized JSON and GeoJSON parameters can be casted as a `pd.DataFrame` or `gpd.GeoDataFrame`. Note that while Fused requires import statements to be declared within the UDF signature, libraries used for typing must be imported at the top of the file.

```python showLineNumbers

@fused.udf
def udf(
    gdf: gpd.GeoDataFrame = None,
    df: pd.DataFrame = None
):
```

## Reserved parameters

When running a UDF with `fused.run`, it's possible to specify the map tile Fused will use to structure the `bounds` object by using the following reserved parameters.

### With `x`, `y`, `z` parameters

```python showLineNumbers
fused.run("UDF_Overture_Maps_Example", x=5241, y=12662, z=15)
```

### Passing a `GeoDataFrame`
```python showLineNumbers

bounds = gpd.GeoDataFrame.from_features(,"geometry":,"id":1}]})
fused.run("UDF_Overture_Maps_Example", bounds=bounds)
```

### Passing a bounding box list

You can also pass a list of 4 points representing `[min_x, min_y, max_x, max_y]`

```python showLineNumbers
fused.run('UDF_Overture_Maps_Example', bounds=[-122.349, 37.781, -122.341, 37.818])
```

### Import functions from other UDFs

UDFs can import functions from other UDFs with `fused.load` in the UDFs GitHub repo or private GitHub repos. Here the commit SHA `05ba2ab` pins the UDF to specific commit for version control (see pinning to commit hash).

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
```

## Return values

UDFs can return the following types of objects. Fused will try to convert the returned object to the requested file format.

### Tables
- `pd.DataFrame`, `pd.Series`, `gpd.GeoDataFrame`, `gpd.GeoSeries`, `shapely.Geometry`
- Arrow-compatible objects (e.g., from DuckDB)

### Arrays
- `numpy.ndarray`, `xarray.Dataset`, `xarray.DataArray`
- `bytes` or `io.BytesIO` are treated as raster images and returned as-is with the raster MIME type.

For the default `png` format (when rendered in workbench), there are some additional limitations:

- Arrays must be 2D or higher, 1D arrays are not supported (prefer returning lists for 1D array instead).
- Rasters without spatial metadata should indicate their tile bounds

When running the UDF through `fused.run()` (or using the `npy` format), those restrictions
do not apply to `numpy.ndarray`, and numpy arrays as return value are supported in general.

### Simple Python objects
- `int`, `float`, `str`, `list`, `tuple`, `set`, `np.integer`, `np.floating`, `NoneType`

`str` is handled as HTML by default. Other types are encoded as JSON.

### Dictionaries
`dict` objects are useful for returning multiple values, e.g., dictionaries of raster numpy arrays. Dictionary values can be any of the types above, while keys must be strings.

## Save UDFs

UDFs exported from the UDF Builder or saved locally are formatted as a `.zip` file containing associated files with the UDFs code, `utils` Module, metadata, and `README.md`.

```
└── Sample_UDF
    ├── README.MD       # Description and metadata
    ├── Sample_UDF.py   # UDF code
    ├── meta.json       # Fused metadata
    └── utils.py        # `utils` Module
```

### In Python: `.to_fused()`

When outside of Workbench, save UDF to your local filesystem with `my_udf.to_directory('Sample_UDF')` and to the Fused cloud with `my_udf.to_fused()`.

This will allow you to access your UDF using a token, from a Github commit or directly importing it in Workbench from the Github URL

### In Workbench: Saving through Github

You can also save your UDFs directly through GitHub as personal, team or community UDF. Check out the Contribute to Fused to see more.

## Update tags and metadata

Modify the UDF's metadata to manage custom tags that persist across the local filesystem, the Fused Cloud, and your team's GitHub repo.

```python showLineNumbers
# Assumging my_udf was loaded or created above
my_udf.metadata['my_company:tags']=['tag_1', 'tag_2']

# Push to Fused
my_udf.to_fused()

# You can reload your UDF and see the updated metadata
fused.load('my_udf').metadata
```

## Debug UDFs

#### UDF builder

A common approach to debug UDFs is to show intermediate results in the UDF Builder runtime panel with `print` statements.

#### HTTPS requests

When using HTTPS requests, any error messages are included in the `X-Fused-Metadata` response header. These messages can be used to debug. To inspect the header on a browser, open the Developer Tools network tab.

[Image: network]

================================================================================

# QUICKSTART

## Data Analytics
Path: quickstart/data-analytics.mdx
URL: https://docs.fused.io/quickstart/data-analytics

# Data Analytics

Quick links for data analysts using Fused.

## Getting Started
- Write your first UDF
- Workbench intro
- Using AI to build

## Explore & Transform Data
- Read data
- Caching for faster iteration

## Build Visualizations
- Interactive charts
- Dashboards
- Standalone maps

## Export & Share
- Download data
- Share via tokens & endpoints
- MCP servers (AI tools)

## Examples
- Zonal stats
- Interactive charts
- Site selection analysis
- Canvas gallery

================================================================================

## Data Engineering
Path: quickstart/data-engineering.mdx
URL: https://docs.fused.io/quickstart/data-engineering

# Data Engineering

Quick links for data engineers using Fused.

## Connect Your Data
- Local files (drag & drop)
- Cloud storage (S3/GCS)
- Databases (Snowflake, BigQuery)

## Data Formats
- Reading data
- Writing data

## Turn Data into APIs
- Shared tokens & endpoints
- Calling UDFs as API
- Download options

## Execution
- Realtime execution
- Scaling out UDFs — parallel execution and dedicated instances

## Advanced Setup
- Environment variables & secrets
- Dependencies
- Git integration
- On-prem setup

## Examples
- Realtime filtering
- Dark vessel detection (AIS ingestion)

================================================================================

## Data Science
Path: quickstart/data-science.mdx
URL: https://docs.fused.io/quickstart/data-science

# Data Science

Quick links for data scientists using Fused.

## Getting Started
- Write your first UDF
- Using AI to build

## Working with Data
- Read data
- Write data

## Execution & Performance
- Caching
- Scaling out UDFs — parallel execution and dedicated instances

## Building Outputs
- Interactive charts
- Dashboards

## Examples
- Zonal stats
- PDF scraping
- Web scraping
- Currency prediction

================================================================================

## Geospatial
Path: quickstart/geospatial.mdx
URL: https://docs.fused.io/quickstart/geospatial-cheatsheet

# Geospatial

Quick links for geospatial workflows in Fused.

## Data Sources
- STAC catalogs
- Google Earth Engine
- Cloud storage (S3/GCS)

## Reading & Writing
- Reading geospatial data
- Writing geospatial data
- Data ingestion (large datasets)

## Spatial Processing
- Geospatial Single vs Tile

## H3 Hexagonal Analytics
- Why H3
- Resolution guide
- Converting to H3
- Aggregations
- Joining datasets
- H3 visualization

## Export & Integrations
- Geospatial exports (DeckGL, Felt, etc.)
- Standalone maps

## Examples
- Zonal statistics
- Climate dashboard
- Dark vessel detection
- Satellite imagery
- Canvas gallery

================================================================================

# EXAMPLES

## AIS Dark Vessel Detection
Path: examples/ais-dark-vessels.mdx
URL: https://docs.fused.io/examples/ais-dark-vessels

_A complete example show casing how to use Fused to ingest data into a geo-partitioned, cloud friendly format, process images & vectors and use UDFs to produce an analysis_

### Requirements
- Access to Fused
- Access to a Jupyter Notebook
- Installing `fused` with `[all]` dependencies (mainly to have `pandas` & `geopandas`):

```python showLineNumbers
pip install "fused[all]"
```

## 1. The problem: Detecting illegal boats

Monitoring what happens at sea isn't the easiest task. Shores are outfitted with radars and each ship has a transponder to publicly broadcast their location (using Automatic Identification System, AIS), but ships sometimes want to hide their location when taking part in illegal activities.

Global Fishing Watch has reported on "dark vessels" comparing Sentinel 1 radar images to public AIS data and matching the two to compare where boats report being versus where they _actually_ are.

In this example, we're going to showcase a basic implementation of a similar analysis to identify _potential_ dark vessels, all in Fused.

[Image: Dark Vessel Detection workflow]

Here's the result of our analysis, running in real time in Fused:

     For the nerds out there, we're using the Ground Range Detected product, not the Radiometrically Terrain Corrected because we're looking at boats in the middle of the ocean, so terrain shouldn't be any issue.
    - This dataset is available as Cloud Optimized GeoTiff through a STAC Catalog, meaning we can directly use this data as is.

</Tabs>

### 3.2 - Writing a UDF to open each AIS dataset

The rest of the logic is write a UDF to open each file, read it as a CSV and write it to parquet.

```python showLineNumbers
@fused.udf()
def read_ais_from_noaa_udf(datestr:str='2023_03_29', overwrite:bool=False):

    # This is the specific URL where daily AIS data is available
    url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler//AIS_.zip'

    # This is our local mount file path
    path=fused.file_path(f'/AIS//')
    daily_ais_parquet = f'/.parquet'

    # Skipping any existing files
    if os.path.exists(daily_ais_parquet) and not overwrite:
        print(f' exist')
        return pd.DataFrame()

    # Download ZIP file to mounted disk
    r=requests.get(url)
    if r.status_code == 200:
        with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
            with z.open(f'AIS_.csv') as f:
                df = pd.read_csv(f)
                # MMSI is the unique identifier of each boat. This is a simple clean up for demonstration
                df['MMSI'] = df['MMSI'].astype(str)
                df.to_parquet(daily_ais_parquet)
                print(f"Saved ")
        return pd.DataFrame()
    else:
        return pd.DataFrame(']})
```

We can run this UDF a single time to make sure it works:

```python showLineNumbers
single_ais_month = fused.run(read_ais_from_noaa_udf, datestr="2024_09_01")

>>> Saved /mnt/cache/AIS/2024_09/01.parquet
```

To recap what we've done so far:
- Build a UDF that takes a date, fetches a `.zip` file from NOAA's AISDataHandler portal and saves it to our UDFs' mount (so other UDFs can access it)
- Run this UDF 1 time for a specific date

### 3.3 - Run this UDF over a month of AIS data

Next step: Run this for a whole period!

Since each UDF takes a few seconds to run per date, we're going to use `fused.submit()` to call a large numbers of UDFs all at once that will each run over a single date.

With a bit of Python gymnastics we can create a `DataFrame` of all the dates we'd like to process. Preparing to get all of September 2024 would look like this:

```python showLineNumbers
# Run this locally - not in Workbench

date_ranges = pd.DataFrame()
print(f"")
print(date_ranges.head())

>>> date_ranges.shape=(30, 1)
      datestr
0  2024_09_01
1  2024_09_02
2  2024_09_03
3  2024_09_04
4  2024_09_05
```

`fused.submit()` requires inputs as a list or `DataFrame`, in which case columns should have the argument names that our UDF expects, in this case `datestr`. We also recommend you always do a first run with `debug_mode=True` to test your submit job:

```python  showLineNumbers
fused.submit(
    read_ais_from_noaa_udf, 
    date_ranges, 
    debug_mode=True
)

>>> Saved /mnt/cache/tmp/AIS/2024_09/01.parquet
  |status  | file_path
-------------------------------------------------
0 |Done    | /mnt/cache/tmp/AIS/2024_09/01.parquet

```

`debug_mode=True` runs the 1st value inside `date_ranges` with `fused.run()`. This allows you to make sure your UDF + arg_list combo is working properly. 

Now that we've seen our UDF is working we can run all 30 jobs in parallel by removing `debug_mode=True`:

```python showLineNumbers
fused.submit(read_ais_from_noaa_udf, date_ranges)
```

We've now unzipped, opened & saved 30 days of data!

`fused.submit()` has more parameters you can control allowing you to change the number of `max_workers`, `engine` or the retry policies. Also take a look at the technical docs for more details.

One handy way to make sure our data is in the right place is to check it in the Workbench File Explorer. In the search bar type: `file:///mount/AIS/2024_09/`:

[Image: Pool Runner live results]

You'll see all our daily files! Notice how each file is a few 100 Mb. These files are still big individual files, i.e. would take a little while to read.

### 3.4 - Ingest 1 month of AIS data into a geo-partitioned format

These individual parquet files are now store on our mount disk. We could save them directly onto cloud storage but before that we can geo-partition them to make them even faster to read. This will make us reduce the time it takes to access our data from minutes to seconds.
Fused provides a simple way to do this with the ingestion process.

[Image: A simple overview of Geoparquet benefits]

_Image credit from the Cloud Native Geo slideshow_

To do this we need a few things:
- **Our input dataset**: in this case our month of AIS data.
   
- **A target cloud bucket**: We're going to create a bucket to store our month of geo-partitioned AIS data in parquet files
- A target number of chunks to partition our data in. For now we're going to keep it at 500
- Latitude / Longitude columns to determine the location of each point

```python showLineNumbers
ais_daily_parquets = [f'file:///mnt/cache/AIS//.parquet' for day in range_of_ais_dates]

job = fused.ingest(
    ais_daily_parquets,
    's3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09',
    target_num_chunks=500,
    lonlat_cols=('LON','LAT')
)
```

We'll send this job to a large instance using `job.run_batch()` as we latency doesn't matter much (we can wait a few extra seconds) and we'd rather have a larger machine & a larger storage:

```python showLineNumbers
job.run_batch(
    instance_type='r5.8xlarge', # We want why big beefy machine to do the partitioning in parallel, so large amounts of CPUs
    disk_size_gb=999 # Set a large amount of disk because this job will open each output parquet file to calculate centroid
)
```

Running this in a notebook gives us a link to logs so we can follow the progress of the job on the offline machine:

[Image: Workbench run remote logs]

Following the link shows us the live logs of what our job is doing:

[Image: Workbench run remote logs]

We can once again check that our geo-partitioned images are available using File Explorer. This time because our files are a lot faster to read we can even see the preview in the map view:

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="60%" width="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/geopartioned_AIS_file_explorer.mp4"/>

Our AIS data is ready to be use for the entire month of September 2024. To narrow down our search, we now need to get a Sentinel 1 image. Since these images are taken every 6 to 12 days depending on the region, we'll find a Sentinel 1 image and then narrow our AIS data to just a few minutes before and after the acquisition time of the Sentinel 1 image.

## 4. Retrieving Sentinel 1 images

Sentinel 1 images are free & open, so thankfully for us others have already done the work of turning the archive into cloud native formats (and continue to maintain the ingestion as new data comes in).

We're going to use the Microsoft Planetary Computer Sentinel-1 Ground Range Detected dataset, because it offers:
- Access through a STAC Catalog helping us only get the data we need and nothing else
- Images are in Cloud Optimized Geotiff giving us tiled images that load even faster
- Examples of how to access the data so most of our work will be copy pasta

    Most of the following section was written in Workbench's UDF Builder rather than in Jupyter Notebooks.

    We'll have the code in code blocks, you can run these anywhere but as we're looking at images, it's helpful to have UDF Builders' live map updated as you write your code.

Let's start with a basic UDF just returning our area of interest:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    # Convert bounds to GeoDataFrame using Fused common function
    bounds = common.bounds_to_gdf(bounds)
    return bounds
```

The code above demonstrates a basic User-Defined Function (UDF) that utilizes Fused's common functions. These utilities provide a some general functions for geospatial operations and transformations. 
The utility function `bounds_to_gdf` is mentioned above is defined as:

```python
def bounds_to_gdf(bounds_list, crs=4326):

    box = shapely.box(*bounds_list)
    return gpd.GeoDataFrame(geometry=[box], crs=crs)
```

This function converts a bounding box into a GeoDataFrame .

This UDF simply returns our Map viewport as a `gpd.GeoDataFrame`, this is a good starting point for our UDF returning Sentinel 1 images

While you can do this anywhere around the continental US (our AIS dataset covered shores around the US, so we want to limit ourselves there), if you want to follow along this is the area of interest we'll be using. You can overwrite this in the UDF directly:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    # Define our specific bounds
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)
    return bounds
```

Following the Microsoft Planetary Computer examples for getting Sentinel-1 we can add a few of the imports we need and call the STAC catalog:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    # Define our specific bounds and convert to GeoDataFrame
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    return bounds
```

We already have a bounding box, but let's narrow down our search to the first week of September:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    time_of_interest="2024-09-03/2024-09-04"
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"")
    return bounds
```

This print statement should return something like:

```bash
Returned 15 Items
```

This will be the number of unique Sentinel 1 images covering our `bounds` and `time_of_interest`.

We can now use the `odc` package to load the first image & we'll use the VV polarisation from Sentinel 1 (VH could also work, and it would be good to iterate on this to visually assess which one would work best. We're keeping it simple for now, but feel free to test out both!).

We'll get an `xarray.Dataset` object back that we can simply open & return as a `uint8` array:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    time_of_interest="2024-09-03/2024-09-04"
    bands = ["vv"]

    # Convert bounds to GeoDataFrame for STAC operations
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"")

    ds = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=bands,
        resolution=10, # We want to use the native Sentinel 1 resolution which is 10m
        bbox=bounds.total_bounds,
    ).astype(float)

    da = ds[bands[0]].isel(time=0)
    image = da.values * 1.0
    return image.astype('uint8')
```

Which gives us a Sentinel 1 image over our area of interest:

[Image: Notebook run remote print]

We've simplified the process quite a bit here, you could also:
- Instead of loading `image.astype('uint8')`, do a more controlled calibration and conversation to dB
- Select a more specific image rather than the 1st one in our stack
- Use a different band or band combination
- Use Radiometrically Terrain Corrected images

### 4.1 Cleaning our Sentinel 1 UDF

Before adding any new functionality, we're going to clean our UDF up a bit more:
- Move some of the functionality into separate functions
- Adding common error catching (so our UDF doesn't fail if no Sentinel 1 images are found within our AOI + date range if it's too narrow)
- Add a cache decorator to code functions that retrieve data to speed up the UDF & reduce costs.

This will allow us to keep our UDF more readable (by abstracting code away) and more responsive. Cached functions store their result to disk, which makes a common query a lot more responsive and less expensive by using less compute

Here's our cleaned up UDF:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    da = get_data(bounds, time_of_interest, resolution, bands)

    image = da.values * 1.0
    return image.astype('uint8')

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    # Convert bounds to GeoDataFrame
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )

    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bbox,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da = ds[bands[0]].isel(time=0)
        return da
```

## 5. Simple boat detection in Sentinel 1 radar images

Now that we have a Sentinel 1 image over a Area of Interest and time range, we can write a simple algorithm to return bounding boxes of the boats in the image. We're going to keep this very basic as we're optimizing for:
- **Speed of execution**: We want our boat detection algorithm to run in a few seconds at most while we're iterating. Especially at first when we're developing our pipeline, we want a fast feedback loop
- **Simplicity**: We're focused on demo-ing how to build an end-to-end pipeline with Fused in this example, not making the most thorough analysis possible. This should be a baseline for you to build upon!

Radar images over calm water tend to look black (as all the radar signal is reflect _away_ from the sensor), while (mostly metallic) boats reflect back to the sensor appearing like bright spots in our image. A simple "boat detecting" algorithm is thus to do a 2D convolution and threshold the output to a certain pixel value:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    return convoled_image.astype('uint8')

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    # Convert bounds to GeoDataFrame
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )

    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da = ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]
```

This returns a filtered image highlighting the brightest spots and reducing the natural speckle of the radar image

[Image: Workbench run remote logs]

This is now relatively simple to vectorise (turn into a vector object, from image to polygons):

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds_gdf = common.bounds_to_gdf(bounds)
    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    gdf_predictions = vectorise_raster(
        raster=convoled_image.astype("uint8"),
        bounds=bounds_gdf,
        threshold=200 # Taking a high threshold in 0-255 range to keep only very bright spots
    )

    # Merging close polygons together into a single polygon so 1 polygon <-> 1 boat
    buffer_distance = 0.0001  # eyeballed a few meters in EPSG:4326 (degrees are annoying to work with ¯\_(ツ)_/¯)
    merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
        -buffer_distance/2
    )
    merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

    return merged_gdf

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)
    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )
    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da =  ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]: # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

@fused.cache
def vectorise_raster(raster, bounds, threshold: float = 0.5):
    from rasterio import features

    transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

    shapes = features.shapes(
        source=raster.astype(np.uint8),
        mask = (raster > threshold).astype('uint8'),
        transform=transform
    )

    gdf = gpd.GeoDataFrame(
        geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
        crs=bounds.crs
    )
    return gdf
```

And that's how we have turned our Sentinel 1 image into a vector `gpd.GeoDataFrame` of bright objects:

[Image: Workbench run remote logs]

## 6. Retrieving AIS data for our time of Interest

To get our AIS data, we now need to retrieve the exact moment our Sentinel 1 images were acquired. We can use this information to only keep AIS points within just a few minutes around that time.

```python showLineNumbers@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds_gdf = common.bounds_to_gdf(bounds)

    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    gdf_predictions = vectorise_raster(
        raster=convoled_image.astype("uint8"),
        bounds=bounds_gdf,
        threshold=200 # Using higher threshold to make sure only bright spots are taken
    )

    # Merging close polygons together
    buffer_distance = 0.0001  # eyeballed value in EPSG:4326 so need to use degrees. I don't like degrees
    merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
        -buffer_distance/2
    )
    merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

    # Keeping metadata close by for merging with AIS data
    merged_gdf['S1_acquisition_time'] = da['time'].values

    return merged_gdf

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )
    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da =  ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

@fused.cache
def vectorise_raster(raster, bounds, threshold: float = 0.5):
    from rasterio import features

    transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

    shapes = features.shapes(
        source=raster.astype(np.uint8),
        mask = (raster > threshold).astype('uint8'),
        transform=transform
    )

    gdf = gpd.GeoDataFrame(
        geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
        crs=bounds.crs
    )
    return gdf
```

`merged_gdf` now returns a column called `S1_acquisition_time` with the time the Sentinel 1 image was taken.

If we save and call this UDF with a token we can call it from anywhere, from a Jupyter Notebook or from another UDF. Let's create a new UDF in Workbench:

```python showLineNumbers
# This is a new UDF
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: int="2024-09-03/2024-09-10",
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    return s1_detections
```

This prints out:

```bash
Found 16 Unique Sentinel 1 detections
Sentinel 1 image was acquired at : 2024-09-04 00:19:09
```

We can now create another UDF that will take this `s1_acquisition_month_day_hour_min` date + a bounding box in input and returns all the AIS points in that time + area.

We're going to leverage code from the community for this part, namely reading the AIS data from a geo-partitioned GeoParquet. Fused allows us to easily re-use any code we want and freeze it to a specific commit so it doesn't break our pipelines (read more about this here)

We can use this bit of code called `table_to_tile` which will either load the AIS data or the bounding box depending on our zoom level to keep our UDF fast & responsive.

    You could write a GeoParquet reader from scratch or call a UDF that you have that already does this, you don't have to use this option. But we want to show you how you can re-use bits of code from others here.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    s1_acquisition_month_day_hour_min:str = '2024-09-04T00:19:09.175874',
    time_delta_in_hours: float = 0.1, # by default 6min (60min * 0.1)
    min_zoom_to_load_data: int = 14,
    ais_table_path: str = "s3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09", # This is the location where we had ingested our geo-partitioned AIS data
    ):
    """Reading AIS data from Fused partitioned AIS data from NOAA (data only available in US)"""

    from datetime import datetime, timedelta

    # Load the utils module
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    zoom = common.estimate_zoom(bounds)

    sentinel1_time = pd.to_datetime(s1_acquisition_month_day_hour_min)
    time_delta_in_hours = timedelta(hours=time_delta_in_hours)

    month_date = sentinel1_time.strftime('%Y_%m')
    monthly_ais_table = f"prod_/"
    print(f"")

    @fused.cache
    def getting_ais_from_s3(bounds, monthly_table):
        return common.table_to_tile(
            bounds,
            table=monthly_ais_table,
            use_columns=None,
            min_zoom=min_zoom_to_load_data
        )

    ais_df = getting_ais_from_s3(bounds, monthly_ais_table)

    if ais_df.shape[0] == 0:
        print("No AIS data within this bounds & timeframe. Change bounds or timeframe")
        return ais_df

    if zoom > min_zoom_to_load_data:
        print(f"Zoom is  | Only showing bounds")
        return ais_df

    print(f"")
    ais_df['datetime'] = pd.to_datetime(ais_df['BaseDateTime'])
    mask = (ais_df['datetime'] >= sentinel1_time - time_delta_in_hours) & (ais_df['datetime'] <= sentinel1_time + time_delta_in_hours)
    filtered_ais_df = ais_df[mask]
    print(f'')
    return filtered_ais_df
```

In workbench UDF builder we can now see the output of both of our UDF:

[Image: Notebook run remote print]

We can now see that 1 of these boats doesn't have an associated AIS point (in red).

    You can change the styling of your layers in the Visualize tab to make them look like the screenshot above

Now all we need to do is merge these 2 datasets together and keep all the boats that don't match an AIS point.

## 7. Merging the 2 datasets together

We can expand the UDF we had started in section 6. to call our AIS UDF by passing a bounding box + `s1_acquisition_month_day_hour_min`.

We'll get the AIS data and join it with the Sentinel 1 detected boats by using geopandas `sjoin_nearest` to get the nearest distance of each boat to an AIS point.

Any point with the closest AIS point >100m from the Sentinel 1 boat will be considered a potentiel "dark vessel".

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: str="2024-09-03/2024-09-10",
    ais_search_distance_in_meters: int=10,
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    @fused.cache()
    def get_ais_from_s1_date(s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds):
        return fused.run("fsh_FI1FTq2CVK9sEiX0Uqakv", s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds)

    ais_gdf = get_ais_from_s1_date()

    # Making sure both have the same CRS
    s1_detections.set_crs(ais_gdf.crs, inplace=True)

    # Buffering AIS points to leverage spatial join
    ais_gdf['geometry'] = ais_gdf.geometry.buffer(0.005)

    joined = s1_detections.to_crs(s1_detections.estimate_utm_crs()).sjoin_nearest(
        ais_gdf.to_crs(s1_detections.estimate_utm_crs()),
        how="inner", # Using left, i.e. s1 as keys
        distance_col='distance_in_meters',
    )

    # Dark vessels will be unique S1 points that don't have an AIS point within 10m
    potential_dark_vessels = joined[joined['distance_in_meters'] > ais_search_distance_in_meters]
    print(f"Found  potential dark vessels")

    # back to EPSG:4326
    potential_dark_vessels.to_crs(s1_detections.crs, inplace=True)
    return potential_dark_vessels
```

And now we have a UDF that takes a `time_of_interest` and bounding box and returns potential dark vessel:

[Image: potential dark vessel detection]

## Limitations & Next steps

This is a simple analysis, that makes a lot of relatively naive assumptions (for ex: all bright spots in SAR are boats for example, which only works in open water and not near the shore or around solid structures like ocean wind mills or oil rigs). There's a lot of ways in which it could be improved but provides a good starting point.

This could be improved in a few ways:
- Masking out any shore or known areas with static infrastructure (to limit potential false positives around coastal wind mill farms)

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/false_positive_wind_mills.mp4" width="80%" />
Example of the Block Island Wind Farm in Rhode Island showing up as false positive "potential dark vessel": Wind mills appear as bright spots but don't have any AIS data associated to them
(`bounds=[-71.08534386325134,41.06338103121641,-70.89011235861962,41.15718153299125]` & `s1_acquisition_month_day_hour_min = "2024-09-03T22:43:33"`)

- Using a more sophisticated algorithm to detect boats. The current algorithm is naive, doesn't return bounding boxes but rather general shapes.
- Return more information derived from AIS data: Sometimes boats go dark for a certain period of time only, making it possible to tie a boat that was dark for a certain time to a known ship when it does have it's AIS on.
- Running this over all the coast of the continental US and/or over a whole year. This would be closer to the Global Fishing Watch dark vessel detection project.

If you want to take this a bit further check out:
- Running UDFs at scale with `run_batch()`. You could use this to run this pipeline over a larger area or over a much longer time series (or both!) to find out more potentiel dark vessels
- More about Fused core concepts like chosing between running UDFs based on Tile or File

================================================================================

## Canvas Gallery
Path: examples/canvas-gallery.mdx
URL: https://docs.fused.io/examples/canvas-gallery

# Canvas Gallery

Explore example Fused Canvases to see what's possible with Fused.

================================================================================

## Climate Dashboard
Path: examples/climate-dashboard.mdx
URL: https://docs.fused.io/examples/climate-dashboard

# Building a Climate dashboard

We're going to build an interactive dashboard of global temperature data, after processing 1TB of data in a few minutes!

### Install `fused`

```python
pip install "fused[all]"
```

Read more about installing Fused here.

In a notebook:

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the link to authenticate.

Read more about authenticating in Fused.

Each file being about 140MB a quick back of the envelope calculation gives us:
```python
recent_days = [day for day in available_days if day.split('datestr=')[1][:7] in recent_months]
len(recent_days) * 140 / 1000 # size in GB of files we'll process
```

```bash
1005.62
```
</details>

Fused allows us to run a UDF in parallel. So we'll process 1 month of data across hundreds of jobs:

```python
results = fused.submit(
  udf, 
  recent_months, 
  max_workers=250, 
  collect=False
)
```

See a progress bar of jobs running:

```python
results.wait()
```

See how long all the jobs took:

```python
results.total_time()
```

```bash
>>> datetime.timedelta(seconds=40, ...)
```

**We just processed 20 years of worldwide global data, over 1TB in 40s!!**

We can now write a UDF that gets all the monthly data and aggregates it by month:

```python
@fused.udf(cache_max_age='0s')
def udf():

    # Listing all files on our mount directory
    monthlys = fused.api.list(fused.file_path(f"monthly_climate/"))
    file_list = "', '".join(monthlys)
    
    result = duckdb.sql(f"""
       SELECT 
           LEFT(datestr, 7) as month,
           ROUND(AVG(daily_mean_temp), 2) as monthly_mean_temp
       FROM read_parquet([''])
       GROUP BY month
       ORDER BY month
    """).df()

    return result
```

Instead of running this locally, we'll open it in Workbench, Fused's web-based IDE:

```python
# Save to Fused
udf.to_fused("monthly_mean_temp")

# Load again to get the Workbench URL
loaded_udf = fused.load("monthly_mean_temp")
```

Return `loaded_udf` in a notebook and you'll get a URL that takes you to Workbench:

```python
loaded_udf
```

Click on the link to open the UDF in Workbench. Click "+ Add to UDF Builder" 

[Image: Monthly temperature aggregation in Workbench]

### Interactive graph (with AI)

You can use the AI Assistant to help you vibe code an interactive timeseries of your data

Simply ask the AI:

```
Make an interactive graph of the monthly temperature data
```

You can then share your graph:
- Save (`Cmd + S` on MacOS or click the "Save" button)
- Click "URL" button to see deployed graph!

Any time you make an update, your graph will automatically update!

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/sharing_grpah.mp4" 
  width="100%" 
/>

================================================================================

## Creating Charts
Path: examples/creating-charts.mdx
URL: https://docs.fused.io/examples/creating-charts

# Interactive Charts

Create interactive charts from any data with AI assistance.

### Prompts Used

```text
Open this dataset as a table and return the first 10 rows: https://survey.stackoverflow.co/datasets/stack-overflow-developer-survey-2024.zip
```

```text
Looking at the dataset, how popular was Python in 2024? Give me the percentage of Python users and total user count
```

```text
How does this compare to the previous year?
```

```text
Show me the evolution of Python popularity over the last 5 years in a table
```

```text
Make this into a simple interactive dashboard. Show only the Python percentage users and total user count into a nice beautiful graph. Make sure to center the y-axis at 0.
```

---

## More Examples

- Climate Dashboard — processing 1TB of weather data
- Analytics is Changing (Again) — blog post on vibe coding

================================================================================

## Currency Prediction
Path: examples/currency-prediction.mdx
URL: https://docs.fused.io/examples/currency-prediction

# Currency Trading Prediction

Get the last 30 days of data for USD, EUR & GBP run a linear regression in Python to predict the price of each currency in the next week

### Prompts

```text
Fetch & Compare the historical daily price of USD, EUR & GBP over the last 30 days as a table
```

```text
I'm interested in buying currencies at the best time. Make a prediction over the prices of each currency on the next 7 days and show me that on a graph with each currency as a different color. Return data in a dataframe for me to view in Table first
```

```text
Based on the prediction make a dashboard that gives me a recommendation for each currency whether to buy or sell. Show these 2 recommendation & the historical + prediction graph
```

### Additional Resources

- Docs page on AI Assistant
- YouTube playlist of vibe coded examples

================================================================================

## Maxar Satellite Imagery
Path: examples/maxar-satellite-imagery.mdx
URL: https://docs.fused.io/examples/maxar-satellite-imagery

_A guide showing how to use Fused to get all of Maxar's Open Data from all the available STAC catalogs and explore the imagery_

### Requirements
- Access to Fused

## Summary 

Working with Fused UDFs also give you the option to easily use functions defined in other UDFs. In practice this means we've created a `common` public UDF that contains some functions we've found useful when working with any type of data.

You can explore it yourself by directly reading the code in Github. If you see any functions you'd like to use, we strongly recommend you use `fused.load()` and pass the latest commit hash at the time you want to use it (see pinning to commit hash):

```python
commit_hash = "fbf5682" # Latest commit hash of https://github.com/fusedio/udfs/blob/main/public/common at time of writing
common = fused.load(f"https://github.com/fusedio/udfs/tree//public/common/")
```

Each Maxar Event itself contains multiple collections. We created a simple function that loops over all the available `UNIQUE_ID/collection.json`, reads them an appends them into a single GeoDataFrame:

Looking at the `WildFires-LosAngeles-Jan-2025/collections.json` file:
```json
,

    ],
    "extent": ,
        "temporal": 
    },
    "title": "Los Angeles Wildfires 2025",
    "description": "Driven by strong Santa Ana winds, multiple wildfires are burning in the Los Angeles, California, area. More than 40,000 acres and more than 12,300 structures have burned; at least 19 people have died.",
    "license": "CC-BY-NC-4.0"
}
```

So we create `stac_to_gdf_maxar` to:
- Loop over all the `UNIQUE_ID/collection.json` files
- Read each `collection.json` file
- Extract the metadata & extent of each collection 
- Convert the `extent` into a GeoDataFrame
- Concat all into a single GeoDataFrame

Once again, you can directly read the code in Github to see exactly how we do this

</details>

You can easily rename your UDFs in Workbench. Rename this UDF to `Maxar_Open_Data_STAC_single_catalog` so we can call it later directly by name.

Make sure to save your UDF with `Cmd + S` (or `Ctrl + S` on Windows / Linux) or in the Workbench UI for these changes to take effect.

<ReactPlayer className="video__player" playing= muted= controls height="100%" width="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/maxar_catalog.mp4" />

And here we get all the images for the Los Angeles Wildfires 2025 event:

[Image: Single collection Maxar STAC]

## Aggregating all available data

### Getting all `events`

To be able to explore all of Maxar's Open Data Program we now need to run this specific UDF over all the available events. 

We'll do this in 2 steps:
- Fetch all the event names 
- Use `fused.submit()` to fetch all the STAC Collections for each event name

```python showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    print(f"")

    return collections
```

Let's break this UDF down:
- We're using `@fused.cache` to cache the Catalog request so our UDF doesn't need to do this request each time we execute it. It prevents being rate limited and doing the same request over and over against the same endpoint
- We're returning a list (`collections`) so if you run this in Workbench you'll notice nothing shows up on the map! That's also why we print the first 5 rows. 

Read through the Best Practices for more handy tips on how to write efficient and easy to debug UDFs

This UDF returns a list of all the available event names currently accessible through Maxar's Open STAC Catalog:

```python 
>>> print(f"")
['BayofBengal-Cyclone-Mocha-May-23', 'Belize-Wildfires-June24', 'Brazil-Flooding-May24', 'Cyclone-Chido-Dec15', 'Earthquake-Myanmar-March-2025']
```

[Image: Maxar STAC Events]

### Preparing `fused.submit()` to run in parallel

We're going to use `fused.submit()` to run our first UDF in parallel. To do this we need a few things:
- Prepare our inputs (in this case the name of all the `events`). We recommend doing this as a dataframe as it's simple to read & work with
- Pass our first UDF to `fused.submit()`

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        debug_mode=True # Using debug to run just the 1st event at first
    )
    print(f"")

    return dfs_out
```

Let's unpack this:
- We're calling the UDF called `"Maxar_Open_Data_STAC_single_catalog"` that we renamed earlier over `collections_df`. At the time of writing this example this represents 46 events.
- We use `fused.submit(..., debug_mode = True)` to run only the 1st value from `collections_df`. This allows us to test that our `fused.submit()` job is written correctly. 

`fused.submit()` allows you to run a single over a list / dataframe of inputs in parallel. Under the hood Fused spins up many realtime instances (see technical docs for details) that will each run the given UDF (in this case `"Maxar_Open_Data_STAC_single_catalog"`) all at the same time.

This is a powerful way to scale a process with just a single function call.

Read the dedicated Docs section on `fused.submit()` for more

[Image: Maxar submit debug mode]

### Getting all Maxar open data

Once we're confident that our `fused.submit()` job setup is correct, we can remove `debug_mode=True` (it's set to `False` by default) and run our UDF across all events.

We can also increase the number of `max_workers`, as we have 46 events and the default `max_workers` is set to 32. We can ask Fused server to thus spin up more instances for us so this parallel job is even faster:

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        max_workers=50, # Increasing the number of max_workers as we have more than events than the default value
    )
    print(f"")

    return dfs_out
```

After a few seconds, we get back a `GeoDataFrame` containing all the Maxar open data STAC catalogs:

[Image: Maxar submit all STACs]

This allows us to do a few different things:
- Explore _all_ of the available Maxar Open Data on a map directly. This helps us see what data Maxar has available that might be of interest, to compare image quality across areas for example. 
- Offer a wide variety of high resolution imagery to query against. For example retrieving as much the cloud free imagery as possible

## Choosing 1 Event to display

With access to all the images from Maxar, we can navigate the map and choose any image we'd like to display. Let's select one and display it in Workbench.

First we can use the Runtime Tab to find the URL of an image we'd like to display:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/maxar_choosing_image2.mp4" width="100%" />

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = True
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")  
        tiles = common.get_tiles(bounds)
    
        arr = common.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

Unpacking this UDF:
- This UDF takes :
    - a `bounds` object. This allows us to pass the current Workbench Map Viewport to our UDF
    - `path` represents the path on S3 to one of the images we want to display
    - `chip_len`: The size of the chip size we'd like our image to display in

These images can be loaded using `bounds` and Tile mode because Maxar has provided these images as Cloud Optimized Geotiffs. This allows us to leverage their tiles & overviews and only load the data we need as we pan around the map

We can check this by reading the metadata in CLI with `gdalinfo` and see that each band has `Block`, meaning is tiled:

```bash  
gdalinfo /vsis3/maxar-opendata/events/Cyclone-Chido-Dec15/ard/38/300200022120/2024-06-11/104001009713BA00-visual.tif

>>>
Driver: GTiff/GeoTIFF

...

Band 1 Block=512x512 Type=Byte, ColorInterp=Red
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 2 Block=512x512 Type=Byte, ColorInterp=Green
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 3 Block=512x512 Type=Byte, ColorInterp=Blue
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
```

Running the above UDF we can for now return the extent of the image:

[Image: maxar return img extent]

This allows us to introduce 2 concepts in Workbench:
- 1. Zoom to layer
- 2. Tile / File modes

### 1. Setting a default view in Workbench

After getting the extent of our image, we're going to Zoom to layer and set this view as the default view:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/zoom_to_layer_default_view2.mp4" width="100%" />

This allows us to to any change we want to this UDF or pan anywhere on the map and always be able to zoom back to this default view!

### 2. Displaying the image

Now we can edit our UDF to return the image by changing `display_extent` to `False`:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = False
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")      
        tiles = common.get_tiles(bounds)
    
        arr = common.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

This change returns the image instead of the extent, but it returns the image all at once, because Workbench is set to "Single" mode by default

[Image: File mode array return]

If you reproduce this yourself and pan around the map you'll notice:
- We see the whole image but with a relatively low resolution.
- Nothing changes as we pan around the map (resolution doesn't change)

We can change this by setting Workbench to "Tile" mode:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/workbench_image_switching_to_tile_video2.mp4" width="100%" />

Under the hood, switching to "Tile" mode tells Workbench to run this UDF not only 1 times, but by breaking it up into Mercantile tiles. This is why you see the image being broken up into a grid of tiles.

These different modes don't change _what_ code is being executed, as our `udf` didn't change. It's only changing what geospatial parameters are being passed:
- "File" mode passes the `bounds` of the current viewport (run 1 time§)
- "Tile" mode passes the `bounds` of the current viewport broken up into a grid of tiles (run 1 per each tile)

## Next steps

We've shown you how to:
- Use `fused.submit()` to run a UDF in parallel
- Use `@fused.cache` to cache requests to reduce costs and improve performance
- Use Workbench to display images in different modes

If you want to go a bit further you could:
- Explore the Best Practices to make the most of UDFs or learn handy tips to use Workbench as its fullest
- Go more in depth with "File" & "Tile" modes in Workbench
- Dive into the different ways Fused allows you to use caching to improve performance

================================================================================

## PDF Scraping
Path: examples/pdf-scraping.mdx
URL: https://docs.fused.io/examples/pdf-scraping

This tutorial will show you how to:
- Search for PDFs online directly in Fused
- Turn tables in PDFs into DataFrames

_Left: Original PDF. Right: DataFrame of PDF in Workbench_

Try out a hosted app for yourself:

## Searching for PDFs

You can search for PDFs directly in Fused:

```text
Search for any PDFs from Census data that would provide me health insurance coverage & type by age. 
I want the data to be available in a table format.
```

The AI Assistant will provide a list of links to PDFs that are as close to your request as possible:

In its current stage we recommend you explore the PDFs for yourself to see which one matches your request best.

The first link, pointing to Census `.gov` data seems to match our request. Opening it and going to the Appendix we find what we're looking for:

## Turning PDF Tables in DataFrames

Next:
- Use the Datalab API to turn a PDF into a JSON
- Write a UDF that turns the JSON into a DataFrame

### Using a preexisting UDF

We built a UDF on top of the Datalab API that you can directly use:

```python
# in a notebook

df = fused.run(
    "fsh_1IH3QxpoAIEz9qtUChfqtS",
    pdf_url="https://www2.census.gov/library/publications/2024/demo/p60-284.pdf",
    raw_table_idx=0
)

print(df)
```

This will _not_ return the table we're looking for. This is a simple UDF that returns each table found in the JSON from DataLab as a DataFrame

The print message will however show us all the available tables found:

```

--- Raw Table 0 ------------------------------------------------
Shape: (14, 5)
          Unnamed: 0   2022     2022.1   2023     2023.1
       Coverage type    NaN  Margin of    NaN  Margin of
                 NaN Number error1 (±) Number error1 (±)
               Total 330000        130 331700        145
     Any health plan 304000        746 305200        704
Any private plan2, 3 216500       1399 216800       1294
------------------------------------------------------------

--- Raw Table 1 ------------------------------------------------
Shape: (35, 10)
    Unnamed: 0  Total              Total.1              Total.2              Total.3              Total.4              Total.5              Total.6     Total.7 Unnamed: 9
           NaN    NaN Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance         NaN        NaN
Characteristic    NaN                  NaN                  NaN                  NaN       Private health                  NaN        Public health  Uninsured4 Uninsured4
           NaN    NaN                  NaN                  NaN                  NaN           insurance2                  NaN           insurance3  Uninsured4 Uninsured4
           NaN    NaN                  NaN            Margin of                  NaN            Margin of                  NaN            Margin of         NaN  Margin of
           NaN Number              Percent           error1 (±)              Percent               error1          (±) Percent               error1 (±) Percent error1 (±)
------------------------------------------------------------

--- Raw Table 2 ------------------------------------------------
Shape: (35, 11)
    Unnamed: 0  Total Total.1    Total.2 Total.3              Total.4       Total.5       Total.6     Total.7 Unnamed: 9  Unnamed: 10
           NaN    NaN     NaN        NaN     NaN Any health insurance           NaN           NaN         NaN        NaN          NaN
Characteristic    NaN     NaN        NaN     NaN       Private health Public health Public health  Uninsured4 Uninsured4          NaN
           NaN    NaN     NaN        NaN     NaN           insurance2           NaN    insurance3  insurance3        NaN          NaN
           NaN    NaN     NaN  Margin of     NaN            Margin of           NaN     Margin of         NaN  Margin of          NaN
           NaN Number Percent error1 (±) Percent               error1   (±) Percent        error1 (±) Percent error1 (±)          NaN
------------------------------------------------------------

--- Raw Table 3 ------------------------------------------------
Shape: (29, 11)
    Unnamed: 0  Total              Total.1              Total.2              Total.3              Total.4              Total.5              Total.6     Total.7 Unnamed: 9  Unnamed: 10
           NaN    NaN Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance         NaN        NaN          NaN
Characteristic    NaN                  NaN                  NaN                  NaN       Private health        Public health        Public health  Uninsured4 Uninsured4          NaN
           NaN    NaN                  NaN                  NaN                  NaN           insurance2                  NaN           insurance3  insurance3        NaN          NaN
           NaN    NaN                  NaN            Margin of                  NaN            Margin of                  NaN            Margin of         NaN  Margin of          NaN
           NaN Number              Percent           error1 (±)              Percent               error1          (±) Percent               error1 (±) Percent error1 (±)          NaN
------------------------------------------------------------

--- Raw Table 4 ------------------------------------------------
Shape: (30, 11)
    Unnamed: 0                Total              Total.1              Total.2              Total.3              Total.4              Total.5       Total.6     Total.7    Total.8  Unnamed: 10
           NaN Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance Any health insurance           NaN         NaN        NaN          NaN
           NaN                  NaN                  NaN                  NaN                  NaN       Private health                  NaN Public health  Uninsured4 Uninsured4          NaN
Characteristic                  NaN                  NaN                  NaN                  NaN           insurance2                  NaN    insurance3  insurance3        NaN          NaN
           NaN                  NaN                  NaN            Margin of                  NaN            Margin of                  NaN     Margin of         NaN  Margin of          NaN
           NaN               Number              Percent           error1 (±)              Percent               error1          (±) Percent        error1 (±) Percent error1 (±)          NaN
------------------------------------------------------------
```

It looks like the table 3 matches what we're looking for:

```python

df = fused.run(
    "fsh_1IH3QxpoAIEz9qtUChfqtS",
    pdf_url="https://www2.census.gov/library/publications/2024/demo/p60-284.pdf",
    raw_table_idx=3 # Table 3, one matching our request
)

print(df)
```

Which returns the following DataFrame:

You may notice the table has some formatting issues. These can be fixed relatively easily by asking the AI Assistant to fix them.

### Build it yourself

You can make a copy of the UDF from the UDF Explorer

You will need to:
- Setup an account on DataLab
- Get an API Key
- Add `datalab` as a secret in the Preferences

And now you get edit this UDF for yourself:

## Next Steps

Now that you have a DataFrame you can explore how to build on top:
- Ask AI to visualize the data
- See how to turn any of your UDFs into an API endpoint
- Let anyone talk to this data through MCP Servers

================================================================================

## Realtime Filtering with DuckDB
Path: examples/realtime-filtering-duckdb.mdx
URL: https://docs.fused.io/examples/realtime-filtering-duckdb

# Realtime data filtering with DuckDB WASM

This tutorial will show you how to build a real time filtering visualization using DuckDB WASM:

Open map in a new tab

This map:
- Fetches data from a Fused UDF (which aggregates & cleans data into hex5), uses DuckDB WASM to filter it and renders the output on a map
- Data comes from Source Cooperative based on 2024 USDA Crop Data Layer

This is how to build this step by step:

## 1. Querying data with DuckDB WASM

The simplest way to use DuckDB in the browser is to use DuckDB WASM to query a hosted file. 

You can directly edit the SQL query directly here:

This UDF:
- Signs S3 once so browser can fetch it
- Loads parquet as view `df`
- Auto-runs query as you type (small debounce)
- 'Run' button stays disabled until data is ready and becomes optional

1. Imports, signing the S3 URL and HTML template start

```python
common = fused.load("https://github.com/fusedio/udfs/tree/6b98ee5/public/common/")
@fused.udf(cache_max_age = 0)
def udf(
    data_url: str = "s3://fused-sample/demo_data/airbnb_listings_nyc.parquet",
    initial_sql: str = """ SELECT * from df """
):
    """
    DuckDB-WASM SQL viewer with VIRTUALIZED table:
    - Loads parquet as view `df`
    - Auto-runs query as you type
    - Table renders only visible rows for performance
    """
    signed_url = fused.api.sign_url(data_url)

    html = f"""<!DOCTYPE html>

```

Key points:

– `fused.api.sign_url` gives a temporary public link; the HTML string is built with an f-string so the signed URL can be interpolated.

2. UI elements (textarea, button, status, result container)

```python

```

Key points 

– The query is auto-executed after a short pause (250 ms) once the user stops typing, but a manual “Run” button and a keyboard shortcut are also provided.

Link to code

Then call this UDF directly in DuckDB WASM:

Link to code

**1️⃣ Load the shared helper library**

```python
common = fused.load("https://github.com/fusedio/udfs/tree/6b98ee5/public/common/")
```

**2️⃣ UDF definition & parameters**

```python
@fused.udf
def udf(
    data_url: str = "https://www.fused.io/server/v1/realtime-shared/UDF_Airbnb_listings_nyc_parquet/run/file?dtype_out_raster=png&dtype_out_vector=parquet",
    initial_sql: str = """\
    SELECT 
      room_type,
      ROUND(AVG(price_in_dollar), 2) AS avg_price,
      COUNT(*) AS listings
    FROM df
    WHERE price_in_dollar IS NOT NULL AND room_type IS NOT NULL
    GROUP BY room_type
    ORDER BY avg_price DESC;"""
):
```
- `data_url` – where DuckDB will read the Parquet file from.
- `initial_sql` – the starter query placed in the textarea.

**3️⃣ Docstring (high-level description)**

```python
    """
    DuckDB-WASM SQL viewer :
    - Loads parquet from a public/accessible URL as view `df`
    - Auto-runs query as you type (small debounce)
    - Run button disabled until data is ready
    """
```

**4️⃣ HTML skeleton (head, style & body)**

```python
    html = f"""<!DOCTYPE html>

```
The HTML places the UI elements on the page and injects the `data_url` and `initial_sql` values.

**5️⃣ JavaScript – DuckDB-WASM loading & data registration**

```javascript

```

Key points 

– The query is auto-executed after a short pause (250 ms) once the user stops typing, but a manual “Run” button and a keyboard shortcut are also provided.

Link to code

1️⃣ Imports & UDF signature

```python
common = fused.load(
    "https://github.com/fusedio/udfs/tree/abf9c87/public/common/"
)

@fused.udf
def udf(
    data_url: str = "https://www.fused.io/server/v1/realtime-shared/…/run/file?dtype_out_raster=png&dtype_out_vector=parquet",
    mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMi…",
    center_lng: float = -73.9857,
    center_lat: float = 40.7484,
    zoom: int = 9,
    initial_sql: str = "SELECT * FROM df LIMIT 100;",
):

```

common = fused.load(...) – loads a small helper library that provides html_to_obj, which later converts the raw HTML string into an object that the Workbench can render.
The UDF parameters let the user change the data source, Mapbox token, map centre, zoom level and the default SQL query without editing the code.

2️⃣ The HTML template (client-side UI)

```python
html = f"""<!DOCTYPE html>

  …

## Real world example: Visualizing Crops in the USA

Data: USDA Crop Data Layer (2024)
From: Source Cooperative
Format: Fused repartitioned this into H3 hexagons as hex7

```bash
https://source.coop/fused/hex/release_2025_04_beta/cdl/hex7_2024.parquet
```

### Processing Data with DuckDB

Before we display the data we want to:
- Remove any nodata (`0`) in CDL
- Calculate the percentage that each crop type makes up of the total area of each hex8 cell 
- Return `lat / lon` coordinates for each hex8 cell to be able to display the data on a map

We can write a simple UDF to do this for us:

Link to code

This returns a dataframe directly. We can then pass this data to another UDF to render it on a map.

## Live filtering on Map

Link to code

Here's how the map is built:

1. Data Source: Fetches Parquet from previous UDF
2. Browser Database: Loads into DuckDB WASM with H3/spatial extensions
3. Real-time Filtering: Applies SQL WHERE conditions on user input
4. Geometry Generation: Converts H3 cells to hexagon boundaries
5. Map Visualization: Renders colored hexagons with tooltips
6. Live Statistics: Shows count, area, and averages for filtered data

### Explore on canvas

You can see this tutorial as a Fused Canvas:

Open in a new tab

## Want to try it yourself?

Sign up for a free Fused account and build on top of any of the User Defined Functions directly in Fused!

================================================================================

## Sharing Canvas Dashboards
Path: examples/sharing-canvas-dashboards.mdx
URL: https://docs.fused.io/examples/sharing-canvas-dashboards

# Create Interactive Dashboards

Any Canvas can be turned into a shareable dashboard by clicking the "Share" button in the top right corner.

[Image: Share modal]

This gives you a few options:

- **Embed mode** — Hide all UI chrome for embedding in external sites
- **Lock view** — Prevent panning and zooming
- **Show outline** — Show outline panel in sidebar
- **Show code panel** — Show code/AI panel when a node is selected

For example, selecting "Embed mode" allows you to `iframe` a canvas as a dashboard:

<iframe
  src="https://unstable.fused.io/canvas/fc_4Dr6z6OuYHboSbs3VbpdEc?embed=true"
  style=}
  allowFullScreen
  title="Interactive Dashboard Example"
/>

## Best Practices

- Use interactive components (see the Standalone Maps reference)
- Add "Text" elements to annotate your dashboard

================================================================================

## Site Selection Analysis
Path: examples/site-selection-analysis.mdx
URL: https://docs.fused.io/examples/site-selection-analysis

This tutorial will:
- Explore 2 store chains in Sacramento, CA
- Create drive-time catchments around each store
- Join catchments with census data (population, income)
- Determine which stores best reach target populations with minimal competition 

This tutorial shows the prompts we use to build this analysis.

Since AI is non-deterministic, prompts may not always work identically. You might need to manually adjust prompts or code.

Consider prompts as starting points, not final answers.

## Getting Data

We'll use these online datasets for this analysis:
- Starbucks point locations as our target stores. Available on Kaggle
- McDonald's point locations as our competitor stores. Available on Kaggle
- Census Data to get population and average income of the area around each store.

### Starbucks & McDonald's locations

Ask the AI to write a UDF loading data from the Kaggle cURL API:

[Image: Downloading Starbucks data from Kaggle]

```text
Open this specific dataset and return points as a dataframe based on this Kaggle curl request:

#!/bin/bash
curl -L -o ~/Downloads/store-locations.zip\
  https://www.kaggle.com/api/v1/datasets/download/starbucks/store-locations
```

To simplify this tutorial, we've pre-downloaded the data:

- Starbucks: `s3://fused-sample/demo_data/catchment_analysis/starbucks_location.pq`
- McDonald's: `s3://fused-sample/demo_data/catchment_analysis/mcdonalds_location.pq`

### Census Data

Get Census data from the official US Census website.

For simplicity, we've pre-downloaded Census Block Groups (BG) data:

- Sacramento, CA Census Block Groups: `s3://fused-sample/demo_data/catchment_analysis/acs_bg_ca_2022_sacramento_geoparquet.parquet`

## Exploring Data in Fused

Create 1 UDF per dataset to explore them as individual layers on the Map View. 

### Starbucks

Create a new UDF and ask the AI:

```text
can you open this starbucks location file (s3://fused-sample/demo_data/catchment_analysis/starbucks_sacramento.pq) and return as geodataframe 
```

### McDonald's

Create another UDF and ask the AI:

```text
can you open this macdonalds location file (s3://fused-sample/demo_data/catchment_analysis/mcdonalds_sacramento.pq) and return as geodataframe 
```

### Census Data

Create another UDF and ask the AI:

```text
Can you open this file using geopandas as return df s3://fused-sample/demo_data/catchment_analysis/acs_bg_ca_2022_sacramento_geoparquet.parquet
```

After renaming each UDF and changing store colors in the Visualization Tab:

You can copy paste these JSONs into the Visualization Tab to have similar colored stores:

**Starbucks**

```
,
    "getFillColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```

**McDonalds**

```
,
    "getFillColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```

</details>

[Image: Stores and Census Block Groups in Sacramento]

## Creating isochrones

The Fused AI Assistant uses other UDFs as inspiration to adapt existing code for new use cases.

We'll use the Get Isochrone UDF as AI inspiration. This UDF:
- Uses the Valhalla API to create drive-time isochrones around points
- Returns a GeoDataFrame with isochrone polygons

Perfect for our Starbucks analysis! We need:
- 15-minute drive time (`auto` costing mode)

Since this API is slow, we'll break down the problem:
1. Use `Get_Isochrone` UDF for 3 points first
2. Validate isochrones look correct
3. Manually increase API calls

Steps:
- Open the `Get_Isochrone` UDF in Workbench (no copy needed)
- Go to your `starbucks_location` UDF chat
- Include `@Get_Isochrone` in your AI request for context

Ask the AI (in the chat for your `starbucks_location` UDF):
```text
Use @Get_Isochrone to create 15-minute driving isochrones around each starbucks location. 

Test this out with just 3 points first so we make sure this is working correctly at first
```

You'll see 3 isochrones on the map:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" width="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/catchments_from_get_isochrone_compressed2.mp4" />

You can manually increase the API calls if desired. For simplicity, we provide the output file:

`s3://fused-sample/demo_data/catchment_analysis/starbucks_isochrones_15min.pq`

## Joining catchments & census data

Next, we'll:
- Take each store's **15-minute catchment polygon** and find intersecting **block groups**
- From intersecting BGs, compute catchment metrics:
    - **Population** sum inside catchment
    - **Median income**
    - **Block group count** (number of intersecting BGs)

In a new UDF, ask the AI:

```text
Write a UDF that intersects each store’s 15-minute catchment polygon (from catchment_udf) with Sacramento block groups.  
For each store, use gpd.overlay to compute:  
- Total population (sum across intersecting block groups)  
- Median household income (median of intersecting block groups)  
- Block group count (distinct GEOIDs touched)  
Aggregate results per store into a clean summary table and return that.

The block groups for sacramento are here: `s3://fused-sample/demo_data/catchment_analysis/sacramento_block_groups.pq`

The starbucks catchments are here: `'s3://fused-sample/demo_data/catchment_analysis/starbucks_isochrones_15min.pq'`

use 4326 projection for all
Return the catchment geometries so I can still visualize the gdf on a map
```

## Competitor Analysis

In the same UDF, ask the AI for competitor analysis:

```text
Enhance the catchment UDF to measure competition:  
- Load McDonald’s store locations from  
  `s3://fused-sample/demo_data/catchment_analysis/mcdonalds_sacramento.pq`.  
- For each Starbucks 15-minute catchment polygon, count how many McDonald’s points fall inside (point-in-polygon).  
- Add this as a new column `Competition` to the per-store summary table.  
How many mcdonalds (competitor to starbucks in this case) are present in each catchment?
```

Track how many of *our own stores* are in each catchment:

```text
Enhance the catchment UDF to measure cannibalization:  
- Load Starbucks store locations from  
  `s3://fused-sample/demo_data/catchment_analysis/starbucks_sacramento.pq`.  
- For each Starbucks 15-minute catchment polygon, count how many *other* Starbucks points fall inside (exclude the store itself).  
- Add this as a new column `Cannibalization` to the per-store summary table.  
```

## Ranking stores

Rank stores based on:
- Highest catchment population
- Highest median income
- Lowest competition
- Lowest cannibalization (i.e. number of our own stores)

So let's ask the AI:

```text
Based on all the info here, give me a ranking of which stores I should invest in the most. I care about starbucks stores that have:
- The highest income 
- Highest population
- Least amount of competitors
- Least amount of cannibilization

Create a ranking formula for Starbucks stores based on these criteria
```

Ask the AI to plot a chart. When your UDF returns a dataframe, AI chat suggests chart creation:

[Image: AI suggesting to create a chart from the dataframe]

This visualizes the results:

[Image: Starbucks location ranking showing population, income, competition and cannibalization metrics]

The AI ranking shows top stores have:
- Highest income
- Lowest population

Since all AI output is Python code, you can examine and fine-tune the ranking logic!

## Next Steps

Improvement ideas:
- Fine-tune ranking, drive times, etc.
- Add more data sources
- Scale to entire US using `fused.submit()`

================================================================================

## Standalone HTML Maps
Path: examples/standalone-html-maps.mdx
URL: https://docs.fused.io/examples/standalone-html-maps

# Create Standalone Maps

In Fused Canvas you can create workflows by connecting UDFs together and visualizing them on a map.

We've created a simple UDF to serve as a map utils library to help you create standalone maps.

## How to visualize your UDF on a standalone map

In Canvas, create a simple UDF that returns a GeoDataFrame or a DataFrame with hex cells.

```python
# vancouver_open_data
@fused.udf
def udf():

    @fused.cache
    def load_geojson():
        # Load GeoJSON directly from the URL into a GeoDataFrame
        DATA_URL = "https://raw.githubusercontent.com/visgl/deck.gl-data/master/examples/geojson/vancouver-blocks.json"
        return gpd.read_file(DATA_URL)

    return load_geojson()
```

We have built a utils library to create common map visualizations using DeckGL:

```python
@fused.udf
def udf():
    data = fused.run('vancouver_open_data')
    
    # Load map utilities for creating interactive maps
    map_utils = fused.load("https://github.com/fusedio/udfs/tree/7848ea5/community/milind/map_utils/")

    # Widgets give greater control over the layout of the map widgets
    widgets = ,
      "legend": False,
      "geocoder": "top-left",
    }

    config = 
        },
        "tooltip": ["valuePerSqm", "growth"]
    }
    
    return map_utils.deckgl_layers(
        layers = [], 
        widgets=widgets
    )
```

This creates a simple map that you can then embed anywhere:

We put together a template Canvas you can use to see how to visualize:
- Static data (GeoDataFrame, DataFrame, etc.)
- Tiled data for any dataset too large to serve as a single file

You can download this canvas and then import it by drag & dropping the `.zip` file in your Workbench.

<LazyReactPlayer
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/dashboards/map_templates_canvas_compressed.mp4"
  controls=
  width="100%"
  style=}
/>

================================================================================

## Web Scraping
Path: examples/web-scraping.mdx
URL: https://docs.fused.io/examples/web-scraping

This tutorial demonstrates how to use Fused User Defined Functions (UDFs) to scrape web pages and extract structured data. We'll walk through building a dataset of public schools from a specific geographic area using three different scraping approaches.

## Introduction

Imagine you need to compile a comprehensive list of public schools for a specific area. A quick Google search leads us to the Top Ranked Public Schools website. This site contains multiple subpages for different locations like Connecticut or New York.

Each page displays only a limited number of schools initially, requiring users to click "see more" to load additional results. Our goal is to programmatically extract data from these pages and generate CSV files for any location without manual navigation.

## Method 1: Individual Page Scraping

When you know the exact page URL and whether it contains paginated content, use the scrapegraph web scraper UDF to extract structured data from single pages.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The target webpage URL to scrape |
| **query** | string | Natural language description of data to extract |
| **output_schema** | dict (optional) | Expected structure of the output data |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python

df = fused.run(
    "fsh_3A1QcdR5kJEwmDSkYxc934",
    url="https://www.publicschoolreview.com/top-ranked-public-schools/connecticut/tab/all/num/1",
    query="Extract school names, ranks, and addresses",
    pagination_pages=2
)
df.head()
```

**Output:**

## Method 2: Batch Scraping Multiple Pages

When you need to scrape data from multiple known URLs simultaneously, use the scrapegraph multi scraper UDF. This UDF creates a unified output schema across all pages and processes them in parallel using fused.submit calls.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **urls** | list | List of webpage URLs to scrape in batch |
| **query** | string | Natural language description of data to extract |
| **pagination_pages** | int (optional) | Number of paginated pages to scrape per URL |
| **scroll_pages** | int (optional) | Number of scroll actions for infinite scroll pages |

### Example Usage

**Using the Python SDK:**

```python

df = fused.run(
    "fsh_5WETmX04oWgWtSCxwv1ZNr",
    urls=[
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/1",
        "https://www.publicschoolreview.com/top-ranked-public-schools/new-york/tab/all/num/3",
    ],
    query="Extract school names, grade ranges, and addresses for all NYC schools"
)
df.head()
```

**Output:**

## Method 3: Intelligent Crawling and Scraping

When you only know the top-level domain but not the specific page URLs, use the firecrawl search UDF to automatically discover and scrape relevant pages. This UDF crawls the website, identifies the most relevant pages based on your search criteria, and extracts the requested data.

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| **url** | string | The base URL to crawl and search |
| **search_prompt** | string | Natural language description of content to find |
| **extraction_prompt** | string | Natural language description of data to extract |

### Example Usage

**Using the Python SDK:**

```python

df = fused.run(
    "fsh_6mpu2dqoEBc1W80GjhZLSM",
    url="https://www.publicschoolreview.com",
    search_prompt="best public schools in connecticut",
    extraction_prompt="Extract school names, grade ranges, and addresses"
)
df.head()
```

**Output:**

## Using HTTPS API Endpoints

For integration into external systems or automated workflows, you can generate shared HTTPS endpoints for these UDFs. This allows you to retrieve data in CSV format using simple HTTPS requests.

### Available Endpoints

**Individual Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_3A1QcdR5kJEwmDSkYxc934/run/file?format=csv"
```

**Multi-Page Scraper:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_5WETmX04oWgWtSCxwv1ZNr/run/file?format=csv"
```

**Intelligent Crawler:**
```bash
curl "https://www.fused.io/server/v1/realtime-shared/fsh_6mpu2dqoEBc1W80GjhZLSM/run/file?format=csv"
```

## Making it Your Own

These are community UDFs that you can fork and customize for your needs. Simply click on the "Make a copy to modify" in the Fused Workbench to create your own copy.

**API Setup Required**: You'll need to configure your own API keys for third party services as fused secrets

## See Also

- Turning any UDF into an API
- Letting anyone talk to your data through MCP Servers
- Scaling out UDFs with `fused.submit()`

*Note: These web scraping features are currently in active development and you may encounter occasional issues.*

================================================================================

## Zonal Stats
Path: examples/zonal-stats.mdx
URL: https://docs.fused.io/examples/zonal-stats

_A step-by-step guide for data scientists._

### Requirements

- access to Fused
- access to a Jupyter Notebook
- the following Python packages installed locally:
  - `fused`
  - `pandas`
  - `geopandas`

## 1. Using Fused for a Zonal Statistics Example

In this guide, we'll estimate how much alfalfa grows in zones defined by polygon geometries.
This will show you how to:
- Bring in your data
- Write a UDF to process the data
- Run the UDF remotely & in parallel
- Create an app that shows your results and can be shared with anyone

```mermaid
---
title: Overview of Zonal Stats
---
graph LR
    A("Continuous Field<br/>(raster image)") --> D
    B("Areas of Interest<br/>(vector polygons)") --> D
    D --> E(Areas of Interest<br/>w/ Attributes)
```

## 2. Bring in your data

You'll first upload your own vector table with `fused.ingest`. This spatially partitions the table and writes it in your specified S3 bucket as a GeoParquet file. You'll then calculate zonal stats over a raster array of alfalfa crops in the USDA Cropland Data Layer (CDL) dataset.

This example shows how to geo partition polygons of Census Block Groups for Utah, which is a Parquet table with a `geometry` column. You can follow along with this file or any vector table you'd like. Read about other supported formats in Ingest your own data.

First, set up a local Python environment, install the latest Fused SDK with `pip install fused`, and authenticate.

Now, write the following script to geo partition your data. Pass the URL of the table to `fused.ingest`. When you kick off an ingest job with `run_batch`, Fused spins up a server to geo partition your table and writes the output to the path specified by the `output` parameter. In the codeblock below, `fd://tl_2023_49_bg/` is the base path to your account's S3 bucket.

```python showLineNumbers

  job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER2023/BG/tl_2023_49_bg.zip",
    output="fd://tl_2023_49_bg/"
  )
  job_id = job.run_batch()
  ```

After running the preceeding code, open fused.io/jobs to view the job status and logs.

Once the job is complete, you can preview the output dataset in the File Explorer.

You can also ingest data without installing anything by using this Fused App.

For the next step you can use the path to the data you just ingested or, if you prefer, this public sample table: `s3://fused-asset/data/tl_2023_49_bg/`.

[Image: Zonal Stats Parquet Files in Workbench File Explorer]

## 3. Write a UDF to process the data

To see the data as we process it, we will write a UDF in the Fused Workbench. As you write code in the UDF Builder you'll see how visualization results, logs, and errors show up immediately.

To write a UDF simply wrap a Python function with the decorator `@fused.udf`.

The first parameter of this UDF, `bounds`. It is reserved for Fused to pass a `GeoDataFrame` which the UDF may use to spatially filter datasets, and usually corresponds to a web map tile. This enables Fused to run the UDF for each tile in the viewport to distribute processing across multiple workers.

The `year` parameter is used to structure the S3 path of the CDL GeoTiff which the utility function `read_tiff` reads for the area defined by `bounds`. The `crop_id` parameter 36 corresponds to alfalfa the CDL colormap, which the UDF uses to mask the raster array.

Fused lets you import utility Modules from other UDFs with `fused.load`. Their code lives in the public UDFs repo.

- `read_tiff` loads an array of the CDL dataset for the specified `bounds` extent and `year`
- `table_to_tile` loads the table you geo partitioned for the specified `bounds` extent
- `geom_stats` calculates zonal statistics by aggregating the `arr` variable over the geometries specified by the `gdf`

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    year: int = 2020,
    crop_id: int = 36
):

    # Convert bounds to tile
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    zoom = common.estimate_zoom(bounds)
    tile = common.get_tiles(bounds, zoom=zoom)

    # Load CDLS data
    arr = common.read_tiff(
      tile,
      input_tiff_path=f"s3://fused-asset/data/cdls/_30m_cdls.tif"
    )

    # Mask for crop
    arr = np.where(np.isin(arr, [crop_id]), 1, 0)

    # Load polygons
    gdf = common.table_to_tile(
      bounds,
      table='s3://fused-asset/data/tl_2023_49_bg/',
      min_zoom=5,
      use_columns=['NAMELSAD', 'GEOID', 'MTFCC', 'FUNCSTAT', 'geometry']
    )
    gdf.crs = 4326

    # Calculate zonal stats
    return common.geom_stats(gdf, arr)
```

Try running the UDF in the UDF Builder and visually inspect the output on the map. See what happens when you change `year`. Try introducing print statements such as `print(arr)` and `print(gdf)` to show logs in the console.

[Image: Zonal Stats Map]

You might receive a timeout error in the `Results` tab. Try zooming into Utah on the map where the zonal areas are highlighted, to reduce the size of the tile being passed into the UDF from the map.

```json showLineNumbers
,
  "rasterLayer": ,
  "vectorLayer": ,
    "getLineColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```

</div>

\
Click "Copy shareable link" to share the app with others!

## 6. Conclusion and next steps

We've shown how you can use Fused to develop a distributed Python workflow to power an app. Through a simple sequence of steps we loaded data, wrote analytics code, and created an app to interact with the data. With a single click you went from experimental development code to a live application.

We hope this overview gives you a glimpse of what you can build with Fused. You can continue to learn how to read data, process data, and integrate with other applications.

Find inspiration for your next project, ask questions, or share your work with the Fused community.

- GitHub
- Discord
- LinkedIn
- Twitter

================================================================================

# PYTHON SDK

## fused.api
Path: python-sdk/api-reference/api.mdx
URL: https://docs.fused.io/python-sdk/api-reference/api

## Module Functions

The following functions can be called directly from the `fused.api` module:

```python

fused.api.function_name()
```

---

## whoami

```python
whoami()
```

Returns information on the currently logged in user

---

## delete

```python
delete(path: str, max_deletion_depth: int | Literal['unlimited'] = 3) -> bool
```

Delete the files at the path.

**Parameters:**

- **path** (<code>str</code>) – Directory or file to delete, like `fd://my-old-table/`
- **max_deletion_depth** (<code>int | Literal['unlimited']</code>) – If set (defaults to 3), the maximum depth the operation will recurse to.
  This option is to help avoid accidentally deleting more data that intended.
  Pass `"unlimited"` for unlimited.

**Examples:**

```python
fused.api.delete("fd://bucket-name/deprecated_table/")
```

---

## list

```python
list(path: str, *, details: bool = False) -> list[str] | list[ListDetails]
```

List the files at the path.

**Parameters:**

- **path** (<code>str</code>) – Parent directory URL, like `fd://bucket-name/`
- **details** (<code>bool</code>) – If True, return additional metadata about each record.

**Returns:**

- <code>list[str] | list[ListDetails]</code> – A list of paths as URLs, or as metadata objects.

**Examples:**

```python
fused.api.list("fd://bucket-name/")
```

---

## get

```python
get(path: str) -> bytes
```

Download the contents at the path to memory.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- `bytes` – bytes of the file

**Examples:**

```python
fused.api.get("fd://bucket-name/file.parquet")
```

---

## download

```python
download(path: str, local_path: str | Path) -> None
```

Download the contents at the path to disk.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`
- **local_path** (<code>str | Path</code>) – Path to a local file.

---

## upload

```python
upload(
    local_path: str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame,
    remote_path: str,
    timeout: float | None = None,
) -> None
```

Upload local file to S3.

**Parameters:**

- **local_path** (<code>str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame</code>) – Either a path to a local file (`str`, `Path`), a (Geo)DataFrame
  (which will get uploaded as Parquet file), or the contents to upload.
  Any string will be treated as a Path, if you wish to upload the contents of
  the string, first encode it: `s.encode("utf-8")`
- **remote_path** (<code>str</code>) – URL to upload to, like `fd://new-file.txt`
- **timeout** (<code>float | None</code>) – Optional timeout in seconds for the upload (will default to `OPTIONS.request_timeout` if not specified).

**Examples:**

To upload a local json file to your Fused-managed S3 bucket:

```py
fused.api.upload("my_file.json", "fd://my_bucket/my_file.json")
```

---

## sign_url

```python
sign_url(path: str) -> str
```

Create a signed URL to access the path.

This function may not check that the file represented by the path exists.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>str</code> – HTTPS URL to access the file using signed access.

**Examples:**

```python
fused.api.sign_url("fd://bucket-name/table_directory/file.parquet")
```

---

## sign_url_prefix

```python
sign_url_prefix(path: str) -> dict[str, str]
```

Create signed URLs to access all blobs under the path.

**Parameters:**

- **path** (<code>str</code>) – URL to a prefix, like `fd://bucket-name/some_directory/`

**Returns:**

- <code>dict\[str, str\]</code> – Dictionary mapping from blob store key to signed HTTPS URL.

**Examples:**

```python
fused.api.sign_url_prefix("fd://bucket-name/table_directory/")
```

---

## get_udfs

```python
get_udfs(
    n: int | None = None,
    *,
    skip: int = 0,
    by: Literal["name", "id", "slug"] = "name",
    whose: Literal["self", "public", "community", "team"] = "self"
) -> dict
```

Fetches a list of UDFs.

**Parameters:**

- **n** (<code>int | None</code>) – The total number of UDFs to fetch. Defaults to All.
- **skip** (<code>int</code>) – The number of UDFs to skip before starting to collect the result set. Defaults to 0.
- **by** (<code>Literal['name', 'id', 'slug']</code>) – The attribute by which to sort the UDFs. Can be "name", "id", or "slug". Defaults to "name".
- **whose** (<code>Literal['self', 'public', 'community', 'team']</code>) – Specifies whose UDFs to fetch. Can be "self" for the user's own UDFs or "public" for
  UDFs available publicly or "community" for all community UDFs. Defaults to "self".

**Returns:**

- <code>dict</code> – A list of UDFs.

**Examples:**

Fetch UDFs under the user account:

```py
fused.api.get_udfs()
```

---

## job_get_logs

```python
job_get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list[Any]</code> – Log messages for the given job.

---

## job_print_logs

```python
job_print_logs(
    job: CoerceableToJobId, since_ms: int | None = None, file: IO | None = None
) -> None
```

Fetch and print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- **file** (<code>IO | None</code>) – Where to print logs to. Defaults to sys.stdout.

**Returns:**

- <code>None</code> – None

---

## job_tail_logs

```python
job_tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = True,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) – how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) – if true, print out only a sample of logs. Defaults to True.
- **timeout** (<code>float | None</code>) – if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

## job_get_status

```python
job_get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

## job_cancel

```python
job_cancel(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – A new job object.

---

## job_get_exec_time

```python
job_get_exec_time(job: CoerceableToJobId) -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns:**

- <code>timedelta</code> – Time the job took. If the job is in progress, time from first to last log message is returned.

---

## job_wait_for_job

```python
job_wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until this job has finished

**Parameters:**

- **poll_interval_seconds** (<code>float</code>) – How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) – The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> – if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

## access_token

```python
access_token() -> str
```

Get an access token for the Fused service.

Returns the Bearer token for the authenticated user. Use this for authenticating API requests outside of the Fused SDK.

**Returns:**

- `str` – The access token string.

---

## logout

```python
logout()
```

Log out the current user.

Deletes the credentials saved to disk and resets the global Fused API.

---

## team_info

```python
team_info() -> dict
```

Get information about the current user's team.

**Returns:**

- `dict` – Team information including name, members, and settings.

---

## schedule_udf

```python
schedule_udf(
    udf: BaseUdf | str,
    minute: list[int] | int,
    hour: list[int] | int,
    day_of_month: list[int] | int | None = None,
    month: list[int] | int | None = None,
    day_of_week: list[int] | int | None = None,
    udf_args: dict[str, Any] | None = None,
    enabled: bool = True,
    **kwargs
) -> CronJob
```

Schedule a UDF to run on a cron schedule.

**Parameters:**

- **udf** (`BaseUdf | str`) – The UDF to schedule, either as a UDF object or name.
- **minute** (`list[int] | int`) – Minute(s) to run (0-59).
- **hour** (`list[int] | int`) – Hour(s) to run (0-23).
- **day_of_month** (`list[int] | int | None`) – Day(s) of month to run (1-31).
- **month** (`list[int] | int | None`) – Month(s) to run (1-12).
- **day_of_week** (`list[int] | int | None`) – Day(s) of week to run (0-6, where 0 is Sunday).
- **udf_args** (`dict[str, Any] | None`) – Arguments to pass to the UDF.
- **enabled** (`bool`) – Whether the schedule is enabled. Defaults to True.

**Returns:**

- `CronJob` – The created cron job object.

**Example:**

```python
# Run a UDF every day at 8:00 AM
fused.api.schedule_udf(
    udf="my_udf",
    minute=0,
    hour=8
)

# Run every Monday and Friday at noon
fused.api.schedule_udf(
    udf=my_udf,
    minute=0,
    hour=12,
    day_of_week=[1, 5]
)
```

---

## schedule_list

```python
schedule_list()
```

List all cron jobs scheduled for the current user.

**Returns:**

- List of `CronJob` objects.

---

## get_apps

```python
get_apps(
    n: int | None = None,
    *,
    skip: int = 0,
    by: Literal['name', 'id', 'slug'] = 'name',
    whose: Literal['self', 'public'] = 'self'
) -> dict
```

Get apps for the current user or public apps.

**Parameters:**

- **n** (`int | None`) – Maximum number of apps to return. None for all.
- **skip** (`int`) – Number of apps to skip.
- **by** (`Literal['name', 'id', 'slug']`) – Sort order.
- **whose** (`Literal['self', 'public']`) – Whose apps to get.

**Returns:**

- `dict` – Dictionary of apps.

---

## resolve

```python
resolve(path: str) -> str
```

Resolve a path from `fd://` to the full S3 URI.

**Parameters:**

- **path** (`str`) – The path to resolve (e.g., `fd://my-bucket/data/`).

**Returns:**

- `str` – The resolved S3 path.

**Example:**

```python
s3_path = fused.api.resolve("fd://my-data/output.parquet")
# Returns: "s3://fused-users/my-team/my-data/output.parquet"
```

---

## enable_gcs

```python
enable_gcs()
```

Save GCS credentials from AWS secret manager into a temporary local file and set its path to the environment variable.

Use this to enable Google Cloud Storage access in your UDFs.

---

## job_get_results

```python
job_get_results(job: CoerceableToJobId) -> list[Any]
```

Get the results of a completed job.

**Parameters:**

- **job** (`CoerceableToJobId`) – The identifier of a job or a `RunResponse` object.

**Returns:**

- `list[Any]` – List of results from the job.

---

## job_wait_for_results

```python
job_wait_for_results(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None
) -> list[UdfEvaluationResult]
```

Block until a job completes and return its results.

Combines `job_wait_for_job` and `job_get_results` into a single call.

**Parameters:**

- **job** (`CoerceableToJobId`) – The identifier of a job or a `RunResponse` object.
- **poll_interval_seconds** (`float`) – How often to poll for status. Defaults to 5.
- **timeout** (`float | None`) – Maximum time to wait in seconds. None for no timeout.

**Returns:**

- `list[UdfEvaluationResult]` – List of UDF evaluation results.

---

## FusedAPI Class Methods

The following methods require creating a `FusedAPI` instance first:

```python
from fused.api import FusedAPI
api = FusedAPI()
api.method_name()
```

## FusedAPI

```python
FusedAPI(
    *,
    base_url: str | None = None,
    shared_udf_base_url: str | None = None,
    set_global_api: bool = True,
    credentials_needed: bool = True
)
```

API for running jobs in the Fused service.

Create a FusedAPI instance.

**Other Parameters:**

- **base_url** (<code>str | None</code>) – The Fused instance to send requests to. Defaults to `https://www.fused.io/server/v1`.
- **shared_udf_base_url** (<code>str | None</code>) – The shared UDF instance to send requests to. Defaults to `https://www.udf.ai`.
- **set_global_api** (<code>bool</code>) – Set this as the global API object. Defaults to True.
- **credentials_needed** (<code>bool</code>) – If True, automatically attempt to log in. Defaults to True.

---

### create_udf_access_token

```python
create_udf_access_token(
    udf_email_or_name_or_id: str | None = None,
    /,
    udf_name: str | None = None,
    *,
    udf_email: str | None = None,
    udf_id: str | None = None,
    client_id: str | Ellipsis | None = ...,
    public_read: bool | None = None,
    access_scope: str | None = None,
    cache: bool = True,
    metadata_json: dict[str, Any] | None = None,
    enabled: bool = True,
) -> UdfAccessToken
```

Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.

**Parameters:**

- **udf_email_or_name_or_id** (<code>str | None</code>) – A UDF ID, email address (for use with udf_name), or UDF name.
- **udf_name** (<code>str | None</code>) – The name of the UDF to create the token for.

**Other Parameters:**

- **udf_email** (<code>str | None</code>) – The email of the user owning the UDF, or, if udf_name is None, the name of the UDF.
- **udf_id** (<code>str | None</code>) – The backend ID of the UDF to create the token for.
- **client_id** (<code>str | Ellipsis | None</code>) – If specified, overrides which realtime environment to run the UDF under.
- **cache** (<code>bool</code>) – If True, UDF tiles will be cached.
- **metadata_json** (<code>dict\[str, Any\] | None</code>) – Additional metadata to serve as part of the tiles metadata.json.
- **enabled** (<code>bool</code>) – If True, the token can be used.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.create_udf_access_token()`

---

### upload

```python
upload(
    path: str,
    data: bytes | BinaryIO,
    client_id: str | None = None,
    timeout: float | None = None,
) -> None
```

Upload a binary blob to a cloud location

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.upload()`

---

### start_job

```python
start_job(
    config: JobConfig | JobStepConfig,
    *,
    instance_type: WHITELISTED_INSTANCE_TYPES | None = None,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: Sequence[str] | None = ("FUSED_CREDENTIAL_PROVIDER=ec2",),
    image_name: str | None = None,
    send_status_email: bool | None = None,
    cache_max_age: int | None = None
) -> RunResponse
```

Execute an operation

**Parameters:**

- **config** (<code>JobConfig | JobStepConfig</code>) – the configuration object to run in the job.

**Other Parameters:**

- **instance_type** (<code>WHITELISTED_INSTANCE_TYPES | None</code>) – The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- **region** (<code>str | None</code>) – The AWS region in which to run. Defaults to None.
- **disk_size_gb** (<code>int | None</code>) – The disk size to specify for the job. Defaults to None.
- **additional_env** (<code>Sequence\[str\] | None</code>) – Any additional environment variables to be passed into the job, each in the form KEY=value. Defaults to None.
- **image_name** (<code>str | None</code>) – Custom image name to run. Defaults to None for default image.
- **send_status_email** (<code>bool | None</code>) – Whether to send a status email to the user when the job is complete.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.start_job()`

---

### get_jobs

```python
get_jobs(
    n: int = 5,
    *,
    skip: int = 0,
    per_request: int = 25,
    max_requests: int | None = 1
) -> Jobs
```

Get the job history.

**Parameters:**

- **n** (<code>int</code>) – The number of jobs to fetch. Defaults to 5.

**Other Parameters:**

- **skip** (<code>int</code>) – Where in the job history to begin. Defaults to 0, which retrieves the most recent job.
- **per_request** (<code>int</code>) – Number of jobs per request to fetch. Defaults to 25.
- **max_requests** (<code>int | None</code>) – Maximum number of requests to make. May be None to fetch all jobs. Defaults to 1.

**Returns:**

- <code>Jobs</code> – The job history.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.get_jobs()`

---

### get_status

```python
get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.get_status()`

---

### get_logs

```python
get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> – Log messages for the given job.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.get_logs()`

---

### tail_logs

```python
tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = False,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) – how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) – if true, print out only a sample of logs. Defaults to False.
- **timeout** (<code>float | None</code>) – if not None, how long to continue tailing logs for. Defaults to None for indefinite.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.tail_logs()`

---

### wait_for_job

```python
wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until the given job has finished

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **poll_interval_seconds** (<code>float</code>) – How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) – The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> – if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.wait_for_job()`

---

### cancel_job

```python
cancel_job(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – A new job object.

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.cancel_job()`

---

### auth_token

```python
auth_token() -> str
```

Returns the current user's Fused environment (team) auth token

**Usage:** `from fused.api import FusedAPI; api = FusedAPI(); api.auth_token()`

---

================================================================================

## fused.core
Path: python-sdk/api-reference/core.mdx
URL: https://docs.fused.io/python-sdk/api-reference/core

## `run_tile`

```python showLineNumbers
def run_tile(email: str,
             id: Optional[str] = None,
             *,
             x: int,
             y: int,
             z: int,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_shared_tile`

```python showLineNumbers
def run_shared_tile(token: str,
                    *,
                    x: int,
                    y: int,
                    z: int,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    _client_id: Optional[str] = None,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_file`

```python showLineNumbers
def run_file(email: str,
             id: Optional[str] = None,
             *,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF.

---
## `run_shared_file`

```python showLineNumbers
def run_shared_file(token: str,
                    *,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  The response from the server after executing the operation on the file.

**Raises**:

- `Exception` - Describes various exceptions that could occur during the function execution, including but not limited to invalid parameters, network errors, unauthorized access errors, or server-side errors.

This function is designed to access shared operations that require a token for authorization. It requires network access to communicate with the server hosting these operations and may incur data transmission costs or delays depending on the network's performance.

---
## `run_tile_async`

```python showLineNumbers
async def run_tile_async(
        email: str,
        id: Optional[str] = None,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to asynchronously run a UDF on a specific tile defined by its x, y, and z coordinates. It supports customization of the output data types for vector and raster data, and accommodates additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - User's email address. Used to identify the user's saved UDFs. If the ID is not provided, the email is also used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF on the specified tile and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_tile_async`

```python showLineNumbers
async def run_shared_tile_async(
        token: str,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared tile-based UDF using a specific access token.

This function constructs a URL for running an operation on a tile, defined by its x, y, and z coordinates, accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation on the specified tile.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the specified tile and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---
## `run_file_async`

```python showLineNumbers
async def run_file_async(
        email: str,
        id: Optional[str] = None,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a file-based UDF associated with the specific email and ID.

This function constructs a URL to run a UDF on a server, allowing for output data type customization for vector and raster outputs and supporting additional parameters for the UDF execution. If no ID is provided, the user's email is used as the identifier.

**Arguments**:

- `email` _str_ - The user's email address, used to identify the user's saved UDFs. If the ID is not provided, this email will also be used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the function fetches the user's email as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_file_async`

```python showLineNumbers
async def run_shared_file_async(
        token: str,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared file-based UDF using the specific access token.

Constructs a URL to run an operation on a file accessible via a shared token, enabling customization of the output data types for vector and raster data. It accommodates additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the file and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

================================================================================

## @fused.cache
Path: python-sdk/api-reference/fused-cache.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-cache

# @fused.cache

```python
cache(
    func: Callable[..., Any] | None = None,
    cache_max_age: str | int = DEFAULT_CACHE_MAX_AGE,
    cache_folder_path: str = "tmp",
    concurrent_lock_timeout: str | int = 120,
    cache_reset: bool | None = None,
    cache_storage: StorageStr | None = None,
    cache_key_exclude: Iterable[str] = None,
    cache_verbose: bool | None = None,
    **kwargs: Any
) -> Callable[..., Any]
```

Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function to cache its return values. The cache behavior can be customized through keyword arguments.

## Parameters

- **func** (`Callable`) – The function to be decorated. If None, this returns a partial decorator with the passed keyword arguments.
- **cache_max_age** (`str | int`) – A string with a numbered component and units. Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. "48h", "10s", etc.).
- **cache_folder_path** (`str`) – Folder to append to the configured cache directory.
- **concurrent_lock_timeout** (`str | int`) – Max amount of time in seconds for subsequent concurrent calls to wait for a previous concurrent call to finish execution and to write the cache file.
- **cache_reset** (`bool | None`) – Ignore `cache_max_age` and overwrite cached result.
- **cache_storage** (`StorageStr | None`) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will automatically select the storage location defined in options (mount if it exists, otherwise local) and ensures that it exists and is writable. Mount gets shared across executions where local will only be shared within the same execution.
- **cache_key_exclude** (`Iterable[str]`) – An iterable of parameter names to exclude from the cache key calculation. Useful for arguments that do not affect the result of the function and could cause unintended cache expiry (e.g. database connection objects).
- **cache_verbose** (`bool | None`) – Print a message when a cached result is returned.

## Returns

`Callable` – A decorator that, when applied to a function, caches its return values according to the specified keyword arguments.

## Examples

Use the `@cache` decorator to cache the return value of a function in a custom path.

```python
@cache(path="/tmp/custom_path/")
def expensive_function():
    # Function implementation goes here
    return result
```

If the output of a cached function changes, for example if remote data is modified, it can be reset by running the function with the `cache_reset` keyword argument. Afterward, the argument can be cleared.

```python
@cache(path="/tmp/custom_path/", cache_reset=True)
def expensive_function():
    # Function implementation goes here
    return result
```

================================================================================

## fused.context
Path: python-sdk/api-reference/fused-context.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-context

# context

Access runtime context information from within a UDF. These functions return information about the current execution environment.

```python

@fused.udf
def udf():
    # Check if running in realtime or batch
    if fused.context.in_realtime():
        print("Running in realtime")
    
    # Get request headers
    headers = fused.context.get_headers()
```

## Functions

### get_headers

```python
get_headers() -> dict[str, str]
```

Get the request headers that were passed to the UDF.

Returns a dictionary of header names to values for all headers passed in the HTTP request.

### get_header

```python
get_header(name: str) -> str | None
```

Get a specific request header by name (case-insensitive).

**Parameters:**
- **name** (`str`) – The header name to retrieve.

**Returns:** The header value, or `None` if not found.

### get_recursion_factor

```python
get_recursion_factor() -> int | None
```

Get the recursion factor from the current context.

Returns the recursion factor when running in a tiled context, or `None` otherwise.

### get_realtime_client_id

```python
get_realtime_client_id() -> str | None
```

Get the realtime client ID from the current context.

Returns the client ID when running in realtime mode, or `None` otherwise.

### get_user_email

```python
get_user_email() -> str | None
```

Get the user email from the current context.

Returns the email of the authenticated user running the UDF, or `None` if not available.

### in_realtime

```python
in_realtime() -> bool
```

Return `True` if the context is in a realtime job.

Use this to conditionally execute code based on execution mode.

### in_batch

```python
in_batch() -> bool
```

Return `True` if the context is in a batch job.

Use this to conditionally execute code based on execution mode.

### get_global_context

```python
get_global_context() -> ExecutionContextProtocol | None
```

Get the global execution context object.

Returns the full context object for advanced use cases.

## Usage Example

```python

@fused.udf
def udf():
    # Conditional behavior based on execution mode
    if fused.context.in_batch():
        # Use larger chunk sizes for batch
        chunk_size = 10000
    else:
        # Smaller chunks for realtime
        chunk_size = 1000
    
    # Access custom headers passed via API
    api_key = fused.context.get_header("X-API-Key")
    
    # Log who is running this UDF
    user = fused.context.get_user_email()
    print(f"Running for user: ")
    
    return process_data(chunk_size)
```

================================================================================

## fused.download
Path: python-sdk/api-reference/fused-download.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-download

# download

```python
download(url: str, file_path: str, storage: StorageStr = 'auto') -> str
```

Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.

## Parameters

- **url** (`str`) – The URL to download.
- **file_path** (`str`) – The local path where to save the file.
- **storage** (`StorageStr`) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will automatically select the storage location defined in options (mount if it exists, otherwise local) and ensures that it exists and is writable. Mount gets shared across executions where local will only be shared within the same execution.

## Returns

- `str` – The function downloads the file only on the first execution, and returns the file path.

## Example

```python
@fused.udf
def geodataframe_from_geojson():

    url = "s3://sample_bucket/my_geojson.zip"
    path = fused.core.download(url, "tmp/my_geojson.zip")
    gdf = gpd.read_file(path)
    return gdf
```

================================================================================

## fused.file_path
Path: python-sdk/api-reference/fused-file-path.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-file-path

# file_path

```python
file_path(
    file_path: str, mkdir: bool = True, storage: StorageStr = "auto"
) -> str
```

Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF. It takes a relative file_path, creates the corresponding directory structure, and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files, such as when writing intermediary files to disk when processing large datasets. `file_path` ensures that necessary directories exist. The directory is kept for 12h.

## Parameters

- **file_path** (`str`) – The relative file path to locate.
- **mkdir** (`bool`) – If True, create the directory if it doesn't already exist. Defaults to True.
- **storage** (`StorageStr`) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will automatically select the storage location defined in options (mount if it exists, otherwise local) and ensures that it exists and is writable. Mount gets shared across executions where local will only be shared within the same execution.

## Returns

- `str` – The located file path.

================================================================================

## fused.find_dataset
Path: python-sdk/api-reference/fused-find-dataset.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-find-dataset

# find_dataset

Find the dataset that contains a specific location URL.

```python
fused.find_dataset(
    location: str,
    base_url: str | None = None
) -> dict[str, Any]
```

Uses hierarchical prefix matching: if the exact path isn't registered, searches progressively shorter prefixes to find the containing dataset.

## Parameters

- **location** (`str`) – Full URL to search for (`s3://`, `gs://`, `http://`, etc.). Can be a file, partition, or directory path.
- **base_url** (`str | None`) – Base URL for API. If None, uses current environment.

## Returns

- `dict` – Dataset dict with keys: `id`, `location`, `description`, `storage_type`, `owner`, `public`, `created_at`, `updated_at`, etc.

## Raises

- `requests.HTTPError` – If dataset not found (404) or request fails.

## Example

```python

# Find dataset containing a file
dataset = fused.find_dataset("s3://bucket/data/year=2024/file.parquet")
print(f"Found dataset: ")
```

## See also

- `fused.register_dataset` – Register a new dataset

================================================================================

## fused.get_chunk_from_table
Path: python-sdk/api-reference/fused-get-chunk-from-table.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-get-chunk-from-table

# get_chunk_from_table

```python
get_chunk_from_table(
    url: str,
    file_id: Union[str, int, None],
    chunk_id: Optional[int],
    *,
    columns: Optional[Iterable[str]] = None
) -> gpd.GeoDataFrame
```

Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

## Parameters

- **url** (`str`) – URL of the table.
- **file_id** (`Union[str, int, None]`) – File ID to read.
- **chunk_id** (`Optional[int]`) – Chunk ID to read.
- **columns** (`Optional[Iterable[str]]`) – Read only the specified columns.

## Returns

`gpd.GeoDataFrame` – The chunk data as a GeoDataFrame.

================================================================================

## fused.get_chunks_metadata
Path: python-sdk/api-reference/fused-get-chunks-metadata.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-get-chunks-metadata

# get_chunks_metadata

```python
get_chunks_metadata(url: str) -> gpd.GeoDataFrame
```

Returns a GeoDataFrame with each chunk in the table as a row.

## Parameters

- **url** (`str`) – URL of the table.

## Returns

`gpd.GeoDataFrame` – A GeoDataFrame where each row represents a chunk in the table.

================================================================================

## fused.ingest_nongeospatial
Path: python-sdk/api-reference/fused-ingest-nongeospatial.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-ingest-nongeospatial

# ingest_nongeospatial

```python
ingest_nongeospatial(
    input: str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame,
    output: str | None = None,
    *,
    output_metadata: str | None = None,
    partition_col: str | None = None,
    partitioning_maximum_per_file: int = 2500000,
    partitioning_maximum_per_chunk: int = 65000
) -> NonGeospatialPartitionJobStepConfig
```

Ingest a non-geospatial dataset into the Fused partitioned format.

## Parameters

- **input** (`str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame`) – A DataFrame or a path to file or files on S3 to ingest. Files may be Parquet or another data format.
- **output** (`str | None`) – Location on S3 to write the `main` table to.
- **output_metadata** (`str | None`) – Location on S3 to write the `fused` table to.
- **partition_col** (`str | None`) – Partition along this column for nongeospatial datasets.
- **partitioning_maximum_per_file** (`int`) – Maximum number of items to store in a single file. Defaults to 2,500,000.
- **partitioning_maximum_per_chunk** (`int`) – Maximum number of items to store in a single chunk. Defaults to 65,000.

## Returns

`NonGeospatialPartitionJobStepConfig` – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

## Example

```python
job = fused.ingest_nongeospatial(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).execute()
```

================================================================================

## fused.ingest
Path: python-sdk/api-reference/fused-ingest.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-ingest

# ingest

```python
ingest(
    input: str | Path | Sequence[str | Path] | gpd.GeoDataFrame,
    output: str | None = None,
    *,
    output_metadata: str | None = None,
    schema: Schema | None = None,
    file_suffix: str | None = None,
    load_columns: Sequence[str] | None = None,
    remove_cols: Sequence[str] | None = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: bool | None = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: int | float | None = None,
    partitioning_maximum_per_chunk: int | float | None = None,
    partitioning_max_width_ratio: int | float = 2,
    partitioning_max_height_ratio: int | float = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: float | None = None,
    subdivide_stop: float | None = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 500,
    lonlat_cols: tuple[str, str] | None = None,
    partitioning_schema_input: str | pd.DataFrame | None = None,
    gdal_config: GDALOpenConfig | dict[str, Any] | None = None,
    overwrite: bool = False,
    as_udf: bool = False
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

## Parameters

- **input** (`str | Path | Sequence[str | Path] | gpd.GeoDataFrame`) – A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- **output** (`str | None`) – Location on S3 to write the `main` table to.
- **output_metadata** (`str | None`) – Location on S3 to write the `fused` table to.
- **schema** (`Schema | None`) – Schema of the data to be ingested. Optional, will be inferred if not provided.
- **file_suffix** (`str | None`) – Filter which files are used for ingestion (e.g. `.geojson`).
- **load_columns** (`Sequence[str] | None`) – Read only this set of columns. Defaults to all columns.
- **remove_cols** (`Sequence[str] | None`) – Columns to drop when ingesting.
- **explode_geometries** (`bool`) – Whether to unpack multipart geometries to single geometries. Defaults to `False`.
- **drop_out_of_bounds** (`bool | None`) – Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- **partitioning_method** (`Literal['area', 'length', 'coords', 'rows']`) – The method to use for grouping rows into partitions. Defaults to `"rows"`.
- **partitioning_maximum_per_file** (`int | float | None`) – Maximum value for `partitioning_method` to use per file.
- **partitioning_maximum_per_chunk** (`int | float | None`) – Maximum value for `partitioning_method` to use per chunk.
- **target_num_chunks** (`int`) – The target for the number of files. Defaults to 500.
- **lonlat_cols** (`tuple[str, str] | None`) – Names of longitude, latitude columns to construct point geometries from.
- **gdal_config** (`GDALOpenConfig | dict[str, Any] | None`) – Configuration options to pass to GDAL for reading files.
- **overwrite** (`bool`) – If True, overwrite the output directory if it already exists. Defaults to False.
- **as_udf** (`bool`) – Return the ingestion workflow as a UDF. Defaults to False.

## Returns

`GeospatialPartitionJobStepConfig` – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

## Example

```python
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).execute()
```

---

## job.run_batch

```python
def run_batch(
    output_table: Optional[str] = ...,
    instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
    *,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: List[str] | None = None,
    image_name: Optional[str] = None,
    ignore_no_udf: bool = False,
    ignore_no_output: bool = False,
    validate_imports: Optional[bool] = None,
    validate_inputs: bool = True,
    overwrite: Optional[bool] = None
) -> RunResponse
```

Begin execution of the ingestion job by calling `run_batch` on the job object.

### Parameters

- `output_table` – The name of the table to write to. Defaults to None.
- `instance_type` – The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` – The AWS region in which to run. Defaults to None.
- `disk_size_gb` – The disk size to specify for the job. Defaults to None.
- `additional_env` – Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` – Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` – Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` – Ignore validation errors about not specifying output location. Defaults to False.

---

## Monitoring jobs

Calling `run_batch` returns a `RunResponse` object with helper methods.

```python
# Declare ingest job
job = fused.ingest(
  input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
  output="s3://fused-sample/census/ca_bg_2022/main/"
)

# Start ingest job
job_id = job.run_batch()
```

```python
job_id.get_status()      # Fetch the job status
job_id.print_logs()      # Fetch and print the job's logs
job_id.get_exec_time()   # Determine the job's execution time
job_id.tail_logs()       # Continuously print the job's logs
job_id.cancel()          # Cancel the job
```

---

## job.run_remote

Alias of `job.run_batch` for backwards compatibility. See `job.run_batch` above for details.

================================================================================

## fused.load_async
Path: python-sdk/api-reference/fused-load-async.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-load-async

# load_async

Asynchronously load a UDF from various sources.

```python
await fused.load_async(
    url_or_udf: str | Path,
    /,
    *,
    cache_key: Any = None,
    import_globals: bool = True
) -> Udf
```

This is the async-optimized version of `fused.load()` that uses async HTTP requests to avoid blocking the event loop during UDF metadata fetching.

## Parameters

- **url_or_udf** (`str | Path`) – A string representing the location of the UDF, or the raw code of the UDF.
- **cache_key** (`Any`) – An optional key used for caching the loaded UDF.
- **import_globals** (`bool`) – Expose the globals defined in the UDF's context as attributes on the UDF object.

## Returns

- `Udf` – An instance of the loaded UDF.

## Example

```python

# Load a UDF asynchronously
udf = await fused.load_async("username@fused.io/my_udf_name")

# Use in parameter validation
if not isinstance(udf, Udf):
    udf = await fused.load_async(udf)
```

## See also

- `fused.load` – Synchronous version
- `fused.run_async` – Run a UDF asynchronously

================================================================================

## fused.load
Path: python-sdk/api-reference/fused-load.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-load

# load

```python
load(
    url_or_udf: Union[str, Path],
    /,
    *,
    cache_key: Any = None,
    import_globals: bool = True,
) -> AnyBaseUdf
```

Loads a UDF from various sources including GitHub URLs and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused platform-specific identifier composed of an email and UDF name. It intelligently determines the source type based on the format of the input and retrieves the UDF accordingly.

## Parameters

- **url_or_udf** (`Union[str, Path]`) – A string representing the location of the UDF, or the raw code of the UDF. The location can be a GitHub URL starting with "https://github.com", a Fused platform-specific identifier in the format "email/udf_name", or a local file path pointing to a Python file.
- **cache_key** (`Any`) – An optional key used for caching the loaded UDF. If provided, the function will attempt to load the UDF from cache using this key before attempting to load it from the specified source. Defaults to None, indicating no caching.
- **import_globals** (`bool`) – Expose the globals defined in the UDF's context as attributes on the UDF object (default True). This requires executing the code of the UDF. To globally configure this behavior, use `fused.options.never_import`.

## Returns

- **AnyBaseUdf** – An instance of the loaded UDF.

## Raises

- `ValueError` – If the URL or Fused platform-specific identifier format is incorrect or cannot be parsed.
- `Exception` – For errors related to network issues, file access permissions, or other unforeseen errors during the loading process.

## Examples

Load a UDF from a GitHub URL:

```python
udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
```

Load a UDF using a Fused platform-specific identifier:

```python
udf = fused.load("username@fused.io/REM_with_HyRiver")
```

================================================================================

## fused.register_dataset
Path: python-sdk/api-reference/fused-register-dataset.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-register-dataset

# register_dataset

Register a dataset for indexed queries.

```python
fused.register_dataset(
    dataset_path: str,
    base_url: str | None = None
) -> dict[str, Any]
```

This function registers a directory in your file storage as a dataset, enabling fast geospatial queries using H3 indexing.

## Parameters

- **dataset_path** (`str`) – Path to the dataset directory. The path should point to a directory containing parquet files.
- **base_url** (`str | None`) – Base URL for API. If None, uses current environment.

## Returns

- `dict` – Dictionary with registration results:
  - `dataset_id`: ID of the created/updated dataset
  - `location`: Normalized URL of the dataset
  - `visit_status`: Status of the dataset visit (success/timeout/error)
  - `items_discovered`: Total number of items found
  - `new_items`: Number of new items added

## Raises

- `requests.HTTPError` – If the API request fails.

## Example

```python

# Register a dataset from your storage
result = fused.register_dataset("s3://my-bucket/my-data/buildings/")
print(f"Registered dataset with ID: ")
print(f"Found  files")
```

- Regular users can use any storage paths they have access to
- Datasets are registered as private (only accessible to your team)
- Files are automatically queued for metadata extraction

## See also

- `fused.find_dataset` – Find a registered dataset

================================================================================

## fused.run_async
Path: python-sdk/api-reference/fused-run-async.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-run-async

# run_async

Async version of `run()` that executes a UDF asynchronously.

```python
await fused.run_async(
    udf: str | Udf | UdfJobStepConfig | None = None,
    *,
    x: int | None = None,
    y: int | None = None,
    z: int | None = None,
    engine: Literal['remote', 'local'] | None = None,
    instance_type: str | None = None,
    type: Literal['tile', 'file'] | None = None,
    max_retry: int = 0,
    cache_max_age: str | None = None,
    cache: bool = True,
    parameters: dict[str, Any] | None = None,
    disk_size_gb: int | None = None,
    **kw_parameters
) -> xr.Dataset | pd.DataFrame | gpd.GeoDataFrame | UdfEvaluationResult
```

This function provides the same functionality as `fused.run()` but with async execution.

## Parameters

- **udf** (`str | Udf | UdfJobStepConfig`) – The UDF to execute. Can be:
  - A string representing a UDF name or shared token
  - A UDF object
  - A `UdfJobStepConfig` object for detailed execution configuration
- **x, y, z** (`int | None`) – Tile coordinates for tile-based UDF execution.
- **engine** (`Literal['remote', 'local'] | None`) – The execution engine to use.
- **instance_type** (`str | None`) – Instance type for remote execution (`'realtime'`, `'small'`, `'medium'`, `'large'`, or a specific instance type).
- **type** (`Literal['tile', 'file'] | None`) – The type of UDF execution.
- **max_retry** (`int`) – Maximum retries if the UDF fails. Defaults to 0.
- **cache_max_age** (`str | None`) – Maximum cache age (e.g., `"48h"`, `"10s"`).
- **cache** (`bool`) – Set to `False` to disable caching.
- **parameters** (`dict[str, Any] | None`) – Additional parameters to pass to the UDF.
- **disk_size_gb** (`int | None`) – Disk size in GB for remote execution (batch only).
- ****kw_parameters** – Additional parameters to pass to the UDF.

## Returns

The result of the UDF execution, which varies based on the UDF and execution path.

## Example

```python

# Run a UDF saved in the Fused system asynchronously
result = await fused.run_async("username@fused.io/my_udf_name")

# Run a UDF saved in GitHub asynchronously
loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
result = await fused.run_async(loaded_udf, bbox=bbox)

# Run a UDF saved in a local directory asynchronously
loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
result = await fused.run_async(loaded_udf, bbox=bbox)
```

## See also

- `fused.run` – Synchronous version
- `fused.load_async` – Load a UDF asynchronously

================================================================================

## fused.run
Path: python-sdk/api-reference/fused-run.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-run

# run

```python
run(
    udf: Union[str, None, UdfJobStepConfig, Udf, UdfAccessToken] = None,
    *,
    x: Optional[int] = None,
    y: Optional[int] = None,
    z: Optional[int] = None,
    sync: bool = True,
    engine: Optional[Literal["remote", "local"]] = None,
    instance_type: Optional[InstanceType] = None,
    type: Optional[Literal["tile", "file"]] = None,
    max_retry: int = 0,
    cache_max_age: Optional[str] = None,
    cache: bool = True,
    parameters: Optional[Dict[str, Any]] = None,
    disk_size_gb: Optional[int] = None,
    **kw_parameters
) -> Union[ResultType, Coroutine[ResultType, None, None]]
```

Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote), with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and allows for both synchronous and asynchronous execution.

## Parameters

- **udf** (`str, Udf or UdfJobStepConfig`) – The UDF to execute. Can be specified as:
  - A string representing a UDF name or UDF shared token.
  - A UDF object.
  - A UdfJobStepConfig object for detailed execution configuration.
- **x, y, z** (`int`) – Tile coordinates for tile-based UDF execution.
- **sync** (`bool`) – If True, execute the UDF synchronously. If False, execute asynchronously.
- **engine** (`Optional[Literal['remote', 'local']]`) – The execution engine to use ('remote' or 'local').
- **instance_type** (`Optional[InstanceType]`) – The type of instance to use for remote execution ('realtime', or 'small', 'medium', 'large' or one of the whitelisted instance types). If not specified, gets the default from the UDF or defaults to 'realtime'.
- **disk_size_gb** (`Optional[int]`) – The size of the disk in GB to use for remote execution (only supported for a batch instance type).
- **type** (`Optional[Literal['tile', 'file']]`) – The type of UDF execution ('tile' or 'file').
- **max_retry** (`int`) – The maximum number of retries to attempt if the UDF fails. By default does not retry.
- **cache_max_age** (`Optional[str]`) – The maximum age when returning a result from the cache. Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. "48h", "10s", etc.). Default is `None` so a UDF run with `fused.run()` will follow `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (`bool`) – Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **parameters** (`Optional[Dict[str, Any]]`) – Additional parameters to pass to the UDF.
- **\*\*kw_parameters** – Additional parameters to pass to the UDF.

## Raises

- `ValueError` – If the UDF is not specified or is specified in more than one way.
- `TypeError` – If the first parameter is not of an expected type.

## Returns

The result of the UDF execution, which varies based on the UDF and execution path.

## Examples

Run a UDF saved in the Fused system:

```python
fused.run("username@fused.io/my_udf_name")
```

Run a UDF saved in GitHub:

```python
loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

Run a UDF saved in a local directory:

```python
loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

This function dynamically determines the execution path and parameters based on the inputs. It is designed to be flexible and support various UDF execution scenarios.

================================================================================

## fused.secrets
Path: python-sdk/api-reference/fused-secrets.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-secrets

# secrets

Access secrets stored in the Fused backend for the current kernel.

```python

# Access a secret by name
api_key = fused.secrets['MY_API_KEY']
```

## Overview

`fused.secrets` is a `SecretsManager` object that provides access to secrets stored in your Fused account. Secrets are accessed as attributes using dot notation.

Secrets can be managed in Workbench Settings under the "Secrets" tab, or via the API.

## Usage

### Accessing secrets

```python

@fused.udf
def udf():
    # Access a secret by name
    api_key = fused.secrets['OPENAI_API_KEY']
    
    # Use the secret

    client = openai.OpenAI(api_key=api_key)
    ...
```

## Security notes

- Secrets are stored encrypted in the Fused backend
- Secrets are only accessible within UDFs running on Fused infrastructure
- Secrets are scoped to your account/organization
- Never log or return secret values from UDFs

## See also

- Environment Variables for runtime configuration
- Cloud Storage for connecting to cloud providers

================================================================================

## fused.submit
Path: python-sdk/api-reference/fused-submit.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-submit

# submit

```python
submit(
    udf: AnyBaseUdf | FunctionType | str,
    arg_list: list | pd.DataFrame,
    *,
    engine: Literal["remote", "local"] | None = "remote",
    instance_type: InstanceType | None = None,
    max_workers: int | None = None,
    n_processes_per_worker: int | None = None,
    max_retry: int = 2,
    debug_mode: bool = False,
    collect: bool = True,
    execution_type: ExecutionType = "thread_pool",
    cache_max_age: str | None = None,
    cache: bool = True,
    ignore_exceptions: bool = False,
    flatten: bool = True,
    **kwargs
) -> JobPool | ResultType | pd.DataFrame
```

Executes a user-defined function (UDF) multiple times for a list of input parameters, and returns immediately a "lazy" JobPool object allowing to inspect the jobs and wait on the results.

Each individual UDF run will be cached following the standard caching logic as with `fused.run()` and the specified `cache_max_age`. Additionally, when `collect=True` (the default), the collected results are cached locally for the duration of `cache_max_age` or 12h by default.

## Parameters

- **udf** (`AnyBaseUdf | FunctionType | str`) – The UDF to execute. See `fused.run` for more details on how to specify the UDF.
- **arg_list** (`list | pd.DataFrame`) – A list of input parameters for the UDF. Can be specified as:
  - a list of values for parametrizing over a single parameter
  - a list of dictionaries for parametrizing over multiple parameters
  - A DataFrame for parametrizing over multiple parameters where each row is a set of parameters
- **engine** (`Literal['remote', 'local'] | None`) – The execution engine to use. Defaults to 'remote'.
- **instance_type** (`InstanceType | None`) – The type of instance to use for remote execution ('realtime', or 'small', 'medium', 'large' or one of the whitelisted instance types). Defaults to 'realtime'.
- **max_workers** (`int | None`) – The maximum number of workers to use. Defaults to 32 for realtime (max 1024), and 1 for batch instances (max 5).
- **n_processes_per_worker** (`int | None`) – The number of processes to use per worker. For realtime instances, defaults to 1. For batch instances, defaults to the number of cores.
- **max_retry** (`int`) – The maximum number of retries for failed jobs. Defaults to 2.
- **debug_mode** (`bool`) – If True, executes only the first item in arg_list directly using `fused.run()`, useful for debugging. Default is False.
- **collect** (`bool`) – If True, waits for all jobs to complete and returns the collected DataFrame. If False, returns a JobPool object. Default is True.
- **execution_type** (`ExecutionType`) – The type of batching to use. Either "thread_pool" (default) or "async_loop".
- **cache_max_age** (`str | None`) – The maximum age when returning a result from the cache.
- **cache** (`bool`) – Set to False to disable caching.
- **ignore_exceptions** (`bool`) – Set to True to ignore exceptions when collecting results. Defaults to False.
- **flatten** (`bool`) – Set to True to receive a flat DataFrame of results. Defaults to True.
- **\*\*kwargs** – Additional (constant) keyword arguments to pass to the UDF.

## Returns

`JobPool | ResultType | pd.DataFrame` – JobPool, or DataFrame depending on execution_type and collect parameters.

## Examples

Run a UDF multiple times for the values 0 to 9:

```python
df = fused.submit("username@fused.io/my_udf_name", range(10))
```

Using async batch type:

```python
df = fused.submit(udf, range(10), execution_type="async_loop")
```

Being explicit about the parameter name:

```python
df = fused.submit(udf, [dict(n=i) for i in range(10)])
```

Get the pool of ongoing tasks:

```python
pool = fused.submit(udf, [dict(n=i) for i in range(10)], collect=False)
```

================================================================================

## fused.types
Path: python-sdk/api-reference/fused-types.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-types

# types

Type annotations for UDF parameters. Use these to get proper type hints and enable Fused-specific behaviors.

```python

from fused.types import Bounds, TileGDF

@fused.udf
def udf(bounds: Bounds = None):
    ...
```

## Type Variables

### Bounds

```python
Bounds = TypeVar("Bounds", bound=list)
```

A bounding box as a list of `[min_x, min_y, max_x, max_y]`. When used as a UDF parameter type hint, Fused automatically structures the bounds object based on how the UDF is called.

```python
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    # bounds is structured by Fused based on x, y, z or bbox input
    return gdf[gdf.geometry.intersects(bounds.geometry[0])]
```

### Bbox [Legacy]

```python
Bbox = TypeVar("Bbox", bound=shapely.geometry.polygon.Polygon)
```

A bounding box as a Shapely Polygon geometry.

### Tile [Legacy]

```python
Tile = TypeVar("Tile", bound=gpd.GeoDataFrame)
```

A GeoDataFrame representing a tile. Alias for `TileGDF`.

### TileGDF [Legacy]

```python
TileGDF = TypeVar("TileGDF", bound=gpd.GeoDataFrame)
```

A GeoDataFrame representing a tile with geometry and metadata columns.

### TileXYZ [Legacy]

```python
TileXYZ = TypeVar("TileXYZ", bound=mercantile.Tile)
```

A mercantile Tile object with `x`, `y`, `z` attributes.

### ViewportGDF [Legacy] 

```python
ViewportGDF = TypeVar("ViewportGDF", bound=gpd.GeoDataFrame)
```

A GeoDataFrame representing the current viewport bounds.

## Exception Types

### UdfTimeoutError

Raised when a UDF execution exceeds the time limit.

### UdfRuntimeError

Raised when a UDF encounters a runtime error during execution.

### UdfSerializationError

Raised when a UDF result cannot be serialized for return.

================================================================================

## @fused.udf
Path: python-sdk/api-reference/fused-udf.mdx
URL: https://docs.fused.io/python-sdk/api-reference/fused-udf

# @fused.udf

```python
udf(
    fn: Optional[Callable] = None,
    *,
    name: Optional[str] = None,
    cache_max_age: Optional[str] = None,
    instance_type: Optional[str] = None,
    disk_size_gb: Optional[int] = None,
    region: Optional[str] = None,
    default_parameters: Optional[Dict[str, Any]] = None,
    headers: Optional[Sequence[Union[str, Header]]] = None,
    **kwargs: dict[str, Any]
) -> Callable[..., Udf]
```

A decorator that transforms a function into a Fused UDF.

## Parameters

- **name** (`Optional[str]`) – The name of the UDF object. Defaults to the name of the function.

- **cache_max_age** (`Optional[str]`) – The maximum age when returning a result from the cache.

- **instance_type** (`Optional[str]`) – The type of instance to use for remote execution ('realtime', or 'small', 'medium', 'large' or one of the whitelisted instance types). If not specified (and also not specified in `fused.run()`), defaults to 'realtime'.

- **disk_size_gb** (`Optional[int]`) – The size of the disk in GB to use for remote execution (only supported for a batch instance type).

- **default_parameters** (`Optional[Dict[str, Any]]`) – Parameters to embed in the UDF object, separately from the arguments list of the function. Defaults to None for empty parameters.

- **headers** (`Optional[Sequence[Union[str, Header]]]`) – A list of files to include as modules when running the UDF. For example, when specifying `headers=['my_header.py']`, inside the UDF function it may be referenced as:

  ```python

  my_header.my_function()
  ```

  Defaults to None for no headers.

## Returns

- `Callable[..., Udf]` – A callable that represents the transformed UDF.

## Example

```python
@fused.udf
def udf(bbox, table_path="s3://fused-asset/infra/building_msft_us"):
    ...
    gdf = table_to_tile(bbox, table=table_path)
    return gdf
```

================================================================================

## fused.h3
Path: python-sdk/api-reference/h3.mdx
URL: https://docs.fused.io/python-sdk/api-reference/h3

## run_ingest_raster_to_h3

```python
run_ingest_raster_to_h3(
    src_path: str | list[str],
    output_path: str,
    metrics: str | list[str] = "cnt",
    res: int | None = None,
    k_ring: int = 1,
    res_offset: int = 1,
    chunk_res: int | None = None,
    file_res: int | None = None,
    overview_res: list[int] | None = None,
    overview_chunk_res: int | list[int] | None = None,
    max_rows_per_chunk: int = 0,
    include_source_url: bool = True,
    target_chunk_size: int | None = None,
    debug_mode: bool = False,
    remove_tmp_files: bool = True,
    tmp_path: str | None = None,
    overwrite: bool = False,
    steps: list[str] | None = None,
    extract_kwargs: list[str] | None = ,
    partition_kwargs: list[str] | None = ,
    overview_kwargs: list[str] | None = ,
    **kwargs: list[str] | None
)
```

Run the raster to H3 ingestion process.

This process involves multiple steps:

- extract pixels values and assign to H3 cells in chunks (extract step)
- combine the chunks per partition (file) and prepare metadata (partition step)
- create the metadata `_sample` file and overviews files

**Parameters:**

- **src_path** (<code>str, list</code>) – Path(s) to the input raster data.
  When this is a single path, the file is chunked up for processing
  based on `target_chunk_size`. When this is a list of paths, each
  file is processed as one chunk.
- **output_path** (<code>str</code>) – Path for the resulting Parquet dataset.
- **metrics** (<code>str or list of str</code>) – The metrics to compute per H3 cell.
  Supported metrics are either "cnt" or a list containing any of
  "avg", "min", "max", "stddev", "mode" (i.e. most common value),
  and "sum".
- **res** (<code>int</code>) – The resolution of the H3 cells in the output data.
  The pixel values are assigned to H3 cells at resolution
  `res + res_offset` and then aggregated to `res`.
  By default, this is inferred based on the resolution of the
  input data ensuring the H3 cell size is close to the pixel size
  (e.g. for a raster with pixel size of 30x30m, a resolution of 11
  is inferred).
- **k_ring** (<code>int</code>) – The k-ring distance at resolution `res + res_offset`
  to which the pixel value is assigned (in addition to the center
  cell). Defaults to 1.
- **res_offset** (<code>int</code>) – Offset to child resolution (relative to `res`) at
  which to assign the raw pixel values to H3 cells.
- **file_res** (<code>int</code>) – The H3 resolution to chunk the resulting files of the
  Parquet dataset. By default will be inferred based on the target
  resolution `res`. You can specify `file_res=-1` to have a single
  output file.
- **chunk_res** (<code>int</code>) – The H3 resolution to chunk the row groups within
  each file of the Parquet dataset (ignored when `max_rows_per_chunk`
  is specified). By default will be inferred based on the target
  resolution `res`.
- **overview_res** (<code>list of int</code>) – The H3 resolutions for which to create
  overview files. By default, overviews are created for resolutions 3
  to 7 (or capped at a lower value if the `res` of the output dataset
  is lower).
- **overview_chunk_res** (<code>int or list of int</code>) – The H3 resolution(s) to chunk
  the row groups within each overview file of the Parquet dataset. By
  default, each overview file is chunked at the overview resolution
  minus 5 (clamped between 0 and the `res` of the output dataset).
- **max_rows_per_chunk** (<code>int</code>) – The maximum number of rows per chunk in the
  resulting data and overview files. If 0 (the default), `chunk_res`
  and `overview_chunk_res` are used to determine the chunking.
- **include_source_url** (<code>bool</code>) – If True, include a `"source_url"` column in
  the output dataset that contains a list of source URLs that
  contributed data to each H3 cell. Defaults to True, set to False
  to omit this column.
- **target_chunk_size** (<code>int</code>) – The approximate number of pixel values to
  process per chunk in the first "extract" step. Defaults to
  10,000,000 for ingesting a single file or a few files. If ingesting
  more than 20 files, each file is processed as a single chunk by
  default, but you can override this by specifying a specific
  `target_chunk_size` value, or by specifying `target_chunk_size=0` to
  always process each file as a single chunk.
- **debug_mode** (<code>bool</code>) – If True, run only the first two chunks for
  debugging purposes. Defaults to False.
- **remove_tmp_files** (<code>bool</code>) – If True, remove the temporary files after
  ingestion is complete. Defaults to True.
- **tmp_path** (<code>str</code>) – Optional path to use for the temporary files.
  If specified, the extract step is skipped and it is assumed that
  the temporary files are already present at this path.
- **overwrite** (<code>bool</code>) – If True, overwrite the output path if it already
  exists, by first removing the existing content before writing the
  new files. Defaults to False, in which case an error is raised if
  the `output_path` is not empty.
- **steps** (<code>list of str</code>) – The processing steps to run. Can include
  "extract", "partition", "metadata", and "overview". By default, all
  steps are run.
- **extract_kwargs** (<code>dict</code>) – Additional keyword arguments to pass to
  `fused.submit` for the extract step.
- **partition_kwargs** (<code>dict</code>) – Additional keyword arguments to pass to
  `fused.submit` for the partition step.
- **overview_kwargs** (<code>dict</code>) – Additional keyword arguments to pass to
  `fused.submit` for the overview step.

The extract, partition and overview steps are run in parallel using
`fused.submit()`. By default, the function will first attempt to run this using
"realtime" instances, and retry any failed runs using "large" instances.

You can override this behavior by specifying the `engine`, `instance_type`,
`max_workers`, `n_processes_per_worker`, etc parameters as additional
keyword arguments to this function (`**kwargs`). If you want to specify
those per step, use `extract_kwargs`, `partition_kwargs`, and `overview_kwargs`.
For example, to run everything locally on the same machine where this
function runs, use:

```
run_ingest_raster_to_h3(..., engine="local")
```

To run the extract step on realtime and the partition step on medium
instance, you could do:

```
run_ingest_raster_to_h3(...,
    extract_kwargs=,
    partition_kwargs=,
)
```

In contrast to `fused.submit` itself, the ingestion sets `n_processes_per_worker=1`
by default to avoid out-of-memory issues on batch instances. You can
increase this if you know the instance has enough memory to process multiple
chunks in parallel.

---

================================================================================

## fused.ingest
Path: python-sdk/api-reference/job.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/api-reference/job

## `fused.ingest`

```python showLineNumbers
def ingest(
    input: Union[str, Sequence[str], Path, gpd.GeoDataFrame],
    output: Optional[str] = None,
    *,
    output_metadata: Optional[str] = None,
    schema: Optional[Schema] = None,
    file_suffix: Optional[str] = None,
    load_columns: Optional[Sequence[str]] = None,
    remove_cols: Optional[Sequence[str]] = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: Optional[bool] = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: Union[int, float, None] = None,
    partitioning_maximum_per_chunk: Union[int, float, None] = None,
    partitioning_max_width_ratio: Union[int, float] = 2,
    partitioning_max_height_ratio: Union[int, float] = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: Optional[float] = None,
    subdivide_stop: Optional[float] = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: Optional[Tuple[str, str]] = None,
    gdal_config: Union[GDALOpenConfig, Dict[str, Any], None] = None
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Arguments**:

- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

        ```python showLineNumbers
        fused.ingest(
            ...,
            lonlat_cols=("x", "y")
        )
        ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        ```python showLineNumbers
        fused.ingest(
            ...,
            gdal_config=
            }
        )
        ```

**Returns**:

Configuration object describing the ingestion process. Call `.run_batch` on this object to start a job.

**Examples**:

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).run_batch()
```

---
#### `job.run_batch`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_batch` on this object to start the ingestion job.

```python
def run_batch(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Begin job execution.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_batch()` returns a `RunResponse` object which has the following methods:

#### `get_status`

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

---

#### `print_logs`

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

**Returns**:

  None

---
#### `get_exec_time`

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

---
#### `tail_logs`

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1) -> None
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.

================================================================================

## JobPool
Path: python-sdk/api-reference/jobpool.mdx
URL: https://docs.fused.io/python-sdk/api-reference/jobpool

## JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### cancel

```python
cancel(wait: bool = False)
```

Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

---

### retry

```python
retry()
```

Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

---

### total_time

```python
total_time(since_retry: bool = False) -> timedelta
```

Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

---

### times

```python
times() -> list[timedelta | None]
```

Time taken for each task.

Incomplete tasks will be reported as None.

---

### done

```python
done() -> bool
```

True if all tasks have finished, regardless of success or failure.

---

### all_succeeded

```python
all_succeeded() -> bool
```

True if all tasks finished with success

---

### any_failed

```python
any_failed() -> bool
```

True if any task finished with an error

---

### any_succeeded

```python
any_succeeded() -> bool
```

True if any task finished with success

---

### arg_df

```python
arg_df()
```

The arguments passed to runs as a DataFrame

---

### status

```python
status()
```

Return a Series indexed by status of task counts

---

### wait

```python
wait()
```

Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### tail

```python
tail(stop_on_exception = False)
```

Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### results

```python
results(return_exceptions = False) -> list[Any]
```

Retrieve all results of the job.

Results are ordered by the order of the args list.

---

### results_now

```python
results_now(return_exceptions = False) -> dict[int, Any]
```

Retrieve the results that are currently done.

Results are indexed by position in the args list.

---

### df

```python
df(
    status_column: str | None = "status",
    result_column: str | None = "result",
    time_column: str | None = "time",
    logs_column: str | None = "logs",
    exception_column: str | None = None,
    include_exceptions: bool = True,
)
```

Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

---

### get_status_df

```python
get_status_df()
```

---

### get_results_df

```python
get_results_df(ignore_exceptions = False)
```

---

### errors

```python
errors() -> dict[int, Exception]
```

Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

---

### first_error

```python
first_error() -> Exception | None
```

Retrieve the first (by order of arguments) error result, or None.

---

### logs

```python
logs() -> list[str | None]
```

Logs for each task.

Incomplete tasks will be reported as None.

---

### first_log

```python
first_log() -> str | None
```

Retrieve the first (by order of arguments) logs, or None.

---

### success

```python
success() -> dict[int, Any]
```

Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

---

### pending

```python
pending() -> dict[int, Any]
```

Retrieve the arguments that are currently pending and not yet submitted.

---

### running

```python
running() -> dict[int, Any]
```

Retrieve the results that are currently running.

---

### cancelled

```python
cancelled() -> dict[int, Any]
```

Retrieve the arguments that were cancelled and not run.

---

### collect

```python
collect(ignore_exceptions = False, flatten = True)
```

Collect all results into a DataFrame

---

================================================================================

## fused.options
Path: python-sdk/api-reference/options.mdx
URL: https://docs.fused.io/python-sdk/api-reference/options

## options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

**Examples:**

Change the `request_timeout` option from its default value to 60 seconds:

```py
fused.options.request_timeout = 60
```

## Options

### __dir__

```python
__dir__() -> List[str]
```

### base_url

```python
base_url: str = PROD_DEFAULT_BASE_URL
```

Fused API endpoint

### shared_udf_base_url

```python
shared_udf_base_url: str = PROD_SHARED_UDF_DEFAULT_BASE_URL
```

Shared UDF API endpoint

### auth

```python
auth: AuthOptions = Field(default_factory=AuthOptions)
```

Options for authentication.

### show

```python
show: ShowOptions = Field(default_factory=ShowOptions)
```

Options for object reprs and how data are shown for debugging.

### max_workers

```python
max_workers: int = 16
```

Maximum number of threads, when multithreading requests

### run_timeout

```python
run_timeout: float = 130
```

Request timeout for UDF run requests to the Fused service

### request_timeout

```python
request_timeout: Union[Tuple[float, float], float, None] = 5
```

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### metadata_request_timeout

```python
metadata_request_timeout: float = 60.0
```

Request timeout for file metadata requests (e.g., /file-metadata endpoint).
These requests may involve processing large parquet files and can take longer.

### request_max_retries

```python
request_max_retries: int = 5
```

Maximum number of retries for API requests

### request_retry_base_delay

```python
request_retry_base_delay: float = 1.0
```

Base delay before retrying a API request in seconds

### realtime_client_id

```python
realtime_client_id: Optional[StrictStr] = None
```

Client ID for realtime service.

### max_recursion_factor

```python
max_recursion_factor: int = 5
```

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

```python
save_user_settings: StrictBool = True
```

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

```python
default_udf_run_engine: Optional[StrictStr] = None
```

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

```python
default_validate_imports: StrictBool = False
```

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

```python
prompt_to_login: StrictBool = False
```

Automatically prompt the user to login when importing Fused.

### no_login

```python
no_login: StrictBool = False
```

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

```python
pyodide_async_requests: StrictBool = False
```

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

```python
cache_directory: Path | None = None
```

The base directory for storing cached results.

### data_directory

```python
data_directory: Path = None
```

The base directory for storing data results. Note: if storage type is 'object', then this path is relative to
fd_prefix.

### temp_directory

```python
temp_directory: Path = Path(tempfile.gettempdir())
```

The base directory for storing temporary files.

### never_import

```python
never_import: StrictBool = False
```

Never import UDF code when loading UDFs.

### gcs_secret

```python
gcs_secret: str = 'gcs_fused'
```

Secret name for GCS credentials.

### gcs_filename

```python
gcs_filename: str = '/tmp/.gcs.fused'
```

Filename for saving temporary GCS credentials to locally or in rt2 instance

### gcp_project_name

```python
gcp_project_name: Optional[StrictStr] = None
```

Project name for GCS to use for GCS operations.

### logging

```python
logging: StrictBool = Field(default=False, validate_default=True)
```

Control logging for Fused

### verbose_udf_runs

```python
verbose_udf_runs: StrictBool = True
```

Whether to print logs from UDF runs by default

### default_run_headers

```python
default_run_headers: Optional[Dict[str, str]] = 

```

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

```python
default_dtype_out_vector: StrictStr = 'parquet'
```

Default transfer type for vector (tabular) data

### default_dtype_out_raster

```python
default_dtype_out_raster: StrictStr = 'npy,tiff'
```

Default transfer type for raster data

### default_dtype_out_simple

```python
default_dtype_out_simple: StrictStr = 'json'
```

Default transfer type for simple Python data (bool, int, float, str, list)

### fd_prefix

```python
fd_prefix: Optional[str] = None
```

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

```python
verbose_cached_functions: StrictBool = True
```

Whether to print logs from cache decorated functions by default

### local_engine_cache

```python
local_engine_cache: StrictBool = True
```

Enable UDF cache with local engine

### default_send_status_email

```python
default_send_status_email: StrictBool = True
```

Whether to send a status email to the user when a job is complete.

### cache_storage

```python
cache_storage: StorageStr = 'auto'
```

Specify the default cache storage type

### use_process_pool_for_local_submit

```python
use_process_pool_for_local_submit: StrictBool = False
```

Use ProcessPoolExecutor instead of ThreadPoolExecutor for local engine submit.
This avoids GDAL thread-safety issues and Python GIL limitations, but has higher
memory overhead and slower startup time.

### row_group_batch_size

```python
row_group_batch_size: int = 32768
```

Target size in bytes for combining adjacent row group downloads.
Adjacent row groups from the same file will be combined into single downloads
until this threshold is reached. Default is 32KB (32768 bytes), which is
optimized for S3 performance.

### base_web_url

```python
base_web_url
```

### save

```python
save()
```

Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

================================================================================

## Udf (class)
Path: python-sdk/api-reference/udf.mdx
URL: https://docs.fused.io/python-sdk/api-reference/udf

## Udf

The `Udf` class is the object you get when defining a UDF with the
`@fused.udf` decorator, or when loading
a saved UDF with `fused.load()`.

## Properties

### name

The name of the UDF. Defaults to the function name.

### code

The source code of the UDF as a string.

### headers

Sequence of header files included with the UDF.

### metadata

Optional dictionary of metadata associated with the UDF.

### parameters

Dictionary of default parameters for the UDF.

### cache_max_age

Maximum cache age in seconds. `None` for default caching behavior.

### instance_type

The instance type to use for remote execution (e.g., `'realtime'`, `'small'`, `'medium'`, `'large'`).

### disk_size_gb

Disk size in GB for batch execution.

### region

AWS region for execution.

### entrypoint

The name of the entrypoint function within the UDF code.

### catalog_url

Returns the URL to open this UDF in the Workbench Catalog, or `None` if the UDF is not saved.

```python
udf = fused.load("my_udf")
print(udf.catalog_url)
# https://www.fused.io/workbench/catalog/my_udf-abc123
```

---

## Methods

### to_directory

```python
to_directory(where: str | Path | None = None, *, overwrite: bool = False)
```

Write the UDF to disk as a directory (folder).

**Parameters:**

- **where** (`str | Path | None`) – Path to a directory. If not provided, uses the UDF function name.
- **overwrite** (`bool`) – If True, overwriting is allowed.

---

### to_file

```python
to_file(where: str | Path | BinaryIO, *, overwrite: bool = False)
```

Write the UDF to disk as a zip file.

**Parameters:**

- **where** (`str | Path | BinaryIO`) – Path to a file or a file-like object.
- **overwrite** (`bool`) – If True, overwriting is allowed.

---

### to_fused

```python
to_fused(overwrite: bool | None = None)
```

Save this UDF to the Fused service.

**Parameters:**

- **overwrite** (`bool | None`) – If True, overwrite existing remote UDF.

**Example:**

```python
@fused.udf
def my_udf():
    return "Hello"

my_udf.to_fused()  # Save to Fused
my_udf.to_fused(overwrite=True)  # Update existing
```

---

### run_local

```python
run_local(*, inplace: bool = False, validate_imports: bool | None = None, **kwargs) -> UdfEvaluationResult
```

Evaluate this UDF locally against a sample.

**Parameters:**

- **inplace** (`bool`) – If True, update this UDF object with schema information.
- **validate_imports** (`bool | None`) – Whether to validate imports.
- ****kwargs** – Additional arguments to pass to the UDF.

**Example:**

```python
result = udf.run_local(x=0, y=0, z=0)
```

---

### schedule

```python
schedule(
    minute: list[int] | int,
    hour: list[int] | int,
    day_of_month: list[int] | int | None = None,
    month: list[int] | int | None = None,
    day_of_week: list[int] | int | None = None,
    udf_args: dict[str, Any] | None = None,
    enabled: bool = True
) -> CronJob
```

Schedule this UDF to run on a cron schedule.

**Parameters:**

- **minute** (`list[int] | int`) – Minute(s) to run (0-59).
- **hour** (`list[int] | int`) – Hour(s) to run (0-23).
- **day_of_month** (`list[int] | int | None`) – Day(s) of month (1-31). Default: every day.
- **month** (`list[int] | int | None`) – Month(s) (1-12). Default: every month.
- **day_of_week** (`list[int] | int | None`) – Day(s) of week (0-6, 0=Sunday). Default: every day.
- **udf_args** (`dict[str, Any] | None`) – Arguments to pass to the UDF.
- **enabled** (`bool`) – Whether the schedule is active.

**Example:**

```python
# Run every day at 8:00 AM
udf.schedule(minute=0, hour=8)

# Run every Monday at noon
udf.schedule(minute=0, hour=12, day_of_week=1)
```

---

### get_schedule

```python
get_schedule() -> CronJobSequence
```

Retrieve any scheduled runs of this UDF.

**Returns:**

- `CronJobSequence` – List of scheduled cron jobs.

---

### set_parameters

```python
set_parameters(
    parameters: dict[str, Any],
    replace_parameters: bool = False,
    inplace: bool = False
) -> Udf
```

Set the parameters on this UDF.

**Parameters:**

- **parameters** (`dict[str, Any]`) – The new parameters dictionary.
- **replace_parameters** (`bool`) – If True, unset any parameters not in the new dict.
- **inplace** (`bool`) – If True, modify this object. If False, return a new object.

**Example:**

```python
udf = udf.set_parameters()
```

---

### create_access_token

```python
create_access_token(
    *,
    client_id: str | None = None,
    public_read: bool | None = None,
    access_scope: str | None = None,
    cache: bool = True,
    metadata_json: dict[str, Any] | None = None,
    enabled: bool = True
) -> UdfAccessToken
```

Create an access token for sharing this UDF.

**Parameters:**

- **client_id** (`str | None`) – Override which realtime environment to run under.
- **public_read** (`bool | None`) – Whether the token allows public read access.
- **access_scope** (`str | None`) – Access scope for the token.
- **cache** (`bool`) – If True, UDF tiles will be cached.
- **metadata_json** (`dict[str, Any] | None`) – Additional metadata for tiles.
- **enabled** (`bool`) – If True, the token can be used.

**Returns:**

- `UdfAccessToken` – The created access token.

---

### get_access_tokens

```python
get_access_tokens() -> UdfAccessTokenList
```

Get all access tokens for this UDF.

**Returns:**

- `UdfAccessTokenList` – List of access tokens.

---

### delete_cache

```python
delete_cache()
```

Delete the cached results for this UDF.

---

### delete_saved

```python
delete_saved(inplace: bool = True)
```

Delete this UDF from the Fused service.

**Parameters:**

- **inplace** (`bool`) – If True, update this object to reflect deletion.

---

### eval_schema

```python
eval_schema(inplace: bool = False) -> Udf
```

Reload the schema saved in the code of the UDF.

Note: This will evaluate the UDF function.

**Parameters:**

- **inplace** (`bool`) – If True, update this UDF object. Otherwise return a new UDF object.

---

### from_gist

```python
@classmethod
from_gist(gist_id: str) -> Udf
```

Create a UDF from a GitHub gist.

**Parameters:**

- **gist_id** (`str`) – The GitHub gist ID.

**Example:**

```python
udf = Udf.from_gist("abc123def456")
```

================================================================================

## Batch jobs
Path: python-sdk/batch.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/batch

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

```python showLineNumbers

@fused.udf
def udf(
    source_s3_path = '',
    destination_s3_path = ''
):

    file_path = fused.download(source_s3_path, source_s3_path.split('/')[-1])

    # Create a temporary directory to extract the files
    s3 = s3fs.S3FileSystem()
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/unzipped_files')

    # Upload each file to the S3 bucket
    for root, dirs, files in os.walk('/tmp/unzipped_files'):
        for file in files:
            file_path = os.path.join(root, file)
            print('file_path', file_path, destination_s3_path)
            s3.put(file_path, destination_s3_path)

    return pd.DataFrame() # UDFs must return a table or raster

```

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_batch()` to kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

```python showLineNumbers
job = udf(
    source_s3_path = 'https://datadownload-production.s3.amazonaws.com/WCMC_carbon_tonnes_per_ha.zip', 
    destination_s3_path = 's3://fused-users/fused/<YOUR_DIR>/dswid/WCMC_carbon_tonnes_per_ha_10gb/'
)
job_id = job.run_batch()
```

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

```python showLineNumbers
fused.api.job_tail_logs("df335890-4406-4832-bf93-6a3b092e496d")
```

================================================================================

## Changelog
Path: python-sdk/changelog.mdx
URL: https://docs.fused.io/python-sdk/changelog

# Changelog

## v1.30.1 (2026-01-22)

**New Features**
- Experimental: Fused Apps can be added to canvas.
- Experimental: JSON-Render UI can be added to canvas.
- `Udf.get_access_token` function.

**Improvements**
- Various canvas improvements.
- AI tools run in the Workbench.

**Bug Fixes**
- Workbench will prevent editing in multiple tabs that could result in unintended changes.
- Fixed flickering of the canvas sidebar.
- Some canvas nodes (batch jobs and others) will have opaque backgrounds to make them easier to read.
- Fixed auto retry of ingestion batch jobs.
- Fixed UDFs losing position in canvas.
- Fixed a bug where batch jobs would take longer to start.
- Fixed a bug where inline diffs could overlap.
- Fixed a bug where stdout (results panel) could fail to show for some UDFs.

## v1.30.0 (2026-01-14)

**New Features**
- You can Command-click (Ctrl-click on Windows/Linux) on output from UDF to navigate to the line of code that produced that output.
- Added an API to get the currently executing UDF.
- `n_processes_per_worker` supports different return types.
- AI settings have been overhauled. Old profile prompts can be retrieved from the Preferences page.
- Job results will show timeline of starting the job in the Jobs Page
- File explorer has a separate play/pause state.

**Improvements**
- Tile UDFs won't show multiple "Large data" warnings.
- Fixes for ingesting large string columns.
- `fused.submit` will not run the current UDF recursively.
- Many bug fixes included in this release.
- Many improvements to UDF connections (edges) and messaging in Canvas.

## v1.29.0 (2025-12-18)

**New Features:**
- Winter holiday decorations are now added at the top of the Workbench page (this can be disabled in
  the preferences page.)
- `fused.submit` supports `n_processes_per_worker` to batch work together when calling realtime
  instances.
- Experimental: A passcode can be set on a shared canvas.

**Improvements:**
- Canvas UDF list has been redesigned to keep the UI more consistent and discoverable.
- Canvas catalog will be cached locally.
- Workbench Tokens and Preferences pages now show as modals above your currently opened page.
- Upgraded DuckDB to v1.4.3.
- Batch jobs page shows run time, shows exact memory and disk usage, and Fused will keep track
  of additional status information about batch jobs.
- "Fix" prompts for AI now quickly show a few options.
- Various AI and canvas interface improvements.
- `fused.ingest` accepts a job name, which will be shown on the batch jobs page.

**Bug Fixes:**
- Fixed a bug where 0 byte and unusually nanmed objects could not be deleted through the file explorer
- Optimized memory usage when ingesting raster data.
- Batch jobs page is more accurate about errors in logs.
- Fixed a bug with using arrow keys to navigate on the canvas page.
- Fixed a bug with passing some numpy types in `fused.submit`.
- File Explorer will always show the file preview, even when the canvas or map builder is paused.

## v1.28.0 (2025-12-08)

**New Features:**
- Jobs page in Workbench now shows metrics, progress, and what UDF was run.
- Batch jobs will now be named after the UDF being run, by default.
- Raster outputs now support float and various integer types for PNG and JPG output. Values are assumed to be 0-1 for float and `min-max` for each integer type.
- Significantly improved the performance of loading large datasets in Workbench.

**Improvements:**
- Workbench now supports `job://` style URLs for batch job IDs.
- Various improvements to ingesting H3 datasets.
- Improved the suggestion and code results shown in the AI chat.
- Performance improvements in Workbench code editor.
- Show progress bar from fused.submit in batch jobs

**Bug Fixes:**
- Various Canvas bug fixes.
- Don't show duplicate status of AI on canvas nodes.
- Fixed an issue where certain UDF responses could be truncated.
- Batch jobs can be run with more parameters being optional.
- Batch jobs with multiple parameters will not incorrectly be cached.
- Shared canvas pages will not have the AI profile selector.
- Fixed an issue where team UDFs may be `load`ed inadvertently instead of a user's own UDF.
- Fixed a visual bug with the Add Billing button.
- Sticky notes in Canvas will not show as loading.
- Fixed situations where code performance profile would not be sent back to Workbench when lots of `print` statements were run.

## v1.27.0 (2025-11-24)

**New Features:**
- Added Gemini 3 model to AI options.
- Added `fused.h3.run_ingest_raster_to_h3` allowing for raster ingestion into H3 cells.
- Batch jobs started from Workbench will be named after the UDF they were started from. They can also have the disk size specified.

**Improvements:**

Batch jobs:
- We began overhauling the jobs list page in Workbench. It should be more performant, show additional instance-related details, and fix bugs where previously selected jobs would be shown.
- Renaming a UDF will show a confirmation modal.
- On-prem batch jobs will have access to the calling user's email address.
- Adjusted the default disk sizes for batch jobs.

Canvas:
- Canvas features have been moved to their own section of the preferences page.
- Canvas shows an indicator when a UDF will not trigger another UDF because it is unsaved.
- Added new Canvas search interface.
- Added a dedicated docs page with examples of Canvas

Code Editor:
- AI code changes can be partially accepted/rejected.

**Bug Fixes:**
- The code editor gutter will no longer flash forever when loading some UDFs.
- Fixed bugs with batch jobs with unusual settings not being runnable and not being cancelable, and with batch jobs in GCP.
- Fixed bugs with canvas not connecting UDF nodes, looping rerunning UDFs, or showing incorrect UDF run states.
- When running multiple UDFs from Workbench, you will not have to individually cancel running batch jobs if batch job UDFs are selected.
- Fixed various UI and navigation bugs in Workbench.
- Max data auto load limit will be applied to tile UDF requests.
- Adjusted AI prompts.

## v1.26.3 (2025-11-19)

**Bug Fixes:**
- Fixed a bug calling shared token UDFs in Fused Apps.

## v1.26.2 (2025-11-17)

**New Features:**
- You can filter for UDFs in the canvas UDF list.
- Currently open UDFs will show in omnisearch.
- Updated available AI models.

**Improvements:**
- Canvas outlines have been improved.
- Added a Switch repository button to the version page.
- Batch jobs default to a larger disk size.
- DuckDB upgraded to v1.4.2.
- Improved performance of rendering some HTML responses in Workbench.

**Bug Fixes:**
- Fixed a bug with calling `fused.submit` in a loop.
- Fixed passing environment variables to batch jobs running in GCP.
- Fixed a layout shift in omnisearch.
- Fixed running some async UDFs.

## v1.26.1 (2025-11-13)

**Improvements:**
- Added package `whitebox-workflows`.

**Bug Fixes:**
- Profile page debug tabs are condensed into a single debug tab.
- Removed some more unused debug information from canvas exports.
- Adjusted `fused.submit` sleep times when running more tasks than workers.
- Improved performance of file explorer in Firefox.
- Fixed bugs where AI diffs could lock up the page.
- Fixed bugs where AI diffs could disappear without being accepted.
- Fixed bugs with AI queries against loaded data when the UDF had been renamed.
- Changed Canvas drag behavior.
- Fixed the code timing profile not appearing.

## v1.26.0 (2025-11-12)

**New Features:**
- AI can run queries against loaded data using DuckDB in Explain mode.
- UDFs can send parameters to other UDFs, which will appear as edges and can trigger runs.
- New experimental option for "floating" edges in canvas.
- New experimental snap-to-guideline in canvas.
- Hovering over a UDF in canvas will show a code preview.
- New enterprise deployment mode.

**Improvements:**
- HTML output of UDFs in Canvas is adjusted to be easier to see.
- Workbench will show a message if duplicate UDF names or IDs are loaded.
- Simplified the UDF conflict modal when possible.
- GCP environments now support more and larger instance types (M3).
- GCP batch jobs support caching.
- "Reset" in Workbench will also reset parameters on UDFs.
- Map visualization settings now support AI changes.
- Workbench will now cache UDF content locally.
- UDF share URLs can now have the output format specified using a file extension.
- UDF name changes will be saved immediately.
- Added GLM-4.6 model option.
- Adjusted how raster (image) data will render in Canvas.

**Bug Fixes:**
- Fixed bugs with inline diffs generated by AI.
- URLs highlighted in Workbench will include parts after `&`.
- Fixed bugs with collapsing content in the UDF list in canvas.
- Fixed bugs with starting batch jobs in GCP.
- Fixed `all_succeeded` in `fused.submit` with batch jobs.
- Fixed bugs with importing UDFs from GitHub having conflicts.
- Restored table view in file explorer.
- Fixed a bug with the map visualization preset button not applying, and with clicking off the form.
- Fixed a bug with public UDF tokens not navigating to the catalog.
- Canvas save-all should be faster.
- Fixed bugs with finished or failed batch jobs appearing as In Progress.
- Disabled `tqdm` progress bar output in batch jobs.
- Fixed possible recursion storm with batch jobs.
- AI prompts from Canvas will switch to the Build profile.
- Canvas will not draw edges from a UDF to itself.
- Fixed being able to remove a shared token from Workbench when the token is no longer valid.
- Fixed batch jobs incorrectly showing as running in canvas.
- Fixed Reset to GitHub not resetting some state.
- Fixed the versions page causing UDFs to re-run.
- Fixed scaling issues with some API calls.
- Fixed Workbench switching pages under some modals.
- `fd` paths will be resolved in Workbench and replaced with the appropriate `s3` (or other) URL.
- Removed some unused debug information from canvas exports.
- Fixed the Omnisearch input getting focus after closing a modal.

## v1.25.1 (2025-10-29)

**New Features:**
- Experimental relationships (edges) between canvas nodes, allowing for creating a new UDF from an existing data UDF, seeing the data relationships between UDFs, and triggering a chain of UDFs.

**Improvements:**
- Canvas nodes will show "Rendering" when waiting for the browser.
- AI chat can now see the list of functions and UDFs.
- AI chat can call a tool to execute the UDF itself.
- AI chat now features exacto models.
- Workbench will ask to confirm if you leave the tab while a save is in progress.
- UDF Explorer in canvas now includes open shared URL button.
- Code folding is now configured to not fold the `udf` function.
- Random UDF names are chosen from a larger pool.
- Improved performance of loading UDF endpoints.
- HTML output used in Canvas will default to having some additional code and styling added.
- `Xarray` upgraded to 2025.10.01.
- Parameters passed to `fused.run` will be more consistently serialized.

**Bug Fixes:**
- Adjusted the UI for paused canvas.
- UDF actions in canvas will look the same as the rest of the app.
- Adjusted Hyparquet link location.
- Fixed deleting S3 directories with many objects.
- Fixed save all UDFs when names conflict.
- UDF favorites can be saved in the cloud.
- AI chat will have more consistent ordering of context items.
- AI model selected for inline edits will be persisted.
- Fixed bugs with UDFs showing as "running" in canvas even though they were not actually running.
- Fixed bugs where the UDF Explorer listing would get truncated for users with many UDFs.
- Canvas will not reorder nodes in shared mode.
- Fixed an error when running UDFs with many lines of code and the profiler option was on.
- Fixed "remove" being disabled on some UDFs on canvas.
- Canvas sticky notes being removed will result in them being deleted permanently.
- UDFs deleted from settings will be deleted from Workbench too.
- Fixed a bug where UDFs called within a UDF called from a shared token would themselves not be able to call UDFs by name.
- Fixed the error message when an invalid `cache_max_age` value was passed.

## v1.25.0 (2025-10-22)

**New Features:**
- Workbench now auto-analyzes error messages from UDFs & suggest an AI fix
- Added `fused.context` API for accessing advanced parts of the UDF request.
- Added Claude Haiku model support.
- UDFs can now be favorited in Workbench.

**Improvements:**
- Shared tokens will now use `udf.ai`.
- Shared token URLs now default to HTML when no format is specified.
- UDF list in canvas shows the UDF status.
- Results panel will no longer pop-up in canvas.
- Workbench's UDF Explorer will now open quicker with cached UDFs.
- `fused.run` will serialize more values for passing to UDFs.
- Canvas has a new auto-arrange feature.
- Workbench pages such as jobs list and file explorer are now modal overlays.
- Run/Freeze button has moved from the results panel back to the Canvas panel.
- Adjusted how AI finishes responses, adding new summaries and auto-naming.
- Dataset index now includes column statistics.
- Workbench table view now shows records count.
- Canvas now supports Parameter-type messages sent to/from UDFs.
- *Reset* button in Workbench now resets parameters set on UDFs.
- `fused.cache` no longer takes (potentially changed) variables into account that are defined outside of the function body
- Improvement for caching of "nested UDFs" (a udf defined in the code and run by the main udf UDF), to not consider the full code but only the code used by that nested UDF

**Bug Fixes:**
- Fixed various bugs with the canvas editor and canvas share mode.
- Fixed various bugs with the AI panel's context selection menu and send button.
- Fixed local engine cache when loading UDFs with `fused.load`.
- Fixed job details scrolling to top when new logs were available.
- Fixed showing many toast messages when saving a canvas.
- Shared tokens and batch jobs will now load UDFs the same as how they would when run by their original author.
- Fixed some broken Omnisearch shortcuts.
- `fused.submit` UDF and arguments are not longer required to be positional.
- Prevented infinite recursion when determing the cache key for `fused.cache`.
- Fixed using an incorrect Git path to fetch content from community repos.
- AI thinking now supports auto-scroll.
- AI panel persists the prompt when changing model.
- Fixed a bug where some `print` statements could result in logs that error'd when sending back to Workbench.
- Adjusted how many UDFs are returned from the server to Workbench at once.

## v1.24.5 (2025-10-13)

**New Features:**
- You can now directly paste context items into AI chat with text like `[@my_other_udf]`.
- It is now possible to retrieve header values from within UDFs.

**Improvements:**
- Improved feedback that a UDF is running in Workbench.
- The default AI profiles in Workbench have been renamed to "Fast", "Explain", and "Smart".
- Enabled nesting of UDFs under certain circumstances.
- File explorer is now shown above the currently open page.
- Upgraded DuckDB to v1.4.1.
- Workbench will show cached UDF Explorer data until it is refreshed.
- `Cmd+Shift+F` is now bound to toggle code folding.

**Bug Fixes:**
- Smoothed out the experience working with Canvas. Many bugs, keyboard shortcuts, UI elements, and interactions were adjusted.
- Collections were renamed to Canvas in a few places.
- Fixed a bug when opening Workbench with an AI prompt provided in the URL.
- Fixed a bug with opening pull requests on the public UDF repo from within Workbench.
- Adjusted the AI prompts in Workbench.
- UDF details page now shows code first.
- Adjusted versions page styling.
- Fixed the changes modal empty state styling.
- Uploaded UDFs will no longer inherit version control metadata.
- It is no longer possible to hide the Canvas in the Canvas page.
- Upload modal will now open data with file opener UDFs.
- `Udf.to_fused` is now more robust for flaky network situations.
- Fixed bugs where saving UDFs would not break the Canvas cache.

## v1.24.4 (2025-10-06)

**New Features:**

- Drag & Drop upload of files anywhere in Canvas view! 

**Improvements:**

- `fused.submit` now supports `instance_type` parameter.
- Workbench will remember which page you were on before switching to some tabs, so you can go back.
- Returning numpy arrays with 4 or more dimensions will be truncated to 3 dimensions for PNG visualization.
- Supervisor profile is available without enabling an experimental feature.

**Bug Fixes:**

- Various bug fixes for Canvas UI.
- Workbench will clean up old AI chats from your browser.
- Fixed `fused.api.job_get_exec_time`.
- Fixed fetching a single result for large (batch) UDF runs.
- `fused.run` will pass `cache_max_age` for large jobs.

## v1.24.3 (2025-10-03)

**New Features:**

- Sticky notes are supported in canvas view.
- Canvas view now has an auto-arrange button.
- It is possible to start AI changes directly from the canvas view.

**Improvements:**

- Improved how UDF context is passed to AI chat, and removed Rename from default tools.
- File Explorer will open files in the canvas view instead of the UDF builder view.
- Fused-py will prefer `npy` format for numpy arrays.
- UDFs can be deleted from the settings modal.

**Bug Fixes:**

- Fixed UI layout bugs in canvas view and AI chat.
- Fixed bugs with starting ingest jobs.
- Fixed bugs with AI auto-retry.

## v1.24.2 (2025-10-01)

**New Features:**

- Added Claude Sonnet 4.5 model.
- Canvas mode has a UDF selection list in its home view.
- AI chats can continue in the background.

**Improvements:**

- Improved highlighting of syntax errors in Workbench.
- Improved highlighting of UDF execution errors in Workbench.
- Adjusted the design of omnisearch at the top panel of Workbench.
- Batch jobs in Workbench run based on cache_max_age.
- Various canvas UI improvements.
- Upgraded package `arraylake`, and added `icechunk`.

**Bug Fixes:**

- Fixed pending changes still showing after switching UDFs.
- Fixed an issue with saving UDFs and shared tokens slowing down.
- Fixed stale data being cached locally when importing a canvas (collection).
- Fixed an issue where UDF renames would incorrect choose to show the "duplicate UDF name" modal.

## v1.24.0 (2025-09-29)

**New Features:**

- Canvas view now default in Workbench!
- UDF are now only shared to "Team" by default rather than "Public"

**Improvements:**

Canvas View:
- Ability to share Canvas in a single click
- Added Team / Public sharing options for Canvas (default to Team)
- Easily see if your Canvas was successfully run or failed
- Simplified 2 panel layout: Jump between code & AI on the left and Canvas on the right 
- Hide / Show UDFs 

AI Chat:
- Much better support for multiple AI chats running at the same time in different UDFs
- Improvements to Experimental Supervisor mode
- Adding inline diff view to code changes made by AI
- Improved charting ability in AI chat

Workbench:
- Added S3 policy tab to profile modal

Dependencies:
- Update `pyogrio` to 0.11
- Update `duckdb` to 1.4.0

**Bug Fixes:**

- Fixed bug in Version tab 

## v1.23.0 (2025-09-18)

**New Features:**

- New experimental "Supervisor" mode in AI chat, which features more Workbench capabilities and enhanced search capabilities. Different tools in Supervisor mode can be turned on and off in the AI chat settings.
- Workbench can now run UDFs in batch mode when `instance_type` is set on `@fused.udf()`.
- AI changes now show an inline diff in the code editor.

**Improvements:**

- Many UDF actions can now be done directly inside Canvas.
- Improvements to prompts and automatic suggestions in AI chat.
- You can now stop dictation and send by clicking the send button in AI chat.
- HTML view in Workbench will again show UDF output as it will be embedded.
- AI chat settings page has been visually updated to be easier to follow.
- Added "Fold all code" and "Unfold all code" actions in the search bar, and Workbench will now remember what code blocks were folded in a UDF.
- *Inline AI* and *Auto share on save* are now general preferenecs, rather than experimental.

**Bug Fixes:**

- Provided more informative warning messages in the Python SDK.
- Allowed non-authenticated users to retrieve public UDFs.
- Fixed JSON serialization of GeoDataFrames without geometry columns.
- Fixed `/public` UDF page.
- AI chat will scroll to bottom more consistently.
- AI chat will show tool calls in thinking blocks.
- Backend-informed autocomplete is re-enabled.

**API Changes:**

- Removed deprecated `job` function.
- Made batch job logs more concise.
- Upgraded DuckDB to v1.4.0.

## v1.22.8 (2025-09-11)

**Bug Fixes:**
- Fixed passing a parameter named `url` to `fused.run`.
- Fixed data table filter interactions with overflow and fullscreen, and fixed min/max labels.
- Fixes for the view changes modal and fixed a crash when duplicating UDFs.
- Fixed an issue where large error messages could cause no useful information to appear.
- Fixed an issue where GitHub PR information would not synchronize well.
- Fixed Workbench not showing a UDF was running and improved it to show the elapsed time.

## v1.22.7 (2025-09-09)

**New Features:**

- Experimental: Dataset discovery in AI chat.
- New Kimi K2 model added to AI chat.

**Improvements:**

AI & Chat:
- Improved AI chat window with better error handling and scrollback/redo functionality.
- AI can now fetch datasets via tools and S3 URLs in chat are now clickable.
- Better context management and the context size is now shown.
- Model selector synchronizes with profile model selector.

Workbench:
- Enhanced version control with improved diff viewer and more prominent titles.
- Auto-save UDFs after commit creation and when resolving stale UDFs.
- Enhanced data table with improved copy functionality, date filtering/sorting, and better header responsiveness.
- File UDFs now duplicate instead of showing conflict modals for better UX.
- Better UDF management with streamlined rename handling and duplicate detection.
- Theme fixes and UI improvements across the interface.

Data & Datasets:
- UTF-8 encoding by default for HTML data returns.
- Improved dataset sorting and search functionality.

**Bug Fixes:**

- Fixed serialization errors for UDFs returning interval categories with better error messages.
- Resolved UDF conflict modal issues and improved saving from action buttons.
- Improved UI performance when renaming UDFs, and in the AI chat.
- Corrected UDF icon display on hover in version pages.
- Fixed slider component thumb positioning.
- Fixed various canvas and UI stability issues.

**API Changes:**

- Polling-based UDF execution now enabled by default for `fused.run()`.
- Single serialization output format parameter in UDF run URLs, via `format`. Backward compatibility maintained for `dtype_out_vector/raster` parameters.

## v1.22.6 (2025-09-02)

**New Features:**

Re-Launching udf.ai! Let anyone talk to your data!

Learn how to connect your own data or analysis here

- Experimental: "MCP Creator" AI profile to simplify creation of MCP servers from UDFs. Read more.
- Experimental auto-retry on errors: AI will try fixing the error until it can either fix it or hits its limit. 
- Experimental: Dataset discovery scheduling in unstable environments.
- Experimental: Collection share tokens.

**Improvements:**
- AI Chat: More models, faster inference, better UX and error handling.
- AI now better understands data & has significantly improved ability to make charts especially for larger datasets.

- Workbench: Enhanced version page, improved UDF management flow, better data table filtering.
- Git integration: improved experience when pushing UDFs to GitHub.

**Bug Fixes:**
- AI & UDFs: Prevented AI writing to read-only UDFs, fixed serialization errors, JSON args, and File UDF save states.
- Version Control: Fixed stale UDF updates, staging detection, and backendId preservation.
- UI: Fixed Windows Ctrl+Click, modal flickering, autosave callbacks, and selection issues.
- [Experimental] Canvas: Fixed node persistence.

## v1.22.5 (2025-08-26)

**New Features:**
- Experimental Canvas mode.

**Improvements:**
- Added Deepseek v3.1 model to AI chat.

- AI chat has better layout and indicators for its settings. AI chat also shows user input when asking for more instructions.
- Versions button has moved to the bottom of the Workbench sidebar.

- Versions page shows the difference between upstream and a saved copy of a UDF.
- Versions page shows team repositories first, and will prompt when changing repository to a public one.
- Data table widget has `bigint` support, copy to clipboard, improved headers, and better filter support.

- Added html5lib package to the runtime.

**Bug Fixes:**
- AI chat will not suggest charting very small dataframes, along with other improvements to its suggestions and auto-fixes.
- Fixed bugs with the visualization menu in the map view. It works again and will not crash when no UDF is selected.
- Workbench won't show delete and versions button when they cannot do anything.
- Bug fixes for the conflict modal in Workbench not working or not updating the existing UDF.
- Fixed Workbench profile text for enterprise users.

**API changes:**
- Large jobs can now be run using `fused.run` and the `instance_type` parameter.

**Deprecations:**
- Parameters under the UDF in the advanced UDF editor is deprecated. It can be turned back on from the Workbench Preferences.

## v1.22.1 (2025-08-19)

**Improvements:**
- Inline AI editing is enabled by default.
- Improved data table component filtering.
- `fused.submit` results are cached by default.
- AI chat will now suggest new users to login after first prompt 
- Multiple open tabs message will appear at the top of the page instead of as a modal.

**Bug Fixes:**
- Fixed a bug when loading some HTML UDF outputs that caused them to not load scripts.
- Fixed bugs with AI prompts.
- Fixed bugs with navigating to particular UDFs on the version page.
- You can opt to keep a changed version of a UDF instead of reloading with changes.
- Fixed file explorer for basic tier users.
- Fixed the Add Billing modal appearing when it was not supposed to.
- The UDF selector has a minimum width and is no longer editable.

## v1.22.0 (2025-08-18)

**New Features:**
- Free tier is now open for all. Signing in to Fused now gives you access to a shared, free compute environment with a daily quota.
- New simplified UDF editor layout. The previous UDF editor with map view is still available as the *Advanced UDF editor*.

**Improvements:**
- Added inline AI editing features as an experimental feature.
- Added `@`-sign to select context in AI chat.
- Added profile selection in AI chat.
- Added new models to AI chat.
- AI chat context will understand the results history.
- Many adjustments to AI prompts.
- Table widget has more filter capabilities.
- Many widgets now load via Parquet for faster and better loading.
- UDF list has had many buttons consolidated into the three dot menu in the advanced UDF builder.

**Bug Fixes:**
- Significant improvements to self-service in Workbench.
- Fixed duplicate UDF and conflicting UDF modal bugs.
- Versions page will remember the last selected repository.
- Workbench will prompt to load new versions from GitHub.
- File explorer will show `/mount` instead of `file:///` URL.
- Fixed issues where HTML results would not be rendered with UTF-8 charset.
- Fixed issues where share token may not get cleared in Workbench when duplicating UDFs.
- Fixed bugs with the canvas map widget loading, filtering, and coloring. 
- Improved error messages and `run_*_async` when using `fused` from within Pyodide, for Pyodide-specific considerations.
- `to_fused(overwrite=True)` will delete the original UDF as expected.
- stderr will always be displayed for UDF runs.

**API changes:**
- Renamed `run_remote` to `run_batch`.
- Deprecate `fused.utils`, which has been replaced with `fused.load`.

## v1.21.6 (2025-07-31)

**New Features:**
- Added Qwen 3 to AI chat.

- Added a Table frame type.

**Improvements:**

AI:
- AI chat can be docker on the right or the left.
- You can now specify specific UDFs in AI chat.

Versioning: 
- Version control page will ask to confirm pushing to a different repository.

Rendering:
- HTML (previously *Embed*) view is always available.

**Bug Fixes:**
- Fixed bugs with caching POST requests for some UDFs.
- Fixed bugs with AI chat context.
- Fixed bugs with self service onboarding and billing portal.
- Fixed bugs where Table mode would not open by default in Workbench.
- Fixed bugs with cache_storage.

## v1.21.5 (2025-07-30)

**New Features:**
- Added self service signup for new users!
- Added voice input to udf.ai and to rich text frames.

**Improvements:**

AI Assistant:
- AI chat can now see results printed from a UDF.
- AI chat can now see sample rows returned from a UDF.
- AI chat can now show the code diff with generated code.

General:
- Profile modal now shows usage quota, when applicable.
- `@fused.cache` functions can now be profiled, and now supports object storage.
- Canvas now shows controls on hover.
- Canvas now has an Embed, Table, and Histogram frame types.
- Sped up UDF executions when `cache_max_age=0` was specified.
- Default UDF in Workbench is much simpler.

Canvas:
- Canvas now shows controls on hover.

**Bug Fixes:**
- Fixed saving UDFs where the UDF had previously been deleted on the server.
- Adjusted the default run timeout to match the backend run timeout.
- Fixed `fused.run` with `sync=False`.

## v1.21.4 (2025-07-24)

Minor bug fixes.

## v1.21.3 (2025-07-23)

Launching udf.ai! Ask your data questions, get AI to help you build answers.

**New Features:**
- Added support for speak-to-AI in the AI chat.
- Improved AI chat with one-click prompt suggestions and better tooltips.

**Improvements:**
- Improved color code handling in the editor.
- Moved experimental inline AI features behind feature flag.
- File-opener UDFs moved to the community catalog.

**Bug Fixes:**
- Fixed issues with undo-ing AI chat changes.
- Fixed issues with AI chat disconnections.
- Fixed caching for HTML return values from UDFs.
- Fixed handling of request parameters for UDF runs.

## v1.21.2 (2025-07-21)

**New Features:**
- Added ability to record audio into AI input.
- Added auto-share UDF on save functionality. (a shared token is created on first save for all new UDFs)
- Added compact mode for Canvas view.
- Added connection and save notifications for Canvas.
- Added ability to copy common load commands to clipboard.
- Added cache hit rate display on user profile.

**Improvements:**
- Enhanced AI Assistant with better UI/UX for changes and settings menu.
- Workbench will automatically add a default UDF, and updated the default UDF template.
- String return types will be assumed to be HTML by default.
- Improved version control page performance and styling. Clicking on the changes pending asterisk (`*`) will now go directly to the version control page.
- Improved performance of the GitHub integration.
- Improved saving all UDFs to do so in parallel.
- Enhanced map widget integration in Canvas.
- Shift+Enter will turn on the currently selected UDF.
- Changed default for `fused.ingest` to `target_num_chunks=500`.

**Bug Fixes:**
- Fixed issues with `fused.run` behaving differently for saved UDFs.
- Fixed issues with new apps not showing changes indicator
- Fixed selected preview tab in UDF builder flickering.
- Fixed preview image uploads during GitHub pushes.
- Fixed user assignment issues on GitHub PR creation.
- Fixed an issue with linking to GitHub accounts.
- Fixed an issue with tailing batch job logs.
- Fixed `fused.__version__` not being populated correctly.

## v1.21.1 (2025-07-14)

**`fused-py`**

- Updated LanceDB to 0.24.1.
- Fixed issues starting batch jobs.

**Workbench**

- AI Assistant now sees all UDFs in Workbench & has access to updated Fused documentation
- App builder now integrates with the new version control page.
- Fixed scrolling on the version control page.
- Fixed saving and layout issues on the Canvas view.

## v1.21.0 (2025-07-11)

**`fused-py`**

New Features:
- Added `fused.api.resolve`.
- Added `fused.api.team_info`.
- IPython magics now load automatically.
- When running a large (batch) job, it is now possible to specify the job's name.
- Upgraded `xarray` to 2025.4.0, DuckDB to 1.3.2, and H3 to 4.3.0.
- The `fd://` filesystem scheme will automatically be registered with `fsspec`.
- UDFs that return HTML will be loaded as `str` objects from `fused.run`.
- UDFs saved on Fused server now have a `catalog_url` property to get the Workbench UDF URL.
- `npy` output format is now supported for numpy (raster) return values.
- Arrow-compatible return values are now accepted.
- `fused.cache` will detect changes to referenced UDFs.
- `fused.run` accepts `verbose` keyword.

Bug Fixes:
- Fixed bugs with `udf.render` on some IPython versions.
- Fixed bugs with `repr`s for access tokens and UDFs.
- Fixed calling `fused.run` with UDF objects and `sync=False`.
- Renamed the UDF class to `Udf`.
- Removed some unused and deprecated code.
- Fixed bugs with `fused.cache` showing as not found, having stale files, or not passing arguments through.
- Removed the `n` keyword argument from `get_udfs` and `get_apps`.
- Fixed bugs with HEAD requests to UDF endpoint.
- `fused.submit` will warn about conflicting arguments.
- `/tmp/` size has been increased in realtime instances.
- `fused.api.list` and related functions supports `/mount` paths.
- Improved the performance of GitHub sync.
- Changed defaults for `JobPool.cancel` and fixed a bug where it would continue to retry.
- Fixed bugs with `JobPool.df` and UDF runs that result in exceptions.
- Fixed encoding URL paths in `fused.api.download` and related functions.

**Workbench**

New Features:
- Added AI editing and AI chat capabilities in the UDF builder.
- Workbench now has a code profiler: each line of code will show its execution runtime
- Added new Canvas dashboard builder mode (experimental).
- Added new Table data view mode.
- GitHub integration has a new page and is no longer beta.
- GitHub integration remembers relevant open PRs better.
- Fused apps uses a newer version of Streamlit and Stlite.
- Added a menu item to take a screenshot in higher-than-screen resolution.
- Added type-to-filter in File Explorer.
- File Explorer can be browsed without logging in.
- File Explorer now shows a summary of the current directory.
- File preview UDFs can now be specified with regular expressions.
- Cmd+Click on shared tokens will now take you to the UDF Explorer.

Bug Fixes:
- Results panel shows when memory usage is unknown.
- Share code is more consistent with the selected output format.
- Fixed bugs with read-only app UI.
- The large data warning will now show in File Explorer as well when applicable.
- Fixed bugs with app/UDF Explorer layout and sorting.
- Adjusted the UI for visualization settings and parameters in the UDF list.
- Reordered menu items in File Explorer.
- Cursor position will be remembered when switching between UDFs.
- Share tokens that are already shown on the page are no longer redacted.
- Map tooltip can now be scrolled.

## v1.20.1 (2025-06-09)

**`fused-py`**

New Features:
- Added `fused.api.resolve` and `fused.api.team_info`.
- IPython magics will automatically be loaded when importing `fused`.
- `run_batch` (batch jobs) can now accept a job name.

Bug Fixes:
- Fixed the `render` method of UDF objects.
- Fixed access tokens for apps being rendered in IPython.
- Fixed calling `fused.run(udf, sync=False)` with UDF objects.
- Removed some deprecated fields and arguments.

**Workbench**

- Workbench will now show the amount of time taken in UDF functions as a heatmap.
- Memory bars in the Results panel will show when usage is unknown.
- Update how shared token URLs are generated for different output formats.
- Updated stlite to 0.82.0.

## v1.20.0 (2025-06-03)

**`fused-py`**

New Features:
- Running a UDF with engine `local` will cache similarly to how it would when running on `remote` (supporting `cache_max_age` to control the caching).
- Large (batch) jobs will be in a Pending state if they cannot start immediately.
- UDFs can now accept `**kwargs` parameters, which will always be passed in as strings.
- `@fused.cache` has a new `cache_verbose` option. If set to `True` (default), it prints a message when a cached result is returned.
- `@fused.cache` renamed the `reset` parameter to `cache_reset`. The existing `reset` parameter is deprecated.
- Some file listing APIs like `fused.api.list` will work for public buckets when on free accounts.
- `fused.load` accepts `import_globals` (default `True`) for controlling importing UDF globals. Also, when globals cannot be imported, a warning is emitted instead of an exception.

Bug Fixes:
- Clarified login-needed message in Fused Apps.
- Fixed bugs with `@fused.cache` results not being ready.
- Fixed bugs with `@fused.cache` not detecting changes in the cached function.
- Loading a UDF from a file will autodetect the UDF function name.
- Fixed bugs with returning GeoDataFrames that do not contain geometry.
- Fixed calling `to_fused` on an app.

**Workbench**

- A message will appear above the UDF body when parameters are set in the UDF list.
- A lock icon will be shown next to read-only UDFs and Apps in Workbench.
- When pushing UDFs to GitHub, the preview image will be pushed to a public URL so that the README in GitHub is rendered correctly.
- When pushing UDFs to GitHub, Workbench will assign the PR to you if possible.
- It is now possible to delete UDFs and Apps directly from the catalog.
- Added a "Reload Collection" button. This pulls all latest version of UDF currently in your Collection.
- Workbench will minimize more changes from the PRs it creates on GitHub.
- "Open in Kepler.gl" supports H3 (string) data.
- The visibility button for a Fused App will now reset the app.
- A new Reset 3D View button is added to the UDF Builder map, and the keyboard shortcut has been updated to `Cmd+Shift+UpArrow` on MacOS (`Ctrl+Shift_UpArrow` on Windows / Linux).
- Workbench will show the current environment name above the map by default.
- Workbench will remember which UDF was selected when reopening the page.
- Adjusted which UDF mode label is shown when automatically detecting the UDF mode.
- Fixed some bugs with dynamic output mode.
- UI updates for the Pull Changes (history) and Push Changes views, including showing the README file in both views.
- Drag&Drop UDF into Workbench now works on the entire tab
- Added a button to download usage table in the Profile view.
- Fixed some visual bugs with light mode.
- File Explorer will no longer show file opener UDFs saved on your personal catalog, and will clarify when file opener UDFs are from your team catalog.

## v1.19.0 (2025-05-19)

**`fused-py`**

Breaking changes:
- Large (batch) jobs have been updated to pass parameters into the UDF the same way as other UDF runs. For compatibility, if the parameters passed into the job do not correspond with the parameters, a dictionary parameter is passed into the UDF instead. This will be deprecated and removed in a future release.
- The `context` and `bbox` parameters to a UDF are no longer treated as special.
- Python 3.9 support, which was previously deprecated, is now removed. The minimum Python version for the `fused` package is now 3.10.

New Features:
- PyArrow upgraded to version 16.0.
- Ingestion now supports input files without extensions, and filters out files with `.` or `_` prefix.
- `cache=False` is now a shortcut for disabling cache, e.g. `cache_max_age=0`.
- UDFs now support parameters annotated as `shapely.Geometry` or `shapely.Polygon`.
- `fused.load` now support loading UDFs from Github Pull Request URLs.
- Added a timeout parameter for `fused.api.upload`.
- Fused API functions now support `/mount` without `file://` prefix.
- `fused.api.download` now supports downloading files from `/mount`.
- `fused.api.list` now supports listing an individual file under `/mount`.
- `max_deletion_depth` in `fused.api.delete` default changed from 2 to 3.

Bug Fixes
- Fixed pickling UDF objects.
- Fixed UDF equality checks not conforming to Python specifications.
- Many fixes to ensure compatibility between the `fused` module available on PyPI and the `fused` module within the Fused backend.
- Fixed returning `pd.Timestamp` objects.
- Fixed bugs with handling of stdout if the UDF is async.
- Fixed bugs with UDF object `repr` in Jupyter.
- Fixed `@fused.cache` in Fused Apps.
- Fixed "multiple auth mechanisms" error when retrieving job results.
- Fixed deserialization of GeoDataFrame without geometry column.
- Fixed cases where UDFs would be indented when run.
- Various stability and performance updates, including new self-healing capabilities on the Fused backend.

**Workbench**

- Upgraded Streamlit in apps to 1.44.
- Fused Apps is no longer "beta".
- Fused Apps will now highlight syntax errors.
- Fused Apps will now autocomplete the `fused` module correctly.
- "Changes pending" in the map has been renamed to "Running" and now shows the time elapsed without needing to hover over it.
- UDF builder tooltips have been refreshed, it is now possible to click on data on the map to pin the tooltip on screen. Pinned tooltips show how many data records were under the mouse and allow paging through them.
- Workbench can now highlight H3 hexagons on click.
- Workbench can now detect decimal H3 indexes and Cmd+Click works on them.
- `udf://` URLs in Workbench now entirely overwrite parameters of the selected UDF.
- Fixed UDF builder showing partially updated map states for Tile UDFs.
- Collections catalog can now be sorted.
- Fixed a bug where Workbench would not detect newly added utils on UDFs.
- Parameters and Visualization sections are now styled slightly differently to make it easier to pick out your UDFs.
- Renamed Visualization "Surprise me" button to "Preset".
- Fixed a visual bug with the job status.
- Updated the share UDF and share app pages.
- Fixed bugs with UDF or app catalog showing the wrong content.
- Renamed "History" to "Pull Changes" and updated styling of that page.
- Fixed Pull Changes not showing diffs if utils had been added or deleted.
- Fixed Workbench showing the original Github link for forked UDFs.
- Workbench code editors will now remember scroll position.
- Clicking the viewport location label will now copy it.

## v1.18.0 (2025-04-28)

**`fused-py`**

Breaking changes:
- `fused.submit` now raises an error by default, if there is any run erroring

New Features:
- It is now possible set the cache storage location with `@fused.cache(storage=...)`.
- `@fused.cache` can now exclude arguments from the cache key.
- `@fused.cache` uses Pandas' own way of hashing DataFrames.
- Added storage argument to `fused.file_path(storage=...)`.
- Large jobs now pick up AWS credentials more consistently.
- Auth redirect can dynamically select the port when logging in locally.
- `udf.to_fused` will show the diff when UDF name conflicts with an existing UDF.
- `fused.load` can load by the unique UDF ID again.
- UDFs can be run by username and UDF name in addition to email and UDF name. (ex: `fused.run(user@team.com/my_cool_udf)`). 
- Preview images can now be specified in-directory in Github.
- Adjusted UDF caching behavior for performance.

Bug Fixes
- Fixed behavior when loading and running UDFs with code outside of the UDF function.
- Fixed `fused.api.list` being incompatible with some async stacks.
- Fixed a bug where strings inside UDFs would get extra spaces added to them.
- Shared tokens can be created by a team account.
- Fixed a bug that could occur where Fused would try to duplicate index column names of the returned DataFrame.
- Fixed various bugs when a UDF closes `stdout`.
- Fixed a bug where `fused.run` would not return printed messages from a UDF.
- Fixed a bug where `fused.load` would crash on very large strings.
- Fixed various bugs with exporting UDFs from within fused-py.
- Fixed a bug where `partitioning_schema_input` would not be found when ingesting.
- Fixed a bug where UDFs might import incorrectly when several pushes happen in quick succession in a linked Github repo.

**Workbench**

- `File (Viewport)` renamed to `Single (Viewport)`.
- Added `Single (Parameter)` UDF type that behaves like `Single (Viewport)` but does not pass the viewport bounds.
- File Explorer will show a per-user home favourite.
- Your UDF list will now sync across different browser tabs.
- Preference toggles are added to the Command Palette.
- The share page has been redesigned.
- Collections catalog page now shows the UDF in a given Collection by hovering over them.
- When adding a duplicate UDF to Workbench, you will be prompted to duplicate or replace it.
- Memory usage for UDFs can be found in the results panel (once displaying memory usage was been turned on in Preferences)
- Workbench will indicate that UDF runs were cached in all circumstances.
- The public map page is now compatible with any public-readable shared token.
- Added `udf://name?param=value` URL support to Workbench.
- Reduced the size of metadata diffs generated by Workbench when pushing to Github.
- Various performance improvements.
- Fixes for various UI layout bugs.

## v1.17.0 (2025-04-10)

**`fused-py`**

- Team UDFs can be loaded or run by specifying the name "team", as in: `fused.load("team/udf_name")`
- `Udf.to_fused` supports overwriting the UDF when saving.
- Added `fused.api.enable_gcs()` to configure using the Google Cloud Platform secret specified in Fused secret manager.
- `@fused.cache` locking mechanism has changed and will not allow multiple concurrent runs.
- Upgraded DuckDB to v1.2.2.
- Running a saved UDF by token or name will now also show the logs, including print statements and error tracebacks.
- All functions interacting with the Fused server will now retry automatically, by default 3 times.
- Python 3.9 support is deprecated. The next release of `fused` will require Python 3.10+.
- Deprecated `fused_batch` module is removed.

**Workbench**

- Cached UDF runs will show the original logs.
- "Change output parameters" in the Share UDF screen shows all detected parameters.
- Added a copy viewport bounds button in the Results panel.
- Improved the performance of the catalog screen.
- Fixed the job page showing times in inconsistent time zones.

App Builder:
- Deprecated `fused_app` module is removed.

## v1.16.3 (2025-04-03)

**`fused-py`**

- It is now possible to return general `list`s and `tuple`s from UDFs. (Note: a tuple of a raster and bounds will be treated as a raster return type.)

**Workbench**

- Workbench will now prompt you when loading large UDF results that could slow down or overwhelm your browser. The threshold for this prompt is configurable in your Workbench preferences.
- Fixed bugs with loading large UDF results.
- UDF list will show an error if a UDF has an empty name.
- Fixed running some public UDFs in Workbench.

## v1.16.2 (2025-04-01)

**`fused-py`**

- It is now possible to return dictionaries of objects from a UDF, for example a dictionary of a raster numpy array, a DataFrame, and a string.
- Whitespace in a UDF will be considered as changes when determining whether to return cached data. (a UDF with different whitespace will be rerun rather than cached)
- Fixed calling `fused.run` in large jobs.

**Workbench**

- Added experimental AI agent builder.
- Workbench will now prompt you to replace an existing UDF when adding the same UDF (by name) from the catalog.
- Added ability to download & upload an entire collection.
- Fixed saving collections with empty names.

Visualization:
- Added an H3-only visualization preset.
- Fixed a bug where changing TileLayer visualization type could result in a crash.

App Builder:
- Updated the runtime.

## v1.16.0 (2025-03-27)

**`fused-py`**

- The result object of running in `batch` now has `logs_url` property.
- Fixed `fused.submit` raising an error if some run failed.

**Workbench**

- Added a Download UDFs button for downloading an entire collection.
- Results will show a message at the top if UDF execution was cached.
- Non-visible UDFs will have a different highlight color on them in the UDF list.
- Collections will show as modified if the order of UDFs has been changed.
- Fixes for Collections saving the ordering and visibility of UDFs.
- Fixed the Team Jobs page in Workbench crashing in some cases.

**Shared tokens**

- Shared token URLs can be called with an arbitrary (ignored) file extension in the URL.

## v1.15.0 (2025-03-20)

**`fused-py`**

- Loading UDFs now behaves like importing a Python module, and attributes defined on the UDF can be accessed.
- The `fused.submit()` keyword `wait_on_result` has been renamed to `collect`, with a default of `collect=True` returning the collected results (pass `collect=False` to get the JobPool object to inspect individual results).
- New UDFs default to using `fused.types.Bounds`.
- Upgraded `duckdb` to v1.2.1.
- UDFs can now return simple types like `str`, `int`, `float`, `bool`, and so on.
- Files in `/mount/` can be listed through the API.
- UDFs from publicly accessible GitHub repositories can be loaded through `fused.load`.
- `fused.load` now supports loading a UDF from a local .py file or directory
- The `x`, `y` and `z` aren't protected arguments when running a UDF anymore (previously protected to pass X/Y/Z mercantile tiles).

**Workbench**

New:
- Added a new account page and redesigned preferences page.
- You can now customize the code formatter settings (available under Preferences > Editor preferences).
- UDFs can optionally be shared with their code when creating a share token.

General:
- Moved shared token page to bottom left bar, and adjusted the icons.
- The ordering of UDFs in collections is now saved.

App Builder:
- Updated app list UI.
- Fixed bugs with shared apps showing the wrong URL in the browser.

## v1.14.0 (2025-02-25)

v1.14.0 introduces a lot of new changes across `fused-py` and Workbench

**`fused-py`**

- Introducing `fused.submit()` method for multiple job run
- Improvement to UDF caching
    - All UDFs are now cached for 90 days by default
    - Ability to customize the age of cached data & UDFs with the new `cache_max_age` argument when defining UDFs, running UDFs or when caching regular Python functions
- `pandas` & `geopandas` are now optional for running non-spatial UDF locally
- Removed hardcoded `nodata=0` value for serializing raster data

**Workbench**

New:
- Introducing Collections to organize & aggregate UDFs together
- Redesigned "Share" button & page: All the info you need to share your UDFs to your team or the world

General:
- Improvements to Navigation in Command Palette. Try it out in Workbench by doing `Cmd + K` (`Ctrl + K` on Windows / Linux)
- Autocomplete now works with `Tab` in Code Editor with `Tab`
- Added a Delete Button in the Shared Tokens page (under Account page)
- Ability to upload images for UDF Preview in Settings Page
- Adding “Fullscreen” toggle in Map View
- Improved `colorContinuous` in Visualize Tab
- Allowing users to configure public/team access scopes for share tokens 
- No longer able to edit UDF & App name in read-only mode
- Fixing job loading logs

File Explorer:
- Download directories as `zip`
- Adding favorites to file path input search results 
- Ability to open `.parquet` files with Kepler.gl

## v1.13.0 (2025-01-22)

- Fixed shared UDFs not respecting the Cache Enabled setting.
- Added a cache TTL (time-to-live) setting when running a UDF via a shared token endpoint.
- Tags you or your team have already used will be suggested when editing a UDF's tags.
- Team UDFs will be shown as read-only in Workbench, similar to Public UDFs.
- File Explorer shows deletion in progress.
- File Explorer can accept more S3 URLs, and uses `/mount/` instead of `/mnt/cache`.
- UDF Builder will no longer select a UDF when clicking to hide it.
- Fixed how Push to Github chooses the directory within a repository to push to.
- Fixed the browser location bar in Workbench updating on a delay.
- Fixed writing Shapefile or GPKG files to S3.
- (Beta) New fusedio/apps repository for public Fused Apps.
- Navigating to Team UDFs or Saved UDFs in the UDF Explorer will now prompt for login.
- Fixed the "Select..." environment button in Workbench settings.
- UDF Builder will no longer replace all unaccepted characters with `_` (underscore).
- Fixed loading team UDFs when running a UDF with a shared token.
- Batch jobs that use `print` will now have that output appear in the job logs.
- Apps in the shared token list show an app icon.
- Removed some deprecated batch job options.
- Installed `vega-datasets` package.

## v1.12.0 (2025-01-10)

- (Beta) Added an App catalog in Workbench, and a new type of URL for sharing apps.
- Added `/mount` as an alias for `/mnt/cache`.
- More consistently coerce the type of inputs to UDFs.
- Added more visualization presets to UDF builder in Workbench.
- Fixed an issue where the tab icon in Workbench could unintentionally change.
- Fixed bugs in Workbench File Explorer for `/mnt/cache` when browsing directories with many files.
- Fixed bugs in `fused` Python API not being able to list all files that should be accessible.
- Fixed bugs in the Github integration, command palette, and file explorer in Workbench.
- Fixed bugs in caching some UDF outputs.
- The shareable URL for public and community UDFs will now show in the settings tab for those UDFs.
- UDFs can customize their data return with `Response` objects.

## v1.11.9 (2024-12-19)

- Accounts now have a *handle* assigned to them, which can be used when loading UDFs and pushing to community UDFs
- Account handle can be changed once by the user (for more changes please contact the Fused team.)
- Added a command palette to the Workbench, which can be opened with Cmd-k or Ctrl-k.
- When creating a PR for a community UDF or to update a public UDF, it will be under your account if you log in to Fused with Github.
- Bug fixes for pushing to Github, e.g. when pushing a saved UDF, and for listing the Fused bot account as an author.
- Batch (`run_batch`) jobs can call back to the Fused API.
- Team UDFs can be pinned to the end of the featured list.
- Speed improvements in ingestion.
- Ingestion will detect `.pq` files as Parquet.
- Format code shortcut in Workbench is shown in the keyboard shortcut list and command palette.
- Workbench will hide the map tooltip when dragging the map by default.
- Workbench will now look for a `hexLayer` visualization preset for tabular results that do not contain `geometry`.
- Workbench file explorer can now handle larger lists of files.
- Fix for browsing disk cache (`/mnt/cache`) in Workbench file explorer.
- Teams with multiple realtime instances can now set one as their default.
- Fix for saving UDFs with certain names. Workbench will show more descriptive error messages in more cases for issues saving UDFs.

## v1.11.8 (2024-12-04)

- New File Explorer interface, with support for managing Google Cloud Storage (GCS) and `/mnt/cache` files.
- Workbench will show an error when trying to save a UDF with a duplicate name.
- Fixed a few bugs with Github integration, including the wrong repository being selected by default when creating a PR.
- Updated `fsspec` and `pyogrio` packages.

## v1.11.7 (2024-11-27)

- Decluttered the interface on mobile browsers by default.
- Fixed redo (Cmd-Shift-z or Ctrl-Shift-z) sometimes being bound to the wrong key.
- Tweaked the logic for showing the selected object in Workbench.

## v1.11.6 (2024-11-26)

- Added Format with Black (Alt+Shift+f) to Workbench.
- Fix the CRS of DataFrame's returned by get_chunk_from_table.
- Added a human readable ID to batch jobs.
- Fused will send an email when a batch job finishes.
- Fix for opening larger files in Kepler.gl.
- Fix for accessing UDFs in a team.
- Improved messages for UDF recursion, UDF geometry arguments, and returning geometry columns.
- Adjusted the UDF list styling and behavior in Workbench.
- Fix for secrets in shared tokens.

## v1.11.5 (2024-11-20)

- Show message for keyword arguments in UDFs that are reserved.
- Added reset kernel button.
- Workbench layers apply visualization changes immediately when the map is paused.
- Show the user that started a job for the team jobs list.
- Fix for running nested UDFs with utils modules.
- Fix for returning xarray results from UDFs.
- Fix for listing files from within UDFs.
- Upgraded to GeoPandas v1.

## v1.8.0 (2024-06-25) :package:

- Added Workbench tour for first-time users.
- Undo history is now saved across UDFs and persists through reloads.
- Added autocomplete when writing UDFs in Workbench.
- Added `colorBins`, `colorCategories`, and `colorContinuous` functions to Workbench's Visualize tab.
- Migrated SDK to Pydantic v2 for improved data validation and serialization.
- Fixed a bug causing NumPy dependency conflicts.

## v1.7.0 (2024-06-04) :bird:

- Execution infrastructure updates.
- Update DuckDB package to v1.0.0.
- Improve responsivity of Workbench allotments.
- Crispen Workbench UI.

## v1.6.1 (2024-05-06) :guardsman:

_GitHub integration_

- Updates to team GitHub integration.
- Users are now able to create shared UDF token from a team UDF both in Workbench and Python SDK.

## v1.6.0 (2024-04-30) :checkered_flag:

- The Workbench file explorer now shows UDFs contributed by community members.
- Team admins can now set up a GitHub repository with UDFs that their team members can access from Workbench.

## v1.5.4 (2024-04-15) :telescope:

- Button to open slice of data in Kepler.gl.
- Minor UI design and button placement updates.

## v1.5.3 (2024-04-08) :duck:

- Improved compatibility with DuckDB requesting data from shared UDFs.
- Geocoder in Workbench now supports coordinates and H3 cell IDs.
- GeoDataFrame arguments to UDFs can be passed as bounding boxes.
- The package ibis was upgraded to 8.0.0.
- Utils modules no longer need to import fused.

## v1.5.2 (2024-04-01) :tanabata_tree:

- File browser can now preview images like TIFFs, JPEGs, PNGs, and more.
- Users can now open Parquet files with DuckDB directly from the file browser.

## v1.5.0 (2024-03-25) :open_file_folder:

- The upload view in Workbench now shows a file browser.
- Users can now preview files in the file browser using a default UDF.

## v1.4.1 (2024-03-19) :speech_balloon:

- UDFs now support typed function annotations.
- Introduced special types  `fused.types.TileXYZ`, `fused.types.TileGDF`, `fused.types.Bbox`.
- Workbench now autodetects Tile or File outputs based on typing.
- Added button to Workbench to autodetect UDF parameters based on typing.

## v1.1.1 (2024-01-17) :dizzy:

- Renamed `fused.utils.run_realtime` and `fused.utils.run_realtime_xyz` to `fused.utils.run_file` amd `fused.utils.run_tile`.
- Removed `fused.utils.run_once`.

## v1.1.0 (2024-01-08) :rocket:

- Added functions to run the UDFs realtime.

## v1.1.0-rc2 (2023-12-11) :bug:

- Added `fused.utils.get_chunk_from_table`.
- Fixed bugs in loading and saving UDFs with custom metadata and headers.

## v1.1.0-rc0 (2023-11-29) :cloud:

- Added cloud load and save UDFs.
- `target_num_files` is replaced by `target_num_chunks` in the ingest API.
- Standardize how a decorator's headers are preprocesses to set `source_code` key.
- Fixed a bug loading UDFs from a job.

## v1.0.3 (2023-11-7) :sweat_drops:

_Getting chunks_

- Added `fused.utils.get_chunks_metadata` to get the metadata GeoDataFrame for a table.
- `run_local` now passes a copy of the input data into the UDF, to avoid accidentally persisting state between runs.
- `instance_type` is now shown in more places for running jobs.
- Fixed a bug where `render()`ing UDFs could get cut off.
- Fixed a bug with defining a UDF that contained an internal `@contextmanager`.

## v1.0.2 (2023-10-26) :up:

_Uploading files_

- Added `fused.upload` for uploading files to Fused storage.
- Added a warning for UDF parameter names that can cause issues.
- Fixed some dependency validation checks incorrectly failing on built-in modules.

## v1.0.1 (2023-10-19) :ant:

- Added `ignore_chunk_error` flag to jobs.
- Added warning when sidecar table names are specified but no matching table URL is provided.
- Fixed reading chunks when sidecars are requested but no sidecar file is present.
- Upgraded a dependency that was blocking installation on Colab.

## v1.0.0 (2023-10-13) :ship:

_Shipping dependencies_

- Added `image_name` to `run_batch` for customizing the set of dependencies used.
- Added `fused.delete` for deleting files or tables.
- Renamed `output_main` and `output_fused` to `output` and `output_metadata` respectively in ingestion jobs.
- Adjusted the default instance type for `run_batch`.
- Fixed `get_dataframe` sometimes failing.
- Improved tab completion for `fused.options` and added a repr.
- Fixed a bug where more version migration messages were printed.
- Fixed a bug when saving `fused.options`.

</details>

================================================================================

# WORKBENCH

## Account
Path: workbench/account.mdx
URL: https://docs.fused.io/workbench/account

On the Account page, you can edit your Username, view basic account information, and access the Usage, Debug, and S3 Policy sections.

 <ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/account_page_edit6.mp4" width="100%" />

================================================================================

## AI Assistant
Path: workbench/ai-assistant.mdx
URL: https://docs.fused.io/workbench/ai-assistant

# AI Assistant

The AI Assistant is a tool that allows you to use AI to help you code directly in the Workbench.

Watch our playlist of Vibe Coding for more examples!

### Get Started with the AI Assistant

1. Open the AI Assistant tab
2. Type your prompt! (Or click the "microphone" icon to directly speak your prompt!)
3. See your results! (or press Shift + Enter if it didn't auto run)

### What it can help you with

AI Assistant sees all the UDFs you have open in Workbench. So AI Assistant can help you:
- Fix your queries 
- Create code to load data you don't know how to load 

### Tips & Tricks

- Ask as specific questions as possible. The more precise your query, the more likely you are to get a response you like.
- Try out the suggestions! Our models auto suggest what you could do next based on all your code
- Ask crazy things you don’t know how to do!
- Come back tomorrow, we'll most certainly have some new stuff! 🧑‍💻

### Personalize your AI Assistant

Pass a custom prompt to the AI Assistant based on your preferences:

- Click the ⚙️ icon in the top right corner
- Add your custom prompt
- Click "Save"

Examples of custom prompts:

```text
I'm a data scientist familiar with Python, don't put comments for basic code
```

```text
Use DuckDB for most queries whenever possible rather than Pandas
```

### Examples

- See how we're using AI to build a climate dashboard
- Creating Interactive charts with AI
- Ask AI to help you configure sharable HTML maps

================================================================================

## Adding a Map to your App
Path: workbench/app-builder/add-a-map.mdx
URL: https://docs.fused.io/workbench/app-builder/app-map

For many geospatial applications you will want to add a map to your Fused App, especially if your UDF returns a Map `Tile`.

This section shows a few examples of how you can do that. While we do recommend you use `pydeck` (the Python implementation of deck.gl) for its versatility, you can use other options like `folium`

    Note that you need to install dependencies with `micropip` inside your Fused app. More on this here.

## Pydeck

Create a pydeck GeoJsonLayer that plots a simple GeoDataFrame.

You can find this app right here and test it for yourself!

Here's what this would look like:

[Image: ImgStdout]

```python showLineNumbers
# installing pydeck & geopandas inside Fused app

await micropip.install(['pydeck', 'geopandas'])

st.write("# Hello World! 👋")
st.write("Here's a simple example of a Fused app plotting NYC metro stations")

DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
gdf = gpd.read_file(DATASET)
# We buffer the points to make them more visible on our map
gdf.geometry = gdf.geometry.buffer(0.001)

# Creating an empty pydeck element
deck = st.empty()

# Initiating pydeck element with view over NYC
view_state = pdk.ViewState(
    latitude=40.73,
    longitude=-73.96,
    zoom=10,
    pitch=0
)

# Creating a GeoJSON layer with our GeoDataFrame
geojson_layer = pdk.Layer(
    'GeoJsonLayer',
    gdf,
)

updated_deck = pdk.Deck(
    layers=[geojson_layer],
    initial_view_state=view_state,
    map_style='mapbox://styles/mapbox/light-v9'
)

deck.pydeck_chart(updated_deck)
```

Read more about how to use Pydeck on their official documentation.

    This example shows how to plot a `GeoDataFrame` directly, but you could swap this out for a UDF that returns a `GeoDataFrame` too:

    ```python showLineNumbers
    # DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
    # gdf = gpd.read_file(DATASET)
    # highlight-next-line
    gdf = fused.run("YOUR_UDF_RETURNING_A_GDF")
    ```

    Read more about `fused.run` in the `fused.run` reference

## Folium

Create a streamlit-folium `TileLayer` that calls a UDF HTTPS endpoint.

```python showLineNumbers

from streamlit_folium import st_folium

m = folium.Map(location=[22.5, -115], zoom_start=4)
url_raster = 'https://www.fused.io/server/v1/realtime-shared/fsh_3QYQiMYzgyV18rUBdrOEpO/run/tiles///?format=png'
folium.raster_layers.TileLayer(tiles=url_raster, attr='fu', interactive=True,).add_to(m)
st_folium(m)
```

================================================================================

## App Builder
Path: workbench/app-builder/app-builder.mdx
URL: https://docs.fused.io/workbench/app-builder/app-builder

Learn everything there is about Fused Apps

<DocCardList className="DocCardList--no-description"/>

================================================================================

## App Builder Overview
Path: workbench/app-builder/app-overview.mdx
URL: https://docs.fused.io/workbench/app-builder/app-overview

# App Builder Overview

The App Builder is an IDE to transform User Defined Functions (UDFs) into interactive, shareable apps.

Data scientists often need to make analytics interactive and accessible to broader audiences. However, building traditional React apps with maps and widgets can be impractical, especially considering prototypes might be discarded. Additionally, frontend frameworks are not well-suited for transforming data or handling large datasets.

With this in mind, the App Builder enables users to build and run apps with serverless Streamlit, an open source framework to deliver dynamic data apps with just a few lines of Python. These are some of its capabilities to keep in mind:

- Build apps
- Install dependencies
- Troubleshoot
- Call UDFs and cache responses
- Share live apps

\
Try running the code snippets below to acquaint yourself with the App Builder.

```python showLineNumbers

st.write("Hello, *Fused!* :rocket:")
```

## Dependencies

To set Python packages for your app, only packages compatible with Pyodide are supported. Please get in touch if you need help with a specific package.

You may also choose to install dependencies at runtime to reduce start-up time. Use micropip to install packages at runtime. 

```python showLineNumbers

await micropip.install(["geopandas", "mercantile"])
```

## Write UDFs

You may define UDFs in the App Builder's code editor and invoke them with `fused.run`. This snippet creates a UDF that returns a `DataFrame` with a column of zeros with a length determined by a slider widget.

```python showLineNumbers

count = st.slider("Count", 1, 10, 4)

@fused.udf
def udf(count: int = 1):

    return pd.DataFrame()

df = fused.run(udf, count=count)
st.write(df)
```

You may also run the UDF on a remote worker by setting `engine='remote'` in the `fused.run` call.

```python showLineNumbers
df = fused.run(udf, count=count, engine='remote')
```

## Call UDFs

Apps may call UDFs and load their output into memory. This enables them to run resource-intensive operations and use libraries unsupported by Pyodide. These snippets illustrate a few ways to call UDFs.

### With `fused.run` (beta)

Call a UDF by its shared token with `fused.run` and pass parameters from a slider.

```python showLineNumbers

threshold = st.slider("Count filter", 0, 400, 25)
df = fused.run('UDF_DuckDB_H3_SF', count=threshold)
```

### HTTPS endpoints

Call UDF HTTPS endpoints with the requests library and pass parameters from a dropdown selectbox.

```python showLineNumbers

city = st.selectbox("Select city", ("Boston", "Paris", "New York"))
url = f"https://www.fused.io/server/v1/realtime-shared/fsh_2wEv0k8Xu2grl4vTVRlGVk/run/file?format=geojson&city="
response = requests.get(url)
st.json(response.json())
```

Render the raster response of UDFs as images.

```python showLineNumbers

st.image('https://www.fused.io/server/v1/realtime-shared/fsh_7Yuq2R1Ru1x5hgEEfNDF5t/run/tiles/11/583/787?format=png')
```

## Caching

It can be helpful to cache the response of UDF calls. To cache a function in Streamlit, decorate it with `@st.cache_data`.

```python showLineNumbers

@st.cache_data
def cached_output():
    return fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9')

df = cached_output()

st.write(fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9'))
```

## Share

The App Builder settings menu includes options to generate a URL to share the app or embed it with an `<iframe>`.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/app_builder_share_edit8.mp4" width="100%" />

### Shareable links
The app is saved to Fused and referenced by a token, such as `https://www.fused.io/app/fsh_7hVSIymGijZ53YGmEs2EIM`.

## Troubleshoot

Click "Rerun" on the top-right menu of the App view in case things aren't working as expected.

[Image: Rerun App]

================================================================================

## File Explorer
Path: workbench/file-explorer.mdx
URL: https://docs.fused.io/workbench/file-explorer

The File Explorer provides a streamlined interface to browse, preview, and open files in cloud object storage and the mounted disk. Fused supports Amazon S3 and Google Cloud Storage, with more integrations coming soon.

When working with data, it can be time-consuming to track down datasets, request access, download gigabytes of data, and write boilerplate code to read files. The File Explorer simplifies this process by enabling users to easily browse any object storage bucket, visualize file contents without writing code, and quickly create User Defined Functions (UDFs) in the UDF Builder with templates for specific file types.

- Browse object storage buckets and list their files
- Quickly preview files
- Create a new UDF from a template to open the file
- Connect an S3 or GCS bucket

## Browse

Browse directories and files in buckets. Use the search bar and filter options to quickly locate specific files, and "Favorite" files for quick access. To explore a bucket, find it in the "Favorites" dropdown or paste its path. To connect private buckets, contact Fused.

[Image: File Explorer Search]

## Preview

The Explorer displays a bucket's directories and objects as folders. Each listed file shows metadata such as file size and path, along with utilities to download or delete the file, copy its path, generate a signed URL, and create a UDF to open it.

Click on a file to preview its content. If the file has a spatial component, it will be displayed on the map, allowing you to zoom and pan to explore the rendered data. For images or other file types, Fused will make a best-effort to render and display the content.

[Image: File Explorer Preview]

## Create UDF

Create Fused UDFs using templates for common file types. Double-click on a file to create a new templated UDF that reads the file, or find additional readers in the file's kebab menu. Parquet tables show an "Open Table" button to open them at the directory level.

[Image: File Explorer Create UDF]

## Template UDFs

Template UDFs are available for common file types (like `CSV`, `Parquet`) and tools (like `DuckDB` and `GeoPandas`). See the latest template UDFs in the UDFs repo.

Supported file types for vector tables include `parquet`, `JSON`, `CSV`, `excel`, `zip`, and `KML`. For raster files `GeoTIFF` and `NetCDF` are supported. If you need a file type that isn't supported, request it on the Fused Discord channel or contribute a template to the community.

## Upload / Download / Edit 

File Explorer also allows you to:
- Upload files (drag & drop)
- Download files
- Create new directories

In File Explorer directories will not be saved if they do not have content in them.

## Connect your own bucket

Alternatively, use this Fused app to automatically structure the policy for you.

The bucket must enable the following CORS settings to allow uploading files from Fused.

### Google Cloud Storage (GCS)

To connect a Google Cloud Storage bucket to your Fused environment, you'll need to follow these steps:

1. Create a Service Account in GCS
Set up a Google Cloud service account with permissions to read, write, and list from the GCS bucket. See the Google Cloud documentation for instructions to:
- Create a Service Account
- Set permissions for the Service Account

2. Download the JSON Key File
Download the JSON Key file associated with the Service Account. This file contains credentials that Fused will use to access the GCS bucket.

3. Set the JSON Key as a Secret
Set the JSON Key as a secret in the secrets management UI. The secret must be named `gcs_fused`.

You then need to write these credentials to a JSON file and pass them to Google as:

```python
@fused.udf
def udf():
    from google.cloud import storage

    # get GCP secrets
    with open("/tmp/gcs_key.json", "w") as f:
        f.write(fused.secrets["gcs_fused"])
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcs_key.json"

    # your code here
```

================================================================================

## Free Tier
Path: workbench/free-tier.mdx
URL: https://docs.fused.io/workbench/free-tier

# Free Tier

Fused is accessible to anyone for free by simply creating an account! Head over to Workbench and sign up!

## Features

You get access to:
- Daily compute credits
- Access to AI Assistant
- Saving UDFs & creating shared tokens

All users on the free tier share the same compute resources.
Do not share sensitive data that others might be able to access on the free tier. 

Upgrade to a higher tier to get your own dedicated environment & compute resources.

================================================================================

## Overview
Path: workbench/overview.mdx
URL: https://docs.fused.io/workbench/overview

Workbench is a browser-based platform to build with data. Write Python, visualize results instantly, and share your work.

## Views

**Canvas** — Default view for building workflows. Connect UDFs together, create dashboards, and see results side-by-side.

**UDF Builder** — Advanced view for geospatial workflows. Visualize multiple UDFs as map layers or inspect individual Table outputs.

## Tools

**File Explorer** — Browse and open files directly from cloud storage (S3, GCS, Azure).

**UDF Explorer** — Organize and share your UDFs. Supports version control through GitHub.

Try Workbench at fused.io/workbench

================================================================================

## Preferences
Path: workbench/preferences.mdx
URL: https://docs.fused.io/workbench/preferences

You can access the Preferences page from Workbench by clicking on Preferences in the bottom-left corner.

This page contains general user preferences, experimental features, theme selection, data settings, and other customization options.

[Image: Preferences button location]

This page is roughly split in 2 sections:

[Image: preferences page sections]

- **Environment**: Setting up kernels, Secret management and environment variables. See below for details.
- **General Preferences**: Such as theme, file formats, etc. These are also searchable from Command Palette (`Cmd/Ctrl + K`).

## Kernel

Selecting a kernel in Workbench sets the environment for your session, providing access to Python libraries at runtime. Contact the Fused team through Slack, Discord or email to set up new kernels.

[Image: kernel]

## Secrets management

Store and manage secrets that are securely encrypted in the backend.

[Image: secrets management]

Add secrets directly in the UI by clicking "+ Add new secret"

You can then retrieve secrets from your UDF:

```python showLineNumbers

fused.secrets["my_secret"]
```

You can also list all available secrets with:

```python showLineNumbers
dir(fused.secrets)
```

## Environment variables

[Image: env variables]

Environment variables in Fused are pre-configured by the Fused team. Contact the Fused team to set new ones. 

These variables can be accessed in the usual way through Python's os module:

```python showLineNumbers

os.environ["ENV_VAR_NAME"]
```

================================================================================

## Canvas
Path: workbench/udf-builder/canvas.mdx
URL: https://docs.fused.io/workbench/udf-builder/canvas

A Canvas is a way for users to organize their UDFs into different projects. This allows you to use Workbench for completely different, unrelated projects and organise your UDFs in a cleaner way. 

With Canvas, you can:
- Save all current open UDFs into a Canvas (after giving it a name)
- Upload / Download Canvas via Workbench (Click the 3 dots next to Canvas name)
- Delete Canvas (this doesn't delete the UDFs inside, only the Canvas)

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/canvas4.mp4" width="100%" />

We are actively working on expanding Canvas features!

### Examples of Canvas in use

- See how we recommend using Canvas as part of our Workbench intro

================================================================================

## Code Editor
Path: workbench/udf-builder/code-editor.mdx
URL: https://docs.fused.io/workbench/udf-builder/code-editor

The Code Editor is where developers write UDFs using standard Python libraries and installed dependencies. The Editor tab contain functionality to organize code, create HTTPS endpoints, and configure the UDF.

[Image: Code Editor]

## Editor

The editor contains the UDF's function declaration. Whenever code is updated, Fused automatically runs the function named `udf` that is decorated with `@fused.udf` and returns the output. Other UDFs declared in the editor are ignored unless referenced by the main `udf` function.

## Default Parameters hierarchy

Just like any other Python function, UDFs can have default parameters. In Workbench, these can be set in Python directly.

The hierarchy is as follows:
1. In-Python parameter (when defining the function)
2. HTTPS Request / Calling UDF with shared token

[Image: Default parameters hierarchy]

Let's start with a simple UDF:
```python
@fused.udf
def udf(my_default_param: float = 1.5):
    
    print(f"")
    return my_default_param
```

In Workbench, without defining else we would get `1.5` back:

[Image: Default parameters hierarchy]

Now we can set default parameters in the UDF layer UI:

## Debug

The code editor highlights errors in the code and shows error logs to help debug.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/debug_code_editor_edit6.mp4" width="100%" />

## Profiler

UDF Builder comes with a built-in profiler that can be used to analyze the performance of a UDF.

This gives you the line-by-line execution time in the UDF.

Note:
- Values are only available for the current run of the UDF.
- Running the same UDF twice might lead to different runtimes especially if you call cached functions or cached UDFs.
- When a line is called multiple times the profiler shows the sum time of all calls & number of hits:

[Image: multiple hits profiler]

## Save a UDF

UDFs show an asterisk (`*`) next to their name when changes have been made since the last save. 

You can Save a UDF in 2 ways:
- Click on 3 dots -> "Save"
- `Cmd + S` (MacOS) / `Ctrl + S` (Win / Linux)

Clicking the "Save" icon saves the present state of the UDF under your account's UDFs.

If the "Save" icon appears greyed out, it means you're viewing a read-only version of the UDF. Make a copy to create a new version than can be modified and saved.

[Image: Save Icon]

## Auto, Tile, and Single (Viewport, Parameter)

On UDF Builder, UDFs can explicitly be set to run as Tile or File - or autoselect between the two if the `bounds` object is typed.

[Image: File]

## Utils Module [Legacy]

It is only available as a legacy feature (available to turn on in the Workbench Preferences).

A Fused UDF can import Python objects from its accompanying utils Module, defined in the Utils Tab's code editor. You can import functions from it in your UDF with `from utils import my_function`.

Here is an example in an old version ofthe Public Overture_Maps_example UDF:

```python showLineNumbers
# This is an old version of the code for demonstration purposes
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    release: str = "2025-01-22-0",
):
    from utils import get_overture

    gdf = get_overture(
        bounds=bounds,
        release=release,
    )
    return gdf
```
[Image: UDF Builder Utils]

================================================================================

## Map
Path: workbench/udf-builder/map.mdx
URL: https://docs.fused.io/workbench/udf-builder/map

As developers edit UDFs in the Code Editor and explore data, they can receive immediate visual feedback on how the code's transformations affect the data.

## Geospatial data

Fused will render `gpd.GeoDataFrame`, `gpd.GeoSeries`, and `shapely geometry` UDF outputs as geometries on the map if their CRS is `EPSG:4326`. If the CRS differs, Fused will make a best-effort to project and render the geometries correctly.

To render array (raster) objects on the map, they must be `uint8` and define their spatial extent. Objects like `xarray.DataArray` already contain spatial metadata. The spatial extent of arrays without spatial metadata, like `numpy.ndarray`, can be specified with a geometry object or an array bounds as `[xmin, ymin, xmax, ymax]`. If the bounds are not present, they default to `(-180, -90, 180, 90)`.

```python showLineNumbers
return np.array([[…], […]]), [xmin, ymin, xmax, ymax]
```

For UDFs that return map Tiles, Fused runs the UDF for only the Tiles in the viewport. This enables efficient analysis on a fraction of a dataset.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/map_overture3.mp4" width="100%" />

### Map controls

The map can be panned by dragging the viewport, zoomed in and out, and rotated with `Cmd` + `Click` + drag on MacOS (`Ctrl` + `Click` + drag on Windows / Linux)

The top of the map has controls to interact with the viewport. These include an address search bar, a basemap selector, a fullscreen toggle, data visualization icon, and a three-dot menu providing access to additional map options.

You can change the basemap by setting it in the map style settings, located at the top right of the UDF Builder map. Currently, light, dark, satellite, and blank basemaps are supported.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/mapcontrols_edit3.mp4" width="100%" />

## Debug

Clicking a rendered feature enters debug mode. To exit, press `Escape` or click the `X` in the map tooltip header."

When data renders successfully on the map, clicking or hovering on it shows attributes for selected pixels or geometries. When data doesn't render, clicking errored tiles shows an error code, and the full error details can be copied as JSON. Additional debugging information can be found in the Runtime pane.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/debugclick6.mp4" width="100%" />

================================================================================

## Navigation
Path: workbench/udf-builder/navigation.mdx
URL: https://docs.fused.io/workbench/udf-builder/navigation

The Navigation pane on the left side of the UDF Builder displays the UDFs available to the user. UDFs are listed as layers. Their order corresponds to their stacking order on the map. Each layer includes icons to delete, reorder, and toggle the visibility of UDFs. The selected layer becomes the "active" layer. Its code appears in the Code Editor and its output in the Runtime pane.

## Zoom to Layer

"Zoom to layer" allows you to quickly zoom the map to the location of the relevant UDF layer. You would use it when you want to focus the map on a specific area or features defined by the layer, especially when the UDF has a spatial component or a default view state.
UDFs that return an object with a spatial component or have a default view state show a "zoom to layer". Clicking it zooms the map to the relevant location.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/zoom_to_layer_edit6.mp4" width="100%" />

## Layer Visibility

The layer toggle feature allows users to show or hide specific visualizations on the map. These layers provide enhanced map insights, making spatial data easier to understand and interpret.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/layers_edit6.mp4" width="100%" />

## UDF Options

The UDF list includes several interactive options to manage and organize functions efficiently.

### Drag and Drop

Users can reorder UDFs by dragging them up and down within the list. To do this, they need to hover over the UDF name to see the drag handle.

### Close/Delete

If the UDF is saved, a "Close UDF" button is available to exit the editor. If the UDF is unsaved and still being edited, a "Delete UDF" button replaces it, allowing users to discard the changes.

[Image: udfOpt]

## Share

The Settings button in the three-dot menu provides access to all options to share the UDF.

[Image: Share UDF]

### Share snippets

UDFs saved in the UDF Builder can be called with HTTPS endpoints using the public UDF name and/or tokens.

The "Shared token" section shows snippets to run the UDF using a public token. This allows any application to invoke the UDF without authentication - including `cURL` calls, Lonboard, Leaflet, Mapbox, Google Sheets, DuckDB, the Fused App Builder, and Python applications with `fused.run`.

### Private snippets

Private snippets are now located under the "Shared token" section in the "Share" view. The "Private snippets" section shows snippets that can only be called by services authenticated with a private token. These include `Links` and `Python snippets`.

### Metadata

#### Image preview

[Image: image preview]

#### Tags

UDF tags can be set to help with discoverability in the UDF Explorer.

[Image: Image tags]

#### Description

UDFs can be documented using Markdown with a brief description of their purpose, code, and associated datasets. The description appears in the UDF profile and `README.md` file.

## Toolbar

The toolbar under the UDF name includes buttons to Share, Save and Download. Additional actions like Duplicate, Delete, Pull changes, and Push changes are available via the three-dot menu on the right.

### GitHub

Organizations with the GitHub Integration enabled can push UDFs to a GitHub repository as a Pull Request or restore a prior version of a UDF from the commit history.

### Download

Clicking "Download" downloads a `.zip` file with the UDF code, module, and configuration.

[Image: File]

================================================================================

## Runtime
Path: workbench/udf-builder/results.mdx
URL: https://docs.fused.io/workbench/udf-builder/runtime

Exploring and analyzing data involves scanning logs, previewing intermediary outputs, and debugging errors.

The Runtime section of the UDF Builder dynamically displays information related to UDF execution, including visual output, debug info, and request details available via the toolbar.

### Toolbar

The toolbar at the top of the Runtime pane includes a menu button for displaying debug info and copying request details.

[Image: resultstoolbar]

================================================================================

## UDF Builder
Path: workbench/udf-builder/udf-builder.mdx
URL: https://docs.fused.io/workbench/udf-builder/udf-builder

The UDF Builder is an IDE to write User Defined Functions (UDFs) and visualize their output.

Data analysis is inherently iterative. It involves loading data, writing code, executing the code, and visualizing results. As developers write and debug their code in the UDF Builder, Fused gives them immediate feedback by automatically re-running the UDF and showing the output. Developers can write UDFs in the Code Editor, preview the output on the Map, and style the visualization. To help debug, the Results pane displays logs and errors generated by the UDF.

- Write UDFs in the Code Editor
- Manage UDFs as map layers in the Navigation pane
- Preview the UDF's output on the Map and edit the Layer Styling its visualization
- View the UDF's logs in the Runtime pane

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/docs_udfbuilderv3.mp4" width="100%" />

================================================================================

## Layer Styling
Path: workbench/udf-builder/viz-styling.mdx
URL: https://docs.fused.io/workbench/udf-builder/styling

# Layer Styling: Visualization 

The UDF builder displays data from your UDF on the map. You can customize the visual representation using the visualization icon located on the map.

If you're using `map_utils` to build standalone maps in Canvas, see Standalone Maps instead. This page covers styling for UDF Builder layers.

**Presets**

Use the "Preset" button to quickly generate styling configurations. You can always undo changes with `Ctrl + Z`.

[Image: Preset button]

## Layer Types

### Vector: Custom Map (Mapbox GL)

Use when your UDF produces vector tiles (MVT) and you want full control using Mapbox GL styling and interactions.

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

  ```py
  @fused.udf(cache_max_age=0)
  def udf(
      token: str = "UDF_Overture_Maps_Example",
      host: str = "https://www.fused.io", 
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      style_url: str = "mapbox://styles/mapbox/dark-v10",
      center_lng: float = -122.4194,
      center_lat: float = 37.7749,
      zoom: float = 16,
      pitch: float = 30,     
      bearing: float = 20,  
      minzoom: int = 6,
      maxzoom: int = 14,
      layer_id: str = "fused-vector-layer",
      source_layer: str = "udf"
  ):
      """
      Minimal Mapbox map (no input UI) that loads Fused XYZ vector tiles (MVT) from `token`.
      Tiles URL: /server/v1/realtime-shared//run/tiles///?dtype_out_vector=mvt
      """
      from jinja2 import Template
      html = Template(r"""

  """).render(
          token=token,
          host=host,
          mapbox_token=mapbox_token,
          style_url=style_url,
          center_lng=center_lng,
          center_lat=center_lat,
          zoom=zoom,
          pitch=pitch,
          bearing=bearing,
          minzoom=minzoom,
          maxzoom=maxzoom,
          layer_id=layer_id,
          source_layer=source_layer,
      )

      common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
      return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

  ```py
  @fused.udf(cache_max_age=0)
  def udf(
      # H3 data (each row: )
      data_url: str = "https://www.fused.io/server/v1/realtime-shared/UDF_DuckDB_H3_SF/run/file?format=json",

      # Mapbox + camera
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      style_url: str = "mapbox://styles/mapbox/dark-v10",
      center_lng: float = -122.417759,
      center_lat: float = 37.776452,
      zoom: float = 11.0,
      pitch: float = 50.0,
      bearing: float = -10.0,

      # Layer tuning
      elevation_scale: float = 20.0,     # count * elevation_scale
      max_count_for_color: float = 500.0, # for [255, (1 - count/max)*255, 0]
      wireframe: bool = False,
  ):

      from jinja2 import Template

      """
      Deck.gl 3D H3HexagonLayer over Mapbox, using your JSON (hex,count) data.
      - Elevation: count * elevation_scale
      - Color: [255, (1 - count/max_count_for_color)*255, 0]
      - Popup shows hex & count
      """
      html = Template(r"""

  """).render(
      data_url=data_url,
      mapbox_token=mapbox_token,
      style_url=style_url,
      center_lng=center_lng,
      center_lat=center_lat,
      zoom=zoom,
      pitch=pitch,
      bearing=bearing,
      elevation_scale=elevation_scale,
      max_count_for_color=max_count_for_color,
      wireframe=wireframe,
  )

      common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
      return common.html_to_obj(html)

  ```

  ```json
  ,
    "hexLayer": ,
      "getLineColor": [200, 200, 200],
      "getElevation": ,
      "elevationScale": 2
    }
  }
  ```

  ```py
  DEFAULT_CONFIG = r""",
    "hexLayer": ,
      "getLineColor": [200, 200, 200],
      "getElevation": ,
      "elevationScale": 2
    }
  }"""

  @fused.udf(cache_max_age=0)
  def udf(
      tile_url_template: str = "https://www.fused.io/server/v1/realtime-shared/UDF_Ookla_Download_Speeds/run/tiles///?dtype_out_vector=json",
      config_json: str = DEFAULT_CONFIG,
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      center_lng: float = -98.5,
      center_lat: float = 39.5,
      zoom: float = 3
  ):
      from jinja2 import Template
      
      html = Template(r"""

  """).render(
          tile_url_template=tile_url_template,
          config_json=config_json,
          mapbox_token=mapbox_token,
          center_lng=center_lng,
          center_lat=center_lat,
          zoom=zoom,
      )

      common = fused.load("https://github.com/fusedio/udfs/tree/abf9c87/public/common/")
      return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": 
  }
  ```

  ```py

@fused.udf(cache_max_age=0)
def udf(
    token: str = "UDF_CDLs_Tile_Example",      
    host: str = "https://www.fused.io", 
    mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
    style_url: str = "mapbox://styles/mapbox/dark-v10",
    center_lng: float = -121.16450354933122,
    center_lat: float = 38.44272969483187,
    zoom: float = 8.59,
    minzoom: int = 6,
    maxzoom: int = 14,
    layer_id_raster: str = "fused-raster-layer",
    raster_tile_size: int = 256,
    raster_opacity: float = 0.95
):
    from jinja2 import Template

    """
    Mapbox map that loads **raster** XYZ tiles from a Fused UDF token.

    Tiles URL:
      /server/v1/realtime-shared//run/tiles///?dtype_out_raster=png
    """
    html = Template(r"""

""").render(
        token=token,
        host=host,
        mapbox_token=mapbox_token,
        style_url=style_url,
        center_lng=center_lng,
        center_lat=center_lat,
        zoom=zoom,
        minzoom=minzoom,
        maxzoom=maxzoom,
        layer_id_raster=layer_id_raster,
        raster_tile_size=raster_tile_size,
        raster_opacity=raster_opacity,
    )

    common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
    return common.html_to_obj(html)

  ```

  ```json
,
  "rasterLayer": ,
  "vectorLayer": 
}
  ```

  ```py
@fused.udf(cache_max_age=0)
def udf(
    token: str = "UDF_Overture_Maps_Example",
    host: str = "https://www.fused.io",
    mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
    style_url: str = "mapbox://styles/mapbox/dark-v10",
    center_lng: float = -122.4194,
    center_lat: float = 37.7749,
    zoom: float = 12,
    minzoom: int = 0,
    maxzoom: int = 15,
    layer_id: str = "vector-fill",
    source_layer: str = "udf",
    fill_color: str = "#35AF6D",
    fill_opacity: float = 0.55,
    outline_color: str = "#0b0b0b",
):
    from jinja2 import Template
    html = Template(r"""

""").render(
        token=token, host=host, mapbox_token=mapbox_token, style_url=style_url,
        center_lng=center_lng, center_lat=center_lat, zoom=zoom,
        minzoom=minzoom, maxzoom=maxzoom, layer_id=layer_id,
        source_layer=source_layer, fill_color=fill_color, fill_opacity=fill_opacity,
        outline_color=outline_color,
    )
    common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
    return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

  ```py
  @fused.udf(cache_max_age=0)
  def udf(
      token: str = "UDF_DSM_Zonal_Stats",
      host: str = "https://www.fused.io",
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      style_url: str = "mapbox://styles/mapbox/dark-v10",
      center_lng: float = -122.4194,
      center_lat: float = 37.7749,
      zoom: float = 12,
      minzoom: int = 0,
      maxzoom: int = 15,
      layer_id: str = "vector-choro",
      source_layer: str = "udf",
      value_attr: str = "stats",
      domain_min: float = 0.0,
      domain_mid: float = 50.0,
      domain_max: float = 100.0,
      color_min: str = "#2b65a0",
      color_mid: str = "#35af6d",
      color_max: str = "#e8ff59",
      fill_opacity: float = 0.65,
      outline_color: str = "#111111",
  ):
      from jinja2 import Template
      html = Template(r"""

  """).render(
          token=token, host=host, mapbox_token=mapbox_token, style_url=style_url,
          center_lng=center_lng, center_lat=center_lat, zoom=zoom,
          minzoom=minzoom, maxzoom=maxzoom, layer_id=layer_id, source_layer=source_layer,
          value_attr=value_attr, domain_min=domain_min, domain_mid=domain_mid, domain_max=domain_max,
          color_min=color_min, color_mid=color_mid, color_max=color_max,
          fill_opacity=fill_opacity, outline_color=outline_color,
      )
      common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
      return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": ,
      "lineWidthMinPixels": 1,
      "getFillColor": 
    }
  }
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
    }
  }
  ```

  ```py
@fused.udf(cache_max_age=0)
def udf(
    token: str = "UDF_FEMA_Buildings_US",
    host: str = "https://www.fused.io",
    mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
    style_url: str = "mapbox://styles/mapbox/dark-v10",
    center_lng: float = -74.0110,   # longitude
    center_lat: float = 40.7133,  
    zoom: float = 14,
    minzoom: int = 0,
    maxzoom: int = 15,
    source_layer: str = "udf",
    layer_id: str = "fema-categories",
    attr_name: str = "OCC_CLS",
    domain: list = None,
    # CARTOColors Bold palette (7 colors)
    colors: list = None,
    fill_opacity: float = 0.65,
    outline_color: str = "#0b0b0b"
):
    from jinja2 import Template

    if domain is None:
        domain = [
            "Assembly",
            "Commercial",
            "Utility and Misc",
            "Residential",
            "Industrial",
            "Education",
            "Government",
        ]
    if colors is None:
        colors = ["#7F3C8D", "#11A579", "#3969AC", "#F2B701", "#E73F74", "#80BA5A", "#E68310"]

    html = Template(r"""

""").render(
        token=token,
        host=host,
        mapbox_token=mapbox_token,
        style_url=style_url,
        center_lng=center_lng,
        center_lat=center_lat,
        zoom=zoom,
        minzoom=minzoom,
        maxzoom=maxzoom,
        source_layer=source_layer,
        layer_id=layer_id,
        attr_name=attr_name,
        domain=domain,
        colors=colors,
        fill_opacity=fill_opacity,
        outline_color=outline_color,
    )

    common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
    return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": ,
      "getElevation": ,
      "elevationScale": 10
    }
  }
  ```

  ```py
  @fused.udf(cache_max_age=0)
  def udf(
      data_url: str = "https://www.fused.io/server/v1/realtime-shared/UDF_DuckDB_H3_SF/run/file?format=json",
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      style_url: str = "mapbox://styles/mapbox/dark-v10",
      center_lng: float = -122.417759,
      center_lat: float = 37.776452,
      zoom: float = 11.0,
      pitch: float = 45.0,
      bearing: float = 0.0,
      elevation_scale: float = 10.0,
      wireframe: bool = False,
      domain_min: float = 0.0,
      domain_max: float = 1000.0,
      null_color: tuple = (184, 184, 184),
  ):
      from jinja2 import Template
      html = Template(r"""

  """).render(
          data_url=data_url, mapbox_token=mapbox_token, style_url=style_url,
          center_lng=center_lng, center_lat=center_lat, zoom=zoom,
          pitch=pitch, bearing=bearing, elevation_scale=elevation_scale,
          wireframe=wireframe, domain_min=domain_min, domain_max=domain_max,
          null_color=list(null_color),
      )
      common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
      return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

  ```py
  @fused.udf(cache_max_age=0)
  def udf(
      token: str = "UDF_DSM_Zonal_Stats",
      host: str = "https://www.fused.io",  # or https://staging.fused.io
      mapbox_token: str = "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
      style_url: str = "mapbox://styles/mapbox/dark-v10",
      center_lng: float = -122.4194,  # SF
      center_lat: float = 37.7749,
      zoom: float = 16.0,
      pitch: float = 60.0,
      bearing: float = -20.0,
      minzoom: int = 0,
      maxzoom: int = 15,
      elevation_scale: float = 1.0,   # stats -> height multiplier
  ):
      from jinja2 import Template

      """
      Minimal Deck.gl 3D vector map (no raster). Requires server to serve MVT:
        /...//run/tiles///?dtype_out_vector=mvt
      """
      html = Template(r"""

  """).render(
          token=token,
          host=host,
          mapbox_token=mapbox_token,
          style_url=style_url,
          center_lng=center_lng,
          center_lat=center_lat,
          zoom=zoom,
          pitch=pitch,
          bearing=bearing,
          minzoom=minzoom,
          maxzoom=maxzoom,
          elevation_scale=elevation_scale,
      )

      common = fused.load("https://github.com/fusedio/udfs/tree/351515e/public/common/")
      return common.html_to_obj(html)
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": ,
    "loadingLayer": ,
    "errorLayer": 
  }
  ```

  Let's take the example of a UDF that returns a `GeoDataFrame` with `hex` values:
  
  ```python showLineNumbers
  @fused.udf()
  def udf(
      bounds: fused.types.Bounds = None,
  ):  
      # get_hex() is a non-important function for this demo that gives us US counties
      df_hex = get_hex(gdf, hex_res)
      df_hex['state_id'] = [id[:2] for id in df_hex["GEOID"]]
      
      return df_hex 
  ```

  And our visualization JSON looks like this:
  ```json
    
    }
  }
  ```

  You should make sure:
  1. `hexLayer > getFillColor > attr` is set to a column that exists in the `GeoDataFrame` (in this case `state_id`)
  2. Make sure your `attr` column is in either `int` or `float` type, not in `str`. In this case we should cast `state_id` to `int`:

  ```python  showLineNumbers
  @fused.udf()
  def udf(
      bounds: fused.types.Bounds = None,
  ):
      df_hex = get_hex(gdf, hex_res)
      df_hex['state_id'] = [id[:2] for id in df_hex["GEOID"]]
      df_hex['state_id'] = df_hex['state_id'].astype(int)
      
      return df_hex 
  ```

  3. Making sure your values are within the correct domain (`hexLayer > getFillColor > domain`). In our case, we're showing US States, so the domain should be `[0, 50]`.

  **Common 3D visualization issues:**

  1. **Wrong elevation function**: Don't use distance functions like `haversine` for elevation
     ```json
     // ❌ Wrong - haversine is for distances
     "getElevation": 
     
     // ✅ Correct - direct property reference
     "getElevation": "@@=properties.HEIGHT"
     ```

  2. **Missing required properties**: Ensure you have both `extruded: true` and a height column
     ```json
     "vectorLayer": 
     ```

  3. **Low transparency**: Avoid very low alpha values that make buildings invisible
     ```json
     // ❌ Too transparent (barely visible)
     "getFillColor": [255, 255, 255, 25]
     
     // ✅ Good visibility
     "getFillColor": [100, 150, 200, 180]
     ```

  **Common visibility issues:**

  - **Pickable setting**: Check that `pickable: true` is set for interactivity
  - **Data type mismatch**: Verify your UDF returns the expected data type (`GeoDataFrame` for vector, `array` for raster)
  - **Zoom level issues**: Ensure zoom levels (`minZoom`, `maxZoom`) include your current map zoom level
  - **Missing geometry**: GeoJsonLayer requires a geometry column in your GeoDataFrame
  - **Incorrect layer type**: Make sure you're using the right `@@type` for your data (GeoJsonLayer vs H3HexagonLayer)

</details>

For more details on DeckGL properties, see the DeckGL documentation.

================================================================================

## UDF Explorer
Path: workbench/udf-catalog.mdx
URL: https://docs.fused.io/workbench/udf-explorer

The UDF Explorer offers a searchable collection of User Defined Functions (UDFs) that can be imported into the UDF Builder for editing. It facilitates sharing and discovering UDFs within teams and the broader Fused community.

Data teams frequently encounter silos when looking to share and reuse code snippets & data across different projects. Traditional notebooks pose challenges in version control, sharing individual utility functions, managing dependencies, and reproducing results in new environments. Additionally, assets often end up disconnected from the code that generated them. UDFs on the other hand encapsulate modules, outputs, and code, which addresses issuess related to reproducibility and lineage.

- Create a new UDF
- Search the explorer across UDF categories
- View the detailed profile for each UDF
- Add any UDF from its Github URL
- Delete a UDF you created yourself
- Upload UDFs from JSON ot ZIP files

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/udf_catalog_edit9.mp4" width="100%" />

## Add a UDF

Create a new UDF by clicking the top-right "New UDF" button. This opens a UDF with a base template.
Alternatively, you can use the "Upload UDF" button to import an existing UDF from your local machine.

[Image: UDF Explorer]

## Search

Open the UDF Explorer by Clicking "UDF Explorer". Search UDFs by name, sort them, and toggle between gallery and list views. Click UDF cards to view their profiles or add them to the UDF Builder.

[Image: Add UDF Button]

### UDF categories

- **All UDFs:** View all available UDFs in one place, including all categories listed below.
- **Public UDFs:** Verified and accessible to all users
- **Community UDFs:** Shared by the community and accessible to all users
- **Team UDFs:** Shared privately within a team in a GitHub repo
- **Saved UDFs:** Private UDFs in the user's account

## Add from GitHub URL

If GitHub integration is enabled on your Workbench, you can paste the link of any UDF from GitHub to open it. This allows you to open a UDF from any branch or revision from GitHub.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/import_udf_from_github_edit7.mp4" width="100%" />

## UDF profile

Select a UDF to view its profile which includes the `README.md` description, last update date, author, link to its code on GitHub, and code preview.

## Contribute to Fused

Fused welcomes your skills and enthusiasm in support of the geospatial community!

There are numerous opportunities to get involved, from contributing code to engaging the community on Discord, LinkedIn, and other social media platforms.

### Where to start?

UDFs can be easily re-used, so before you build something new or share yours with the community, check the UDF Explorer to see if someone has already built something you might benefit from! Here are a few examples of Public & Community UDFs:

- Opening Overture Building Datasets to browse the latest building dataset from Overture's open data on Source Coop
- Exploring Sentinel 2 satellite imagery by using Microsoft's Planetary Computer or AWS Open Data Program
- Computing height for buildings in the US using the ALOS 30m Digital Surface Model

### Publish a UDF to a GitHub repository

Once you write a UDF, you can use the Push to GitHub button in Fused Workbench to publish or update your UDF in a GitHub repository. You will be able to select the repository (e.g., `fusedudf`, `community`, or `public`) and automatically create a pull request on GitHub. Read how to enable the GitHub integration.

To add a UDF to the community repository, select community from the dropdown before creating a pull request.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench/walkthrough-videos/docs_rewrite/push_udf_to_github_edit9.mp4" width="100%" />

Note that once the UDF is committed to the main branch of the repo, you can expect to wait up to 3 minutes for it to deploy and appear in the UDF Explorer.

================================================================================

# BLOG POSTS

## Introducing Fused AI Canvas
Path: blog/2025-10-30-intro-canvas/index.mdx
URL: https://docs.fused.io/blog/2025-10-30-intro-canvas

# Introducing Fused AI Canvas

Today we're introducing Fused AI Canvas, a new way to organize your UDFs on a free form canvas

## A Free Form Canvas

Organize & move your UDFs as you build your workflow on a free form canvas, allowing you to build analysis & workflows without being limited to a linear flow!

<img 
    style=}
    src="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-10-30-intro-canvas/moving_elements_on_canvas.gif" 
    alt="Moving elements on canvas"
/>

## Create workflows

Fused Canvas still runs individual serverless UDFs but you can now more easily connect them together

<img 
    style=}
    src="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-10-30-intro-canvas/connecting_udfs.gif" 
    alt="Connecting UDFs"
/>

## Share in 1 click

Share your work to anyone with a single click

<img 
    style=}
    src="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-10-30-intro-canvas/share_in_one_link.gif" 
    alt="Share canvas"
/>

## Try it out

Try out Fused Canvas right away for free

================================================================================

## Building Geospatial Real-Time Analysis in Fused
Path: blog/2025-10-24-chris-webinar/index.mdx
URL: https://docs.fused.io/blog/2025-10-24-chris-webinar

Chris Kyed, Data Scientist at Pacific Spatial Solutions, showcases 3 real time geospatial analysis uses cases for Fused. 

Last year Chris showcased how Pacific Spatial uses Fused to analyze 100 Billion drive records. Pacific Spatial Solutions is a company that provides geospatial analysis & data science services for various industries in Japan.

### Ocean Health Mapping

River plumes carry nutrients/sediment to the ocean and are split in 2 broad categories:
- Good plume: natural sediment/nutrient redistribute & add biodiversity
- Bad plume: excessive sediment/waste water and toxins/agricultural fertilizer with negative biodiversity impacts

Mapping them can be helpful for understanding the health and human activities impact on the ocean.

[Image: Ocean health mapping]

These datasets are open and available through the Jaxa Earth API :
- Chlorophyll-a concentration (Daytime/Half-monthly)
- Sea surface temperature (Daytime/Half-monthly)
- Precipitation Rate (Half-monthly)

### Rainfall & Incidence Data

Traffic accidents and traffic safety has been one of the most important problems for the last many decades. Now that we have publicly accessible data regarding the accidents, we need to find ways to access and analyze it.

The Japanese National Police Agency published yearly traffic incidence data all around the country. 

- Index of Open data on traffic accident statistics

[Image: Japan incidence data]

Explore the code directly in Fused:
- Incidence Data as tiled H3
- Japanese Police Intersection Accidents

### Tokyo Shadow Map

Japanese summers can get very hot, which is amplified in large urban areas creating urban heat islands. 

Prolonged exposure to the sun can cause dehydration and sunstroke, so we want to give Tokyo residents and tourists an app that can help them navigate the city in a more comfortable way.

Pacific Spatial Solutions is creating shadow maps for the city of Tokyo by combining 121 point clouds in batch and transforming them into 0.5m resolution DSM data and dynamically rendering them in Fused:

[Image: Tokyo shadow maps]

- Online Japan Shadow Map
- 3D point cloud data

### Learn more about Fused

- Try out Fused for yourself for free!
- Dive deeper in the Geospatial with Fused tutorials
- Learn more about working with H3 Tiles

================================================================================

## Analytics is Changing (Again)
Path: blog/2025-08-01-analytics_is_changing/index.mdx
URL: https://docs.fused.io/blog/2025-08-01-analytics_is_changing

# Analytics is Changing (Again)

AI has revolutionized how we write code; now it's reshaping how data teams can work.

    className="video__player"
    playing=
    muted=
    controls
    height="450px"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/from_udf_ai_to_dashboard_exploring_data_compressed.mp4"
    width="800px"
    style=}
/> */}

### Data + Compute + LLMs = Analytics at the Speed of Thought

The way we **interact with data changes constantly**; we only need to look at the last few years to see that we are seeing a shift as part of the "Great Decoupling” – a broader movement toward zero-infrastructure. 

By adding Serverless Compute – via **cloud functions** or the **browser** – to formats like Apache Parquet and engines such as DuckDB, we unlock blazing fast analytics without clusters or VMs. Now AI is entering the mix, and we think this is the perfect timing to put all the pieces together.

We don’t think analysts, data engineers & scientists are going anywhere. We believe that “AI won’t replace you, a human using AI will”. AI amplifies the creativity and speed of the people already doing the work.

Here’s how we think the future of analytics looks like:

- **Data context limits hallucination**: Having LLMs know exactly about the structure and content of your data means AI is much less likely to hallucinate.
- **Code execution as validation**: AI code editors help write code, but can’t actually run it. At Fused we’re building on top of our real time execution engine, so errors in code are spotted and fixed much faster, all in the same workflow.
- **Human analysts to guide the work**: LLMs can write every possible query imaginable to parse a dataset, or make whichever chart you want. Answering the questions becomes easier, so asking the right questions becomes more valuable.

[Image: Three ways working with AI is changing analytics]

### So... What is Vibe Analytics?

A few months ago Andrej Karpathy coined the term Vibe Coding: letting LLMs loose and just give in to whatever code suggestion comes around, to quickly get a prototype of a new project, specifically for web development.

We’ve since seen the term Vibe Analytics starting to pop up around, applying a similar approach but to analytics: leveraging LLMs to make it easier and faster to create analysis, make dashboards and share them to the rest of the team. 

### Come Build With Us!

Help us build the future of data analytics. Reach out to us directly (info@fused.io) or see our open positions.

---
We did take inspiration from Andrej Karpathy's recent Software is Changing (Again) talk at AI Startup School, as you may have noticed from the title.

================================================================================

## Fused is SOC2 Type 1 Compliant!
Path: blog/2025-07-08-soc2_type1/index.mdx
URL: https://docs.fused.io/blog/2025-07-08-soc2_type1

# Fused is now SOC 2 Type 1 compliant!

We're excited to announce that Fused has achieved SOC 2 Type 1 compliance. This is a significant milestone for us, as it demonstrates our commitment to security and transparency.

At Fused, we want to allow all data teams to get work done quickly. We want to make sure to do this on top of a secure and compliant platform.

Through the comprehensive auditing process overseen by Insight Assurance, we've demonstrated our adherence
to the stringent requirements outlined by the SOC 2 Type 1 standard, reinforcing our dedication to safeguarding
sensitive data and maintaining operational resilience.

Request the full report by email: `info@fused.io`. 

We're actively pursuing SOC 2 Type 2 certification, building on our existing Type 1 compliance.

================================================================================

## Notes from EO Summit 2025
Path: blog/2025-06-19-eo_summit/index.mdx
URL: https://docs.fused.io/blog/2025-06-19-eo_summit

# Notes from EO Summit 2025

Last week, we were at EO Summit, Fused came out of beta, we officially launched our new site and are open for business! 

Our mission is to help data teams get stuff done quickly, which is relevant for the people we talked to at EO Summit.

[Image: Fused at EO Summit]

Here are some of our takeaways:

### 1. AI makes writing code simpler, but executing it at scale is still hard

A lot of us are going through a bit of an existential crisis, while realising that damn, yep, AI can help us write code to glue datasets together quickly. Knowing the intricacies of a Python library isn’t a competitive advantage for individuals and companies to build the best analytics anymore. 

That’s all well and good when working on a small, one-off project. But conferences like EO Summit keep showing that there’s more data than ever before. Archives of imagery, 3D point clouds and any dataset keeps growing in scale, resolution & time backlog. Problems aren’t always local, nor limited to a small time & place. 

AI is changing what it means to build software & products, there’s not doubt about that. But executing it at scale is still a dark art. 

### 2. So much data, yet we still struggle to get things done in a timely manner

At another conference, the Cloud Native Geopaptial last month Brianna Pagán shared her story of the Los Angeles fires taking down her home, and while having so much data available, so little was actually accessible in a helpful manner quickly. This topic came again at this conference, wildfires being the prime example of the complexity of making timely and updated use of data in times of need despite having so much.

A few months ago, our own Milind Soni made a quick interactive dashboard in a few hours as a proof of concept of what rapid tools could look like for wildfire updates using Fused.

These conversations are directly going into our internal product development discussion; as we write this, we're developing tools to build these dashboard for rapid iteration. Stay tuned for more on that soon! 

### 3. The gap between data & applications

Nadine Alameh, at the head of the Taylor Geospatial Institute & former CEO of the OGC, the consortium in charge of standards for all things geospatial, mentioned the lack of companies in the middle solving problems between data providers and companies building analytics products. This is something Aravind, head of Terrawatch -who organised this conference- has been saying for a while.  

That’s why we were excited about being at EO Summit, we think we can make a difference tackling these problems!

[Image: Talking to attendees]

We’re building tools that make getting data from all types of places & formats together, execute code (however it was written) at any scale with a few lines of code and sharing it all to whomever needs it, be it interactive dashboard, CSVs or tile servers. 

If you’d like to learn more about Fused, book some time with Max, our Developer Advocate right here to get a demo!

================================================================================

## Scaling Environmental Insights with Fused and H3
Path: blog/2025-05-27-environmental-insights/index.mdx
URL: https://docs.fused.io/blog/2025-05-27-environmental-insights

# Scaling Environmental Insights with Fused and H3

Farmers and analysts face a familiar challenge: weather and crop data is fragmented, slow to process, and hard to act on.

We worked with Emma Quirk (Senior Data Analyst) and Majid Alivand (Senior Data & Analytics Manager) to showcase how Fused can help bring all these datasets together. In this webinar they give an overview of the industry challenges interfacing backend data analytics with frontend data consumption. Emma walks through the notebook she used to model climate and irrigation patterns for vineyards.

## Addressing the Challenge

Building useful & actionable weather models for environmental insights requires bringing datasets from:
- Different sources
- Different resolutions
- Different formats

To address these challenges, Emma & Majid used Fused + H3 to bring together datasets like GridMET climate, CDL crops, LANID irrigation, and gSSURGO soils by converting them from raster to H3 to build unified parquet files.

By building UDFs to manipulate each dataset, the team can iterate on their analysis in seconds while H3 allows them to more easily combine all the datasets together.

## Why H3?

- Harmonized format across datasets and regions
- Scalable queries at multiple resolutions
- Compact storage parquets with improved spatial performance
- No reprojection needed for global analyses
- Equal neighbors for clean spatial logic

## Try it out for yourself

Try out the notebook from the presentation for yourself:
- Colab Notebook

Sign up for a free Fused account and try it out for yourself!

================================================================================

## Launching Fused Apps
Path: blog/2025-05-20-launching-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-launching-fused-apps

# Launching Fused Apps

Today we're launching Fused Apps, allowing you to write Python in the browser, creating, editing & sharing interactive apps 

## 🐍 Write Python in the browser

Fused Apps are one of the the fastest way to write Python, directly in the browser. 
- No environment setup, just start writing Python
- Create interactive apps using Streamlit 
- Save & rename your Apps

## 🔗 Share your apps with anyone

As soon as you save your Fused App, you can create a shareable link for anyone to open

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>

## 🔒 Standalone & Private, running in your browser 

Fused Apps are built on top of Pyodide, Streamlit & Stlite but offer many features:
- Github Integration for team collaboration
- Run light-weight LLMs locally in your browser with libraries like `transformers.js.py`

[Image: CDL Bunnies]

## 📚 Access a Catalog of existing Fused Apps to get started

We've curated a catalog of existing Fused Apps to get you started, including:
- 🌽 Explore different crops around the US
- 🛰️ Visualize the trend of objects sent to Space
- 🌲 Pan a map of Forest statistics per global municipality

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/App_catalog_browsing.mp4"
/>

You can try out Fused Apps for yourself directly in Fused Workbench and read our dedicated Docs page

## ⚙️ How we built Fused Apps: Technical Details

We wrote a dedicated technical blog post going into details of how Fused Apps are built:
- How we originally built Fused Apps
- The challenges of Python in the browser
- Our approach to building a product

## Join our webinar on May 22nd!

On Thursday May 22nd we'll be hosting a webinar showcasing how our customers use Fused

- Join on LinkedIn

[Image: Webinar thumbnail]

================================================================================

## Inside Fused Apps: Python in The Browser
Path: blog/2025-05-20-inside-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-inside-fused-apps

# Inside Fused Apps: Python in The Browser

This is a technical deep dive into how we built **Fused Apps**, a way to build a Python-based workflow in your browser, that you can save and share with someone. You can read the full product announcement here.

At Fused, we’re building tools to help data scientists work more efficiently: we want to give them the ability to work on any dataset, create an analysis and scale it to the whole world with just a few lines of code. But data scientists don’t work in a vacuum, and analysis aren’t (always) done because people are simply curious about a topic. 

We built Fused Apps in the spirit of allowing a single person to do all the work themselves; and in this first episode of *Inside Fused*, a series of blogposts about how we’re building Fused, we want to take you behind the curtain to show how Fused Apps is built. 

### Fused Apps at a glance

Fused provides both a Python package to run User Defined Functions (UDFs), and Workbench, a browser-based IDE to write, execute, visualize them as well as create Fused Apps to make interactive frontends. 

Here’s what Fused Apps look like:

[Image: Fused apps preview]

_Fused Apps. From left to right: the list of apps that have been loaded in Workbench, the app code editor, and the running app itself._

The App code editor & renderer allow users to write their own Python code using Streamlit to build a frontend entirely in Python, a language most data scientists already work with.

We want data scientists to be able to go from “hey, that’s a cool idea” to “here’s what it looks like” without tech getting in their way. Especially in a world where LLMs make writing code simpler, the bottleneck becomes the speed at which data scientists can execute & ship code, not write it.

Fused Apps offers a way for data scientists to orchestrate their entire workflow using Python, without having to worry about backends, scaling, or clusters. Fused Apps complement our UDF builder, which offers a way to build data processing and backend functions, by offering an end-to-end workflow. At Fused, we use this for ingesting and managing datasets, managing resources we make available to our UDFs, and finalizing analyses.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/Vision_Pro_cdl_exploring_sped_up.mp4"
/>

_Fused Apps work on *any* device, as long as there's a browser!_

### How we built it

In short, Fused Apps tie together a frontend application for users to write Python code with Streamlit, a backend that saves & serves these apps and provides shareable links, and a product experience tying all of this together (error handling, autocomplete suggestions, async UI functionalities, etc.)

Fused Apps are built on top of Stlite & Streamlit. Streamlit being a library allowing you to write a frontend application, entirely in Python, and Stlite an in-browser version of Streamlit. This allows people familiar with Python but not so much frontend development (like data scientists) to have something rendered on screen in HTML, but only with writing Python. 

Stlite provides the ability to run apps in the browser, and while Stlite Sharing does support URL-encoded mechanisms for app sharing, the URL is the same as the code, preventing users from *updating* said code; there’s also no such thing as app catalogs, etc. So we ended up using Stlite as the engine and built a product experience around that. 

The frontend for building a Python-in-the-browser application already exists, with Streamlit & Stlite. However the backend had to be built from the ground up.

Originally, we didn’t even have a way for people to save their apps! Our internal workflow while developing & testing this was to have a Slack channel where we pasted our app links in to be able to find them later on

[Image: Early days Slack sharing links]

This is at the the core of our development philosophy: Build a prototype, use it a bunch, find the pain points, fix them, build more. In this case, saving & tracking apps was the next piece to build.

This is how we added functionality like Github integration, sharing options, and an app catalog. All these were only added after we got some traction internally and from early customers.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>
_Sharing Fused Apps is just a couple of clicks now_

### Python, in the browser

The next piece of this puzzle is to realise that running Python locally on your machine and in the browser is quite different. We want to make the experience for the user as seamless as possible: we’re building products for data scientists to build everything in Python.

We use Pyodide to run Python in WASM. This enables the same Python language to be used in a new environment – the browser. The browser environment is key because it gives us a way to safely ship applications to users.

Safely shipping software to people wasn’t always the simplest. Previous efforts via Flash and Java applets enabled a generation of rich web content. The promise of Java applets was run anywhere – that the same code could run on anyone’s computer. These technologies died out mainly because of security model problems. (Java lives on in Blu-ray disks.)

At the time of writing this, in mid 2025, browser applications are considered well sandboxed (with the possible exceptions of RowHammer/Spectre type issues), browser applications open when you want them, and go away completely when you dismiss them, all the while performing more and more complex tasks. Browser applications handle video encoding/decoding for video conferencing, graphics rendering for games, maps, and design, and more frequently, for programming. Companies like Figma are building complex, professional grade applications for the browser first.

From a development perspective, the browser becomes an operating system. It becomes less important to know what exact hardware & underlying operating system the users have, and we develop applications for the browsers’ APIs. Those applications are inherently portable to new devices, because the heavy lifting is porting the browser to these new devices. 

### Building a Product

There are other projects out there that allow building hosted Stlite applications, however these miss some of the features that people expect from a well rounded product. Here are few things we’ve added to Fused Apps:
- Package Handling (Every Fused Apps comes with `fused` and `pyarrow` pre-installed)
- Error handling 
- Naming & saving of Apps
- Github integration for teams
- Creating shareable links allowing users to send their apps to anyone with a browser

Beyond the technical challenges we want to build a product that helps data scientists build, iterate and ship faster. A data scientist can build an analysis that takes in different parameters and make an interactive graph for their project manager to test out directly all without needing support

A big part of this is the philosophy of how we build things at Fused: We’re a team of engineers & data scientists. A lot of the features we’re listing above here come directly from our own usage of Fused Apps while making real applications with some of our customers.

We care about how fast it takes for a new user to click on a Fused App and start running it, or what the experience of saving & sharing an app looks like. 

Every Fused App comes with some packages pre-installed, such as `fused` and `pyarrow`, which are helpful for data scientists. (Streamlit comes with a number of other common packages like Pandas and NumPy already.) But this comes at the expense of loading time, as each new app is a self-contained application which requires downloading all the required packages. This is leading us to spend time optimizing the initial loading time of Pyodide.

### What can we expect from Pyodide

Fused Apps, and any implementation of Python in the browser, isn’t as customizable as a fully local setup. Packages with native code that are not prebuilt for WASM will not work. At the time of writing, some popular packages like Pandas & Numpy are built and supported, but others aren’t. The backbone of the map processing pipeline, GDAL, for example, isn’t currently supported. This isn’t from lack of popularity, but reflects the complexity of building GDAL.

Some packages will need architecture updates. As another example people currently call into ffmpegfrom Python using the CLI. The WASM environment does not have a CLI concept, and this would need to be replaced with library calls. Other packages which are not well suited to this architecture (such as Torch) might have alternatives developed because the wasm ecosystem is so attractive a development target.

#### LLMs in the Browser

An example of this is transformers.js.py. This library (developed by Yuichiro Tachibana, also behind Stlite) needed more than one level of architectural adaptation to get running in the browser, but once adapted it brought the capability of running lightweight LLMs. This allows us to ship applications that run small LLMs models, running locally in the browser! 

We made a Fused App exploring the USDA’s Crop Data Layer (CDL) dataset, a dataset of different crop types in the US. Instead of showing all 130+ categories, we can just take a text input, run a similarity analysis on the fly against all the categories and find the closest CDL crop type! 

[Image: CDL Bunnies]

_You too can find out what Bunnies prefer by trying this Fused App here_

#### Compatibility & load times

This is a fast changing ecosystem though, for example we’ve made a small contribution of a build of the H3 package specifically because we wanted to have H3 supported in Python in the browser. The list of supported packages built in Pyodide is growing quickly. 

Moving large amounts of data in and out of the browser is slow, so data-intensive libraries like Torch or Dask are not well-suited for this. We solve this by bringing in the compute performance & flexibility of Fused UDFs, which run native (non-WASM) Python in a cloud environment. Fused UDFs run code much closer to the data, improving performance, and are not limited by what packages are available for Pyodide, improving flexibility.

Install times for Pyodide are relatively long from a user point of view, as the application isn’t stored locally and installed each time it's opened. This will most likely keep improving over time as the technology is developed further. For example Pyodide wants to add memory snapshotting to help with this, but it isn’t stable yet.

Even when the application is stored locally, we found we needed to reinitialize Pyodide in some cases. There is no concept of switching virtual environments in Pyodide, since running a new script reuses the same Python VM. When switching between apps in Fused, we reinitialize Pyodide in order to prevent packages from one app interfering with another. If we didn’t do this, a user might accidentally rely on a package installed by App A in developing App B, which would then not work when sharing App B.

We also needed to be careful with when the app can run, since the app has access to the user’s browser context. We chose to give users the chance to inspect the app’s code before running it if it would have access to their Fused account.

Code written in Stlite and Pyodide looks almost the same as regular Python, but there are slight differences. Many of these come in the form of adapting synchronous and asynchronous code. Stlite for example allows for top-level `await` because the browser’s event loop is being used. This can be tricky to work with because regular advice for working around asynchronous code in Python does not work with Pyodide.

[Image: CDL loading async model & logging]
_Async-aware UI allows us to provide improved feedback as an app loads a light weight LLM model or data to display_

In order to create a good product experience, we added our own syntax checking and linting on top of Pyodide. 

[Image: Error handling]

_Lots of small features go a long way to delighting users with a smooth experience._

### Delivering at near-zero cost

Fused Apps are part of the free tier of Fused. You can use Fused Apps without logging in at all, and with a free login you can save and share your apps. We want to take a moment here to explain why this is naturally a free offering and will continue to be free.

As a browser-based application, all of the code execution and data transfer happens in the user's browser. This means that we do not need to sandbox code execution or pay for cloud compute resources to run anyone’s Fused Apps. 

Where we do incur costs are in the shared control plane layer of Fused. Technically, the control place layer doesn’t see much difference between an app and a UDF. As a result, the incremental cost of serving an app user is very low and it is easy for us to offer that for free. The core offering of Fused is the backend serverless execution of code, which is our paid product.

### Try it out for yourself 

Don’t take our word for it, give Fused Apps a try for yourself! As we mentioned, Fused Apps are free and don’t require login. You can check out our catalog of public Apps in Fused Workbench.

Recently we announced re-partitioning the Crop Data Layer dataset into H3 hexagons for anyone to use and hosted the resulting dataset on Source Cooperative.

Alongside this we created a public Fused App allowing you to explore any crop for the 2024 dataset

### We're hiring: Help us build the future of data science workflows!

We firmly believe data scientists need tools that give them the independence to do their work rather than asking for support to scale their analysis or share their results. 

We need smart people to help us build all of this. We are hiring for:

- Deep knowledge of Python & Pyodide
- Opinionated thinking in building the future of data science pipelines
- People wanting to join a fast moving startup and build things

If you’d like to join the team, **send us your info here**!

[Image: The team]

_Join the team!_

================================================================================

## How Sylvera uses Fused to prototype and power DeckGL applications
Path: blog/2025-05-16-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2025-05-16-danieljahn

**TL;DR Sylvera quickly builds and tests new app features by serving data to DeckGL applications using Fused HTTPS endpoints.**

At its core, Sylvera rates carbon projects. Our ratings are powered by several earth observation and geospatial analysis data products. From climate risk data, and deforestation indicators, to biomass-predicting ML models, a wealth of data goes into generating a single-letter rating.

```javascript showLineNumbers
const BOUNDS_AFRICA: [number, number, number, number] = [
  -25.35, -46.95, 51.35, 37.35,
];
const UDF_H_AOI_FILE_CALL = "FUSED_UDF";

function createTileLayer(id: string, data: string[]): TileLayer<ImageBitmap>  = props;

      return [
        new BitmapLayer(otherProps, ),
      ];
    },
  });
}

function createBitmapLayer(
  id: string,
  image: string,
  bounds: [number, number, number, number]
) );
}

const createFusedFileLayer = (
  layerId: string,
  fusedId: string,
  bounds: [number, number, number, number],
  year: number
) => -$`;
  const param = `year=$`;
  const imageUrl = `https://www.fused.io/server/v1/realtime-shared/$/run/file?dtype_out_raster=png&dtype_out_vector=csv&$`;

  const layer = createBitmapLayer(key, imageUrl, bounds);

  return layer;
};

const createBasemapLayer = () => //.png",
  ]);
};

  const bounds = BOUNDS_AFRICA;
  const year = 2000;

  const basemap = createBasemapLayer();
  const fused = createFusedFileLayer(
    "fused",
    UDF_H_AOI_FILE_CALL,
    bounds,
    year
  );

  return (

  );
}

  createRoot(container).render(<App />);
}
```
</details>

## Conclusion and future work

The application we built isn't yet fully featured to be put in front of users – but that's the point. We were not aiming for a finished product yet. Instead, we achieved rapid iteration that enabled us to gather relevant stakeholder feedback.

The speed we could reach wouldn't have been possible without Fused's development platform. Fused unifies three traditionally separate stages — prototyping, scaling, and visualization—into a single seamless solution. Thanks to this, Fused was an indispensable tool for product iteration.

In the future, we would like to explore the coming integration with Zarr stores. Being able to not only visualize the results but also to immediately persist them into a Zarr store will be a game-changing capability for anyone who uses Zarr as the persistence layer.

================================================================================

## Repartitioning Crop Data Layer & US Census into H3 hexagons
Path: blog/2025-05-06-cdl-census-hex/index.mdx
URL: https://docs.fused.io/blog/2025-05-06-cdl-census-hex

# Repartitioning Crop Data Layer & US Census into H3 hexagons

Fused has repartitioned the USDA Crop Data Layer and US Census into H3 hexagons.

These 2 datasets are both available on Source Cooperative for anyone to use, free of charge!

[Image: Source coop CDL hex]

### What are these datasets?

We've taken 2 popular datasets and made them available in H3 Hexagons, this provides a number of benefits:
- Much faster ability to aggregate data at different scales
- A simple way to join datasets together (as they, and any other H3 tiled dataset are using the same grid)

You can read more about H3 Indexes on the dedicated page.

We're providing:

1. Crop Data Layer (from USDA CroplandCros)

- Available for `[2012, 2014, 2016, 2018, 2020, 2022, 2024]`
- Available in `hex7` and `hex8` resolutions

2. US Census (from data.census.gov)

- Available for `2020`
- Available in `hex7` and `hex8` resolutions

### Using these datasets

These datasets are completely free for anyone to use, with or without Fused. To showcase what's possible we have made a public Fused UDF anyone can use without an account to explore these.

[Image: CDL from source udf]

You can try it here without any account.

================================================================================

## Fused featured in Maxar TED Talk
Path: blog/2025-04-08-TED/index.mdx
URL: https://docs.fused.io/blog/2025-04-08-TED

# Fused featured in Maxar TED Talk

[Image: Fused Powering Maxar]

Fused is excited to be featured in a TED Talk today by Peter Wilczynski, CPO at Maxar, which explored a multi-source site monitoring scenario of Vancouver's greening initiatives over the past five years powered by the Fused platform.

[Image: Fused Maxar Workbench]

Site monitoring scenarios are at their most effective when data from disparate sources is combined—from satellite imagery and map data to municipal and demographic data. Analytics today requires bringing all these different sources of data together: from raw pixels to contextual intelligence — in real-time, seamlessly, and at scale.

This is an iterative process, each dataset being in its own format & hosted on different cloud providers, updated independently and on different schedules.

Fused is built by data scientists, engineers & open source contributors who know the reality of building pipelines with breaking changes in libraries, new file formats that require rethinking how data is ingested and business requirements that constantly change.

We firmly believe we need tools that help us bring as many datasets together as possible, especially in a world where LLMs and AI can increasingly talk directly to data.

If you’d like to try out an instance of the Fused workbench with pre-loaded data from Maxar later this month, which Peter demoed on screen, sign up for the waitlist on fused.maxar.com to get access and see for yourself this is more than a pre-recorded tech demo!

================================================================================

## Announcing Fused AI Builder
Path: blog/2025-04-01-announcing-ai-builder/index.mdx
URL: https://docs.fused.io/blog/2025-04-01-announcing-ai-builder

# Announcing Fused AI Builder

**Fused AI Builder let's you create LLM Agents that can directly call & execute deployed Python code through Fused UDFs!**

### Give LLMs the ability the talk to your data & code

We're launching Fused AI Builder, a simple way for you to give an LLM access to any UDF you want

</Tabs>

================================================================================

## Announcing Fused 2.0
Path: blog/2025-02-25-fused/index.mdx
URL: https://docs.fused.io/blog/2025-02-25-fused

# Announcing Fused 2.0

**Fused 2.0 is our biggest update to date, with changes across Workbench & `fused-py`!**

### A New UDF Editor in Workbench

[Image: New Fused Workbench]

#### Introducing Collections: A simple way to organize your UDFs

Collections now allows you to organize your UDFs together as you work on different projects, save them together and keep your editor focused on the project at hand.

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/load_collections_new.mp4" width="100%" />

Collection is still in early access so you need to enable it under "Preferences -> Enable UDF Collections [Beta]" to access it

#### Redesigned UDF Editor

UDF Editor now comes with a host of new features & redesigned UI:
- Adding a new Full Screen Map View
- The Visualize Tab is now under the UDF expanded parameters, allowing you to hide your Code Editor and just focus on tweaking the visualization of your data
- Split screen "Editor" & "Module" on top of each other: Keeping your code clean in the main "Editor" Tab is now easier by moving functions under the "Module" tab.

#### A New Share Page

We've moved all the tools & information you need to easily share your UDFs into a dedicated page (and button). You can easily:
- Create a token to share your UDFs 
- Edit the Description, Tags & Image preview of your UDFs

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/share_udf.mp4" width="100%" />

### Changes to `fused-py`

Our Python library `fused-py` is getting some updates to make processing data in Python simpler, from a small one time task to processing huge datasets

#### Simplifying how Fused handles geometries

- Moving away from Fused having many different `fused.types` to only having 2 simple types: `fused.types.Bounds` and `fused.types.Tile`.
- Replacing `bbox` object with `bounds`: a more generic term to pass geometries to UDFs

#### `fused.submit()` for simple, multi job runs

We're introducing `fused.submit()` as a simple way to run many UDFs all at once in parallel

[Image: fused.submit() demo]

This can significantly speed up parallel tasks like:
- Fetching lots of individual data points from an API
- Scaling small processing steps to lots of data points

#### Improved caching

Under the hood we've significantly improved how Fused caches results of recurring UDFs & cached functions + we've introduced new tools for developers to control caching:
- A default 90 days cache time for all UDFs
- Editing the cache duration with the new `cache_max_age` argument

Read more about Caching

### Full Fused 2.0 Changelog

Read our Changelog to see every change happening with Fused 2.0

================================================================================

## Enhance your data with GERS IDs
Path: blog/2025-02-18-jennings/index.mdx
URL: https://docs.fused.io/blog/2025-02-18-jennings

**TL;DR Enriching your spatial data with GERS IDs can make it interoperable across the data ecosystem. You can use Fused UDFs to create custom HTTPS endpoints to enrich your data with GERS.**

The Global Entity Reference System (GERS) is a framework that structures, encodes, and matches map data to a shared universal reference within Overture. GERS helps organizations identify and reference their own datasets with standard identifiers to Overture data to help unify datasets.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gers_sheets.mp4" width="100%" />

\
In this blog post we show how to create simple endpoints with Fused UDFs to enrich a dataset with GERS IDs. We'll use the Overture Building footprints to first enrich a polygon with GERS IDs then another one to look-up metadata for a specified GERS ID.

## The benefit of GERS

When third-party dataset is spatially matched to an Overture feature it's "enriched" with that feature's GERS ID and becomes "GERS-enabled". This makes it easy to associate it by ID to any other GERS-enabled dataset.

```
                            gers_id             buliding_name
0  08b2a1072534cfff020018b8a6efde22  James A. Farley Building
1  08b2a100d2cb6fff02000821de8bdff1      Pennsylvania Station
```

For example, a municipal government with a dataset of building footprints for local offices, coffee shops, and museums could match those entities to a GERS ID. This would enable the government to easily join its data other "GERS-enabled" datasets to enrich them with additional information such as insurance data, historical property values, restaurant reviews, fire risk, or rooftop solar potential.

## Create a UDF to enrich a polygon with GERS IDs

We can create a UDF that takes in a polygon and returns a GERS ID. This UDF will spatially match the polygon to Overture Buildings and return the GERS ID of the building that intersects the polygon. This is useful for enriching a dataset with GERS IDs.

Users can design and preview the workflow interactively, allowing them to test assumptions and visualize their effect. Parameters can be adjusted, and the output can be previewed before running the UDF on the entire dataset.

```python showLineNumbers

aoi = json.dumps(,"geometry":}]})

@fused.udf
def udf(bbox: fused.types.TileGDF=aoi):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Convert bbox to GeoDataFrame
    if isinstance(bbox, str):
        bbox = gpd.GeoDataFrame.from_features(json.loads(bbox))

    # 2. Load Overture Buildings that intersect the given bbox centroid
    gdf = utils.get_overture(bbox=bbox.geometry.centroid, overture_type='building', min_zoom=10)

    # How many Overture buildings fall within the bbox centroid?
    print("Buildings in centroid: ", len(gdf))

    # 3. Rule to set only one GERS on the input polygon
    bbox['id'] = gdf.id.values[0]

    return bbox
```

### Create an HTTPS endpoint

With Fused, it's easy to turn your UDF into an HTTPS endpoint. This enables you to run the UDF it programmatically via HTTPS requests to integrate the functionality into various workflows and applications.

This endpoint runs a public UDF with the code above. You can call it with a geojson of a single polygon in the bbox query parameter and it will return a geojson with the polygon and an assigned GERS ID.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Enrich/run/file?dtype_out_vector=geojson&bbox=,"geometry":}]}
```

## Create a UDF to look-up metadata for a GERS IDs

We can also create a sample UDF to do a reverse operation: look-up a Building and its attributes by passing a GERS id. A user will be able to pass a GERS id and the UDF will look-up the building and return its geometry along with attributes about the building.

To do this, we create a UDF that takes in a `gers_id` parameter. Because the first 16 digits of GERS correspond to an H3 cell, we can use the ID to create a polygon to spatially filter the dataset. It'll bring up any buildings that intersect the H3 cell. Once we have the building, we can easily work with its geometry object and attributes using GeoPandas.

```python showLineNumbers
@fused.udf
def udf(gers_id: str='08b2a100d2cb6fff02000821de8bdff1'):

    from shapely.geometry import Polygon

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. H3 from GERS
    h3_index = gers_id[:16]
    print('h3_index', h3_index)

    # 2. Polygon from H3
    bounds = Polygon([coord[::-1] for coord in h3.cell_to_boundary(h3_index)])
    bbox = gpd.GeoDataFrame()

    # 3. Load Overture Buildings
    gdf = utils.get_overture(bbox=bbox, overture_type='building', min_zoom=10)

    # 4. Subselect building
    gdf = gdf[gdf['id'] == gers_id]

    # 5. De-struct the names column
    normalized_df = pd.json_normalize(gdf['names'])
    gdf = gdf.reset_index(drop=True).join(normalized_df)

    return gdf[['id', 'primary', 'subtype', 'class', 'geometry']]
```

### Create an HTTPS endpoint

Here's how you can create and use an HTTPS endpoint for your GERS building lookup UDF.

This endpoint returns a CSV table of the building's GERS ID, primary name, subtype, class, and geometry. You can use this endpoint to enrich your dataset with GERS IDs by calling it with a GERS ID query parameter.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Lookup/run/file?08b2a100d2cb6fff02000821de8bdff1&dtype_out_vector=csv
```

For example, you could call this endpoint from a Google Sheet to enrich a dataset with GERS IDs. This sample Google Sheet returns the enriches the "primary" name column for any given Building GERS. Just drag the formula to apply it to any row below. It works by calling the "GERS lookup" endpoint with a GERS ID query parameter.

================================================================================

## We're partnering with Overture to make their Data easily accessible with Fused
Path: blog/2025-02-11-overture/index.mdx
URL: https://docs.fused.io/blog/2025-02-11-overture

**_TL;DR: We've made it easier to work with Overture data by leveraging Fused._**

Fused has been working with the team at The Overture Maps Foundation to enable direct access to their data through Fused UDFs. We are excited to share that the Overture docs now show examples on how to see how to integrate any Overture data into your workflows using Fused.

[Image: Alt]

Overture Maps aims to provide foundational building blocks of data across various themes designed to be broadly applicable across industries. Our goal at Fused is to make it easy to work with Overture data and adopt standards (such as GERS). To this end, we are creating easy abstractions to access data, tools to perform foundational operations such as conflation and enrichment, and example workflows to inspire and help you understand how to leverage this data.

One of the key usecases for Overture + Fused is enriching datasets with Overture Maps data. This tutorial showcases 2 simple Python workflows that do this by performing a spatial join with Overture Buildings. This example of a simple enrichment operation will help you understand how to work with Overture data in your own workflows.

To follow along, check out the:
- Overture Maps Docs "Getting Data" Page
- Overture Maps Docs "Examples" Page
- Overture Maps Example UDF
- Overture + NSI UDF

## Overview

The Overture Buildings dataset is dividen into themes. Two key themes are:
- Buildings is composed of building footprints represented as polygons
- Places is composed of business establishment locations and associated metadata, represented with point coordinates.

We'll first show how you can load Overture data by reusing an existing Fused UDF, then write a User Defined Function (UDF) with custom logic to perform enrichment with a spatial join. You'll be able to run the resulting UDF for any custom area of interest (AOI).

## Step 1: Load data with the Overture Maps UDF

Fused has a catalog of pre-made UDFs you can easily copy and repurpose for your own data analysis workflows. In the catalog, you'll find the Fused Overture UDF which enables you to quickly load Overture data from any of the themes for an area of interest (AOI). You can run the UDF with `fused.run` and specify an AOI to load data for using the bbox parameter. You may also pass optional parameters to select between Overture releases, themes, and columns - that way you can fetch only the data you need. In this example, we can specify the 'building' theme by setting the `overture_type` parameter.

```python showLineNumbers

bounds = [-73.9847, 40.7666, -73.9810, 40.7694]

fused.run('UDF_Overture_Maps_Example', bounds=bounds, overture_type='building')
```

The output should look like this:

```python showLineNumbers

        id	                                geometry	                                        class   ...
24134	08b2a100d65a6fff0200b45ce7e2b99b	POLYGON ((-73.98552 40.76736, -73.98557 40.767...	apartments  ...
24135	08b2a1008b259fff02007917db1c32d3	POLYGON ((-73.98441 40.76703, -73.98431 40.767...	apartments  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	apartments  ...
24179	08b2a100d6516fff02006dc174022a7e	POLYGON ((-73.98346 40.76623, -73.98327 40.766...	commercial  ...
24180	08b2a1008b248fff0200315983940aa8	POLYGON ((-73.98407 40.76749, -73.98402 40.767...	None    ...

```

By browsing the UDF's catalog page, you can see its code and even copy it to run it interactively on the Fused Workbench. You'll notice that the UDF uses the `get_overture` helper function to read from spatially partitioned parquets of the overture data releases, hosted in a Source Cooperative S3 bucket. The source code of the helper function is fully open and hosted on GitHub here.

Here's a simplified version to show the core of what's going on in `get_overture`. It constructs the table path on S3 and then uses the table_to_tile helper function from Fused to load data that falls within the specified bounding box. This approach allows you to efficiently perform spatial queries on a large dataset and load only the records within the given area.

```python showLineNumbers
# Structure the table path with input parameters
table_path = f"s3://us-west-2.opendata.source.coop/fused/overture//theme=/type="

# Load the data within the bounding box
df = utils.table_to_tile(bbox, table=part_path)
```

## Step 2: Write a Custom UDF to join Places with Buildings
The example above shows how to run an existing UDF to load Overture data, but it's likely you want to write your own data transformations. You can borrow `get_overture` to load data into your own UDFs. As an example, here is a UDF to load Overture Buildings polygons and Overture Places points. The UDF perform a spatial join between them to determine which points fall within each building.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Buildings
    gdf_buildings = utils.get_overture(bbox=bbox, theme='buildings')

    # 2. Load Places
    gdf_places = utils.get_overture(bbox=bbox, theme='places')

    # 3. Create a column with the Buliding Name
    gdf_buildings['primary_name'] = gdf_buildings['names'].apply(lambda x: x.get('primary') if isinstance(x, dict) else None)

    # 4. Spatial join between Places and Buildings
    gdf_joined = gdf_places.sjoin(gdf_buildings[['geometry', 'primary_name']])[['id', 'names', 'primary_name', 'geometry']]

    return gdf_joined
```

To run this UDF, you simply call it with your AOI. Fused will execute the code with the given parameter then return the UDF's output.

```python showLineNumbers

bbox = gpd.GeoDataFrame(geometry=[shapely.box(-73.9847, 40.7666, -73.9810, 40.7694)], crs=4326)

fused.run(udf, bbox=bbox)
````

This will return a GeoDataFrame with the geometry of the Place, the `primary_name` of the building it falls within, and other attributes as defined in the UDF's return statement.

The output should look like this:

```python showLineNumbers
        id	                                names                                                   primary_name	    geometry
3883	08f2a100d65160860308e7269804dcb7	 className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/join_places.mp4" width="100%" />

## Step 3: Write a Custom UDF to join Buildings with the NSI dataset

As a second example, you can also enrich the Overture Buildings dataset with metadata data from the National Structure Inventory (NSI) API. The NSI API offers point data on buildings in the U.S. that is relevant to hazard analyses.

This UDF loads the Overture and NSI datasets, performs a spatial join to enrich the building polygons with hazard metadata, and returns the enriched GeoDataFrame. It can be used within a larger analysis workflow to enrich building polygons to calculate risk indices. You can read more about performing spatial operations to enrich Overture Buildings with NSI in our geospatial processing guide.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Overture Buildings
    gdf_overture = utils.get_overture(bbox=bbox)

    # 2. Load NSI from API
    response = requests.post(
        url="https://nsi.sec.usace.army.mil/nsiapi/structures?fmt=fc",
        json=bbox.__geo_interface__,
    )

    # 3. Create NSI gdf
    gdf = gpd.GeoDataFrame.from_features(response.json()["features"])

    # 4. Join Overture and NSI
    cols = ["id","geometry","metric","ground_elv_m","height","num_floors","num_story"]
    join = gdf_overture.sjoin(gdf, how='left')
    join["metric"] = join.apply(lambda row: row.height if pd.notnull(row.height) else row.num_story*3, axis=1)
    return join[cols]

```

The output should look like this:

```python showLineNumbers

	    id	                                geometry	                                        val_struct      med_yr_blt  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	348190.820	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
```

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/overture_nsi_2025.mp4" width="100%" />

## Conclusion

In this short tutorial, we outlined how you can integrate Overture data into your workflows using Fused. We saw how you can use Fused to load the data, write a custom Python workflow, and run it for a custom AOI. We hope the these foundational pieces help you see how you can unlock the full potential of Overture Maps and start creating your own workflows to enrich your own datasets.

================================================================================

## How Pilot Fiber creates internal tools to support telecom operations
Path: blog/2025-01-23-kyle/index.mdx
URL: https://docs.fused.io/blog/2025-01-23-kyle

**TL;DR Pilot Fiber creates apps with Fused to quickly identify and resolve service interruptions for its New York City customers.**

Pilot Fiber is a commercial Internet Service Provider primarily in New York City. Our primary value proposition in competing with national-scale ISPs is our commitment to customer experience–we are fast and flexible in responding to customer needs in all aspects of the business:

- During the sales process, we aim to answer customer questions quickly and accurately, including technical details and routing.
- When designing our deployment into new buildings, we equip our engineers with as much detail as possible before they arrive on-site to maximize the efficiency of time spent with building engineers.
- When an incident interrupts a service we will immediately jump into action to address the cause and restore connectivity.

We use Fused to support all of these areas, and this post focuses on the last one: incident management.
- What happens when a service interruption occurs?
- How do we identify the likely location of an issue and get to a solution as quickly as possible?

### The Problem: Why Speed Matters

One of the most common ways a customer's service can be impacted is through damage to the physical fiber cables connecting them back to a data center and the internet. Almost all fiber optic cables in Manhattan run through a shared manhole-and-duct system beneath the streets. As such, road construction or work by other providers in a manhole has the potential to damage the equipment of multiple providers. When that damage occurs, it is first come, first served to get your network repaired and customers back online. Field teams from multiple providers can't work in the same manhole simultaneously, so being onsite first and ready to repair can mean a difference of hours in customer downtime.

Because of this, Pilot uses an active fiber monitoring system across our network. Sophisticated devices in our data centers are constantly shooting light down the fibers in our network looking for potential damage. Those devices return a reflectance signature from the fiber and compare it with a reference "snapshot" created when that fiber was initially installed in a building.

When an anomaly is registered, it immediately fires an alert giving a fiber route and distance to the potential problem (i.e. "There is unexpected light loss on the fiber serving 1234 5th Avenue at a distance of 2.351 kilometers from the data center."). When this happens, our engineering and support teams analyze the data within minutes to determine the issue's exact location and, if necessary, get crews headed to the site to begin repairs.

## The Process: Fused As The Glue-Layer

Historically, this analysis has required the attention of an Outside Plant engineer with access to specialized software and network knowledge, regardless of the time of day or day of the week. This bottleneck is not ideal when time is of the essence, even at 3 a.m. So today, we are creating a more sophisticated future using Fused to make this information accessible to more support team members and make our response times even faster.

Using Fused as a back-end glue layer, we built a web app allowing users to select a route and distance and calculate where the system has registered the fault. We also created a simple user interface that provides the user a view of nearby network infrastructure and automatically generates the reports field crews would need to complete repairs based on that nearby infrastructure.

The workflow requires a series of calls to UDFs that act as intermediaries to a Postgres/PostGIS database, which in turn is sourcing data from other internal sources. This structure allows us to easily keep the business logic organized at the UDF layer while limiting the scope of data access and security via Postgres and internal processes maintaining the sync.

The basic process is seen below:

_Workflow diagram._

Two separate flows are initiated when the user inputs a route and a distance to process. One retrieves the selected route to load onto the Mapbox-based map within the app, while the second kicks off a processing chain to analyze the fault information. This chain utilizes UDFs that assist in isolating the location of the fault and relevant nearby infrastructure and adding elements to the map display to assist the user in visualizing what may be occurring.

_UDF to find fault location._

If you consider the cables you see strung along utility poles, they are not perfectly straight: they can sag, bend, go up and down, and have coils of extra cable along the way. The same is true of our cables under the streets. All of those variations add distance to the run, which needs to be considered when determining where a fault is likely to be located.

Taking those variables into account, we use Fused to apply GeoPandas and PostGIS spatial functionality to assess where the fault is most likely to be located. After calculating that location, the tool loads splice cases along that route that point to where problems are most likely to have occurred, and slack loops built into the route to make the user aware of nearby capacity that could enable faster repair of more significant damage. We next determine which splice cases are closest to the likely damage point and if any of those are within 150m of the automated distance calculation. These manholes would be the first locations our field teams would be sent to investigate.

Once we have determined the relevant nearby splice cases, we use another UDF to build a CSV that reproduces what a splice report ## The Impact of Fused

The impact of Fused across this process is many-fold:
- The ability to easily work with data across several systems.
- Centralizing business logic to the UDFs involved eliminates the tendency for this logic to be spread across client-side processes, server-side processes, and possibly the database itself.
- Modularization of operations into UDFs. For example, the UDF that generates the splice reports for this process can easily be reused in any other method that requires the same functionality.
- The effortless ability for the same UDF to simultaneously serve as a modular processing unit where needed in one workflow and a map service for display in another.
- Using Python and standard libraries enables developers who may not be spatial data experts to read, understand, and modify UDFs as necessary.

## Conclusion

Pilot's success in the market is largely based on our flexibility and responsiveness to customer needs, which is never more important than when physical damage to the network is impacting their service. Within this scenario, Fused is providing a critical layer enabling us to offer more non-technical users access to data in multiple systems through a simple UI that will result in repair teams moving to restore service to our customers as quickly as possible.

Fused is an ideal product for Pilot Fiber in that we can increasingly make highly technical information available in a usable format to additional teams throughout the company in support of our drive to be fast, flexible, and accurate in delivering service to our customers in all aspects of the business.

================================================================================

## How Fused Powers BlackPrint's Acquisition Intelligence Platform
Path: blog/2025-01-22-blackprint/index.mdx
URL: https://docs.fused.io/blog/2025-01-22-blackprint

**TL;DR BlackPrint streamlines and transforms fragmented real estate data into actionable insights across Latin America.**

BlackPrint Technologies began its journey as a satellite mapping service designed to help municipalities modernize their property registries. Recognizing a greater opportunity, we transitioned into the private sector to address a significant gap in the commercial real estate market: the need for accessible, actionable data. Today, our platform empowers professionals with precise acquisition intelligence, transforming how decisions are made across Mexico and, soon, all of Latin America.

In this blog post, we will explore how BlackPrint built the backend of its intelligence platform with Fused to provide a comprehensive view of the commercial real estate market in Mexico.

## The Problem: Challenges in Real Estate Data
Real estate professionals face fragmented data sources and manual processes. This complexity limits access to critical metrics such as zoning regulations, demographic patterns, and traffic trends, making site selection decisions both high-stakes and prone to errors. BlackPrint recognized this challenge and set out to create an all-in-one platform, transforming disjointed data into actionable intelligence to empower professionals across the commercial real estate landscape in Latin America.
Our Solution: The BlackPrint Approach

BlackPrint's platform offers a comprehensive suite of tools and insights, including property and zoning data, demographic and socioeconomic analytics, and detailed foot and vehicle traffic analysis. Underpinning this solution is a staggering volume of geospatial data sourced from diverse datasets such as cadastral records, demographic studies, and traffic sensors. This vast amount of information is meticulously analyzed to deliver actionable insights, all while being presented through an intuitive and user-friendly interface. By simplifying complex data into accessible formats, BlackPrint empowers users—regardless of their technical expertise—to navigate and leverage insights with ease. Building such a robust platform required not only expertise but also the right tools. That's where Fused came in.

## Leveraging Fused: A Partnership for Efficiency

Fused became an indispensable partner in turning our vision for BlackPrint into reality. Its end-to-end cloud platform allowed us to move from concept to MVP significantly faster by simplifying our data processing and data delivery workflows. Before using Fused, we faced daunting challenges, such as processing and visually inspecting terabytes of traffic data and massive datasets from the Overture Maps Foundation for points of interest.

These tasks, which previously required extensive time and resources, became streamlined and efficient with Fused. Its ability to process geospatial datasets via https-accessible Python functions made the development process seamless, enabling us to focus on creating an intuitive, high-performance platform. With Fused, we could deliver real-time insights and user-friendly visualizations at a scale that was unimaginable only a few years ago.

## Empowering Professionals with Unprecedented Insights

BlackPrint's platform is transforming real estate decision-making by delivering unprecedented insights to professionals. For example, a retailer in Mexico City utilized our tools to identify a high-traffic site with optimal customer reach, saving weeks of manual analysis and significantly improving accuracy.

Our platform's effectiveness is measured through key metrics like time saved, improved precision, and enhanced ROI for our clients. Users have reported up to a 30% increase in efficiency when planning site expansions or evaluating investments.

Looking ahead, BlackPrint's vision extends beyond Mexico, aiming to revolutionize acquisition intelligence across Latin America. With partners like Fused, we continue to innovate, making geospatial analytics scalable, intuitive, and accessible to professionals at every level.

## Conclusion

BlackPrint Technologies is on a mission to redefine real estate intelligence by delivering actionable insights that drive smarter, faster decisions. This journey has been significantly accelerated thanks to Fused, whose serverless data delivery empowered us to process and serve complex geospatial data at an unprecedented scale. Together, we are shaping a future where real estate professionals can access intuitive, data-driven tools that simplify their workflows and enhance their outcomes. Join us in transforming the industry—visit blackprint.ai to see how we are revolutionizing real estate decision-making.

================================================================================

## Hot-spot analysis for invasive species using Overture Maps
Path: blog/2025-01-21-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2025-01-21-elizabeth

**TL;DR Elizabeth Rosenbloom creates hotspot maps to identify key areas where Arundo donax is likely to spread, streamlining analysis to improve invasive species mitigation.**

In 2020 while working in Silicon Valley for the county of Santa Clara Valley, I became obsessed with improving monitoring and prevention efforts surrounding Arundo donax. The search and mitigation process this invasive plant species, Arundo donax, was a Sisophisian struggle that had been subject to the same procedures year in and year out, with no progress on beating the spread. To improve the efficacy and efficiency in battling against this notorious weed, I decided to build a tool that would identify the key areas for mitigation - both for the frequency of propagation (occurrence) and for spread potential.

In this blog post I show how I used Fused to create a map of key hotspots where Arundo donax is likely to spread based on built-environment factors derived from Overture Maps data.

To follow along, you check out the UDF associated with the blog post:
- Invasive_Species_Hotspot UDF

## Introduction

In 2020, the problem with managing Arundo was that many agency employees considered this to be a hopeless pursuit, given the exorbitant amount of time cleaning, layering, and calculating the weighted analysis would take. Despite our vast ArcGIS library of tools, engineers, hydrologists, and GIS managers all warned me that I was going to drive myself crazy trying to get the enterprise software to successfully run my analysis. My only regret in building the tool back then was that I didn't have a tool like Fused to expedite the data pulling, processing, and calculating - as it would have saved me from the very lunacy I was warned about.

The obsession with the grass species, Arundo donax, began with an insight to the positive feedback loops created by increased flooding and the spread of invasive species due to climate change. Arundo donax is one of the most invasive plant species worldwide. In addition to destroying biodiversity and disrupting habitats for native species, this large grass also contributes to significant flooding patterns. As weather events become more severe and biodiversity declines, these changes create compounded consequences in our changing climate.

## Challenges with Hot-spot Analysis

Hot-spot analysis tools using weighted sums can significantly increase accuracy in targeting key areas for prevention. To build a hot-spot analysis for Arundo donax, the following variables need to be scored according to their degree of influence: distance to nutrient loading sources (such as a golf course), distance to a riparian buffer (creek or river), distance to a water-flow disruptor (such as a bridge), and size of the stream.

The key pain points of running a weighted sum on traditional GIS software include:
- Slow calculations: "State-of-the-art" software like ArcGIS can take several hours to calculate weighted sums. Furthermore, running weighted sums on large datasets/geographic areas can be nearly impossible given exhaustive RAM and GPU demands, so analyses over 1000 sq miles often require a user to split analyses into different geographic regions.
- Program crashes: beyond the significant wait time required for typical weighted sum calculations, users of prominent GIS software often experience output delays as a result of runtime errors and other issues spurring a program crash.
- Data transformations, cleaning, and standardization: most users of traditional GIS software will find they need to start from scratch when compiling data for a hot-spot analysis or weighted sum. Sometimes, standard base layers like slope and aspect will be searchable on the local software basis. Still, often, lengthy transformations are required to make the layers compatible with the final overlay calculation.

## How Fused Changed My Workflow

Using cloud-based systems like Fused can significantly increase calculation speeds, program resilience, and access to public Cloud Native datasets.

After encountering the Fused and learning about how I could improve the speed and geographic spread of site suitability analyses, I wanted to put it to the test by expanding on a previous analysis I did in 2021 using ArcGIS. The original 2021 analysis took several months of data collection, interviews with other local agencies, and extensive data cleaning, standardization, and transformations. I experienced all the aforementioned pain points of hot-spot analysis/weighted sum calculations and more.

Flash forward to today, where I am compiling global data sets, layering them, and deriving statistical insights within 1% of the time that it took me using ArcGIS. Running buffer analyses, weighting variables, and procuring data has taken a fraction of the time for GLOBAL data - and if you remember from before, the previous analysis from 2021 took months for a county-wide calculation and final product.

The most satisfying aspect of my new hot-spot analysis application wasn't just the expansive end product; the process was more seamless and engaging than I had imagined.

## Workflow Design

I created a UDF with a simple model to identify hotspots susceptible to arundo. The model uses a weighted sum of several base Overture data classes:

- Golf Courses
- Bridges
- Water bodies (rivers, streams, etc.)

The UDF performs the following steps:
1. Create GeoDataFrames from the Overture maps dataset using get_overture
2. Generate an H3 score based on buffers around each feature
3. Aggregate the H3 scores to create a weighted sum

## Key Takeaways
Given the complexity of procuring, layering, and interweaving data along with significant wait times and resource consumption, many governmental agencies, non-profits, and even private corporations struggle to run spatial analyses such as site-suitability and hot-spot tools. Insights and tools can be created by improving the speed and efficacy of operations such as weighted sums and fuzzy analysis across spatial layers with UDFs.

Site-suitability and hot-spot analysis go beyond species detection. By simplifying the approach to these types of tools, we can more quickly and accurately detect climate-vulnerable zones, prioritize habitat restoration, and create models to build resilient communities. Industries such as real estate development, retail, and logistics can more quickly understand the variables that affect their businesses by using cloud-based systems like Fused, which can easily manage large datasets.

================================================================================

## Calculating Fire Ratings with Overture Buildings and Places
Path: blog/2025-01-20-amico/index.mdx
URL: https://docs.fused.io/blog/2025-01-20-amico

**TL;DR Chris Amico shows how to combine Overture Maps data with fire perimeters to analyze wildfire impact on buildings and businesses.**

As communities continue to rebuild and recover from the devastation caused by natural disasters such as wildfires, the question remains: How can we quantify what was lost, especially in the built environment? With the ability to analyze detailed building footprints and overlay fire boundaries, we can begin to answer this by providing a rough estimate of the damage and identifying which structures were impacted by the flames.

In this blog post, Chris Amico shows how by leveraging data such as Overture Building footprints and fire progression maps, we can gain insight into the extent of fire risk. This enables news agencies to derive insights such as count of shops or homes exposed or even assess the capacity of highway routes for residents to evacuate before a prospect fire reaches them.

To follow along, you check out the UDFs and app associated with the blog post:

- Fire Proximity GERS_Lookup UDF
- Fire Proximity Building_Score UDF
- Fire Proximity Buffer UDF
- Google Sheet that enriches the "Fire Risk" column for any given Building GERS
- App: H3 rollups within water buffer

### Introduction

Fused simplifies the process of spatially joining datasets with Overture Maps data, such as integrating with fire-related datasets.

For this example, we use the Inter Agency Fire Perimeter Historical dataset (published by the National Interagency Fire Center (NIFC)) which includes historical fire perimeters up to 2024. Joining fire perimeters with Overture Buildings and Places data enables us to highlight service gaps or identify regions that may require immediate response.

This demo will first select buildings within a buffer zone to determine which fire perimeter they fall within. Then, it will perform an H3 stats roll-up of business categories from Overture Places, counting the number and types of businesses that fall within each distance range. This will involve rolling up Overture Places business categories into H3 hexagons.

## The Workflow

These UDFs return Overture Buildings and Places within a buffer distance from a fire. They offers a simple way to determine the scope of possible fire damage and quickly assess the number of businesses, homes, and other significant structures within the affected area. By adjusting the buffer or selecting fire extent based on dates, users can fine-tune their analysis to gain deeper insights into how far the fire's reach extends and what establishments were most at risk.

### b. Fire Proximity Building Score

Next, we load the Overture Buildings dataset and spatially join it with the fire buffer zones. This workflow categorizes buildings based on their proximity to the fire, helping us assess which structures are most at risk.

1. Load the NIFC fire perimeter data
2. Create buffer zones around the fire perimeters
3. Load Overture Buildings
4. Spatially join buildings within the buffer zones to categorize them by proximity to the fire

### c. Overture Places Rollup by H3

Finally, we perform a spatial aggregation by calculating the H3 index for the centroids of Overture Places within the fire buffer. This allows us to roll up business categories into H3 hexagons, enabling a holistic overview of business distribution in relation to the fire perimeter.

1. Load the NIFC fire perimeter data
2. Load Overture Places
3. Determine the H3 for the centroid of each building
4. Normalize the 'categories' column into individual columns
5. Roll-up categories by H3, create categories primary set

## Conclusion and Next Steps

This kind of analysis helps understand not only the immediate impact but also in planning for future fire preparedness and recovery efforts. In this post, we saw how Overture and Fire-extent data can help us estimate the extent of the damage, from individual buildings to entire neighborhoods.

Organizations looking to integrate these types of perspectives into their workflows could create apps or API services that deliver derivative products, such as GERS lookups to categorize "fire risk" based on buffer proximity.

They could also use Fused HTTPS endpoints from the UDF to return a CSV with the GERS and "Fire Risk" score for buildings within a defined bounding box, as specified by a query parameter. Additionally, they could also use the HTTPS endpoints to automatically enrich the "Fire Risk" column of an arbitrary dataset for any given Building GERS. Users could apply it to any row of their table, with the functionality powered by a "GERS lookup" endpoint using a GERS ID query parameter.

================================================================================

## Characterize cities with embeddings of Overture Place categories
Path: blog/2025-01-16-maribel/index.mdx
URL: https://docs.fused.io/blog/2025-01-16-maribel

**TL;DR Maribel Hernandez shows how to create clusters of business categories using Overture Places data.**

Maribel Hernandez is a computer scientist and researcher at CINVESTAV, a multidisciplinary academic institution in Mexico. She specializes in graph theory in the field of computational genomics and complex networks. In this blog post, Maribel shows how she characterizes cities based on the distribution of businesses by rolling-up business categories by H3.

As someone who works with urban networks, Maribel's focus is on exploring how cities function and how their design impacts inclusivity. The core question driving this analysis is: Does the city have an even distribution of business services? Or are there shortages such as food deserts or unequal access to health facilities?

To follow along, you can clone and run the associated:
- Colab Notebook
- Overture Places Embedding Clusters UDF

## Introduction

Consider these scenarios:
- How connected are areas dominated by upper-class establishments to the broader city fabric?
- Are health services distributed with equal access from low-income neighborhoods?
- Do certain metropolis have better accessibility and service availability compared to others?

Take, for example, a comparison between 3 key cities in Mexico: Mexico City, León, and Mérida. Are neighborhoods in these cities equally served by essential services such as healthcare, food, and transportation? Could some areas be considered food deserts, while others enjoy easy access to all services?

_Preview of UDF on Workbench._

## Descriptive analysis

When analyzing the urban fabric of cities like Mexico City, Leon, and Merida, we can obsere distinct patterns of service distribution that manifest as rings around the city center.

_Mexico City._

### a. The heart of the city
Especially in Leon and Merida, the central area tends to form a cohesive, dense cluster of services. The core zone houses a variety of businesses including health services, retail, and food outlets, which are generally well-connected and easily accessible. Intuitively, the central cluster can be thought of as the "heart" of the city, serving as the primary hub for commerce, social interaction, and access to essential services.

_Leon._

### b. Islands of Services
Beyond the center, we notice smaller clusters of services scattered across the city, often in the form of "islands." These islands represent pockets of neighborhoods that, while not part of the dense city center, offer an array of unique services. These can serve to highlight the phenomenon of emergent neighborhoods within a greater whole.

_Merida._

### c. Peripheral ring
The periphery of these cities forms another distinct pattern. These outer regions also tend to cluster together in similar service categories, forming a ring that surrounds the central core. It's possible services in these peripheral areas are often more limited in scope and may reflect a focus on residential and less commercial needs, or reflect lower-income neighborhoods with scarcer access to key services.

## Conclusion
This ring-like pattern of service distribution suggests a common trend where the core of the city is highly accessible, while the periphery often lacks the same diversity and depth of business services. The islands of services in between can be seen as attempts to bridge this gap, but they are not always as effective in meeting the needs of the population on the periphery.

These spatial patterns offer valuable insights into the inclusivity of urban networks, highlighting areas that may need attention to ensure strategic business placement and guarantee equitable access to essential services.

## Future work
- Create Network: Create a bipartite network between place categories and H3 indices, using weights as counts for each category.
- Tie in Demographic Data: Integrate INEGI sociodemographic data at the census block group level to understand how services align with population needs.
- Access by Neighborhood: Use origin-destination (OD) movement networks to evaluate service accessibility by neighborhood of residence.
- Segregation Analysis: Identify zones with low visit rates or difficult accessibility. These zones may represent areas of segregation or neglect in the urban network.

================================================================================

## Streamlining the design of parcel delivery routes with H3
Path: blog/2025-01-09-antonius/index.mdx
URL: https://docs.fused.io/blog/2025-01-09-antonius

**TL;DR GLS uses Fused to create internal tooling to optimize routing for parcel delivery operations.**

In the parcel delivery business, geospatial analyses are crucial to answer questions about daily operations. Do delivery drivers visit the same regions each day, letting them know their areas intimately? Or is there a high volatility of the regions? And of course, how do we optimize the routes of multiple drivers servicing the same region?

Those are the questions Antonius is working on at GLS Studio, an innovation lab by GLS (General Logistics Systems) which is an international parcel delivery service provider.

In this blog post, Antonius highlights how he uses Fused to create stable delivery areas for single-day and multi-day aggregates.

## The Challenge Designing Delivery Areas

While the planned areas of driver tours to delivery packages can be rather well-defined, an evaluation of the areas actually served by drivers is equally important and might not be as easy. It can be used for guiding delivery drivers to become more efficient while also enabling managers to identify planned areas that are suboptimal due to built environment features like rivers or big highways.

Creating delivery areas out of single geospatial data points is challenging. A naïve approach would be to use convex polygons, but this causes multiple issues:
- A single data point can have a high influence on the shape and area of the polygon
- A tour consisting of multiple not connected sub-areas is hard to detect and correctly display
- This limits the area to what is covered by historic data which leaves a gap in new target regions
- Calculations using polygons are computationally expensive, hindering ad-hoc changes to the selected time span or the calculation parameters

Building new and sometimes experimental features means that the database is often not optimized for the use case. Data needs to be joined between multiple tables and even between multiple data sources. Therefore, fetching all data can be slow, highly limiting the usefulness of having an experimental feature to play around with.

## Solving the Challenge with H3 and Fused UDFs

### Dynamic Delivery Areas with H3

The solution to the first problem came by using H3 cells instead of polygons. By assigning cells to the drivers based on their historic deliveries to the cell, driver areas result automatically. Using H3 cells across different resolutions also allows us to represent the differences between urban and rural areas which see different parcel volumes. While there exists one "base resolution" to ensure non-overlapping and complete areas, the logical hierarchy among H3 cells can be used to calculate on lower resolutions for rural areas that see fewer deliveries, speeding up the computation and ensuring a broader coverage of those areas beyond the historical data points.

On the other hand, disputed H3 cells can be broken down to a higher resolution and assigned to different drivers or, when the "base resolution" has been reached, assigned to the driver delivering most parcels to the cell. As H3 cells have clearly defined neighborhoods, areas can easily be extended beyond their historical limits when desired, covering the empty space around to include a new parcel that falls outside of historically served area boundaries.

_Fused app to show dynamic delivery areas at different H3 resolutions._

### Streamlining workflows with Fused UDFs and Caching

Fused UDFs helped us solve problems around the latency of querying and calculations. When a user looks at an area for a day, they are probably interested in the same area on some of the previous and following days as well, right? So why not pre-calculate that already?

Using Fused, it is simply a matter of fire-and-forget to trigger the UDF with parameters for some previous and following days which are then already running to cache. So when the user views an adjacent day, the data will already be there. And more broadly, when it is possible to limit the number of parameter combinations in an experimental feature to a manageable amount, this fire-and-have-it-cached approach is not limited to caching data from previous and following days, but can also be used for a range of other cases.

_Sample workflow with Fused UDFs._

## Conclusion

When developing new features that are not yet supported by the current data infrastructure, Fused UDFs enabled us to easily test things without having to change the underlying infrastructure in advance. The UDFs are easily shareable and adjustable, allowing testing by multiple people without having to run code locally while automatically making sure everyone is using the same code that is hosted in the UDF. And because we can easily call UDFs with HTTPS endpoints, when we have verified the feasibility of a feature, it's easy to integrate into our product.

================================================================================

## The Strength in Weak Data Part 3: Prepping the Model Dataset
Path: blog/2024-12-12-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-12-12-kristin

**TL;DR Kristin shares a UDF to create training data for a corn yield prediction model using Zonal Statistics.**

Now, suppose we want to do this where corn is grown in the midwest US. Here is what the states that grew corn in 2018:

Within these states, we have **1,333 counties**. Assuming each is similar in size to my home county of Lyon County, we can calculate:

1,333 counties × 20,000 data points = **26 million data points**

That's **20,000 times** the statistical power! 🎉

Let me say that louder for the people in the back: **26,000,000 vs. 1,333 data points**

And that's just for one time period. If we run a model on 2–4 time periods, we're looking at nearly **100 million data points**. Now, building a model on 100 million data points isn't trivial, but at Fused, this process becomes a breeze.

## Building the Training Dataset

I'm aiming to predict corn yield based on my SIF readings from early May, late May, early June, and late June. So, we need to build out a table with this structure:

| County  | Year | Yield (bushels per acre) | Area of Corn (acres) | Area of County (acres) | SIF Value-201605a (early May) | SIF Value-201605a (late May) |
| --- | --- | --- | --- | --- | --- | --- |
| 17015 | 2016 | 205 | 124,145 | 3,032,0383 | .15 | .65 |
|  |  |  |  |  |  |  |

To quickly validate this table against a map, I'll build out my workflow in Fused using Python and query the table with SQL. In the past, working with these two languages would have required complex tooling, storing data in a warehouse, and roughly **five hours** to run. With Fused, I can simply reference the a GeoDataFrame object and query in SQL with DuckDB all within the same UDF—taking just **five seconds**!

Here's what it looks like:

## Splitting the Data

But we're not stopping there! To ensure our model is both robust and unbiased, we need to carefully split our data. Enter Walk-Forward Cross-Validation — a game-changer for time series data. Think of it like planting seeds each season and harvesting them before the next planting. By always training on past data and testing on future data, we respect the natural flow of time. This method is perfect for corn yields because, just like how last year's harvest influences this year's, our model benefits from understanding those temporal dependencies. Plus, it prevents any sneaky data leakage, ensuring our predictions are based solely on what's known up to that point.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/krv3.mp4" width="100%" />

## Conclusion and Next Steps

Keep going or end here?

By progressively expanding our training set, each fold builds on the last, capturing more nuanced patterns and trends. To bring this to life, I'm leveraging TimeSeriesSplit from **sklearn**, seamlessly integrating it into our workflow. This tool simplifies the process, allowing us to focus on what truly matters—unlocking the full potential of our 100 million data points for actionable predictions.

And there you have it! We've taken weak, infrequent data and transformed it into a powerhouse dataset ready to drive smarter decisions in the $21 billion corn commodities market. Stay tuned for the next part of our journey, where we'll dive into building and fine-tuning our predictive models. Until then, keep cultivating those insights! 🌽

================================================================================

## Streamlining Infrastructure Risk Analysis with Fused
Path: blog/2024-12-11-jacob/index.mdx
URL: https://docs.fused.io/blog/2024-12-11-jacob

**TL;DR Jacob at VIDA uses Fused to streamline processing and rendering of CMIP6 climate risk models, improving data sharing and sanity checks.**

================================================================================

## Map Overture Buildings and Foursquare Places with Leafmap
Path: blog/2024-12-10-qiusheng/index.mdx
URL: https://docs.fused.io/blog/2024-12-10-qiusheng

**TL;DR Dr. Qiusheng walks through how you can call Fused UDFs to load data into leafmap maps using Jupyter Notebooks.**

- Google Colab Notebook Walkthrough
- Leafmap Docs

## Calling Fused UDFs to load data

You first use leafmap to create a bounding box over an area of interest (AOI) `user_aoi` and create a GeoDataFrame `gdf_aoi` with it. Then, you can run the Overture Maps UDF, passing the AOI as a parameter to define the area to fetch data for.

================================================================================

## From query to map: Exploring GeoParquet Overture Maps with Ibis, DuckDB, and Fused
Path: blog/2024-12-06-naty/index.mdx
URL: https://docs.fused.io/blog/2024-12-06-naty

**TL;DR Naty shares a UDF to use Ibis with DuckDB's spatial extension to query and explore Overture Maps data.**

Naty is a Senior Software Engineer and a contributor to Ibis, the portable Python dataframe library. One of her main contributions was enabling the DuckDB spatial extension for Ibis in 2023.

In this blog post, she shows us how to leverage the spatial extension in DuckDB with Ibis to query Overture data. Ibis works by compiling Python expressions into SQL, you write Python dataframe-like code, and Ibis takes care of the SQL. Thanks to Ibis integration with Pandas and GeoPandas, you only need to do `to_pandas()` to get your expression as a GeoDataFrame.

}
  title="Overture H3 Skyline"
/> */}

We first establish a connection to a DuckDB database, in this particular case we have an in-memory connection. Then, we do `read_parquet` and we receive a table expression. Since our result, `t`, is a table expression, we can now run queries against the file using Ibis expressions. In this example, to start, we filter by some infrastructure subtypes (pedestrian, and water), select only a few columns, and limit our "search" to a bounding box `bbox`. Notice that this `bbox` is the Fused bounding box, not the overture maps one.

We then rename the column `class` to avoid conflicts with the deferred operator, and finally filter the expression by specific infrastructure classes like toilets, ATMs, drinking water, and other useful information. Up to this point, we only have a table expression, Ibis has a deferred execution model. It builds up expressions based on what you ask it to do and then executes those expressions on request.

To show an example of an aggregate, we executed and printed the `value_counts()` as a Pandas DataFrame. Ibis can execute the table against the DuckDB backend, and return it as a Pandas DataFrame or a GeoPandas GeoDataFrame (if `geometry` column is present), by only doing `to_pandas()`.

### Conclusion

The synergy between Ibis, DuckDB, and Fused has redefined the ease of querying and visualizing geospatial data. These frameworks provide an intuitive and powerful toolkit, enabling users to express geospatial queries, perform efficient transformations, and access high-performance analytics with minimal setup.

By leveraging this stack, interacting with vast geospatial datasets like Overture Maps becomes straightforward, efficient, and accessible.

### Resources

If you want to learn more about Ibis geospatial capabilities, check some of the geospatial blog posts here.

You might also find these resources useful as you dive into Ibis, DuckDB, and Overture:

- Overture Maps Data Repo
- Ibis Docs
- DuckDB spatial extension
- DuckDB spatial functions docs
- Ibis Zulip Chat

================================================================================

## Creating an app to model road mobility networks in Lima, Peru
Path: blog/2024-12-05-claudio/index.mdx
URL: https://docs.fused.io/blog/2024-12-05-claudio

**TL;DR Claudio used Fused to create an app to model road mobility networks in Lima, Peru, using GeoPandas, and OSMnx.**

On December 2023, I visited the Institute for Metropolitan Planning (IMP) in Lima. The director had invited me to share some of my geospatial analysis projects from my master's studies and explore potential collaborations. Around that time, Lima's mayor had announced a bold infrastructure initiative: building 60 flyover bridges to ease traffic congestion in one of the most gridlocked cities in Latin America.

1. Extracting the Road Network: Using OSMnx, I extracted road networks within a defined Area of Interest (AOI).
2. Enriching Data: Each road segment was assigned speed and travel time values.
3. Defining Population Data: A 1km² grid with population density and zoning data was loaded into a GeoPandas GeoDataFrame.
4. Setting Simulation Parameters:
    - Population size: Derived from density data.
    - Trips per person: Assumed at 2 trips/day (commute to and from work).
    - Origins and Destinations: Residential zones were assigned as homes and commercial zones as workplaces.
    - Trip Schedules: Normal probability distributions were used for departure (6-8 AM) and return times (5-7 PM).

With these parameters, the simulation sampled "home" and "work" nodes, calculated start times, and determined the shortest paths between origins and destinations. Async UDF calls made the process parallelized and efficient. The final output was a GeoDataFrame with:

- Start Time (Unix timestamp)
- Trip Type ("home" or "work")
- Path (list of coordinates)
- Timestamps (for each coordinate)

## Future Plans

This project is far from over. Here are the features I aim to add to make it a valuable tool for urban planners, especially in resource-constrained settings like Lima:

1. Larger AOI Support: Handle bigger datasets and simulate more trips.
2. Multimodal Routing: Incorporate walking, biking, driving, and public transit options, akin to OSRM profiles.
3. Custom Infrastructure: Allow users to model new transit infrastructure within the OSM road network.
4. Mobility Metrics: Provide detailed metrics (e.g., travel times, congestion levels) for each simulation.

With these enhancements, this tool could empower city stakeholders to make data-driven decisions on critical urban interventions—whether it's building flyovers or optimizing public transit routes. The ultimate goal? Improving mobility for over 11 million residents in Lima and beyond.

You can try out the public UDF here
}>
<Iframe
  id="claudio"
  code=
  height="600px"
  useResizer=
/>
</div> */}

================================================================================

## Beyond RGB: Interactive Exploration of NEON's Hyperspectral Data
Path: blog/2024-12-03-guillermo/index.mdx
URL: https://docs.fused.io/blog/2024-12-03-guillermo

**TL;DR Guillermo used Fused to build an interactive tool for exploring NEON hyperspectral data, making large-scale geospatial analysis more accessible and actionable for researchers.**

As a research specialist focused on remote sensing applications in semi-arid rangelands, I'm constantly seeking tools that can enhance our ability to process and analyze large-scale geospatial data. The excitement of discovering new platforms that streamline complex workflows never gets old, especially when dealing with the massive datasets typical in remote sensing research.

My journey with Fused began unexpectedly through the "Minds Behind Maps" podcast, where host Maxime Lenormand interviewed Sina Kashuk, Co-Founder and CEO of Fused (see episode). The conversation sparked my curiosity, leading me to explore Fused's community examples and documentation. After signing up for Fused, I knew exactly how I wanted to test it: an interactive tool for exploring NEON's Airborne Observation Platform (AOP) data.

Find Guillermo's UDF code and App associated with the blog post here:
- NEON Hyperspectral GEE UDF
- NEON Dataset Explorer App

## Making Hyperspectral Data Accessible

For those unfamiliar with NEON AOP, it's an NSF-funded initiative revolutionizing ecological observation. Using advanced imaging spectrometers, NEON collects hyperspectral data across 426 distinct wavelength bands at 1-meter resolution. Imagine having 426 different perspectives of the same landscape, each revealing unique insights about vegetation, soil composition, and ecosystem health.

_Source: NEON Imaging Spectrometer._

The real challenge, however, isn't collecting this rich data - it's making it accessible and actionable for researchers. This is where Fused enters the picture. Diving into their documentation and gallery of click-and-run examples, I found myself inspired by the platform's potential. By combining elements from various examples, I began building my first User Defined Function (UDF), eventually discovering the App Builder - a feature that would prove crucial in creating an interactive interface for hyperspectral data exploration. Having worked extensively with Google Earth Engine (GEE) and NEON data, the recent announcement of NEON AOP's availability through GEE presented the perfect opportunity to test Fused's capabilities. My goal was simple yet powerful: create a user-friendly application that could tap into this wealth of hyperspectral data and make it instantly accessible to researchers.

## Looking Ahead

Looking ahead, I'm already planning the next phase of this app. I just want to add functionality that tracks sampling locations and allows users to The experience of building this demo app has reinforced my belief in the importance of platforms like Fused in the geospatial community. They serve as crucial bridges between massive datasets and practical applications, eliminating infrastructure headaches and letting researchers focus on what matters most - the science.

For those interested in exploring NEON's hyperspectral data or learning more about this application, feel free to connect with me or try the app for yourself here.

The future of remote sensing analysis lies in making powerful data more accessible, and I'm excited to be working in this field.

================================================================================

## How DigitalTwinSim Models Wireless Networks with DuckDB, Ibis, and Fused
Path: blog/2024-11-26-sameer/index.mdx
URL: https://docs.fused.io/blog/2024-11-26-sameer

**TL;DR DigitalTwinSim uses Fused with Ibis and DuckDB to model high-resolution wireless networks.**

Sameer, co-founder of DigitalTwinSim, leads the development of advanced geospatial analysis tools to support the telecom industry in strategic network planning. DigitalTwinSim specializes in using high-resolution data to optimize the placement of network towers ensuring reliable, high-speed connectivity.

In this blog post, Sameer shares how he leverages Ibis with a DuckDB backend, and Fused to model wireless networks at high resolution. This approach enables him to quickly generate network coverage models for his clients. He explains and shares a Fused UDF that processes data in an H3 grid to evaluate optimal locations for network towers.

# Fused for Interactive Processing With Instant Visualization

Here, tools like Fused have become essential. Fused allows us to filter and visualize raw output data in a more interactive way, which we can also share with clients to illustrate network design and coverage areas.

To set up the UDF in Fused, we uploaded our data as a Hive-partitioned Parquet folder and created a UDF in Ibis to generate visualizations on demand based on zoom level and area of interest. At higher zoom levels, we compute the parent H3 index and aggregate data to show broader coverage areas; at lower zoom levels, we display individual H3 indices. The H3 polygons are generated and colored dynamically based on the data in the Parquet folder, allowing us to interactively filter data and share visualizations with clients.

Click here to launch the UDF in Fused Workbench.

# Conclusion

As network demands grow and requirements for high-speed internet access become more stringent, accurate, high-resolution modeling is essential for effective planning and deployment.

DigitalTwinSim's integration of tools like DuckDB and Fused, alongside Ibis and H3 grids, enables us to tackle the challenges of processing, analyzing, and visualizing massive datasets. By leveraging DuckDB's powerful data aggregation capabilities, we can manage and analyze high-resolution data efficiently, irrespective of memory constraints. Meanwhile, Fused empowers us to deliver interactive, client-ready visualizations, allowing stakeholders to better understand network coverage and performance.

================================================================================

## The Fastest Way to Download Foursquare's new POI Dataset
Path: blog/2024-11-21-foursquare-poi/index.mdx
URL: https://docs.fused.io/blog/2024-11-21-foursquare-poi

**TL;DR The Fused Team made Foursquare's open dataset of 100M global places accessible via GeoParquet files which you can access via a UDF.**

Foursquare just released an open dataset of over 100M global places of interest.

We at Fused have partitioned these points into easily accessible GeoParquet files, and hosted them on Source Cooperative

On top of that, we've build a publicly available User Defined Function (UDF) that anyone can use to easily look at & download to GeoJSON, all from the browser

**Try it out for yourself!**

You don't need to login or create an account to easily access the Foursquare POI points

- Try out Fused for yourself for free!
- Get familiar with Fused by checkout out our Quickstart docs
- Follow us on LinkedIn to keep up with updates
- Read this more in-depth look at the whole dataset from our community member Mark Litwintschik

================================================================================

## How I Got Started Making Maps with Python and SQL
Path: blog/2024-11-04-kent/index.mdx
URL: https://docs.fused.io/blog/2024-11-04-kent

**TL;DR Stephen Kent shares his journey making maps with Fused using Python and SQL.**

I am a self taught developer and data enthusiast. I first came across the spatial data community when I saw a Matt Forrest video on LinkedIn where he demonstrated how to visualize buildings from the Vida Combined Building Footprints dataset with DuckDB. Immediately I thought, what if you could see all the buildings in a country, say, Egypt? I set out to do just that and made this map with DuckDB and Datashader.

_Buildings in Egypt._

Find Stephen's UDF code here:
- Five Minutes Away in Bushwick Brooklyn UDF

## Starting with Fused

The day after I posted that image on LinkedIn, in April, 2024, I had a call with Plinio Guzman of Fused. I told him what I had been up to, and he was enthusiastic and confident that Fused would fit my needs. One key feature he mentioned was the live development. While I was developing that Egypt map, I had to start the ETL to the final product over and over until I got it looking the way I wanted.

So I got started right away. I found Fused User Defined Functions (UDFs) like Overture Maps and the S2 Explorer and traveled all over the world looking for stunning images. It was thrilling to fly from New York to Tokyo and see the results render instantly.

_Exploring the world with Sentinel 2._

I then began to change the components of these UDFs to see different Overture types, but at this point I was hesitant to build my own UDF from scratch.

## Instant UDFs

That was until Fused launched its File Explorer. With one click, it was now possible for me to create a UDF from providers like Source Cooperative and visualize with numerous presets like DuckDB or GeoPandas. With this new feature, I recreated my Egypt map with the same Vida dataset, this time using DuckDB with the H3 extension. It was liberating, I came to realize the components were simpler than I thought.

## Local Tests

I used DuckDB with the H3 extension without Fused to query Overture Maps for countries and continents all locally in a Jupyter notebook. The benefit with the H3 extension is that if you set up the query right you can aggregate larger than in memory datasets at ease from your notebook.

_Road Density in Africa._

And made this Egypt building map with H3, how does it compare with the Datashader version up top?

_Egypt Building Density with H3._

## Fused and Overture Maps

In August, Fused announced a tighter partnership with Overture Maps Foundation and that came with even more Overture features. Like with Source Cooperative I could now instantly generate UDF of buildings, places, land use, or roads, etc by joining parquet files (and more). I proceeded to use the framework of that UDF to join all kinds of data.

_Proximity analysis between Road Networks and Hospitals in Paris._

## Joining H3 with GeoJSON

One day I was looking at the DuckDB_H3_Example, and I was struck — what if I joined those cells with Overture Buildings? I learned how to use the DuckDB H3 extension from all of the example UDFs on Fused. So I called that UDF in an Overture UDF and used GeoPandas to join the two. The result is the map below. The color of the buildings comes from the count of corresponding Yellow Cab pickups. There are millions of points in this TLC parquet file, and H3 helped me to aggregate to thousands for an easier spatial join.

_Overture Buildings joined with H3 Yellow Cab pickups in New York City._

I made this particular map with Kepler.gl, with two clicks from the Workbench. I could have also exported the data to tools like Felt and Mapbox. You can find the code I used to recreate this map here.

## App Builder

I just started working on the Fused App Builder, and made a dashboard to view and interact with NYC’s 311 call data as a 3D H3 heatmap. Anyone using it can set the date range and resolution to change the display. Very fast and easy to use.

## Community

There's so much exciting data science happening on Fused. Check out Kevin Lacaille's post on ML-less global vegetation segmentation at scale. And Christopher Kyed's Analyzing traffic speeds from 100 billion drive records, that is the kind of project I would love to work on.

I continuously find inspiration as I browse community UDFs. Here's a join of H3 heatmaps with Overture types. This is a heatmap of connectors (intersections) joined with segments (roads) in London. The darker colors have more intersections. I am looking to incorporate traffic counts.

_Road density in London._

## Conclusion

I have been using Fused for several months but it feels like I am just getting started. It seems like the only real limit here is what I can dream up.

_This is a cross-post of Stephen Kent's Medium Article published October 14, 2024._

================================================================================

## Discovering NYC Chronotypes with Fused
Path: blog/2024-10-30-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2024-10-30-elizabeth

**TL;DR Elizabeth Cultrone analyzed NYC Taxi pickup data to identify neighborhood boundaries based on activity patterns. She created a UDF to implement H3 binning and similarity metrics.**

Neighborhoods within a city have consistent characteristics but often have ill-defined boundaries. Some neighborhoods are more similar than others even though they're not nearby. Understanding these local boundaries and the demographics, dynamics and behaviors of different areas affects a wide range of business applications, including advertising, site selection, business analytics, and many more.

_Highlighting natural catchment area boundaries around Koreatown._

## Statistical Analysis

In the App Builder, we created graphs to summarize the similarity values shown in the map. Histograms of the pickups across the most and least similar hexes to each location confirm that the distributions are different for each. We can also explore the cumulative count of hexes to help determine an optimal threshold for similarity values, depending on the application.

_Comparison between most and least similar hexes of two AOIs._

## Conclusion

The Fused UDF Builder makes developing and iterating on these analyses swift and convenient, with no need to jump between different environments for developing vs viewing the results. And although the taxi dataset is small, the Fused Tiling functionality offer the possibility of developing similar analyses with larger datasets. With more data and richer features this proof-of-concept could be expanded to discover more robust, fully-defined neighborhoods, allowing us to develop data-driven approaches to local geography.

================================================================================

## Earth-scale AI pipelines for Earth Observation (Part 1: Data Curation)
Path: blog/2024-10-29-durkin/index.mdx
URL: https://docs.fused.io/blog/2024-10-29-durkin

**TL;DR Fused simplifies how Earth Observation data is processed to curate training data for AI models. Gabriel Durkin shows a Streamlit app he created to train and run land use and crop detection models.**

The rate of prototyping the Fused App Builder unlocks is unrivaled. Recently I used it to create a sophisticated prototype app to accelerate my ML workflow. You can follow along by using the "Cube Factory" app I built.

## Introduction

The field of AI stands to revolutionize Earth Observation (EO), unlocking unprecedented insights from satellite imagery. This article shows how you can leverage multiple layered Fused components:

- Serverless compute
- Highly customizable Fused UDFs
- The Fused App builder

We will harness these resources to curate training datasets that can streamline deep learning workflows. As a working example, we will demonstrate land use inference and crop detection with multispectral satellite imagery.

EO data offer valuable insights for predicting the impact of severe weather events, quantify environmental changes, and tracking natural disasters. Various sectors stand to benefit, from urban planners to finance gurus in commodities and futures trading. AI techniques combined with geospatial data offer insights into short and long term dynamics on Earth, such as deforestation, wildfire management, and agricultural land use.

However, one of the biggest bottlenecks of processing EO data is the time spent on data engineering, holding back researchers from harnessing the full potential of their data.

In this article, we'll demonstrate the Fused platform in effortlessly curating training data for machine learning model development. The derived model can differentiate between developed areas, forests, open lands, vineyards, and fields of corn and soybean.

Here is a screenshot of the simple Python app built to make the data curation and visualization process relatively effortless for the geospatial data scientist. Every control and widget setting in this front end you see here modifies how the data is transformed in the back end by Fused UDFs, one for each data asset available, and produces the map visualizations you see in the viewport, with spot analytics in the sidebar.

Later, as part of the workflow, I will demonstrate spatial querying that will allow us to choose custom regions on which to train a model - simply by outlining polygons on a map.

## Article structure
This is the first of a 3 part series that will show how Fused can be used in querying, processing/ETL, visualization, and inference tasks across stages of the model development lifecycle:
1. **Data curation:** Prepare and generate training images and masks.
2. **Model training:** Feed curated data into a state-of-the-art multiclass segmentation model.
3. **Model Inference:** Establish generalization limits by evaluating model performance on user-selected holdout regions from different locations and timeframes.

## The role of Fused

Fused is a platform that simplifies the engineering challenges involved in building data workflows for Earth Observation (EO) analysis. Its key features for curating training data include:
- **Python User Defined Functions (UDFs):** UDFs define transformations on data and can easily be called with parameters and across different areas of interest.
- **Remote Accessibility:** UDFs can be called from anywhere via HTTPS requests, delivering data exactly where it's needed without the need to store or transfer large datasets.
- **Parallel Execution:** UDFs can run in parallel, processing thousands of data chips per minute for efficient scaling.
- **UDF Workbench:** The UDF Builder provides instant feedback during algorithm development, allowing users to visually inspect the resulting data chips on a dynamic map. Any changes are immediately deployed upon saving.

The synergy of these Fused components enables researchers to dedicate more time to experimentation and analysis rather than data engineering.

> "Ultimately we want Data Scientists to be able to deliver autonomously — without operational reliance on a dedicated engineering team, especially given the unwieldy scale and volume of earth observation data."

## Problem overview

An example workflow addresses the challenge of developing a model to categorize and quantify agricultural land use across the continental U.S. with multispectral satellite imagery. This task involves complex multi-class segmentation with several key challenges:

- **Image variability:** Satellite images can vary in resolution, quality, brightness, and cloud cover. Crop reflectivity fluctuates across regions and growing seasons, impacting data selection and algorithm design.
- **Engineering limitations:** Traditional approaches restrict the number of iterations researchers can perform to design and tune a model.
- **Increasing data complexity:** The growing number of spectral bands and satellite sources available requires a systematic approach to selecting an index combination.

By automating engineering processes such as image chipping and source harmonization (time, space, and projection) to prepare training data, researchers can have more iteration cycles as they define spectral indices and band combinations that generalize images.

## Input datasets

Our example model will use 3 bands of Sentinel 2 satellite imagery as input to our ML model to predict a CDL crop mask value - the target layer. We will leverage a Fused "Cube Factory" app we built to generate the chips or "datacubes" that contains both inputs and target. Thinking bigger, there are multiple fused assets which can take the role of input or target. A strength of the fused Cube Factory app is the implicit matching of data in both spatial and temporal dimensions. A further goal of this blog is to demonstrate how easily custom apps like this one can be created and shared on the Fused App Builder.

- Sentinel 2 offers 13-bands of 10 meter resolution imagery with broad temporal and spatial coverage. We chose the Glacier index as the Pseudo Red Channel and Channels 8 (NIR) and 11 as Green and Blue, respectively (image above).
- Land Use Land Cover (LULC) dataset provides a global land cover classification
- Digital Elevation Model (DEM) dataset provides terrain elevation
- Sentinel 1 offers HV and VV polarization bands of 10 meter resolution - active imaging data derived from SAR with broad temporal and spatial coverage and a 6 day cadence.
- The USDA Cropland Data Layer (CDL) dataset maps individual pixels of 30m resolution to hundreds of crops and land types like 'soybean' and 'corn', or even 'pumpkins' and 'cherries'. Below you can see a hyperspectral image of central california, and the dominant crop and land classes in the associated CDL image below.

An additional flexibility offered by the app is the variety of Fused assets that can be called as either inputs or targets. For the viewport visualization we set the left side of the split map as the input layers and the right side as the targets.

Returning to our chosen target, the CDL dataset, it contains up to 256 named crop classes (see: CDL_names_codes_colors), we grouped these together into 10 superclasses to simplify our task.

1. Background/Missing BG
2. Wheat WH
3. Corn CO
4. Soybeans SB
5. Grass/shrub/open lands OL
6. Forest and wetlands FW
7. Open water OW
8. Developed DV
9. Other agricultural (catch-all) OA
10. Other/barren OT

```json

```

## Data curation workflows

The Mask2Former model we will employ for this exercise is optimized for 3 channel RGB images, so we'll use Fused UDFs to create a 4 layer datacube, three inputs from Sentinel 2 and one CDL layer as target. Then, we'll call them across a set of tiles to create the training dataset.

For this example, I selected 3 bands to predict with: glacier index (red-green normalized difference index) is on top, and bands 8 and 11 from Sentinel palette in the middle. The CDL mask at the base of the datacube has pixel values that correspond to major crop classes.

## Create datacube

The first 3 channels of the datacube encode Sentinel 2 bands as RGB false color channels, and the fourth channel encodes the CDL mask are associated with the following 3 UDFs:

1. Choose Right and Left map layers from the Fused asset collection.

2. Choose a Survey Period and location using the left side pop-out menu.

3. If using Sentinel 2 - select 3 of the available 13 bands or one of 6 normalized difference indices.

4. If using digital elevation model as input or target (left or right map side), choose whether terrain, or gradient modes, choose scale parameters, and choose a colormap for visualization.
5. Draw polygons on the map.

## Model training

To train a model on our datacube we downloaded the Mask2Former model from Hugging Face and fine-tuned it on the 10,000 datacubes of 240x240 pixels (resulted in ~1GB of training data). This can be done locally on a GPU machine, or in the cloud on colab or paperspace gradient (now digital ocean). In our next post we will demonstrate how to train ML models natively on the Fused platform.

### Model Hosting

The final model is available on a dedicated Inference endpoint hosted by Hugging Face, and accessed via a API call through a custom Fused UDF. It is available in the Cube Factory app for inspection as the Mask2Former ML layer. This allows a qualitative side by side comparison with the CDL layer. The model was trained on July imaged crops in the midwest among other areas, here is how it performs vs ground truth near Jacksonville Illinois. The left side shows the target layer and the right side shows the model prediction.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gabe_slider_HD.mp4" width="100%" />

## Conclusion

This post showed how Fused makes it easy to generate training datasets. It gave a practical example of how with Fused UDFs the data loading, data preprocessing, and data loading happen behind an HTTPS endpoint, enabling easy data retrieval from anywhere. There's no need to pre-compute, store preprocessed data, or manually generate tiles, as Fused dynamically generates data for the specified tiles using simple HTTPS calls.

This approach offers several advantages to build customized training datasets. Since data generation is on demand and gets cached, a data scientist can quickly iterate, adjusting spatial or temporal parameters without worrying about managing storage or running jobs to generate datasets. The flexibility to load data that is needed, when it's needed, accelerates experimentation and refinement of models.

To replicate this work for your own AOIs, you can try out the Cube Factory app yourself or run the underlying UDFs on your own Fused Workbench environment. Please feel free to contact me on LinkedIn.

## References
- [1] L. L. Zhang, S. Dhaka, et al., Building a Crop Segmentation Machine Learning Model with Planet Data and Amazon SageMaker Geospatial Capabilities (2023)
- [2] S. Hamdani, Supervised Wheat Classification Using PyTorch's TorchGeo — Combining Satellite Imagery and Python (2023)
- [3] Sentinel-2 Mission Overview (2023)
- [4] Digital Elevation Model (DEM) Data (2023)
- [5] ESA WorldCover 10m 2020 V100 (2023)

## Additional Resources
- USDA Cropland Data Layer Methodology
- Sentinel 2 Spectral Band Indices
- Fused Documentation

================================================================================

## DuckDB, Fused, and your data warehouse
Path: blog/2024-10-24-stefano/index.mdx
URL: https://docs.fused.io/blog/2024-10-24-stefano

**TL;DR GLS Studio uses Fused to optimize Snowflake queries. This enables route planning in their ParcelPlanner app with H3-partitioned geospatial data served to a Honeycomb Maps frontend.**

We then created two key UDFs within Fused. These work in tandem to handle authentication, caching, and efficient data retrieval for our DuckDB-powered map:

- **The "Public" UDF (Hammer):** This UDF isn't cached and serves as the entry point. It handles authentication and collects the full date range requested by the customer.
- **The "Private" UDF (Nail):** This cached UDF takes a single date and returns the necessary data for that specific day.

The "Hammer" UDF spawns up to 1,000 asynchronous Fused workers, each running an instance of the "Nail" UDF to fetch data for an individual date, as specified with a parameter. Once the data is retrieved, it is stitched together into a single GeoPandas DataFrame, ready for use.

With this approach, historical data only needs to be read once from Snowflake. For the present date, which is subject to updates, we handle caching differently and apply a one-hour cache to optimise performance.

## Conclusion

In the end, Fused allowed us to integrate our Honeycomb maps directly with Snowflake, handling caching and security concerns. This approach saved us significant backend development and data engineering work—all with just a few dozen lines of Python.

================================================================================

## The Strength in Weak Data Part 2: Zonal Statistics
Path: blog/2024-10-22-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-10-22-kristin

**TL;DR Kristin created a UDF to mask cropland areas using USDA data and run a Zonal Statistics workflow for corn yield predictions.**

Farming isn't static—corn fields rotate with soybeans or cover crops yearly, adding noise to our data. Here's a 25 km² area:

This block includes not only farmland but also trees, towns, and water bodies. Our challenge is to isolate the specific areas where corn is grown to enhance the precision of our analysis. Enter Fused, which has a Public CDLs UDF that reads the USDA Cropland Data Layer, letting me specify the year and crop type to pinpoint corn accurately.

## Masking crop areas with a UDF

To tackle this, I created a Fused UDF that loads the USDA Cropland Data Layer for a specified year and crop type to identify corn-growing regions. I then used corn-growing regions to mask a Solar Induced Fluorescence raster. Finally, I calculate its mean values for each county.

Now for the fun part:

1. **SIF Data:** Display SIF for a specific month from a NetCDF file.
2. **Corn Areas:** Map corn cultivation that year from a GeoTIFF file of the Cropland Data Layer (CDL) data product.
3. **Precision Clipping:** Clip layers to show SIF values only where corn grows.
4. **Zonal Statistics:** Aggregate the SIF that incides on corn crops for each county.

You can see the UDF code here and even clone it to your Fused workspace.

**Voila!** From one county's weak data to creating summary statistics for the county. This provides the ingredients to boost the prediction strength and reduce noise in the prediction model I want to build.

## Scaling Up

Applying this to **400 Midwest counties** transforms our dataset from 400 points to **60 million**. The results?

- **Enhanced statistical power:** More data = stronger, more reliable predictions.
- **Improved accuracy:** Predictions are more closely aligned with actual outcomes.

Here is how the data compares on a map.

## Why It's simple with Fused

With Fused, working with rasters and vectors is straightforward. This blog post showed how I'm turning weak, unreliable data into a powerhouse of insights effortlessly.

## Ready to transform?

Curious to see the magic? Interact with the UDF in the Fused UDF Builder and elevate your data from weak to strong. Harness your data's full potential and make impactful decisions!

Feel free to reach out if you have any questions at `info@fused.io`.

================================================================================

## Blazing Fast Geospatial SQL in DuckDB
Path: blog/2024-10-17-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-10-17-isaac

In this video from the FOSS4G 2024 conference, Isaac Brodsky, CTO and co-founder of Fused, shows the power of combining H3 with DuckDB to enhance geospatial data analysis.

As an example, Isaac shows a Fused User Defined Function (UDF) that joins the Overture Places dataset and the Natura 2000 biodiversity areas dataset, achieving significant reductions in file size and query execution times. He showcases how the integration allows for efficient data exploration, filtering, and real-time queries, emphasizing the power of DuckDB's H3 extension and spatial extension.

Isaac explains how H3 simplifies geospatial analytics by offering common spatial index to join datasets and enables efficient storage & processing by converting spatial features into 64-bit integers. Additionally, DuckDB enables developers to conveniently transition between Python and SQL. He also highlights how DuckDB can simplify data processing architectures by querying data in real-time from object storage systems such as S3.

================================================================================

## Analyzing traffic speeds from 100 billion drive records
Path: blog/2024-09-25-pacific/index.mdx
URL: https://docs.fused.io/blog/2024-09-25-pacific

**TL;DR Pacific Spatial Solutions uses Fused to streamline data workflows and feature engineering to predict national traffic risk in Japan.**

Over the last few decades, it has become increasingly evident that passenger vehicles are by far the most dangerous way to travel. As a result, it has become more and more important to find an efficient and effective method to predict traffic risk. However, predicting traffic accidents and where they are likely to occur is a very complex problem, with large amounts of data being needed for most meaningful predictions.

At Pacific Spatial Solutions, we are currently trying to tackle this problem by training a machine learning model to predict road and intersection risk in Japan nationwide. As we are trying to predict traffic risk on a national level it is only natural that the data we use cover the same area.

_Drive recorder data points from a single day in a specific part of downtown Tokyo._

## Moving to Fused
Specifically, what we are trying to achieve is to map the speed values from the drive recorder data points to their nearest road. Using a traditional "nearest neighbor" approach would not be feasible, as we would need to measure the distance between billions of points and thousands of roads.
With our current cloud service provider we therefore had to rely on "clustering", so that data points that are close location wise would be close in memory too. This definitely increases performance, but adds some randomness to the processing time and cost because depending on where the area of interest lies in memory, you might have to search through all of your data to find it. As a result, to keep cost and processing time reasonable, we had to limit the nearest neighbor search area using a very small buffer. This was the only way to make our analysis with a dataset of this magnitude feasible.

_Nationwide drive recorder data points and their Fused spatial partitions._

### UDF Design
1. Use `bbox` to load GPS points and roads in the viewport.
2. Structure `DataFrame`s with the GPS points, road krings and road geometry.
3. For each point identify the road with the closest kring cell within a certain k distance, and map the speed to it.
4. Aggregate all of the speed values.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF=None,
    base_path: str = '...'
):
    from utils import df_to_gdf, list_s3, run_pool, get_GPS_road_data

    # Load ingested GPS and road data
    L = list_s3(f'/GPS_hex/')
    df_GPS, df_road_hex, df_road_geom = get_GPS_road_data(bbox, L)

    # Nearest neighbor calculation
    df_final = df_GPS.merge(df_road_hex, on='hexk')
    df_final['distance'] = (df_final['k']+0.5)*k
    df_final['cnt'] = 1
    dfg = df_final.groupby('segment_id')[['cnt', 'speed', 'distance']].sum().reset_index()
    dfg['speed'] = dfg['speed']/dfg['cnt']
    dfg['distance'] = dfg['distance']/dfg['cnt']

    # Introduce geometry to roads
    df = df_road_geom.merge(dfg)
    df['width_metric'] = df['cnt']**0.5/5
    return df.sjoin(bbox[['geometry']])
```

We now have our result which is a DataFrame representing the road network within our bbox. All the roads have their respective aggregated speed, distance and metric values as well as how many points were used for the aggregation. This result can easily be enriched by bringing in more columns from the base data such as the timestamp. This would make it possible to create hourly speed pattern analysis or maybe even a time series visualization.

For demonstration purposes, the video above shows this UDF running on a fraction of the ingested dataset.

_UDF result of Osaka Japan. Line width shows point density. Brighter yellow colors indicate high speed and darker purple colors low speed._

## Conclusion

By leveraging the spatial partitioning that Fused does during ingestion and the flexibility of the h3 library, we have created a method to reliably map our drive recorder points to their nearest segment.

The natural next step will be to scale our analysis using multiple machines and run on all of our data. To achieve this, we would iterate over each of the chunks that fused produced when ingesting our road data, instead of the bbox. This will ensure that our calculations are only run once for each of our roads. The modification can be achieved fairly easily in Fused and we are very excited to see how well Fused will be able to perform in this case.

================================================================================

## Creating cloud-free composite HLS imagery with Fused
Path: blog/2024-09-24-marie/index.mdx
URL: https://docs.fused.io/blog/2024-09-24-marie

**TL;DR Pachama partnered with Fused to generate cloud-free HLS image composites, improving tropical forest monitoring and canopy height mapping for carbon conservation projects.**

_Example composites highlight how the HLS-L30 product alone can have gaps when attempting to make a seasonal composite, as fewer cloud-free observations._

This blog post explores how Pachama's engineering team partnered with Fused to generate cloud-free seasonal composites using Harmonized Landsat Sentinel-2 (HLS) data, enabling higher quality optical imagery and better canopy height map creating ML model performance.

## Obstacles to create a cloud-free HLS image composite

The HLS dataset is an exciting development put forward by NASA's Satellite Needs Working Group. It provides consistent surface reflectance data with global observations every 2-3 days at a 30-meter resolution. The dataset harmonizes data from Landsats 8 & 9 with the European Space Agency's Sentinel-2A & 2B satellites such that the results are high quality, standardized, and able to be combined [2].

The HLS dataset consists of scene-level harmonized data, and does not create any cloud-free composite images by default. A significant amount of compute power is needed to process and combine this data, which contains multiple petabytes of data. Iteration on the compositing algorithm is also essential to quickly experiment and refine the process.

_Example of HLS image for a region in Brazil with clouds._

One common solution to this problem is to use Google Earth Engine (GEE). However, only the Landsat portion of this dataset (HLS-L30) is available on GEE. Without the Sentinel-2 portion of this dataset (HLS-S30), we do not get a 2-3 day temporal resolution that is required for cloud-free imagery in frequently cloudy areas.

## With Fused

Pachama turned to Fused to create scalable workflows for quickly iterating on a compositing algorithm. Fused's UDF model allowed Pachama to design algorithms that parallelize image processing, generate cloud-free composites, and run these workflows at scale.

### Pachama's UDF workflow

Here's the workflow we created with a Fused User Defined Function (UDF) to generate cloud-free composite HLS imagery.

### 1. Write a UDF to load imagery

This sample UDF loads data for the Landsat and Sentinel2 data products. It queries for a specific date range and does a first pass at filtering out images with too many clouds.

```python showLineNumber
# To Get your username and password, Please visit https://urs.earthdata.nasa.gov
@fused.udf
def udf(
    bbox: fused.types.TileGDF,
    mask_url: str,
    band_url: str,
    username="<INSERT USERNAME>",
    password="<INSERT PASSWORD>",
    env="earthdata",
):

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils
    # Authenticate
    aws_session = utils.earth_session(cred=)
    cred = 
    overview_level = max(0, 12 - bbox.z[0])

    # Read band data
    band_arr = utils.read_tiff(
        bbox,
        band_url,
        overview_level=overview_level,
        cred=cred,
    )

    # Read and apply cloud mask
    mask_arr = utils.read_tiff(
        bbox,
        mask_url,
        overview_level=overview_level,
        cred=cred,
    )
    cloud_mask = (mask_arr & 0b00000010) >> 1
    band_arr = np.where(cloud_mask == 1, np.nan, band_arr)

    # Filter nan's and convert to RGB values
    band_arr = np.where(band_arr == -9999, np.nan, band_arr)
    band_arr = band_arr / 10
    band_arr += 1 # workaround for uint8 and nan values
    band_arr = band_arr.astype("uint8")

    return np.array(band_arr)
```

### 2. Call the UDF asynchronously

This UDF queries the LP DAAC STAC catalog for data that matches the time and location of interest. This UDF then calls the previous one in parallel asynchronously to fetch each cloud-free image in parallel. It then combines the outputs, taking the median of each band to create a cloud-free composite.

```python showLineNumber

@fused.udf
async def udf(
    bbox: fused.types.TileGDF,
    date_range="2023-05/2023-06"
):

    from collections import defaultdict

    RGB_BANDS = ["B04", "B03", "B02"]
    F_MASK_BAND = "Fmask"

    # Query STAC catalog
    band_urls = get_band_urls(bbox, date_range)

    # Call the image loading/masking UDF in parallel
    tasks = defaultdict(list)
    for band in RGB_BANDS:
        for mask_url, band_url in zip(band_urls[F_MASK_BAND], band_urls[band]):
            arr_task = fused.run(
                "<INSERT UDF TOKEN>",
                bbox=bbox,
                sync=False,
                parameters=)
            tasks[band].append(arr_task)

    # Combine each band
    rgb = []
    for band in RGB_BANDS:
        task_results = await asyncio.gather(*tasks[band])
        composite_values = []

        # Convert back to format with nan's
        for arr in task_results:
            arr = arr.image.values.astype("uint8")
            arr = np.where(arr == 0, np.nan, arr)
            arr += 1
            composite_values.append(arr)

        # Take median of the composite values
        band_composite = np.nanmedian(composite_values, axis=0)
        band_composite = band_composite.astype("uint8")
        rgb.append(band_composite)

    return np.array(rgb)
```

The UDF above generates a cloud-free composite image and gives Pachama control and transparency over the image inputs.

_Example of cloud-free HLS image composite for the same region in Brazil._

## Benefits of using Fused

The best part is that Pachama's Data Science team can design UDF while looking at a specific area, and to run it for a different region by simply changing the input bounding box (bbox). This flexibility allows Pachama to create individual image tiles for any location worldwide. They can easily experiment and generate composites for different date ranges by adjusting the input parameters.

- Easy parallelization with simple Python function calls, no need to manage clusters
- Iterate on both UDFs in the same code editor with the UDF Builder
- Instant feedback during algorithm development, no need to wait for pipelines to run
- Invoke UDF and load its data into a Jupyter Notebook with `fused.run` for downstream analysis

## Conclusion

Thanks to Fused, Pachama's scientists and engineers can quickly iterate and experiment with different algorithms to optimize their image composites. Scaling the algorithm to apply to a larger area also becomes trivial by using Fused. Pachama can more efficiently improve transparency into forest carbon projects through better data and better insights, faster.

## References

- [1] On the Advantages of Using Harmonized Landsat Sentinel-2 Data for Monitoring Environmental Change
- [2] An Update on NASA's Harmonized Landsat and Sentinel-2 Project
- [3] An initial evaluation of carbon proxies for dynamic reforestation baselines

================================================================================

## The Strength in Weak Data Part 1: Navigating the NetCDF
Path: blog/2024-09-23-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-09-23-kristin

**TL;DR Fused streamlined Kristin's workflow to integrate CSV and NetCDF data directly from S3.**

Ever tried to make sense of the myriad file types in spatial data science and felt like you've wandered into a linguistic labyrinth? Trust me, you're not alone. As a data scientist who's spent more time wrangling datasets than I care to admit, I thought I'd take a casual stroll down memory lane with an old high school friend: regression models. Just a simple plot of actual vs. predicted, right? But when spatial data's involved, you can't just sit back and relax—you've got to keep one eye on the geometries.

I'm currently working on an agricultural project, and growing up on a farm gives me a personal stake in this. This blog illustrates my solution to the geometry debacle. I'll first take you to the area where I grew up: Lyon County.

The resolution differences are huge—going from 30 square meters up to 5 billion! Traditional tools would have you pulling your hair out, but Fused lets you turn this "weak" data into something powerful.

## Actual Variable: Handling the Data Mismatch

When dealing with data that doesn't quite match up—like trying to combine different resolutions—you need to align everything to the coarsest resolution. In this case, that's the county level.

Here's how I tackled it: I grabbed a CSV file of county ANSI codes along with my actual variable data. Using Fused's Fused's File Explorer, I plotted the data easily. Just a quick visit to the File Explorer S3 bucket, a double-click on the file, and the entire map rendered instantly.

Remember the days of wrestling with shapefile resolutions? No more. I edited the UDF to pull my actual data CSV straight from my S3 bucket in under 30 seconds. Boom.

## Predictor Variable: Navigating the NetCDF

Now, let's get into the predictor variable—a NetCDF file from 5 degrees off the equator, covering around 25 square kilometers. NetCDF files can be a bit tricky to work with due to their complex formats, but Fused's utility modules make it easier. I imported some key functions directly into my UDF to clip the array, convert it into an image, and add a colormap.

```python showLineNumber
@fused.udf
def udf(bbox: fused.types.TileGDF=None, path: str='s3://fused-asset/misc/kristin/sif_ann_201508b.nc'):
    xy_cols=['lon','lat']
    utils = fused.load("https://github.com/fusedio/udfs/tree/057a273/public/common/").utils
    # Get the data array using the constructed path
    da = utils.get_da(path, coarsen_factor=3, variable_index=0, xy_cols=xy_cols)
    # Clip the array based on the bounding box
    arr_aoi = utils.clip_arr(da.values,
                       bounds_aoi=bbox.total_bounds,
                       bounds_total=utils.get_da_bounds(da, xy_cols=xy_cols))
    # Convert the array to an image with the specified colormap
    img = (arr_aoi*255).astype('uint8')
    return utils.arr_to_plasma(arr_aoi, min_max=(0, 1), colormap="rainbow", include_opacity=False, reverse=True)
```
Once I saved the UDF and created an HTTPS endpoint, I visualized the data interactively in the App Builder.

## The Variable That is Going to Make this Weak Data Strong

Okay, I have prepped my actual and predictor variables. Now, I will focus on how to fuse the geometries together using the variable that is going to make this Weak Data Strong (30 square meters). For that, stay tuned for Part 2, where I'll dive into the techniques for aligning and merging these spatial layers into a cohesive analysis. See you in the next installment!

================================================================================

## Enrich your dataset with GERS and create a Tile server
Path: blog/2024-09-19-overture/index.mdx
URL: https://docs.fused.io/blog/2024-09-19-overture

**TL;DR Fused enables on-the-fly enrichment of Overture datasets using simple spatial joins.**

Overture is an open data project that publishes interoperable map datasets. It aims to foster an ecosystem of developers creating downstream map services around its data products. Fused emerged as a solution to enrich Overture datasets on the fly and serve them with XYZ Tile endpoints.

### Why this matters

Datasets often lack the structure required to support the creation of reliable map services and products. A dataset may reference real-world entities using its own conventions, making it challenging to integrate with other datasets. Overture promotes interoperability with GERS (Global Entity Reference System) IDs, which link dataset entities to the same real-world features. This simplifies the process of conflating multiple datasets, ensuring consistency.

The Overture Buildings theme provides a comprehensive collection of global building footprints, but different use cases might call for additional building information. This could include attributes related to risk assessment, census information, zoning regulations, property management details, and more. By enriching datasets with GERS IDs, users can easily link external data sources to power analytical applications.

However, enriching a dataset can be computationally and memory-intensive due to the size of the data. Fused addresses this by enabling live queries and rendering data directly from the source GeoParquet files, eliminating the need to create new tiles each time the underlying data is updated.

## Step-by-step guide

In this post, we show how to enrich your dataset by matching it with the Overture Buildings theme using GERS IDs. This makes your dataset easier to visually inspect and more accessible for others to integrate with their data.

For this example, we created a Fused User Defined Function (UDF) to load the NSI Structures dataset and overlay it on the Overture Buildings dataset. This will let us visualize buildings based on attributes from NSI such as structure value or height. We'll then turn the UDF into an XYZ Tile server to create an app.

================================================================================

## The App That Finds Your City's Rainfall Twin Globally
Path: blog/2024-09-17-milindsoni/index.mdx
URL: https://docs.fused.io/blog/2024-09-17-milindsoni

**TL;DR Milind analyzes global precipitation patterns using H3 indexing, cosine similarity, and Earth Engine data to create an interactive rainfall comparison app.**

## How It Works

Our UDF utilizes the following key components:

1. Earth Engine API: To fetch global precipitation data
2. H3 Index: For efficient spatial indexing
3. DuckDB: For fast query execution on geospatial data
4. Cosine Similarity: To compare rainfall vectors

## The Workflow

1. **Data Aggregation with DuckDB**: The data retrieval process is streamlined using Fused and Xarray:
   - **Fused and Earth Engine**: Fused simplifies access to Google Earth Engine's vast catalog. It provides a more intuitive and faster interface with a much better file manager for working with spatial data compared to the Earth Engine platform itself.

   - **Xarray Integration**: We use Xarray to work with our multi-dimensional rainfall data. It allows for easy handling of labeled arrays and datasets, particularly useful for time-series climate data.

2. **Data Aggregation with DuckDB**: After retrieving the raw data, we use DuckDB to efficiently aggregate it. This involves:
   - Grouping the data by H3 hexagon and month
   - Calculating the average monthly rainfall for each hexagon
   - Creating 12-element vectors representing annual rainfall patterns for each location

3. **Cosine Similarity Calculation**: Finally, we use cosine similarity to compare these rainfall vectors. This allows us to quantify how similar the rainfall pattern of one location is to another, or a reference pattern.

4. **Converting UDF to an app with Fused App Builder**: To make the rainfall similarity comparison UDF accessible and interactive, I used the Fused App Builder to help quickly build an app from the UDF that I just created. Every data scientists favourite prototyping tool is Streamlit which helps to build frontends in Python quickly and that's what the app builder brings to you! Convenience of Streamlit with the Power of Fused.

## The App Builder

If you are familiar with Streamlit, it is super convenient to build UI from just Python code. Folium maps helped me build interactive maps where I can draw areas to compare with and I could also write a custom HTML-based iframe to integrate Mapbox GL within the app itself, the snippets of which again are available in the Fused documentation.

1. **Interactive Folium Map**

I implemented a Streamlit Folium based map that allows users to select a location of interest.

2. **Plotly Charts**

A bar chart displays monthly rainfall data for the selected location in the folium map after querying the UDF and passing the GeoJSON shape as a parameter in the UDF,

3. **Iframe Integration**

- The hex-similarity map shows global rainfall pattern similarities.

### Calling the UDF within the App

Just one line of code to call my UDFs within the app to

- Fetch the historical rainfall data from Google Earth Engine for the marked area.
- Aggregate rainfall vectors
- Calculate the similarities of the location with the vectors in the bounding box in the iframe

It was as easy as `fused_app.run("fsh_****")`

### Performance and Optimization

Fused and Streamlit already have excellent caching mechanisms which helped me cache large amounts of data and information prior to the usage so that the next time the app loads, the computations are much faster! I can compare the rainfall patterns of any two locations on the Earth in seconds with a few lines of code. How cool is that!

> Building scalable Geospatial Applications have never been so quick and easy!

================================================================================

## Six ways to use Fused
Path: blog/2024-09-12-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2024-09-12-danieljahn

**TL;DR: Fused is a versatile platform that serves as a code catalog, a parallel data processing engine, an app creation tool, a serverless HTTPS endpoint generator, and an IDE.**

*Example from How Pachama creates maps on-the-fly with Fused*

## 5. Geospatial Streamlit

Streamlit is a Python library that helps you create and
deploy web apps
with a few lines of code.

Streamlit is also the best first-time-user experience I've had with a library.
Without prior experience, I could immediately go from a Python script straight to an interactive web app.

With Fused's App Builder, any UDF can be turned into an interactive Streamlit app.
Fused also automatically serves the app for you.
While the app itself runs in the browser using Pyodide, it can call any Fused UDF, processing the data using the Fused engine.

  height="800px"
  useResizer=
  requirements=
/> */}

## 6. Geospatial-first IDE

Of the six, this is the most aspirational use case.
It's also potentially the most impactful.

Fused provides the Workbench, a great web-based IDE.
Working with it started changing how I think of developing geospatial applications.

[Image: ]

Today, there are two worlds.

- On one side, the software engineer uses test-driven-development to develop well-designed code in quick iterations.

- On the other side, the data scientist develops code directly against real data using notebooks and visualizations.

Fused can bring these worlds together. Simply annotating your function as `@fused.udf` gives you the ability to immediately visualize the results with real data, over any geographic region.
Fused Workbench does this, but you could equally develop in VSCode and switch to QGIS to immediately inspect the results.

By developing your code as a web of stateless UDFs and utilizing `@fused.cache`, you gain the ability to develop automatically cached pipelines whose results can be inspected in tools like Felt or served with an HTTPS endpoint without any added work.

Often the greatest cost of data pipelines is developer time.
Fused has the potential to tighten the development feedback loop and catch errors early, reducing the time needed to develop robust data pipelines.

## Conclusion

This article gave six concrete examples of how you can use Fused today.

However, the possibilities of Fused are not limited to these examples. With its powerful execution engine, visual IDE, growing host of integrations, and just-copy-the-link app deployment, Fused is generic enough to enable use cases not even the team behind it has thought of.

I'm excited about the future of Fused. I wouldn't be surprised to see it become a ubiquitous tool in the geospatial world.

================================================================================

## AI for object detection on 50cm imagery
Path: blog/2024-09-05-dl4eo/index.mdx
URL: https://docs.fused.io/blog/2024-09-05-dl4eo

**TL;DR Jeff Faudi used Fused for real-time object detection on 50cm satellite imagery, displaying results as an interactive web map.**

In this article I show how to create an object detection layer on 50cm imagery in realtime. It explains how to create a Fused User Defined Function (UDF) to load satellite image tiles to call an inference model, then publish it as an interactive map app.

To display this image on the web, you typically need to project it in Web Mercator projection with gdal and cut it into 256x256 pixels tiles that will be displayed nicely by web-mapping applications such as GoogleMaps, OpenLayers, Mapbox, MapLibre, Leaftlet or Deck.gl.

Until recently, I would have done this physically and generated thousands of tiles. Now, we will do this almost magically with Fused.

## Creating a UDF

Basically, I just have to write the piece of code that generate the content of a tile and Fused takes care of running the code and providing the urls to share the layer in any application. The Python function that I have to write is called a UDF and it has at least one parameter which contains the bounding box (bounds) on which I need to generate the tile.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF = None,
    chip_len: int = 256):

    from utils import read_geotiff_rgb_3857

    geotiff_file = 's3://fused-users/dl4eo/my_image.tif'
    return read_geotiff_rgb_3857(bbox, geotiff_file, output_shape=(chip_len, chip_len))
```

First, it is worth noting that we extract all content from a GeoTIFF image (ideally a COG i.e. Cloud Optimized GeoTIFF) which contains the bands and geometric information about the satellite image. This GeoTIFF is stored anywhere on the cloud. Here, it is stored in the AWS S3 bucket provided by Fused. Also, note that the function returns an array for raster tiles but could return a GeoJSON for vector tiles.

We use the bounding box of the tile provided as a parameter, convert it from lat/long to Web Mercator (EPSG:3857), get the corresponding bounding box in the original image, and project it in Web Mercator projection in the destination array with the correct desired tile size (typically 256x256 pixels).

The Fused UDF Builder enables one to view the result and logs while coding.

## Implementing aircraft detection

Now, if we want to display a real-time aircraft detection layer, we could replicate the previous step: send the resulting image extract to the API and display a vector layer. However, we must avoid applying deep learning algorithms to images that might have been zoomed. These algorithms are typically trained at a specific resolution, and the Web Mercator projection does not preserve size.

_https://en.wikipedia.org/wiki/Mercator_projection_

We read the content of the Pleiades image in its original projection (either the raw geometry or a transverse mercator projection in which the central meridian would pass through the center of the image). In this case, the resolution is guaranteed to be the correct native resolution of the image.

The UDF gets the Pleiades image in the correct projection, then calls the prediction API, and finally returns the predictions in a GeoDataFrame which will be dynamically rendered on the map. For performance, we have added the @fused.cache decorators which make the function automatically cache results for identical parameters. The predictions are returned in pixels in the source image and then converted into lat/long so they render on a map. Then, when we look at the result in the workbench, we get some issues at the border of the tiles.

The reason is that if an aircraft is on the tile border, it will be detected partially on the lower tile and potentially on the upper tile. The two bounding boxes might not align perfectly so we cannot merge them. The solution here is to extract a image larger than the tile: if the center of the predicted box is inside the tile we keep it, if it is outside we discard it. We usually use a margin that is the upper size of the objects we are trying to detect i.e. 100 meters for aircrafts. After these little improvements, the result is much nicer

## Building a web app

Now that everything is running fine in the workbench, it is time to extract the layers and include them in a webpage. Fused provides an easy way to integrate layers in external applications via HTTPS requests. You just need to go to Settings, click Share and copy the provided URL.

Then, you can integrate this URL as the tile source in any mapping application. I am not diving into that here, but you can read how to do this in the DeckGL Fused docs. You can check the code source of the demonstration below. Here is the extract of the JavaScript Deck.gl code where the URL is integrated.

And here it is: the final working demonstration!

## Conclusion

Huge thanks to the amazing team at Fused for their incredible support, and to my former colleagues at Airbus for providing the stunning Pleiades image. I think that this application turned out to be very sleek and powerful. If the underlying satellite image changes, the AI layer gets automatically recomputed on the fly.

I'd love to hear your thoughts!

_This article was originally published in LinkedIn on June 20th 2024._

================================================================================

## Summarizing building energy ratings
Path: blog/2024-09-03-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-09-03-isaac

In this video tutorial, I show a complete data app workflow in Fused. Starting with exploring the data in Fused, the tutorial walks through developing a UDF to serve the data, and then a Fused App to share results.

With Fused, this whole workflow takes just minutes from beginning to end. Fused helps me visualize the data at every step, iterate on my analytical logic, and finally publish a dashboard.

================================================================================

## ML-less global vegetation segmentation at scale
Path: blog/2024-08-29-kevin/index.mdx
URL: https://docs.fused.io/blog/2024-08-29-kevin

**TL;DR Kevin used Fused to create a global vegetation segmentation layer without machine learning, displaying results as an interactive web map.**

================================================================================

## How Pachama creates maps on-the-fly with Fused
Path: blog/2024-08-27-pachama/index.mdx
URL: https://docs.fused.io/blog/2024-08-27-pachama

**TL;DR Pachama uses Fused to create maps on-the-fly for their sustainability platform.**

Pachama is a technology company that harnesses satellite data and AI to empower companies to confidently invest in nature. The engineering team at Pachama created a Land Suitability Tool to help landowners and project developers qualify parcels of land to implement carbon projects. They turned to Fused to simplify their data workflows.

## The Challenge: Quickly Assess Potential Restoration Projects

Pachama harnesses earth observation data and AI to bring unprecedented insight into how forests sequester carbon, protect wildlife, and benefit local communities. By harnessing the latest advancements in technology, they bring transparency and integrity to forest restoration projects, catalyzing funding in natural climate solutions.

Pachama recently built the Land Suitability Tool within their Reforestation Partner Portal to revolutionize how project developers assess the restoration potential of prospective project sites. In this portal, organizations and landowners looking to start a reforestation project define an Area Of Interest (AOI) by drawing or uploading a polygon, then estimate the land's eligibility based on data layers derived from environment models that take into account country-level data about land cover, vegetation history, and natural risks. For example, a project may look to derive credits from carbon sequestration through native reforestation and ally with local communities that earn an income as stewards of the land.

One of Pachama's challenges was making preprocessed data available for user-defined AOIs that aren't known ahead of time. This would require generating and storing data for entire countries, which is expensive given that a preprocessing step is billed for each square kilometer.

Furthermore, the process involved transferring data between backend and frontend teams, each with different requirements. This resulted in converting datasets between formats, workflows with complex infrastructure, long-running jobs, and slow turn-around times.

## The Solution: Serverless Tile Generation with Fused

To overcome these challenges, Pachama turned to Fused to generate maps on the fly with serverless API endpoints. Fused now provides them an elegant way to write custom workflows to crunch data with Python and serve it behind tile endpoints that natively integrate with map tile layers. This makes it possible to process and visualize any dataset with manageable operation costs.

> **"Fused has been critical in our product lifecycle. The speed at which we were able to iterate based on new requirements is unrivaled."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

The ability to trigger a UDF that generates a vector directly from a Zarr file was a game-changer for Pachama's ability to close the gap between their analytics and their end-users. This innovation has made the team more productive and enabled them to streamline complex tasks that were previously cumbersome and impractical.

> **"Fused takes DevOps out of our hands to focus on our core mission, building technology to restore nature."**
>
> **Marie Hoeger, Staff Software Engineer @ Pachama**

The Land Suitability tool covers the contiguous USA, Brazil, Mexico, Argentina, Guatemala, Panama, Paraguay, Colombian Amazon, and the Peruvian Amazon. Pachama plans to expand to more regions around the world. It processes a variety of datasets including Pachama's proprietary canopy height map. Pachama generates regional maps of average top-of-canopy height using a combination of lidar from GEDI and a suite of satellite observations at varying spatial scales, including optical and radar imagery, topography, and climate data. Fused's on-the-fly tiling simplifies the workflows to generate and load the data into the user-facing app.

By combining analytical and visualization capabilities, Fused enables powerful and productive workflows. Instead of pre-computing tiles for entire datasets, Pachama now generates tiles dynamically only for user-defined AOIs, reducing system complexity and cost.

Here's a minimalist example of how Pachama uses a Fused User Defined Function (UDF) to generate a vector from a raster file in COG format:

```python
@fused.udf
def udf(bbox: fused.types.TileGDF=None):

    from utils import raster_to_vector

    table_path = "s3://pachama-fused-data/dataset.tiff"
    gdf = raster_to_vector(table_path, bbox)
    return gdf
```

This UDF can be called via HTTPS request with the following URL structure:

```
https://www.fused.io/server/v1/realtime-shared/fsh_1gcTv/run/tiles///?dtype_out_vector=mvt
```

## Key Features

The Fused automatically provisions an endpoint for each of Pachama's UDFs. The prospecting application then loads the endpoint into a Mapbox application, which consumes the output in MVT format as defined by the `dtype_out_vector` parameter.

- HTTPS Endpoints work with slippy maps, which is standard across map tiling applications.
- Map clients call the endpoint for each tile in the viewport, passing values for z, x, and y. Fused then runs the UDF, passing a GeoDataframe with the Tile coordinates.
- The UDF code spatially filters the referenced dataset, processes the fraction of data, and returns it to the client app as the response of the HTTPS call in the format specified via a query parameter. This avoids the need to pre-compute data or manage files.

## The Result: Simplify Data Workflows By 50%
Fused and its UDF environment revolutionize how Pachama renders tile-based maps by leveraging analytical tools: cloud-optimized data formats, the flexibility of Python for spatial operations, and the scalability of serverless. Engineers at Pachama used to see a gap between the analytical data formats (e.g. COGs & GeoParquets) and visualization data formats (MVT, PMTiles, XYZ Tiles). Fused closed the gap and let them retire a major piece of the pipeline.

> **"Fused replaced 4 steps of the pipeline with a single Fused UDF."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

## Future innovation for Pachama
Looking ahead, Pachama aims to expand this powerful tool worldwide, catalyzing high-integrity reforestation projects in the regions that need it the most. With Fused's infrastructure underpinning its platform, Pachama can stay focused on making powerful science and analytics accessible to everyone through intuitive visual interfaces.

Read about Pachama's mission and learn how they use technology to evaluate forest carbon projects to assess carbon projects.

================================================================================

## Geospatial workflows of any size
Path: blog/2024-04-22-webinar/index.mdx
URL: https://docs.fused.io/blog/2024-04-22-webinar

Isaac Brodsky, the CTO of Fused, delved into the power of Fused during a LinkedIn live session with Matt Forrest. They discussed the contrast of Python vs. SQL for data analytics, the advantages of serverless geospatial processing, and showcased a live demo of the UDF Builder. During the demo, Isaac created a User Defined Function visualize Overture building footprints that are within a certain proximity of water.

You can re-watch the webinar on LinkedIn, YouTube, or below.

================================================================================

## DuckDB + Fused: Fly beyond the serverless horizon
Path: blog/2024-04-09-duckdb/index.mdx
URL: https://docs.fused.io/blog/2024-04-09-duckdb

**TL;DR Fused extends DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.**

The combination of Fused serverless operations and DuckDB offers blazing fast data processing. Fused embraced Python to create serverless User Defined Functions (UDFs). Now, with the help of DuckDB, Fused enables developers to leverage the ease and familiarity of SQL in these functions  -  without compromising performance and parallelism.

This blog explains how Fused User-Defined Functions (UDFs) can extend DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.

The blog post illustrates three complimentary implementations:
1. Run DuckDB in a Fused UDF
2. Call Fused UDFs from DuckDB
3. Integrate DuckDB in applications using Fused

## The evolution of the data processing landscape
For companies with bottom lines that depend on time to insight, the data landscape is driven by the need to process increasing data volumes and make operations easier to express. This section discusses how Fused and DuckDB can address these needs within the context of the latest wave of the data processing ecosystem.

### Increasing data volumes
When the size of data required for an operation is larger than memory, it becomes a bottleneck. In the early 2010's, the effort to process increasing volumes of data created MapReduce, Hadoop, and Spark to help companies scale out clusters. The complexity of managing clusters gave way to managed services like Databricks and Snowflake, but their high cost and inefficient data transfer with Python (by now a staple of data science) still left parts of the market unaddressed.

Many technologies emerged to attempt to address latent gaps, but it was DuckDB that surged around 2020 as a fast, easy to use, and cost effective solution to process large volumes of data with SQL while reducing the switching cost of having to learn new frameworks. At around the same time, serverless solutions to address the scale out problem started to gain traction.

Now, as AI training and inference require ever more data, the speed of processing and the speed of development become critical bottlenecks. DuckDB and serverless processing together enable new applications. DuckDB gives workflows an in-process performant SQL engine with:

- Fast processing of large datasets through larger than memory processing with a vectorized query engine.
- Zero-copy interoperability with Python, thanks to formats like Apache Arrow.
- Portability and unprecedented developer experience with easy set-up and without the need to maintain a database server.
- Extensibility thanks to an ecosystem of plugins and extensions (C++), scalar Python UDFs, and WebAssembly compatibility.

DuckDB's modularity in data interchange and query execution makes it an ideal choice for serverless architectures. The combination of DuckDB and serverless has unique advantages:

- Fast and cheap data access thanks to cloud optimized data formats that enable retrieving part of the file (e.g. Parquet for tabular data, Cloud Optimized GeoTIFF for imagery.)
- Scalability, distributed compute without managing infrastructure and without expense when code is not running.
- Easy to share results and create integrations by triggering jobs and loading data via simple HTTPS calls.

## Python + SQL synergy
Python is the lingua franca of data science and AI. It's an imperative language - which means it's easy to write complex logic without sacrificing readability, and interface a broader range of data formats - enabling operations inaccessible to SQL like calling API clients, fine-grained analytic calculations, and processing arrays and rasters. The Python ecosystem recently adopted Rust to write high performance, memory safe modules. However, Python historically struggled with concurrency and managing the memory of distributed clusters, which hindered its ability to process large datasets.

Declarative languages like SQL offer simple syntax to define data manipulations for performant query engines, but they lack explicit control flow and are limited to select data structures.

Two approaches to intertwine SQL and Python emerged, each with particular tradeoffs in portability and efficiency:
- **SQL queries in Python.** These tend to sacrifice data transfer efficiency between runtimes or require specialized, complicated data warehousing.
- **Python UDFs within SQL.** These tend to incur performance costs and require maintaining a Python runtime within the DBMS.

These are offered, to different extents, by tools like Databricks, BigQuery, and Postgres.
- **Databricks** offers a notebook environment, familiar to the data scientist, that enables workflows to transition between Python and SQL - but requires specialized data warehousing, complicated cluster management, and lacks debuggability.
- **BigQuery** UDFs bring an imperative language to SQL engine - but it's constrained to Javascript which lacks Python's powerful data operations and libraries.
- **Postgres** and other databases can bring SQL to a Python runtime with connector libraries such as Psycopg2 and SQLAlchemy - but this pattern has the infrastructure overhead of needing to run a separate database server.

However versatile, DuckDB is founded on SQL and still needs to rely on Python and plugins for expressibility. But its support for Python UDFs and plugins is yet to mature.
- DuckDB only supports scalar Python UDFs.
- Constrained to the capabilities of the local runtime process.
- There's no seamless way to share Python UDFs across databases or runtimes.
- Plugins are difficult to write and deploy.

## Fused + DuckDB synergy
Fused is a framework to author and run serverless operations. Every Fused UDF is an HTTPS API that can be called to run and load data from any application that can make HTTPS requests. Integrating UDFs into workflows is as easy as passing the endpoint as a string. Spreadsheets, web maps, ETL pipelines, and DuckDB can all load data from HTTPS API endpoints, and dynamically parametrize calls with query parameters.

- Eliminates the need to provision, manage, and scale instances - which is what caused the initial break away from the Map Reduce, Hadoop, and Spark era. Its just-in-time backend scales from zero to cluster as quickly as needed.
- UDFs can call UDFs - which results in blazing fast execution by running thousands of parallel jobs -without worrying about orchestration.
- Pay only when code runs, and run from anywhere - which speaks to market segments unaddressed by managed platforms like Databricks and Snowflake.
- Natively runs on a standard Python interpreter - so it seamlessly runs DuckDB while keeping Python's expressibility and ecosystem of libraries.
- Dovetails with cloud-native data formats. Their atomic data loading and compressed formats make for reduced data transfer between local processes and third party cloud warehouses.

Fused and DuckDB together reduce architectural complexity and make it easy to have cutting-edge analytic processing in any application. Together, they eliminate the need for cumbersome distributed query engines which are slow to start-up and are overkill for smaller datasets.

Fused UDFs are easy to share and can run from anywhere. The examples in this post are available as community UDFs you can find on the open source Github repo and run them in any Python environment with the Fused SDK.

## Example patterns

This section shows and discusses three powerful patterns at the intersection of Fused and DuckDB.
### 1. Run DuckDB in a Fused UDF

DuckDB parallelizes its own operations under the hood thanks to its columnar vectorized query engine that provides compelling performance for querying using SQL. However, there can still be bottlenecks in operations upstream or downstream of DuckDB. To resolve this, Fused UDFs easily run DuckDB and create a seamless experience between Python and SQL.

See the full example in our documentation.

### 2. Call Fused UDFs from DuckDB

Any database that supports querying data via HTTPS can call and load data from Fused UDF endpoints using common formats like Parquet or CSV. This means that DuckDB can dispatch operations to Fused that otherwise would be too complex or impossible to express with SQL, or would be unsupported in the local runtime.

In this example, a Fused UDF returns a table where each record is a polygon generated from the contour of a raster provided by the Copernicus Digital Elevation Model as a Cloud Optimized GeoTIFF. DuckDB can easily trigger a UDF and load its output with this simple query, which specifies that the UDF endpoint returns a Parquet file.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sql.gif" alt="overture" width="600"/>

This pattern enables DuckDB to address use cases and data formats that it doesn't natively support or would otherwise see high data transfer cost, such as raster operations, API calls, and control flow logic.

See the full example in our documentation or open it in this [DuckDB shell%0ALIMIT-10~).

### 3. Integrate DuckDB in applications using Fused

Fused is the glue layer between DuckDB and apps. This enables seamless integrations that trigger Fused UDFs and load their results with simple parameterized HTTPS calls.

DuckDB is an embedded database engine and doesn't have built-in capability to share results other than writing out files. As a corollary of the preceding example, it's possible to query and transform data with DuckDB and seamlessly integrate the results of queries into any workflow or app.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sheets.gif" alt="overture" width="600"/>

To try this example simply make a copy of this Google Sheets spreadsheet (File > Make a copy) and click, and modify the parameters in B2:4 to trigger the Fused UDF endpoint and load data.

See the full example in our documentation.

## Conclusion

While the pendulum of the data landscape swung from distributed compute to single-node, Fused's serverless operations swing the conversation back with a simple and cost-efficient scale-out.

This blog post discussed how gaps in the modern data stack can be addressed by integrating Fused and DuckDB, two emerging data processing tools. The intersection between DuckDB's portable SQL and Fused's scalable python operations creates a stack that is:
Flexible due to the seamless interaction of Python and SQL.
Scalable, simple, and cost efficient.

Easy for data scientists to create, and easy for non-coders to consume.

DuckDB is an early example of how Fused integrates with the modern data stack. We're eager to share the growing list of compelling integrations over the following months.

We would like to extend our thanks to Wes McKinney and Michael Driscoll for reviewing drafts of this post before it went out.

## Get started with Fused

Want to get involved?

- Try out Fused for yourself for free!
Give back to the community by contributing a UDF.
- You can also join the conversation by becoming a member of the Fused Discord community. We are always happy to hear your thoughts.
- Does taking serverless operations to the next level sound exciting to you? Fused is hiring! Shoot us a note at `sina@fused.io`.

================================================================================

## Fused redefines geospatial with instant maps
Path: blog/2024-03-06-pressrelease/index.mdx
URL: https://docs.fused.io/blog/2024-03-06-pressrelease

Fused is a modern geospatial toolkit for companies to code, scale, and ship geospatial workflows of any size.

This week we are unveiling Fused, a toolkit to enable interoperability between all geospatial datasets and tools in the modern data stack. Fused is the glue layer that integrates data platforms with data tools via a managed serverless API.

## Overview

Co-founders Sina Kashuk and Isaac Brodsky met while working at Uber. They co-founded Unfolded to commercialize the popular open source geospatial visualization projects Kepler.gl, Deck.gl, and H3. Unfolded was acquired by Foursquare in June, 2021.

Fused has raised $1 million in pre-seed funding from Fontinalis Partners, Wes McKinney, Michael Driscoll, Jason Richman, and angels from Uber, Airbnb, DoorDash, and others.

Fused delivers serverless geospatial operations at any point of the stack — with a simple HTTPS call. This is like when users pull-up information from map apps, but with custom and transparent logic. This shields developers from hours of burdensome engineering, enabling businesses to serve their customers with timely insights, faster.

Teams supercharge their favorite IDEs, tools, and frameworks with Fused. They build with the Python SDK, preview on the browser with Fused Workbench, and run in their stack via the Hosted API.

_Fused ecosystem and product line._

## The problem

We're now in a moment where large-scale geospatial datasets are migrating to open cloud-enabled formats. However, we have personally seen how it can be challenging to utilize this data at scale. At the same time, there has been a rise in Earth observation imagery, which will only accelerate as we monitor climate change and as satellites continue scaling, enabling. However, the sheer volume of data, complexity of operations, and fragmentation of tooling holds back how we process and present that data that is critical for making informed decisions about critical company operations.

Today, data scientists and analysts manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. The size of data limits the possible depth of insight of last-mile analytics and the speed at which they can be delivered — leaving problems unaddressed. Moreover, data scientists handoff algorithms to data engineers who then translate code to work with orchestrators that run on distributed compute systems maintained by an entire devops team. An analyst needs to navigate a sea of buzzwords like CRS, GDAL, Spark clusters, geo-partitions, raster and vector joins, zonal stats, and census blocks — just to prepare for the analysis they actually want to do.

## The solution

Today, a leading global media company animates atmospheric rivers to report weather news — 36x speed improvement, from hours to minutes. An EV company uses Fused to optimize its EV charging station network planning capabilities — blending data at an unprecedented coverage and detail. A carbon offset company creates custom deforestation basemaps with better operational efficiency — closing the analysis loop for stakeholder reports.

Fused empowers teams in these companies to seamlessly layer weather, infrastructure, road, and deforestation data; while transforming it with custom Python code to create apps for real-time decision making. Fused simplifies workflows so small teams can deliver novel business-critical insights where it wasn't possible before.

Read more: Founder's Blog Post

## Vibrant community

As a founding tenet, Fused promotes open source, transparency, and collaboration. To this end, data scientists and app builders engage the Fused community on GitHub and Discord to find, reuse, and share verified code snippets that they can bring into their workflows.

Community UDF of hydrology model by Taher Chegini
In sum, Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when for any geospatial analysis you would have to send it to your GIS person to analyze it and get it back in 3 weeks, if you get lucky (and forget about integrating that with any other tools). Fused is built to be the interoperable glue between geospatial data systems, and we're excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join the journey to break away from old geospatial infrastructure. Let's revolutionize geospatial technology together! fused.io. 🌎🚀

Join the journey

- Read the announcement on Tech Crunch
- Follow us on Twitter/X
- Follow us on LinkedIn
- Star our GitHub repo
- Join the conversation on Discord
- Read Fused's founding principles

================================================================================

## Founder's blog post: why Fused?
Path: blog/2024-03-01-welcome/index.mdx
URL: https://docs.fused.io/blog/2024-03-01-welcome

Fused enables interoperability between datasets and tools in the modern data stack. It's a glue layer to integrate data platforms with data tools via a managed serverless API.

## Current limitations with data processing

Today, there is a fragmented ecosystem around scalable geospatial data processing. Python geospatial libraries like GeoPandas, Shapely, and Rasterio make it easy to do small jobs but are single-threaded and operate entirely in-memory. For bigger jobs, there are Python parallel processing tools like Dask that require complex installations and are liable to memory pressure errors. Spark-based tools like Apache Sedona and RasterFrames have a steep learning curve and are hard to debug and orchestrate. Postgres and its geospatial extension PostGIS operate on larger-than-memory datasets but are hard to scale larger than the disk of one machine, aren’t designed for OLAP workloads, and can be hard to administer. Cloud data warehouses like Databricks and Snowflake are monolithic systems that tend to bring lock-in and pricing that is hard to anticipate.

Spatial SQL is a great way to run scalable operations on tables with vector data - but falls short on raster data and does not have native access to libraries for the finesse operations of data science. Geospatial data science teams largely use Python and would prefer to use it both in development and in production - but tooling fragmentation forces them to juggle languages and frameworks. The present paradigm accepts the inefficiencies of complexity as a necessary evil because there hasn’t been a better way to work with both raster and vector data at scale. Data teams have an unaddressed need for a friendly Python API that scales. To increase development velocity it’s convenient for most code to run in Python, moving only computationally heavy code into specialized frameworks - as efficiently as possible. Additionally, scaling Python from local development to massive cloud workloads calls for efficient parallelization.

## Seizing the moment

The last several years have seen a commoditization of modular building blocks of OLAP systems and increased adoption of geospatial cloud-native data formats. With the convergence and popularity of columnar memory formats like Apache Arrow and Apache Parquet, easy-to-use columnar OLAP databases like DuckDB, and broader adoption of geospatial cloud-native data formats like Cloud-Optimized GeoTIFF and GeoParquet, we believe there’s a window for a serverless geospatial OLAP engine. Moreover, serverless computing has emerged as a prominent trend, delegating infrastructure management and dynamically scaling resources in response to demand, leading to heightened flexibility and cost efficiency. Leveraging serverless cloud infrastructure like AWS Lambda, Azure Functions, Google Cloud Functions, or Cloudflare Workers enables event-driven processing closer to the data source.

Parquet files have become the standard file format for columnar data and have helped to commoditize the decoupling of storage and compute by enabling queries directly on object storage like AWS S3. GeoParquet – a specification for storing point, line, and polygon geometries in Parquet – has seen recent momentum as a fast storage format for geospatial vector data and has started to be integrated into industry-standard tools like GDAL. Moreover, with spatial partitioning, operations can be broken down into small independent parts that execute simultaneously in multiple processes. For geospatial array data like satellite imagery, Cloud-Optimized GeoTIFF – an extension to GeoTIFF that enables chunked access via HTTPS range requests – has taken hold as the standard way to store geospatial image data, with petabytes publicly available from AWS’ open data program and buy-in from major vendors like USGS and Planet.

Apache Arrow has become the universal in-memory columnar data format for columnar, analytic data because its language-independent specification enables easier movement of data between languages and frameworks. Moreover, GeoArrow – an incubating specification for storing geospatial data in Arrow – gives us a way to move geospatial data from Python to compiled code for free, and will likely serve as the foundation for an ecosystem of large-data geospatial tools. Already in the frontend, deck.gl can use GeoArrow-style data buffers to visualize millions of coordinates with no serialization costs.

As a result of all these trends, smaller data can be transferred to and processed on serverless cloud services in ways that are not possible ever before. Public clouds enable event-driven compute services that automatically scale, which makes for simple infrastructure and dependency management. Managed offerings reduce the complexities of data pipelines enabling geospatial workloads of any size to run on demand – to empower users with the ability to go from code to map, instantly.

## Why Fused?

Fused instantly converts user’s Python code to workflows and maps in Jupyter notebooks, low-code web apps, the Fused Workbench web-app, ETL pipelines, or any tool that consumes HTTPS API endpoints. Fused lets developers run real-time serverless operations at any scale and build responsive maps, dashboards, and reports. Developers develop in production and run on any scale data without infrastructure friction using serverless parallel computing powered by advanced caching of geo-partitioned data. This enables bringing interoperable workflows, apps, and maps to the user's preferred stack and avoiding vendor lock-in.

With Fused, users find, reuse, and share User Defined Functions (UDFs) in the Fused vibrant community. Fused UDFs are building blocks of serverless geospatial operations that integrate across the stack - with Planetary Computer, Google Earth Engine, Big Query, Snowflake, DuckDB, and more. They load datasets from the cloud ecosystem such as NASA, NOAA, US Census, and Overture. Fused serverless API turns these UDFs into live HTTPS endpoints that load their output into any tools.

Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when you manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. Fused is built to be the interoperable glue between geospatial data systems, and we’re excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join us in our journey to break from old geospatial infrastructure. Let's revolutionize geospatial technology together! 🌎🚀

- The Fused Founding Team

================================================================================


---

Generated automatically from Fused documentation. Last updated: 2026-01-29
Total sections: 5
