# Fused Documentation - Complete Reference

> Fused is an end-to-end cloud platform for data analytics, built around User Defined Functions (UDFs): Python functions that can be run via HTTP requests from anywhere, without any install required.

This comprehensive reference contains the complete text of all Fused documentation, including all API methods, examples, tutorials, and guides.

================================================================================

# CORE CONCEPTS

## Call UDFs asynchronously
Path: core-concepts/async.mdx
URL: https://docs.fused.io/core-concepts/async/

A UDF can be called asynchronously using the async/await syntax. A common implementation is to call a UDF multiple times in parallel with different parameters then combine the results.

    Setting `sync=False` in `fused.run` is intended for asynchronous calls when running in the cloud with `engine='remote'`. The parameter has no effect if the UDF is ran in the local environment with `engine='local'`.

To illustrate this concept, let's create a simple UDF and save it as `udf_to_run_async` in the workbench:

```python showLineNumbers
@fused.udf
def udf(date: str='2020-01-01'):

    time.sleep(2)
    return pd.DataFrame()
```
    We can not pass a UDF object directly to `fused.run`. Asynchronous execution is only supported for saved UDFs specifed by name or token.

We can now invoke the UDF asynchronously for each date in the `dates` list and concatenate the results:

```python showLineNumbers
async def parent_fn():

    # Parameter to loop through
    dates = ['2020-01-01', '2021-01-01', '2022-01-01', '2023-01-01']

    # Invoke the UDF as coroutines
    promises_dfs = []
    for date in dates:
        df = fused.run("udf_to_run_async", date=date, engine='remote', sync=False)
        promises_dfs.append(df)

    # Run concurrently and collect the results
    dfs = await asyncio.gather(*promises_dfs)
    return pd.concat(dfs)
```

nest_asyncio might be required to run UDFs async from Jupyter Notebooks.
```python showLineNumbers
!pip install nest-asyncio -q

nest_asyncio.apply()
```

================================================================================

## Build with LLMs
Path: core-concepts/best-practices/build_with_llms.mdx
URL: https://docs.fused.io/core-concepts/best-practices/build_with_llms/

# Building with LLMs: Getting things done, fast.

_Our mission at Fused is to help you get things done, fast. LLMs can help you write User Defined Functions, then leverage Fused to get feedback on your execution and make analytics faster_

This setup helps you setup Cursor code editor in a way that helps you write & debug UDFs fast.

By the end of this guide you'll have:
- A custom Cursor setup that allows you to ask LLMs to write UDFs for you
- Cursor testing + debugging UDFs based on Fused best practices
- Gives you links to open UDFs in Workbench directly

## Requirements

- A Fused Account with an engine
- A Cursor account
- Locally installed Cursor (update to latest version)

## Setup

_If you get blocked, check out some of the common issues in the Getting unblocked section at the end of this guide._

#### 1. Install `fused` locally. 

This is to help Cursor write UDFs, save them to Workbench for you as well as test & debug them. 

```python
pip install "fused[all]" # this installs optional dependencies especially helpful for geospatial operations
```

You also need to be logged in to your Fused account for Cursor to be able to save UDFs to Workbench for you. You can test this with:

```python

fused.api.whoami()

>>>

```

#### 2. (Optional but recommended) Clone the `udfs` public repo

At Fused we want to build things quickly so we made a common set of functions that we often re-use
You can pass this to Cursor to help it write UDFs without having to re-invent the wheel.

```bash
git clone https://github.com/fusedio/udfs.git
cd udfs/
```

#### 3. Start Cursor locally

Either click on it in your applications or:

```bash
cursor .
```

#### 4. Give Cursor knowledge about Fused

Cursor & all AI tools are evolving so quickly we recommend you update Cursor to the latest version. 
Some of the exact layouts might change by the time you read this.

On Mac you can update Cursor by clicking the top left application and selecting "Check for Updates"

[Image: Update Cursor]

If you get errors when trying to run fused commands in Cursor, it's likely because Cursor is not using the right Python environment. Here are a few things to try:

1. Make sure you have the right Python environment activated. You can do this by:
   - Opening the command palette (Cmd/Ctrl + Shift + P)
   - Searching for "Python: Select Interpreter"
   - Selecting the environment where you installed fused

2. If that doesn't work pass what you need to activate the correct environment to Fused as rule:
- Go to Cursor settings -> "Rules" -> "+ Add Rule"
- Give it instructions, for example:

```md
Always run:
`source /Users/user/miniforge3/etc/profile.d/conda.sh` and then `conda activate fused` to use the latest fused packages
```

3. Try restarting Cursor after activating the right environment

You might need to tinker a bit to make sure you've got the proper environment activated. If nothing else, ask Cursor to help you!

This seems to be a bug if you have `zsh` and `powerlevel10k`. 

At time of writing this isn't fully solved but forums recommend editing your `.zshrc` file to remove the `powerlevel10k` plugin.

Cursor & AI tools are evolving so quickly we recommend you update Cursor to the latest version as some of the exact layouts keep changing.

On Mac you can update Cursor by clicking the top left application and selecting "Check for Updates"

[Image: Update Cursor]

You might not be logged into your Fused Account. Try:

```python

fused.api.whoami()

>>>

```

Cursor might also have not been able to save the UDF to Workbench. Sometimes just ask it again! It can also give you the name of the UDF it wrote, which you can then find in your Saved UDFs in Workbench.

</details>

### Share your findings with us!

We're always looking to improve our best practices and make it easier to get things done with LLMs. 

If you have any feedback, let us know on Discord!

================================================================================

## Best Practices
Path: core-concepts/best-practices/index.mdx
URL: https://docs.fused.io/core-concepts/best-practices/

# Best Practices

Get the most out of Fused by following these proven best practices for building efficient UDFs, organizing your workflows, and optimizing performance.

## Documentation overview

<DocCardList />

================================================================================

## Making the most out of UDFs
Path: core-concepts/best-practices/udf-best-practices.mdx
URL: https://docs.fused.io/core-concepts/best-practices/udf-best-practices/

# Build & Running UDFs

_An opinionated guide to making the most out of Fused UDFs_

Fused UDFs are Python functions that run on serverless compute and can be called from anywhere with `fused.run(udf)`. This guide is a resource meant to help you on your way to making the most out of UDFs.

## A short reminder: The anatomy of a UDF

```python showLineNumbers
@fused.udf
def udf(my_awesome_input: int = 1):

    return pd.DataFrame()
```

Each UDF has a few specific elements:
- The `@fused.udf` decorator
- Arguments -ideally typed-
- Imports _inside_ the function
- Some logic
- A supported `return` object

All of this is explained in the "Write UDF" section in much more details.

You can then run UDFs from _anywhere_ with `fused.run(udf)`. These are still Python functions, giving you a lot of flexibility on what oyu can do, but we have some recommendations for keeping them fast & efficient.

## Writing efficient UDFs

### Keep things small

The main benefit of Fused UDFs is how responsive they are. They achieve this by running on Python serverless compute. They can time out, so the best way to keep workflows fast is to keep them small:

- Break pipelines into single-task UDFs
- Leverage `fused.run()` to chain UDFs together
- Or run small tasks in parallel

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):

        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):

        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ```python showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):

        df = fused.run(load_data_udf, data_path=data_path)
        processed_df = fused.run(process_data_udf, df=df)

        return processed_df
    ```

    This is a breakdown of what happens when you run a UDF with `fused.run()` and why we recommend you keep your UDFs at the 30s-1min mark:

    [Image: UDF Design Guidelines]

    Let's imagine we have a `fetch_single_data` that loads data from an API for a large amount of inputs `input_data=[0,1,2,3,4,5,7,8,9]`:

    ```python showLineNumbers
    @fused.udf
    def fetch_single_data(single_input: int):

        # Considering this as our API call, sleeping to simulate the time it takes to get the data
        time.sleep(3)

        return pd.DataFrame("]})
    ```
    If we were to run this UDF in Workbench we would only be able to run it for 1 input at a time, so we could edit our UDF to loop over the inputs:
    ```python showLineNumbers
    @fused.udf
    def fetch_data(inputs: list):

        fetched_data = []
        for i in inputs:
            # Considering this as our API call
            time.sleep(3)
            fetched_data.append(f"processed_")

        return pd.DataFrame()
    ```
    However, running this UDF with `fused.run(fetch_data, inputs=input_data)` will take longer as we add inputs, we could even quickly go over the 120s limit. We still do want to fetch data across all our inputs which is where `fused.submit()` comes in:

    Going back to our original UDF, we can now run it with `fused.submit()` to run it in parallel:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(input_data):

        results = fused.submit(
            fetch_single_data,
            input_data,
            engine='local', # This ensures the UDF is run in our local server rather than spinning up new instances.
        )
        fetched_data = results.collect_df()

        return fetched_data
    ```

    This is of course a simplified example, but it shows how you can use `fused.submit()` to run a UDF in parallel for each input.

    This now runs a lot faster by running the `fetch_single_data` UDF in parallel for each input.

        The example here blocks the main thread until all the `fused.submit()` calls have finished. This means you might have to wait longer in Workbench for the results to show up.
    
    Comparison of both approaches in Workbench:

    Running with `fused.run()`, 30.89s:
    [Image: 10 inputs fused.run]

    Running with `fused.submit()`, 5.3s:
    [Image: 10 inputs fused.submit]

    Re-using the example from keeping things small:

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down AND cache the calls:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):

        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):

        # Some complicated processing logic to create df_processed
        # ...
        return processed_df
    ```

    ```python  showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):

        @fused.cache
        def load_data(data_path):
            return fused.run(load_data_udf, data_path=data_path)

        @fused.cache
        def process_data(df):
            return fused.run(process_data_udf, df=df)

        df = load_data(data_path)
        processed_df = process_data(df)

        return processed_df
    ```

    ```python  showLineNumbers
    @fused.udf
    def udf():

        beginning_time = time.time()

        # long processing step #1
        time.sleep(5)
        end_process_1 = time.time()
        process_time_1 = round(
            end_process_1 - beginning_time, 2
        )
        print(f"")

        # short processing step
        time.sleep(0.2)
        process_time_2 = round(
            time.time() - end_process_1, 2
        )
        print(f"")

        return
    ```

    Would give us:

    ```
    >>> process_time_1=5.0
    >>> process_time_2=0.2
    ```
</details>

### Test out your UDFs before running them in parallel

When using `fused.submit()` to run a UDF in parallel you can use the `debug_mode` to run the 1st argument directly to test if your UDF is working as expected:

```python showLineNumbers
job = fused.submit(udf, inputs, debug_mode=True)
```

This runs `fused.run(udf, inputs[0])` and returns the results. It allows you to quickly test out `udf` before running it on a large number of inputs.

Since UDFs are cached by default, using `fused.submit(udf, inputs, debug_mode=True)` means Fused won't run the 1st input again, as you'll have just run it! (unless `udf` is set with `cache_max_age=0`)

### Join the Discord for support

We host & run a Discord server where you can ask any questions! We or the community will do our best to help you out!

[Image: Discord]

================================================================================

## Workbench Best Practices
Path: core-concepts/best-practices/workbench-best-practices.mdx
URL: https://docs.fused.io/core-concepts/best-practices/workbench-best-practices/

# Workbench best practices

_Tips & Tricks for making Fused Workbench work for you_

[Image: Workbench Overview]

Workbench is a web-IDE built to make working with Fused UDFs even faster! 

## Experimenting with UDFs, fast

In UDF Builder you have access to a Code Editor that runs your UDFs and outputs results directly on the Map View for you. As soon as you make changes they show up in Map View! 

### üí° Leverage all the UDF Best Practices

While this page is for Workbench, it builds on top of all the Best Practices that make your UDF fast & efficient. So if you haven't yet, take a look at our dedicated UDF Best Practices.

### Use `return` to quickly explore data

Your UDF will stop at the first `return` it sees, which you can use to your advantage to return an intermediate result and explore it directly on the map:

    In this example, we're using the Overture Maps Example UDF but not sure exactly what our `bounds` object looks like.

    The easiest way to check is simply to return it inside our UDF before any other logic:

### Format your code for more visibility

You can hit `Opt + Shift + F` (or `Alt + Shift + F` on Windows/Linux) to format your code with a smaller line-length. This comes in handy if you don't want to scroll left and right to read your code, at the expense of having a bit more up and down scrolling to do.

## üó∫Ô∏è Visualizing results

### Visual Debugging Techniques

- **Elevation as a debugging tool**: Use 3D elevation (`extruded: true`) to add an extra dimension to your analysis
- **Multi-dimensional visualization**: Combine opacity, color, and height to encode different aspects of your data
- **Color by data attributes**: Map colors to categorical attributes and height to quantitative metrics

### Optimizing Color Use for Analysis

- **Color categories for types**: Use distinct color schemes like `TealRose` to clearly distinguish between categories
- **Sequential colors for metrics**: Use color gradients for representing continuous values like ratios or densities
- **Diverging color schemes**: Highlight values above and below an interesting mid-point in quantitative data - the middle color is assigned to the critical value with contrasting colors on either end
- **Visibility first**: Choose colors that maintain visibility over your basemap (darker colors for light basemaps, lighter colors for dark basemaps)

For a complete reference of available color schemes, check out CARTO color schemes which are implemented in DeckGL and available in Fused. These include categorical schemes like `Bold` and `Pastel`, sequential schemes like `BluYl` and `Sunset`, and diverging schemes like `TealRose` and `Tropic`.

## Navigating Workbench

### Using Keyboard Shortcuts: Command Palette

Workbench has built-in keyboard shortcuts & quick navigation features: Hit `Cmd + K` (or `Ctrl + K` on Windows / Linux) to bring up Command Palette or use the search bar in the header for quick access:

[Image: Command Palette]

Without lifting your hands from the keyboard you can:
- Open a New UDF
- Search the Docs, directly in Workbench!
- See some of the most helpful Keyboard Shortcuts

You'll find a more extended list of Keyboard Shortcuts in the command palette.

[Image: Preferences - Keyboard Shortcuts]

### Quickly jump from UDF Builder to File Explorer

UDF Builder & File Explorer work well together, so we've made easy to jump from one to the other

- In UDF Builder, `Cmd + Click` on a `s3://...` path will open it directly in File Explorer
- In File Explorer double clicking on a file will prompt Fused to do its best at guessing which Catalog UDF to use to load this file in Code Editor

## Organising your work

### Renaming UDFs

You can easily rename UDFs by clicking on the UDF name in the header and hitting `Enter`

Your team can load your own UDFs by calling it with a team udf name so be sure to give it an explicit name!

### Using tags

You can add tags to your UDF in the Share page. This gives yet another way to find & search your UDFs. We recommend giving tags according to:
- Type of data the UDF works with (e.g. `satellite image`, `elevation model`, `population`)
- Type of analysis the UDF does (e.g. `zonal stats`, `building footprint extraction`, `flood mapping`)
- Type of file the UDF loads (e.g. `vector`, `raster`, `point cloud`)

### Using Collections

You can use Collections to organise your UDFs into different projects. This allows you to:
- Have multiple unrelated projects in Workbench
- Be able to share a set of UDFs at once with your team (by downloading a Collection & sending it to your team mates)

## Troubleshooting

If things feel a bit off, for example your UDF output looks suspicious here are a few things you can do:
- Manually rerun the UDF with `Shift + Enter`
- Check how much RAM your tab is using (in Chrome can easily do so by hovering the tab). Sometimes too much data is brought in to your browser and while we do our best to manage it properly it can get out of hand. A good old tab refresh goes a long way

<ReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/refresh_edit2.mp4" width="100%" />

================================================================================

## Caching
Path: core-concepts/cache.mdx
URL: https://docs.fused.io/core-concepts/cache/

# Caching

_This pages explains how caching makes Fused more responsive & some best practices for making the best use of it_

## Caching Basics

The goal of Fused is to make developing & running code faster for data scientists. This is done by using efficient file formats and making UDFs simple to run. On top of those, Fused relies heavily on caching to make recurring calls much faster.

At a high level, caching is storing the output of a function run with some input so we can directly access the result next time that function is called with the same input, rather than re-computing it to save time & processing cost.

[Image: Function + Input run]

_The first run of a [Function + Input] is processed, but the next time that same combination is called, the result is retrieved much faster_

As soon as either the function or the inputs change however, the output needs to be processed (as the result of this new combination has not been computed before)

[Image: Different Function + Input run]

Fused uses a few different types of cache, but they all work in this same manner

## Caching any Python function: `@fused.cache`

### Locally

Any Python function, either inside a UDF or even locally on your machine can be cached using the `@fused.cache` decorator around it:

```python  showLineNumbers
# This works locally on your machine

from datetime import datetime

@fused.cache(cache_max_age='30s')
def telling_time():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return current_time

telling_time()
```

[Image: Fused_cache_function_locally]

As seen in the debug logs, your cached data will be saved under `/tmp/cached_data/tmp/` locally.

Similar to how this works with `fused.run()`, you can overwrite `cache_max_age` when executing your function directly:

```python showLineNumbers
telling_time(cache_max_age="0s") # Overwrite cache duration to be 0s, i.e. no caching
```

### Inside a UDF

This also works inside a UDF by passing `@fused.cache` decorator around any function:

```python  showLineNumbers
@fused.udf
def udf():

    @fused.cache
    def load_data(i):
        # Do heavy processing here
        return pd.DataFrame()

    df_first = load_data(i=1)
    df_first_repeat = load_data(i=1)
    df_second = load_data(i=2)

    return pd.concat([df_first, df_first_repeat, df_second])
```

Under the hood:
- The first time Fused sees the function code and parameters, Fused runs the function and stores the return value in a cache.
    - This is what happens in our example above, line 10: `load_data(i=1)`
- The next time the function is called with the same parameters and code, Fused skips running the function and returns the cached value
    - Example above: line 11, `df_first_repeat` is the same call as `df_first` so the function is simply retrieved from cache, not computed
- As soon as the function _or_ the input changes, Fused re-computes the function
    - Example above: line 12 as `i=2`, which is different from the previous calls

**Implementation Details**

A function cached with `@fused.cache` is:
- Cached for 12h by default (can be changed with `cache_max_age`)
- Stored as pickle file on `mount/`

### Benchmark: With / without `@fused.cache`

Using `@fused.cache` is mostly helpful to cache functions that have long, repetitive calls like for example loading data from slow file formats.

Here are 2 simple UDFs to demonstrate the impact:
- `without_cache_loading_udf` -> Doesn't use cache
- `with_cache_loading_udf` -> Caches the loading of a CSV

```python  showLineNumbers
@fused.udf
def without_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    # @fused.cache
    def load_ais_data(ais_path: str):

        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

and the same:
```python  showLineNumbers
@fused.udf
def with_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    @fused.cache
    def load_ais_data(ais_path: str):

        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

Comparing the 2:

[Image: Caching benchmark]

### Best Practices: `@fused.cache`

Caching a local function or inside a UDF works best for:
- Loading data from slow formats (CSV, Shapefile)
- Repetitive operations that can take a long amount of processing

However, be wary of relying on `@fused.cache` to load very large (>10Gb) datasets as cache is only stored for a few hours by default and is over-written each time you change the cached function or inputs.

Look into ingesting your data in partitioned cloud native formats if you're working with large datasets.

The line between when to ingest your data or use `@fused.cache` to load data inside a UDF is a bit blurry. Check this section for more

### Example use cases

You can look at some real-world use cases in some of our Examples:
- Caching a STAC catalog request when fetching Sentinel 1 radar satellite image in our Dark Vessel Detection example
- Read about Jeff Faudi's use of `@fused.cache` in running an ML-inference model for aircraft detection. (See the public UDF for yourself)

## Caching a UDF

While `@fused.cache` allows you to cache functions locally or _inside_ UDFs, UDFs ran with `fused.run()` are cached by default on Fused server.

You can create a token for your UDF in Python by first saving your UDF to Fused server:

```python showLineNumbers
@fused.udf
def slow_caching_udf():

    time.sleep(5)
    
    return pd.DataFrame()

fused.run(slow_caching_udf)
```

We can demonstrate this caching with a UDF that has a `time.sleep(5)` in it. Running this same UDF twice:

[Image: Cached_fused_run_udf]

This means that UDFs that are repeatably called with `fused.run()` become much more responsive. Do remember once again that UDFs are recomputed each time either anything in the UDF function or the inputs change!

**Implementation Details**

Cached UDF are:
- Stored for 90d by default (see Python SDK for more details)
- Stored on S3
- You can overwrite the cache age by passing `cache_max_age` either when defining the UDF with `@fused.udf(cache_max_age)` or when running the UDF with `fused.run(udf, cache_max_age)`

Note that UDFs work similarly to regular Python functions, default arguments are evaluated when defining the function, not when calling it.

For example:
```python

@fused.udf
def abc(d: str = datetime.datetime.now().strftime('%Y-%m-%D')):
    print("with default", d)
```

and
```python
@fused.udf
def abc(d: str = None):

    d = datetime.datetime.now().strftime('%Y-%m-%D')
    print("with default", d)
```

Will both be cached similarly (i.e. calling either of these functions 2 times consecutively will return the cached results).

## Advanced

### Caching & `bounds`

Pass `bounds` to make the output unique to each Tile.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def fn(bounds):
        # convert bounds to tile
        common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
        zoom = common_utils.estimate_zoom(bounds)
        tile = common_utils.get_tiles(bounds, zoom=zoom)
        return tile

    return fn(bounds)
```

Note that this means that if you're running your Tile UDF in Workbench, every time you pan around on the map you will cache a new file

For this reason, it's recommend to keep cache for tasks that _aren't_ dependent on your `bounds` when possible, for example:

```python  showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    # convert bounds to tile
    common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = common_utils.estimate_zoom(bounds)
    tile = common_utils.get_tiles(bounds, zoom=zoom)

    # Loading of our slow data does not depend on bounds so can be cached even if we pan around
    gdf = loading_slow_geodataframe()
    gdf_in_bounds = gdf[gdf.geometry.within(tile.iloc[0].geometry)]

    return gdf_in_bounds
```

### Defining your cache lifetime: `cache_max_age`

You can define how long to keep your cache data for with `cache_max_age`. Valid time units include:
- Seconds (`s`)
- Minutes (`m`)
- Hours (`h`)
- Days (`d`)

Examples: `24h` (24 hours), `30m` (30 minutes), `10s` (10 seconds)

**Cache Behavior:** UDF executions are cached by default. To bypass caching and ensure fresh results, pass `cache_max_age="0s"` in your `fused.run()` call.

```python showLineNumbers
@fused.udf
def udf():

    @fused.cache(
        cache_max_age="24h" # Your cache will stay available for 24h
    )
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    return gdf
```

This also works with `@fused.udf()` & `fused.run()`:
```python showLineNumbers
@fused.udf(cache_max_age="24h") # This UDF will be cached for 24h after its initial run
def udf(path):

    gdf = gpd.read_file(path)

    return gdf
```

This UDF will be cached from the moment it's executed with `fused.run(udf)` for as long as is defined in `cache_max_age`:

```python showLineNumbers
fused.run(udf)
```

If you run `fused.run(udf)` again with no changes to `udf`, then for the next 24h `fused.run(udf)` will return a cached result. This is both faster & cheaper (saving on compute) while giving you control over how long to keep your cache for.

You can also overwrite the `cache_max_age` defined in `udf` when running your UDF:

```python showLineNumbers
fused.run(udf, cache_max_age="12h")
```

`udf` results will now only be cached for `12h`, even if `udf` was defined with a `cache_max_age` of `24h`:

The age of your cache is defined as follows:
- By default a UDF is cached for 90 days.
- If `@fused.udf(cache_max_age)` is defined, this new cache age overwrites the default.
- If `fused.run(udf, cache_max_age)` is passed, then this cache age takes priority over default & `@fused.udf(cache_max_age)`

### Resetting cache: `cache_reset`

Sometimes you might want to reset your cache, when for example:
- Running a UDF with unknown `cache_max_age`, and you want to make sure you're getting fresh results
- Having a `try / except` block to reset cache if your default UDF with cache fails.

You can easily do this by passing `cache_reset=True`:

```python showLineNumbers
fused.run(udf, cache_reset=True)
```

This also works in combination with `@fused.cache`:

```python showLineNumbers
@fused.cache(cache_reset=True)
def my_function():
    ...
    return gdf
```

Defining a UDF that simply returns the current time:
```python showLineNumbers

@fused.udf
def udf():
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

Running this a first time:
```python showLineNumbers
fused.run(udf)
```

Returns:
```bash
2025-06-06 10:00:00
```

Running this a second time:
```python showLineNumbers
fused.run(udf)
```

Returns a cached result:
```bash
Cached UDF result returned.
2025-06-06 10:00:00
```

This is because UDFs are cached by default even without passing any `cache_max_age` argument

We can break this cache by passing `cache_reset=True`:

```python showLineNumbers
fused.run(udf, cache_reset=True)
```

Returns:
```bash
2025-06-06 10:00:12
```

</details>

 showLineNumbers
@fused.cache(
    storage="local"
)
def local_function_load(data_path):
    ...
    return gdf

gdf = local_function_load()
```

    Read more about this in the Python SDK page on `@fused.cache`

================================================================================

## Download
Path: core-concepts/content-management/download.mdx
URL: https://docs.fused.io/core-concepts/content-management/download/

# Download

Download remote files to the local system to make them available to UDFs across runs. Files are written to a disk shared across all UDFs in an organization.

## `fused.download`

Download any file to: `/mount/tmp/` which any other UDF can then access.

```python showLineNumbers
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    print(out_path)
```

The `download` function sets a lock to ensure the download happens only once, in case the UDF is called concurrently.

================================================================================

## Environment variables
Path: core-concepts/content-management/environment-variables.mdx
URL: https://docs.fused.io/core-concepts/content-management/environment-variables/

Save constants to an `.env` file to make them available to UDFs as environment variables. You should use the secrets manager for sensitive information like API keys.

First, run a File UDF that sets variables in an `.env` file in the `/mnt/cache/` directory.

```py
@fused.udf
def udf():
    env_vars = """
    MY_ENV_VAR=123
    """

    # Path to .env file in disk file system
    env_file_path = '/mnt/cache/.env'

    # Write the environment variables to the .env file
    with open(env_file_path, 'w') as file:
        file.write(env_vars)
```

Now, any UDF can load the values from `.env` as environment variables with the `load_dotenv` and access them with os.getenv.

```py
@fused.udf
def udf():
    from dotenv import load_dotenv

    # Load environment variable
    env_file_path = '/mnt/cache/.env'
    load_dotenv(env_file_path, override=True)

    # Access environment variable
    print(f"Updated MY_ENV_VAR: ")
```

================================================================================

## File systems
Path: core-concepts/content-management/file-system.mdx
URL: https://docs.fused.io/core-concepts/content-management/file-system/

Fused provides two file systems to make files accessible to all UDFs: an S3 bucket and a disk. Access is scoped at the organization level.

## `fd://` S3 bucket

Fused provisions a private S3 bucket namespace for your organization. It's ideal for large-scale, cloud-native, or globally accessible datasets, such as ingested tables, GeoTIFFs, and files that need to be read outside of Fused.

Use the File explorer to browse the bucket and see its full path.

[Image: file explorer]

Fused utility functions may reference it with the `fd://` alias.

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="fd://census/ca_bg_2022/",
).run_remote()
```

## `/mnt/cache` disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and `fused.download` write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system. For example, to list files in the directory:

```python showLineNumbers
@fused.udf
def udf():

    for each in os.listdir('/mnt/cache/'):
        print(each)
```

### Troubleshooting

If you encounter the following error, it means `/mnt/cache` is not yet configured for your environment. To resolve this issue, please contact the Fused team to enable it.

```text
Error: No such file or directory: '/mnt/cache/'
```

================================================================================

## GitHub Integration
Path: core-concepts/content-management/git.mdx
URL: https://docs.fused.io/core-concepts/content-management/git/

<Tag color="#3399ff">Enterprise</Tag> _This feature is accessible to organizations with a Fused Enterprise subscription._

Github integration allows:
- Collaboration across teams
- Saving & Versioning of UDFs

Fused Github integration plugs directly into your own repositories.

## Configuring Github integration

1. Create a new Github repository. There's no enforced repo structure because Fused scans the entire repo for UDFs, although the Public UDFs repo may serve as guideline.

2. Install the Fused GitHub app for your organization. Navigate to this GitHub URL, click "Configure", and select the GitHub organization that contains the repository.

[Image: install fused github]

3. Scope the app to the target repository. It's recommended to select only the specific repo.

[Image: install fused github 2]

4. Once the above is complete, let the Fused team know the full repo path so they configure it for your organization.

5. Confirm the integration is enabled by checking that repo UDFs appear under the "Team UDFs" tab in the UDF Catalog.

You can see all the repositories you have access to in the Versions Tab:

[Image: Github Versions Tab]

================================================================================

## File Formats
Path: core-concepts/data-ingestion/file_formats.mdx
URL: https://docs.fused.io/core-concepts/data-ingestion/file_formats/

# File Formats

_This page specifies which File Formats for both raster & vector data we prefer working with at Fused, and why_

## What this page is about

Fused works with any file formats you might have, as all UDFs are running pure Python. This means you can use any file formats you want to process your data.
That being said the goal of Fused is to significantly speed up the workflow for data scientists, by leveraging modern cloud compute infrastructure and simplify it.

Some formats like Shapefile, CSV, JSON, while incredibly versatile, aren't the most appropriate for large datasets (even above a few Gb) and are slow to read / write (we consider anything above 10s of seconds to read to be extremely slow).

Take a look at our benchmark to see a comparison between loading a CSV, GeoParquet & Fused-partitioned GeoParquet to see a concrete example of this

To make the most out of Fused, we recommend ingesting your data into the following file formats:

### For rasters (images): Cloud Optimized GeoTIFF

For images (like satellite images) we recommend using **Cloud Optimized GeoTIFFs** (COGs). To paraphrase the Cloud Native Geo guide on them:

> Cloud-Optimized GeoTIFF (COG), a raster format, is a variant of the TIFF image format that specifies a particular layout of internal data in the GeoTIFF specification to allow for optimized (subsetted or aggregated) access over a network for display or data reading

    Fused does not (yet) have a build-in tool to ingest raster data. We suggest you create COGs yourself, for example by using `gdal`'s built-in options or `rio-cogeo`

Cloud Optimized GeoTIFFs have multiple different features making them particularly interesting for cloud native applications, namely:
- **Tiling**: Images are split into smaller tiles that can be individually accessed, making getting only parts of data a lot faster.
- **Overviews**: Pre-rendered images of lower zoom levels of images. This makes displaying images at different zoom levels a lot faster

[Image: A simple overview of Geoparquet benefits]

_A simple visual of COG tiling: If we only need the top left part of the image we can fetch only those tiles (green arrows). Image courtesy of Element 84's blog on COGs_

- Element84 wrote a simple explainer of what Cloud Optimized GeoTiffs are with great visuals
- Cloud Optimized Geotiff spec dedicated website
- Cloud Optimized Geotiff page on Cloud Native Geo guide

### For vectors (tables): GeoParquet

To handle vector data such as `pandas` `DataFrames` or `geopandas` `GeoDataFrames` we recommend using **GeoParquet** files. To (once again) paraphrase the Cloud Native Geo guide:

> GeoParquet is an encoding for how to store geospatial vector data (point, lines, polygons) in Apache Parquet, a popular columnar storage format for tabular data.

[Image: A simple overview of Geoparquet benefits]

_Image credit from the Cloud Native Geo slideshow_

    Refer to the next section to see all the details of how to ingest your data with Fused's built-in `fused.ingest()` to make the most out of geoparquet

- `geoparquet` Github repo
- `geoparquet` 1 page website with a list of companies & projects involved
- GeoParquet page on Cloud Native Geo guide

You can explore some of the uses of GeoParquet in some of our examples:
- Ingesting AIS point data from NOAA into GeoParquet format
- Explore the Overture Buildings dataset in this public UDF, which we repartitioned into GeoParquet. Read more about how we're leveraging this with the Overture Maps Foundation.

### Additional resources

- Read the Cloud-Optimized Geospatial Formats Guide written by the Cloud Native Geo Org about why we need Cloud Native formats
- Friend of Fused Kyle Barron did an interview about Cloud Native Geospatial Formats. Kyle provides simple introductions to some cloud native formats like `GeoParquet`

================================================================================

## Data Ingestion
Path: core-concepts/data-ingestion/index.mdx
URL: https://docs.fused.io/core-concepts/data-ingestion/

# Data Ingestion

UDFs work at their best when data is fast to access. This section is on why this is needed, and how to prepare any data you'd like to use to be as efficient as possible when working with Fused.

## Documentation overview

<DocCardList />

================================================================================

## Ingest your own data
Path: core-concepts/data-ingestion/Ingest-your-data.mdx
URL: https://docs.fused.io/core-concepts/data-ingestion/Ingest-your-data/

_This guide explains how to use `fused.ingest` to geopartition and load vector tables into an S3 bucket so they can quickly be queried with Fused._

## Ingest vector data

To run an ingestion job on vector data we need:
1. **Input data** - This is could be CSV files, a `.zip` containing shapely files or any other sort of non partitioned data
2. **A cloud directory** - This is where we will save our ingested data and later access it through UDFs

We've built our own ingestion pipeline at Fused that partitions data based on dataset size & location.
Our ingestion process:

1. Uploads the `input`
2. Creates geo-partitions of the input data

This is defined with `fused.ingest()`:

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract",
)
```

Ingestion jobs often take more than a few seconds and require a lot of RAM (we open the whole dataset & re-partition it), which makes this a large run so we're going to use `run_remote()` so our ingestion job can take as long as needed.

To start an ingestion job, call `run_remote` on the job object returned by `fused.ingest`.

```python showLineNumbers
job_id = job.run_remote()
```

    Refer to the dedicated documentation page for `fused.ingest()` for more details on all the parameters

Ingested tables can easily be read with the Fused utility function `table_to_tile`, which spatially filters the dataset and reads only the chunks within a specified polygon.

```python showLineNumbers
@fused.udf
def udf(bounds, table="s3://fused-asset/infra/building_msft_us/"):
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    return utils.table_to_tile(bounds, table)
```

The following sections cover common ingestion implementations. It's recommended to run ingestion jobs from a Python Notebook or this web app.

### Ingest a table from a URL

Ingests a table from a URL and writes it to an S3 bucket specified with `fd://`.

```python showLineNumbers

job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract",
).run_remote()
```

If you encounter the message
`HTTPError: `, please contact Fused to increase the number of workers in your account.

### Ingest multiple files

```python showLineNumbers

job_id = fused.ingest(
    input=["s3://my-bucket/file1.parquet", "s3://my-bucket/file2.parquet"],
    output=f"fd://census/dc_tract",
).run_remote()
```

To ingest multiple local files, first upload them to S3 with fused.upload then specify an array of their S3 paths as the input to ingest.

### Row-based ingestion

Standard ingestion is row-based, where the user sets the maximum number of rows per chunk and file.

Each resulting table has one or more _files_ and each file has one or more _chunks_, which are spatially partitioned. By default, ingestion does a best effort to create the number of files specified by `target_num_files` (default `20`), and the number of rows per file and chunk can be adjusted to meet this number.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    explode_geometries=True,
    partitioning_method="rows",
    partitioning_maximum_per_file=100,
    partitioning_maximum_per_chunk=10,
).run_remote()
```

### Area-based ingestion

Fused also supports area-based ingestion, where the number of rows in each partition is
determined by the sum of their area.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_area",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
).run_remote()
```

### Geometry subdivision

Subdivide geometries during ingestion. This keeps operations efficient when geometries have many vertices or span large areas.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_geometry",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
    subdivide_start=0.001,
    subdivide_stop=0.0001,
    subdivide_method="area",
).run_remote()
```

### Ingest GeoDataFrame

Ingest a GeoDataFrame directly.

```python showLineNumbers
job_id = fused.ingest(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).run_remote()
```

### Ingest non-geospatial: `ingest_nongeospatial`

Ingest a table that doesn't have a spatial component.

```python showLineNumbers
job_id = fused.ingest_nongeospatial(
    input=df,
    output="s3://sample-bucket/file.parquet",
).run_remote()
```

### Ingest with a predefined bounding box schema: `partitioning_schema_input`

Here is an example of an ingestion using an existing partition schema which comes from a previously ingested dataset. This assumes you've already ingested a previous dataset with `fused.ingest()`.
This may be useful if you are analyzing data across a time series and want to keep the bounding boxes consistent throughout your analysis.

```python showLineNumbers
@fused.udf
def read_ingested_parquet_udf(path: str = "s3://sample-bucket/ingested_data/first_set/"):

    # Built in fused method to reach the `_sample` file and return the bounding boxes of each parquet
    df = fused.get_chunks_metadata(path)

    # Since we want our `partitioning_schema_input` specified in `ingest()` to be a link to a parquet file containing bounds coordinates, we will save this metadata as a parquet file
    partition_schema_path = path + 'boxes.parquet'
    df.to_parquet(partition_schema_path)

    return partition_schema_path

partition_schema_path = fused.run(read_ingested_parquet_udf)

job_id = fused.ingest(
    input="s3://sample-bucket/file.parquet",
    output="s3://sample-bucket/ingested_data/second_set/",
    partitioning_schema_input=partition_schema_path
).run_remote()
```

## Troubleshooting

As with other Fused batch jobs, ingestion jobs require server allocation for the account that initiates them. If you encounter the following error message, please contact the Fused team to request an increase.

```text
Error: `Quota limit: Number of running instances`
```

================================================================================

## Why we need Ingestion
Path: core-concepts/data-ingestion/why-ingestion.mdx
URL: https://docs.fused.io/core-concepts/data-ingestion/why-ingestion/

# Why we need data Ingestion

_This page will give you all the tools to make your data fast to read to make your UDFs more responsive._

## What is this page about?

The whole purpose of Fused is to speed up data science pipelines.
To make this happen we need the data we're working with to be responsive, regardless of the dataset. The ideal solution is to have all of our data sitting in RAM right next to our compute, but in real-world applications:

- Datasets (especially geospatial data) can be in the Tb or Pb range which rarely fit in storage, let alone RAM
- Compute needs to be scaled up and down depending on workloads.

One solution to this is to build data around **Cloud Optimized formats**: Data lives in the cloud but also leverages file formats that are fast to access. Just putting a `.zip` file that needs to be uncompressed at every read on an S3 bucket is still very slow. Our ingested data should be:

- **On the cloud** so dataset size doesn't matter (AWS S3, Google Cloud Storage, etc.)
- **Partitioned** (broken down into smaller pieces that are fast to retrieve so we can load only sections of the dataset we need)

This makes it fast to read for any UDF (and any other cloud operation), so developing UDFs in Workbench UDF Builder & running UDFs is a lot faster & responsive!

## Benchmark & Demo

We're going to use a real-world example to show the impact of using different file formats & partitioning to make data a lot faster to access. For this demo, we'll be using AIS (Automatic Identification System) data as for our Dark Vessel Detection example.
These are points which represent the location of boats at any given time. We'll be using free & open data from NOAA Digital Coast.

[Image: Dark Vessel Detection AIS]

The NOAA Digital Coast platform gives us 1 zip file per day with the location of every boat with an AIS transponder as CSV (once unzipped).

We'll download 1 day and upload it as a CSV to Fused server with `fused.api.upload()`:

```python showLineNumbers
@fused.udf
def save_ais_csv_to_fused_udf():
    """Downloading a single day of data to Fused server"""

    response = requests.get("https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/AIS_2024_01_01.zip")

    with open("data.zip", "wb") as f:
        f.write(response.content)

    with zipfile.ZipFile("data.zip", "r") as zip_ref:
        zip_ref.extractall("./data")

    csv_df = fused.api.upload("./data/AIS_2024_01_01.csv", "fd://demo_reading_ais/AIS_2024_01_01.csv")
    print(f"Saved data to fd://demo_reading_ais/")

    return pd.DataFrame()
```

And simply running this UDF:

```python
fused.run(save_ais_csv_to_fused_udf)
```

We can check that our CSV was properly ingested with File Explorer by navigating to `s3://fused-users/fused/demo_reading_ais/`

[Image: Our AIS CSV properly uploaded on File Explorer]

That's one big CSV.

But opening it on its own doesn't do all that much for us. We're going to create 3 UDFs to showcase a more real-world application: Opening the dataset and returning a subset inside a bounding box. We'll do this 3 different ways to compare their execution time:
1. From the default CSV
2. From the same data but saved a `.parquet`
3. Ingesting this data with `fused.ingest()` and reading it from our ingested data

Since our AIS data covers the waters around the US, we'll use a simple bounding box covering a small portion of water:

```python showLineNumbers

bounds = gpd.GeoDataFrame(geometry=[shapely.box(-81.47717632893792,30.46235012285108,-81.33723531132267,30.58447317149745)])
```

This `bounds` is purposefully small (`print(smaller_bounds.iloc[0].geometry.area)` returns `0.02`) to highlight loading a very large dataset and recover only a small portion of data.

### 1. Reading directly from CSV

Here's a simple UDF to read our CSV in memory and return only points in within our bounding box:

```python showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):

    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    df = pd.read_csv(path)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 2. Reading from Parquet

First we need to save our AIS data a `.parquet`:

```python showLineNumbers
@fused.udf
def ais_to_parquet():

    # S3 bucket & dir for demo purpose here. Replace with your own
    csv_df = pd.read_csv("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv")
    csv_df.to_parquet("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet")
    print(f"Saved data (as parquet) to fd://demo_reading_ais/")

    return pd.DataFrame()
```

Here's our updated UDF to read a `.parquet` file:

```python  showLineNumbers
@fused.udf
def from_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet"
):

    df = pd.read_parquet(path)
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 3. Reading from Fused Ingested GeoParquet

Now, we're going to ingest the parquet file we have to create geo-partitioned files, using `fused.ingest()`. We'll go in more details on how to ingest your own data in the following section.

```python showLineNumbers
job = fused.ingest(
    "fd://demo_reading_ais/AIS_2024_01_01.parquet",
    "fd://demo_reading_ais/ingested/",
    target_num_chunks=500, # 500 is a rough default to use in most cases. We're not optimizing this value for now
    lonlat_cols=('LON','LAT')
)
```

Ingestion jobs are quite memory hungry so we'll run this on a remote machine as a large run:

```python
job.run_remote(
    instance_type='r5.8xlarge', # 256GiB RAM machine
)
```

Again using File Explorer to inspect our data we can see `fused.ingest()` didn't create 1 file but rather multiple:

[Image: File Explorer view of ingested files]

Our ingestion process broke down our dataset into smaller files (making each file easier to access) and a `_sample.parquet` file containing the bounding box of each individual file. This allows us to first intersect our `bounds` with `_sample` and then only open the smaller `.parquet` files we need:

```python showLineNumbers
@fused.udf
def read_ingested_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/file_format_demo/ingested/"
):

    # convert bounds to tile
    common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = common_utils.estimate_zoom(bounds)
    tile = common_utils.get_tiles(bounds, zoom=zoom)

    # Built in fused method to reach the `_sample` file and return only bounding box of each parquet holding our points
    df = fused.get_chunks_metadata(path)

    # Only keeping the tiles where our bounds is -> Only need to load actual data inside / touching our bounds
    df = df[df.intersects(tile.geometry.iloc[0])]

    # This is based on Fused's ingestion process
    chunk_values = df[["file_id", "chunk_id"]].values
    rows_df = pd.concat([
        fused.get_chunk_from_table(path, fc[0], fc[1], columns=['geometry'])
        for fc in chunk_values
    ])
    df = rows_df[rows_df.intersects(tile.geometry[0])]
    df.crs = tile.crs
    return df
```

    We've implemented a utils function that allows you to more simply read Fused ingested data: table_to_tile

    So instead of re-implementing the above in 2 lines of code you can read your ingested data:
    ```python showLineNumbers
    @fused.udf
    def udf(
        bounds: fused.types.Bounds, path: str='s3://your-bucket/your-dir/demo_reading_ais/ingested/'):
        utils = fused.load('https://github.com/fusedio/udfs/tree/bb712a5/public/common/').utils
        df = utils.table_to_tile(bounds, table=path)
        return df
    ```

### Comparing all 3 runs

We'll run each of these 3 methods in a Jupyter notebook using the `%%time` magic command to compare their run time:

[Image: Comparing all 3 methods of reading AIS]

There are a few conclusions to draw here:
- simply saving a CSV to `.parquet` makes files smaller & significantly faster to read just by itself (4x speed gain in this example)
- But proper geo-partitioning takes those gains ever higher (additional 8x speed gain in this specific example)

In short, by ingesting our data with `fused.ingest()` we're trading some up-front processing time for much faster read time. We only need to ingest our data once and every subsequent read will be fast and responsive.

## When is ingestion needed?

You don't _always_ need to ingest your file into a cloud, geo-partitioned format. There are a few situation when it might be simpler & faster to just load your data.
Small files (< 100Mb ) that are fast to open (already in `.parquet` for example) that you only read once (note that it might be read 1x in your UDF but your UDF might be run many times)

Example of data you should ingest: 1Gb `.zip` of shapefile
- `.zip` means you need to unzip your file each time you open it and then read it. This slows down working with the data _every minute_. This results in each individual files (a CSV when unzipped) containing millions of points.
- shapefile contains multiple files, it isn't the fastest to read

Example of data you don't need to ingest: 50Mb `.parquet`
- Even if the data isn't geo-partitioned, loading this data should be fast enough to make any UDF fast

### Using cache as a single-use "ingester"

We could actually significantly speed up the above example where we loaded the AIS data as a CSV without running `fused.ingest()`, by using cache:

```python  showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.ViewportGDF,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):

    @fused.cache
    def load_csv(path):

        return pd.read_csv(path)
    df = load_csv(path)

    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]

    return bounds_ais
```

The first run of this would still be very slow, but running this a second time would give us a much faster result:

[Image: Comparing first CSV read to cached CSV read]

We're using `@fused.cache` to cache the result of `load_csv()` which is the same regardless of our `bounds`, so this allows us to save an 'intermediate' result on disk.
There are some limitations to this approach though:
- This cache is emptied after 24h.
- This cache is overwritten any time you change the cached function or its inputs

This approach is only helpful if you want to 1 time explore a new dataset in UDF Builder and don't want to wait around for the ingestion run to be done. Beyond that, this will end up being slower (and more frustrating)

================================================================================

## üê≥ On-prem
Path: core-concepts/onprem.mdx
URL: https://docs.fused.io/core-concepts/onprem/

Fused offers an on-prem version of the application in a Docker container. The container runs in your computing environment (such as AWS, GCP, or Azure) and your data stays under your control.

The container image is currently distributed via a private release. Email `info@fused.io` for access.

## Fused On-Prem Docker Installation Guide

[Image: On prem]

_Diagram of the System Architecture_

### 1. Install Docker

Follow these steps to install Docker on a bare-metal environment:

Step 1: Update System Packages

Ensure your system is up-to-date:
```bash
sudo apt update && sudo apt upgrade -y
```

Step 2: Start & Enable Docker
```bash
sudo apt install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable docker
sudo systemctl start docker
```

Step 3: Add Docker Permission to local user (after this command is run, the shell session must be restarted)
```bash
sudo usermod -G docker $(whoami)
```

Step 4: Configure Artifact Registry
```bash
gcloud auth configure-docker us-west1-docker.pkg.dev
```

### 2. Install Dependencies and Create Virtual Environment

Step 1: Install pip
```bash
sudo apt install python3-pip python3.11-venv
```

Step 2: Create virtual environment
```bash
python3 -m venv venv
```

Step 3: Activate virtual environment
```bash
source venv/bin/activate
```

Step 4: Install Fused and dependencies
```bash
pip install pandas ipython https://fused-magic.s3.us-west-2.amazonaws.com/fused-1.14.1.dev2%2B2c8d59a-py3-none-any.whl
```

### 3. Configure Fused on the Docker container

Run the following in a Python environment within the container to configure the on-prem profile. The Fused team will provide values specific to your account via secure communication.

```python showLineNumbers

fused.options.base_url = "***"
fused.options.auth.client_id = "***"
fused.options.auth.client_secret = "***"
fused.options.auth.audience = "***"
fused.options.auth.oauth_token_url = "***"
fused.options.auth.authorize_url = "***"

fused.options.save()
```

The code above only needs to be run once. After this is complete, Fused will use the local configuration for future batch jobs.

If Fused has already been configured for batch jobs, you may need to remove the local `~/.fused` directory before running the above code.

### 4. Authenticate an individual Fused user account

Step 1: Start a Python shell
```bash
python
```
Step 2: Obtain credentials URL

```python showLineNumbers

credentials = fused.api.NotebookCredentials()
credentials.url
```

Step 3:
Go to the credentials URL from the prior step in a web browser. Copy the code that is generated and paste into Python.
```python showLineNumbers
credentials.finalize(code="xxxxxxxxxxxxxxx")
```

### 5. Create Google Cloud service account key and add to Fused

Step 1:
In Google Cloud Console, go to `IAM & Admin > Service Accounts`. Select the service account you want to use, click on the three dots on the right, and select `Manage Keys`. Choose JSON and download the key.

Step 2:
Login to the Fused workbench environment settings. Click `Add new secret`. For name use `gcs_fused` and for value paste the contents of the JSON key file.

### 6. Run Fused API: Test UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

```python showLineNumbers
@fused.udf
def udf(datestr=0):

  loguru.logger.info(f'hello world ')
```

Step 2: Rename this UDF to "hello_world_udf" & Save

[Image: Hello World UDF]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF from Python

```python showLineNumbers

fused.api.FusedAPI()

my_udf = fused.load("hello_world_udf") # Make sure this is the same name as the UDF you saved
job = my_udf(arg_list=[1, 2])
fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1"
  ]
)

job_status = job.run_remote()
job_status.run_and_tail_output()
```

Optionally, to mount a filestore volume to the node that runs the job, add the following to the `additional_docker_args`. This assumes that filestore is mounted at `/mnt/cache` on the host machine.
```python showLineNumbers
additional_docker_args=["-v", "/mnt/cache:/mnt/cache"]
```

### 7. Run Fused API: Example with ETL Ingest UDF

Now that we've tested a simple UDF we can move to a more useful UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

You'll need a GCS Bucket to save this to, pass it to `bucket_name` in the UDF definition for now

```python  showLineNumbers
@fused.udf
def udf(datestr: str='2001-01-03', res:int=15, var='t2m', row_group_size:int=20_000, bucket_name:str):

  path_in=f'https://storage.googleapis.com/gcp-public-data-arco-era5/raw/date-variable-single_level//2m_temperature/surface.nc'
  path_out=f"gs:///data/era5/t2m/datestr=/0.parquet"

  if len(fused.api.list(path_out))>0:
    df = pd.DataFrame([])
    print("Already exists")
    return None

  def get_data(path_in, path_out):
    path = fused.download(path_in, path_in)
    xds = xarray.open_dataset(path)
    df = xds[var].to_dataframe().unstack(0)
    df.columns = df.columns.droplevel(0)
    df['hex'] = df.index.map(lambda x:h3.api.basic_int.latlng_to_cell(x[0],x[1],res))
    df = df.set_index('hex').sort_index()
    df.columns=[f'hour' for hr in range(24)]
    df['daily_min'] = df.iloc[:,:24].values.min(axis=1)
    df['daily_max'] = df.iloc[:,:24].values.max(axis=1)
    df['daily_mean'] = df.iloc[:,:24].values.mean(axis=1)
    return df

  df = get_data(path_in, path_out)

  memory_buffer = io.BytesIO()
  table = pa.Table.from_pandas(df)
  pq.write_table(table, memory_buffer, row_group_size=row_group_size, compression='zstd', write_statistics=True)
  memory_buffer.seek(0)

  gcs = gcsfs.GCSFileSystem(token=json.loads(fused.secrets['gcs_fused']))
  with gcs.open(path_out, "wb") as f:
    f.write(memory_buffer.getvalue())

  print(df.shape)
  return None
```

Step 2: Rename this UDF to "ETL_Ingest"

[Image: Ingest ETL in workbench]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF

```python showLineNumbers

fused.api.FusedAPI()

udf = fused.load("ETL_ingest")
start_datestr='2020-02-01'; end_datestr='2020-03-01';
arg_list = pd.date_range(start=start_datestr, end=end_datestr).strftime('%Y-%m-%d').tolist()
job = udf(arg_list=arg_list)

fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1", "-v", "./.fused:/root/.fused"
  ]
)

job_status = job.run_remote()
job_status.run_and_tail_output()
```

## Commands

### `run-config`

`run-config` runs the user's jobs. The job configuration can be specified either on the command line, as a local file path, or as an S3/GCS path. In all cases the job configuration is loaded as JSON.

```
Options:
  --config-from-gcs FILE_NAME   Job step configuration, as a GCS path
  --config-from-s3 FILE_NAME    Job step configuration, as a S3 path
  --config-from-file FILE_NAME  Job step configuration, as a file name the
                                application can load (i.e. mounted within the
                                container)
  -c, --config JSON             Job configuration to run, as JSON
  --help                        Show this message and exit.
```

### `version`

Prints the container version and exits.

## Environment Variables

The on-prem container can be configured with the followin environment variables.

- `FUSED_AUTH_TOKEN`: Fused token for the licensed user or team. When using the FusedDockerAPI, this token is automatically retrieved.
- `FUSED_DATA_DIRECTORY`: The path to an existing directory to be used for storing temporary files. This can be the location a larger volume is mounted inside the container. Defaults to Python's temporary directory.
- `FUSED_GCP`: If "true", enable GCP specific features. Defaults to false.
- `FUSED_AWS`: If "true", enable AWS specific features. Defaults to false.
- `FUSED_AWS_REGION`: The current AWS region.
- `FUSED_LOG_MIN_LEVEL`: Only logs with this level of severity or higher will be emitted. Defaults to "DEBUG".
- `FUSED_LOG_SERIALIZE`: If "true", logs will be written in serialized, JSON form. Defaults to false.
- `FUSED_LOG_AWS_LOG_GROUP_NAME`: The CloudWatch Log Group to emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_LOG_AWS_LOG_STREAM_NAME`: The CloudWatch Log Stream to create and emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_PROCESS_CONCURRENCY`: The level of process concurrency to use. Defaults to the number of CPU cores.
- `FUSED_CREDENTIAL_PROVIDER`: Where to obtain AWS credentials from. One of "default" (default to ec2 on AWS, or none otherwise), "none", "ec2" (use the EC2 instance metadata), or "earthdata" (use EarthData credentials in `FUSED_EARTHDATALOGIN_USERNAME` and `FUSED_EARTHDATALOGIN_PASSWORD`).
- `FUSED_EARTHDATALOGIN_USERNAME`: Username when using earthdata credential provider, above.
- `FUSED_EARTHDATALOGIN_PASSWORD`: Password when using earthdata credential provider, above.
- `FUSED_IGNORE_ERRORS`: If "true", continue processing even if some computations throw errors. Defaults to false.
- `FUSED_DISK_SPACE_GB`: Maximum disk space available to the job, e.g. for temporary files on disk, in gigabytes.

## Connecting an encrypted S3 bucket

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket. The following steps are required to connect an encrypted S3 bucket:

- Configure KMS policy
```json
,
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

- Configure S3 bucket policy
```json
,
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3        "arn:aws:s3      ]
    }
  ]
}
```

================================================================================

## dependencies
Path: core-concepts/run-udfs/dependencies.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/dependencies/

# Dependencies

To keep things simple, Fused maintains a single runtime image. This means that any UDF you run will be executed with these dependencies by default

## UDF Dependencies

The Python packages are listed below and can also be found in this public `.txt` file.

Get in touch to have a package added to the list of dependencies or to learn about private runtime images for your organization.

## [BETA] Install your own dependencies

The simplest way to add your own library is to run the dedicated Package Management Fused app. This app allows you to create a different environment and add any module you'd like

You need to make sure you have access to Fused Apps to be able to run this.

[Image: Beta package management app]

You'll then need to import the environment path in your UDF:

```python showLineNumbers
@fused.udf
def udf():

  sys.path.append(f"/mount/envs/demo_env/lib/python3.11/site-packages/")

  # Logic using your dedicated package
  return
```

================================================================================

## Run UDFs
Path: core-concepts/run-udfs/index.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/

# Running UDFs

There are 2 main ways to run UDFs:

1. On short, small resources requirements (RAM / CPU / storage) jobs 
2. On longer, higher resources requirements jobs 

## Documentation overview

<DocCardList />

================================================================================

## Large UDF run
Path: core-concepts/run-udfs/large_jobs.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/large_jobs/

Some jobs require more resources than a few Gb of RAM or take more than a few seconds to run. This section will show how to run such larger jobs

### Defining "Large" jobs

Large jobs are jobs to run UDFs that are:
- Longer than 120s to run
- Require large resources (more than a few Gb of RAM)

To run these we use higher-latency instances than for small jobs, but with the ability to specific RAM, CPU count & storage depending on the needs.

This is useful for example when running large ingestion jobs which can require a lot of RAM & storage

### A Simple UDF to demonstrate

We'll use the same UDF as in the running multiple small jobs section:

```python showLineNumbers
@fused.udf
def udf(val):

    return pd.DataFrame()
```

As mentioned in the Small UDF job section, to call it 1 time we can use `fused.run()`:

```python showLineNumbers
fused.run(udf, val=1)
```

## Running a large job: `job.run_remote()`

Running a UDF over a large instance is done in 2 steps:
1. Creating a job with the UDF to run and passing input parameters to run the UDF over
2. Send the job to a remote instance and (optionally) defining the instance arguments

```python showLineNumbers
# We'll run this UDF 5 times with 5 different inputs
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

### Passing UDF parameters with `arg_list`

**Single Parameter**

As mentioned above to pass UDF arguments to a remote job, use `arg_list` to specify a `list` of inputs to run your job over:

```python showLineNumbers
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

**Multiple Parameters**

Currently `arg_list` only supports giving 1 input variables to each UDF. We can work around this by aggregating multiple variables into a `dict` and having a UDF take a `dict` as input:

```python showLineNumbers
@fused.udf
def udf(variables: dict = ):

    # Retrieving each variables from the dictionary
    val1 = variables['val1']
    val2 = variables['val2']

    # Some simple boilerplate logic
    output_value = int(val1)*int(val2)
    df = pd.DataFrame(data=)

    # Saving output to shared file location to access results later
    # `/mnt/cache` is the shared file location for all small & large jobs
    df.to_csv(f"/mnt/cache/demo_multiple_arg_list_run/output_.csv")

    return df
```

  You do need to type the input variable for this to work. If we had defined `variables` without typing it as a `dict`:
  ```python showLineNumbers
  @fused.udf
  def udf(variables = ):
    # Notice the lack of `variables: dict = `
    ...
    return df
  ```
  our remote job run would have fail as Fused server has no way of knowing what to expect from `variables`

We can then call this UDF as a remote job by passing a list of dictionaries to `arg_list`:
```python showLineNumbers
job = udf(arg_list=[, ])
job.run_remote()
```

We can confirm this worked by viewing our results by browsing File Explorer:

[Image: Run remote multi arg]

### `run_remote` instance arguments

With `job.run_remote()` you also have access to a few other arguments to make your remote job fit your needs:

- `instance_type`: Decide which type of machine to run your job on (see below for which ones we support)
- `disk_size_gb`: The amount of disk space in Gb allocated to your instance (between `16` and `999` Gb)

For example if you want a job with 16 vCPUs, 64Gb of RAM and 100Gb of storage you can call:
```python showLineNumbers
job.run_remote(instance_type="m5.4xlarge", disk_size_gb=100)
```

  Fused `run_remote()` `instance_type` are based around AWS General Purpose instance types. We do support the following:

  ```python showLineNumbers
  supported_instance_types = [
    "m5.large",
    "m5.xlarge",
    "m5.2xlarge",
    "m5.4xlarge",
    "m5.8xlarge",
    "m5.12xlarge",
    "m5.16xlarge",
    "r5.large",
    "r5.xlarge",
    "r5.2xlarge",
    "r5.4xlarge",
    "r5.8xlarge",
    "r5.12xlarge",
    "r5.16xlarge",
  ]
  ```

    Running `job.run_remote()` in a notebook gives you a clickable link:

    [Image: run remote notebook]

    Under the "Jobs" tab, on the bottom left of Workbench:

    Each job leads to an email summary with logs upon completion:

    [Image: run remote email]

    A common use case for offline jobs is as a "pre-ingestion" process. You can find a real-life example of this in our dark vessel detection example

    Here all we're returning is a status information in a pandas dataframe, but the our data in unzipped, read and saved to S3:

    ```python showLineNumbers

    @fused.udf()
    def read_ais_from_noaa_udf(datestr='2023_03_29'):

        url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler//AIS_.zip'
        # This is our local mount file path,
        path=f'/mount/AIS_demo//'
        daily_ais_parquet = f'/.parquet'

        # Download ZIP file to mounted disk
        r=requests.get(url)
        if r.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
                with z.open(f'AIS_.csv') as f:
                    df = pd.read_csv(f)

                    # highlight-next-line
                    df.to_parquet(daily_ais_parquet)
            return pd.DataFrame()
        else:
            return pd.DataFrame(']})
    ```

    Data written to `/mount/` can be accessed by any other instance used by anyone on your team so it can be used by any other UDF you run after.

          You can use File Explorer to easily see your outputs! In this case of the above example typing `efs://AIS_.csv` (and replacing `datestr` with your date) will show the results directly in File Explorer!
    
</details>

### Large jobs trade-offs

- Takes a few seconds to startup machine
- Can run as long as needed

## Example use cases

You can explore examples of how we're using `run_remote()` in some of our other examples:
- Scaling a large ingestion job of AIS point data
- Ingesting a large amount of cropland data for zonal statistics.

================================================================================

## Small UDF run
Path: core-concepts/run-udfs/run.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/run/

Fused UDF functions really shine once you start calling them from anywhere. You can call small jobs in 2 main ways:
1.  `fused.run()` in Python. All you need is the `fused` Python package installed
    - Useful when wanting to run UDF as part of another pipeline, inside another UDF or anywhere in Python / code.
2.  **HTTP call** from *anywhere*
    - Useful when you want to call a UDF outside of Python. For example receiving dataframes into Google Sheets or plotting points and images in a Felt map

### Defining "Small" job

"Small" jobs are defined as any job being:
- Less than 120s to execute
- Using less than a few Gb of RAM to run

These jobs run in "real-time" with no start-up time so are quick to run, but with limited resources and time-out if taking too long.

## `fused.run()`

`fused.run()` is the simplest & most common way to execute a UDF from any Python script or notebook.

The simplest way to call a public UDF is using a public UDF name and calling it as: `UDF_` + name. Let's take this UDF that returns the location of the Eiffel Tower in a `GeoDataFrame` as an example:

```python showLineNumbers

fused.run("UDF_Single_point_Eiffel_Tower")
```

[Image: Simple UDF fused.run() returning a geodataframe]

There are a few other ways to run a UDF:

- By name from your account
- By public UDF name
- Using a token
- Using a `udf` object
- From Github URL
- From git commit hash (most recommended for teams)

### _Name_ (from your account)

_When to use: When calling a UDF you made, from your own account._

You can call any UDFs you have made simply by referencing it by name (given when you save a UDF).

(Note: This requires authentication)

[Image: Hello World UDF]

This UDF can then be run in a notebook locally (granted that you have authenticated):

```python showLineNumbers
fused.run("Hello_World_bbox")
```

[Image: Running Hello World UDF]

### _Name_ (from your teammate's account)

_When to use: When calling a UDF someone on your team made, from your own account._

Similarly, you can reference by name and run any UDFs under your teammates' accounts. Simply prefix the UDF name with the person's email address, separated by a `/`.

```python showLineNumbers
fused.run("teammate@fused.io/Hello_World_bbox")
```

Note that both your and your teammate's accounts must belong to the same organization.

### _Public UDF Name_

_When to run: Whenever you want to run a public UDF for free from anywhere_

Any UDF saved in the public UDF repo can be run for free.

Reference them by prefixing their name with `UDF_`. For example, the public UDF `Get_Isochrone` is run with `UDF_Get_Isochrone`:

```python showLineNumbers
fused.run('UDF_Get_Isochrone')
```

### _Token_

_When to use: Whenever you want someone to be able to execute a UDF but might not want to share the code with them._

You can get the token from a UDF either in Workbench (Save your UDF then click "Share") or returning the token in Python.

Here's a toy UDF that we want others to be able to run, but we don't want them to see the code:

```python showLineNumbers

@fused.udf()
def my_super_duper_private_udf(my_udf_input):

    # This code is so private I don't want anyone to be able to read it
    return pd.DataFrame()
```

We then need to save this UDF to Fused server to make it accessible from anywhere.

```python showLineNumbers
my_super_duper_private_udf.to_fused()
```

`my_udf.to_fused()` saves your UDF to your personal user UDFs. These are private to you and your team. You can create a token than anyone (even outside your team) can use to run your UDF but by default these UDFs are private.

We can create a token for this `my_super_duper_private_udf` and share it:

    `fused.submit(udf)` runs all the UDF calls in parallel, making it a helpful tool to run multiple UDFs all at once.

    We can demonstrate this by adding a simple `time.sleep(1)` in our original UDF:
    ```python  showLineNumbers
    @fused.udf
    def udf(val):

        time.sleep(1)
        return pd.DataFrame()
    ```

    In a notebook, we can time how long each cell takes to execute with the `%%time` magic command

    ```python showLineNumbers
    # In a jupyter notebook
    %%time
    fused.run(udf, val=1)
    ```

    [Image: Singe run]

    This takes 2s: A few ms of overhead to send the UDF to Fused server & run + 1s of `time.sleep(1)`

    Now using `fused.submit()` to run this over 50 UDFs:

    [Image: 30 runs]

    This takes a few more seconds, but not 100s. `fused.submit()` is a helpful way to scale a single UDF to many inputs in a timely manner.

</details>

### Example use cases

`fused.submit()` is used in many places across our docs, here are some examples:

- ‚õ¥Ô∏è In the Dark Vessel Detection example to scale retrieving daily AIS `.zip` files from NOAA over 30 days.
- üõ∞Ô∏è Retrieving all of Maxar's Open Data STAC Catalogs across every events they have imagery for.
- üí° Check the Best Practices for more on when to use `submit()` and when to use other methods.

## HTTP requests

In the UDF Builder, you can create an HTTP endpoint for a UDF in the "Snippets" section. This generates a unique URL to call the UDF via HTTP requests. The URL is scoped to that UDF only and it can be revoked to disable access. The same can be done with the Fused Python SDK.

### Shared token

To run a UDF via HTTP request, generate a shared token and use the provided URL. Manage your account's shared tokens in fused.io/profile#tokens.

<ReactPlayer playsinline className="video__player" loop playing= muted controls height="100%" width="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/shared_token_edit3.mp4" />

Structure the URL with the `file` path parameter to run as a single batch operation.

```
https://www.fused.io/server/v1/realtime-shared/******/run/file?dtype_out_raster=png
```

To integrate with a tiling service, structure the URL with the `tiles` path parameter, followed by templated `///` path parameters. See our integration guides for examples.

```
https://www.fused.io/server/v1/realtime-shared/******/run/tiles///?dtype_out_raster=png
```

### Private token

Calling UDFs with Bearer authentication requires an account's private token. The URL structure to run UDFs with the private token varies slightly, as the URL specifies the UDF's name and the owner's user account.

```bash
curl -XGET "https://app.fused.io/server/v1/realtime/fused/api/v1/run/udf/saved/user@fused.io/caltrain_live_location?dtype_out_raster=png" -H "Authorization: Bearer $ACCESS_TOKEN"
```

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

### Specify parameters

When UDF endpoints are called via HTTP requests argument values are specified with query parameters, which require input parameters to be serializable. As such, the UDF should specify the types to cast them to. Read more about supported types for UDF parameters.

### Response data types

The `dtype_out_vector` and `dtype_out_raster` parameters define the output data type for vector tables and raster arrays, respectively.

- The supported types for vector tables are `parquet`, `geojson`, `json`, `feather`, `csv`, `mvt`, `html`, `excel`, and `xml`.
- For raster array: `png`, `gif`, `jpg`, `jpeg`, `webp`, `tif`, and `tiff`.

```
https://www.fused.io/server/v1/realtime-shared/****/run/file?dtype_out_raster=png
```

Read how to structure HTTP endpoints to call the UDF as a Map Tile & File.

## Caching responses

If a UDF's cache is enabled, its endpoints cache outputs for each combination of code and parameters. The first call runs and caches the UDF, and subsequent calls return cached data.

================================================================================

## Why UDFs
Path: core-concepts/why.mdx
URL: https://docs.fused.io/core-concepts/why/

At Fused, our mission is to help get things done, fast. We want every team to be able to get from **Analytics to Action** as quickly as they can. 

## Our core believes

We believe every Analytics team should have the tools to:
- üåç Answer the big picture problems first;
- üí° Iterate on their analysis when new data & algorithms becomes available;
- üèÉ Ship a first version rather than getting it perfect;
- üìä Visualize & report their work to anyone in their team.

## User Defined Functions

A lot of the tools Analytics teams have today slow the process down:
- Python dependency management gets in the way of getting work done.
- A lot of the scientific Python tooling focused more on getting the result down to 10 decimals places rather than answering the big picture

That's why we built Fused around User Defined Functions, UDFs. 

[Image: udf pipeline]

UDFs are the DNA of analytics. They are Python functions that can be called from anywhere:
- üêç No environment setup: Just start writing Python immediately.
- üîó Shareable as HTTPS endpoints in 2 clicks: Ship your work to the rest of the team
- üîÑ Iterable: Edit your code, Save, and see the results downstream immediately.
- üöÄ Scales with your hardware requirements: From running a subset of data to analyzing the entire world.

## UDFs are the DNA of analytics

Making every process of your Analytics a UDF makes it faster:

- **Data needs to be ingested constantly**: UDFs can be edited as datasets change & evolve. They get updated when you save them. 
- **New algorithms come and go**: UDFs allow you to iterate on existing data and swap out just what you need.
- **Reporting & Visualization evolve**: UDFs can take your analysis and render it in dynamic ways.

## From your laptop to the World

Look, we know that many Analytics project start in a notebook on a laptop. 

- üíª Start by running UDFs locally, then in 2 lines of code scale to datasets the size of the world
- üåç UDFs can be called from anywhere: From a notebook, a frontend application or integration platforms
- üîÄ Work locally or in Workbench, our browser based IDE interchangeably  

## Efficiently Scaling

Because Fused is built 
- ‚òÅÔ∏è Serverless computing: Only pay for the processing you actually use
- ‚ö°Ô∏è Caching makes recurring calls faster & cheaper

## Get started using UDFs right now

Check out:
- ‚ö°Ô∏è The Quickstart guide: Learn how to use UDFs in 5 minutes
- üìö UDF Core Concepts: Everything you need to know about building & using UDFs
- üéì Our Examples: Real world examples of how to use UDFs

================================================================================

## Write UDFs
Path: core-concepts/write.mdx
URL: https://docs.fused.io/core-concepts/write/

[Image: udf anatomy]

Follow these steps to write a User Defined Function (UDF).

- Decorate a function with `@fused.udf`
- Declare the function logic
- Optionally cache parts of the function
- Set typed parameters to dynamically run based on inputs
- Import utility modules to keep your code organized
- Return a vector table or raster
- Save the UDF

## `@fused.udf` decorator

First decorate a Python function with `@fused.udf` to tell Fused to treat it as a UDF.

## Function declaration

Next, structure the UDF's code. Declare import statements within the function body, express operations to load and transform data, and define a return statement. This UDF is called `udf` and returns a `pd.DataFrame` object.

```python showLineNumbers
@fused.udf # <- Fused decorator
# highlight-start
def udf(name: str = "Fused"): # <- Function declaration

    return pd.DataFrame(!']})
# highlight-end
```
The UDF Builder in Workbench imports the `fused` module automatically. To write UDFs outside of Workbench, install the Fused Python SDK with `pip install fused` and import it with `import fused`.

Placing import statements within a UDF function body (known as "local imports") is not a common Python practice, but there are specific reasons to do this when constructing UDFs. UDFs are distributed to servers as a self-contained units, and each unit needs to import all modules it needs for its execution. UDFs may be executed across many servers (10s, 100s, 1000s), and any time lost to importing unused modules will be multiplied.

An exception to this convention is for modules used for function annotation, which need to be imported outside of the function being annotated.

## `@fused.cache` decorator

Use the @fused.cache decorator to persist a function's output across runs so UDFs run faster.

```python showLineNumbers
@fused.udf # <- Fused decorator
def udf(bounds: fused.types.Bounds = None, name: str = "Fused"):

    # highlight-start
    @fused.cache # <- Cache decorator
    def structure_output(name):
        return pd.DataFrame(!']})
    # highlight-end

    df = structure_output(name)
    return df
```

## Typed parameters

UDFs resolve input parameters to the types specified in their function annotations.
This example shows the `bounds` parameter typed as `fused.types.Bounds`
and `name` as a string.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, # <- Typed parameters
    name: str = "Fused"
):
```

To write UDFs that run successfully as both `File` and `Tile`, set `bounds` as the first parameter, with `None` as its default value. This enables the UDF to be invoked successfully both as `File` (when `bounds` isn't passed) and as `Tile`. For example:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    ...
    return ...
```

### Supported types

Fused supports the native Python types `int`, `float`, `bool`, `list`, `dict`, and `list`. Parameters without a specified type are handled as strings by default.

The UDF Builder runs the UDF as a Map Tile if the first parameter is typed as `fused.types.Bounds`.

### `pd.DataFrame` as JSON

Pass tables and geometries as serialized UDF parameters in HTTPS calls. Serialized JSON and GeoJSON parameters can be casted as a `pd.DataFrame` or `gpd.GeoDataFrame`. Note that while Fused requires import statements to be declared within the UDF signature, libraries used for typing must be imported at the top of the file.

```python showLineNumbers

@fused.udf
def udf(
    gdf: gpd.GeoDataFrame = None,
    df: pd.DataFrame = None
):
```

## Reserved parameters

When running a UDF with `fused.run`, it's possible to specify the map tile Fused will use to structure the `bounds` object by using the following reserved parameters.

### With `x`, `y`, `z` parameters

```python showLineNumbers
fused.run("UDF_Overture_Maps_Example", x=5241, y=12662, z=15)
```

### Passing a `GeoDataFrame`
```python showLineNumbers

bounds = gpd.GeoDataFrame.from_features(,"geometry":,"id":1}]})
fused.run("UDF_Overture_Maps_Example", bounds=bounds)
```

### Passing a bounding box list

You can also pass a list of 4 points representing `[min_x, min_y, max_x, max_y]`

```python showLineNumbers
fused.run('UDF_Overture_Maps_Example', bounds=[-122.349, 37.781, -122.341, 37.818])
```

## `utils` Module

Define a UDF's `utils` Module file in the Workbench "Module" tab and import it in the UDF. Use it to modularize code to make it readable, maintainable, and reusable.

```python showLineNumbers
from utils import function
```

### Import utils from other UDFs

UDFs import the `utils` Module from other UDFs with `fused.load` in the UDFs GitHub repo or private GitHub repos. Here the commit SHA `05ba2ab` pins `utils` to specific commit for version control.

```python showLineNumbers
utils = fused.load(
    "https://github.com/fusedio/udfs/tree/05ba2ab/public/common/"
)
```

`utils` Module are imported from other UDFs in a user's account.

```python showLineNumbers
utils = fused.load("your@email.com/my_udf").utils
```

## `return` object

UDFs can return the following objects:

- Tables: `pd.DataFrame`, `pd.Series`, `gpd.GeoDataFrame`,  `gpd.GeoSeries`, and `shapely geometry`.
- Arrays: `numpy.ndarray`, `xarray.Dataset`, `xarray.DataArray`, and `io.BytesIO`. Fused Workbench only supports the rendering of `uint8` arrays. Rasters without spatial metadata should indicate their tile bounds.
- Simple Python objects: `str`, `int`, `float`, `bool`.
- Dictionaries: `dict`. Useful to return dictionaries of raster numpy array for example.

## Save UDFs

UDFs exported from the UDF Builder or saved locally are formatted as a `.zip` file containing associated files with the UDFs code, `utils` Module, metadata, and `README.md`.

```
‚îî‚îÄ‚îÄ Sample_UDF
    ‚îú‚îÄ‚îÄ README.MD       # Description and metadata
    ‚îú‚îÄ‚îÄ Sample_UDF.py   # UDF code
    ‚îú‚îÄ‚îÄ meta.json       # Fused metadata
    ‚îî‚îÄ‚îÄ utils.py        # `utils` Module
```

### In Python: `.to_fused()`

When outside of Workbench, save UDF to your local filesystem with `my_udf.to_directory('Sample_UDF')` and to the Fused cloud with `my_udf.to_fused()`.

This will allow you to access your UDF using a token, from a Github commit or directly importing it in Workbench from the Github URL

### In Workbench: Saving through Github

You can also save your UDFs directly through GitHub as personal, team or community UDF. Check out the Contribute to Fused to see more.

## Update tags and metadata

Modify the UDF's metadata to manage custom tags that persist across the local filesystem, the Fused Cloud, and your team's GitHub repo.

```python showLineNumbers
# Assumging my_udf was loaded or created above
my_udf.metadata['my_company:tags']=['tag_1', 'tag_2']

# Push to Fused
my_udf.to_fused()

# You can reload your UDF and see the updated metadata
fused.load('my_udf').metadata
```

## Debug UDFs

#### UDF builder

A common approach to debug UDFs is to show intermediate results in the UDF Builder results panel with `print` statements.

#### HTTP requests

When using HTTP requests, any error messages are included in the `X-Fused-Metadata` response header. These messages can be used to debug. To inspect the header on a browser, open the Developer Tools network tab.

[Image: network]

================================================================================

# TUTORIALS

## Advanced Fused Features
Path: tutorials/fused-advanced.mdx
URL: https://docs.fused.io/tutorials/fused-advanced/

# Advanced Fused Features

Install the `fused` Python package locally:

```bash
pip install "fused[all]"
```

Read more about installing Fused.

Run the following code snippets in a notebook for best interactivity!

### Run 100s of jobs in seconds: `fused.submit()`

Calculate the average prices of houses over the years:

```python

@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
    housing['price_per_area'] = housing['price'] / housing['area'].round(2)

    return housing['price_per_area'].mean()

# List all available files
housing_lists = fused.api.list('s3://fused-sample/demo_data/housing/')

results = fused.submit(
    udf, 
    housing_lists[10:] # Running only first 10 files for demo purposes
)
```

Print results:

```python
results

>>> 
path	
s3://fused-sample/demo_data/housing/housing_1970.csv	1376.804413
s3://fused-sample/demo_data/housing/housing_1971.csv	1492.044819
s3://fused-sample/demo_data/housing/housing_1972.csv	1484.496548
```

Read more about running your code in parallel here

### Batch jobs

Run larger jobs that exceed the memory & time constraints of a single UDF job.

```python
@fused.udf
def udf(val):

    # Simulating a complex processing step
    time.sleep(200) 

    return pd.DataFrame()
    
# Running with 5 inputs
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

Read more about setting up batch jobs here

### Preview large files from your laptop

Use Fused to sample large datasets for local exploration. The data loads inside your UDF and only sends you the sample.

Using the ERA5 weather dataset as an example (~140MB file):
```python
@fused.udf
def udf(path: str='s3://fused-asset/data/era5/t2m/datestr=2024-01-01/0.parquet'):

    df = pd.read_parquet(path)

    return df.head(10)
```

Get a small sample of data:

```python
sample = fused.run(udf)
```

================================================================================

## Quick Wins with Fused
Path: tutorials/fused-quick-wins.mdx
URL: https://docs.fused.io/tutorials/fused-quick-wins/

# Quick Wins with Fused

Quick ways to get started with Fused in Workbench.

### Open a simple dataset

```python
@fused.udf
def udf(path = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
```

### Convert data quickly

```python
@fused.udf
def udf():

    df = pd.read_excel("s3://fused-sample/demo_data/housing_2024.xlsx")
    df.to_parquet("s3://fused-sample/demo_data/housing_2024.parquet")
    
    return df
```

### Turn any data into an API

Turn any code into an API (similar to our Quickstart):

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
    housing['price_per_area'] = round(housing['price'] / housing['area'], 2)
    
    return housing[['price', 'price_per_area']]
```

### Preview datasets

Explore cloud storage directly in the File Explorer tab

<LazyReactPlayer 
  className="video__player"
  playing=
  muted=
  controls
  height="100%"
  width="100%"
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/File_explorer_from_file.mp4"
/>

### Profile your code

Workbench's built-in profiler provides:
- Line by line runtime
- Memory usage
- Total runtime
- Total file size

<LazyReactPlayer 
  className="video__player"
  playing=
  muted=
  controls
  height="100%"
  width="100%"
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/profiling_lines_and_memory.mp4"
/>

### Data First IDE: Table / Map view

Workbench is built for data teams. 

Toggle between Map/Table view based on your data!

<LazyReactPlayer 
  className="video__player"
  playing=
  muted=
  controls
  height="100%"
  width="100%"
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/Map_Table_alternating.mp4"
/>

### Return HTML code & make a chart!

You can return HTML code directly in Fused and turn your data into an embeded Chart:

```python
@fused.udf
def udf(path = "s3://fused-sample/demo_data/housing/housing_2024.csv"):
    from fastapi import Response

    housing = pd.read_csv(path)
    housing['price_per_area'] = round(housing['price'] / housing['area'], 2)
    
    chart_html = alt.Chart(housing).mark_point().encode(
        x='price',
        y='price_per_area'
    ).to_html()

    return Response(chart_html.encode('utf-8'), media_type="text/html")
```

Go to the "Embed" tab to see your chart:

[Image: HTML Chart Example]

================================================================================

## best-practices
Path: tutorials/Geospatial with Fused/best-practices.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/best-practices/

# Best Practices

### Explore data in "Map View"

While you might be tempted to explore a specific row of a `GeoDataFrame` by filtering it and printing it, sometimes it faster to just click on it in Map View. Once selected, use the tooltip copy icon to copy the object as JSON, which you can then inspect elsewhere.

### Tilt Map view to explore 3D datasets

Map View gives you a top-level view of the world by default. But hold `Cmd` (or `Ctrl` on Windows / Mac) to tilt the view!

[Image: Tilted View]

üèòÔ∏è This can be really helpful to explore 3D datasets like in this DSM Zonal Stats UDF.

You can reset the view by running the "Reset 3D view to top-down" shortcut from Command Palette
:::

================================================================================

## Map Tile/Single
Path: tutorials/Geospatial with Fused/filetile.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/filetile/

# Displaying Spatial Data: Tile vs Single (Viewport, Parameter)

When spatial UDFs are called (i.e. that return spatial data like a `GeoDataFrame` or an array of `GeoTiff` tiles), they run and return the output of the execution like any UDF. However they can be called in two ways that influence how Fused handles them: `Tile` and `Single`.

This is an important distinction and can be changed in Workbench at the top of the UDF editor:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/File_Tile_Viewport2.mp4" width="100%" />

We'll demonstrate the differences with a simple UDF that takes a `bounds` object as input and display how it behaves differently in each mode:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds):

    from shapely.geometry import box
    
    # Returning bounds to gdf
    gdf = gpd.GeoDataFrame(geometry=[box(*bounds)], crs=4326)
    return gdf
```

### Single (Viewport)

Selecting `Single (Viewport)` mode, the UDF runs within a single call.

If you run the example UDF, make sure to select `Single (Viewport)` mode in the UDF editor, execute it with "Shift + Enter" and zoom out you'll see that `gdf` covers the viewport you had:

[Image: File (Viewport)]

**We generally recommend using `Single (Viewport)` mode when :**
- Working with smaller datasets that fit into memory
- Wanting to load data that doesn‚Äôt move each time you pan around
- Wanting 1 seamless layer without tiling artifacts
- Wanting data at a specific resolution

### Tile

By contrast, `Tile` mode runs your UDF multiple times over a grid of Mercator Tiles that cover the viewport. We aim to use anywhere from 2 to 15 tiles to cover the current viewport. 

Looking at the same UDF and selecting `Tile` mode, we get many different tiles:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/Tile_bounds_udf.mp4" width="100%" />

You can see a few other differences:
- This UDF is called each time you move around the Map Viewport
- We need to "Freeze" the viewport for it to stop rendering. This is in contrast to `Single (Viewport)` mode where the UDF is called once and panning in Map View doesn't re-run the UDF

**We generally recommend using `Tile` mode when :**
- Loading a lot of data at once (since the UDF is called multiple times over a smaller extent each time)
- Wanting a more dynamic, responsive panning & scrolling experience
- Wanting a dynamic resolution to be calculated for your image rendering based on your current zoom level

The mode you select for a UDF is saved with the UDF. You can decide which mode you prefer people to use for your UDF by selecting the mode in the UDF editor.

## The `bounds` object

A UDF may use the `bounds` parameter to spatially filter datasets and load into memory only the data that corresponds to the `bounds` spatial bounds. This reduces latency and data transfer costs.

Cloud-optimized formats are particularly suited for these operations - they include Cloud Optimized GeoTiff, Geoparquet, and GeoArrow.

### `bounds` object types

The `bounds` object defines the spatial bounds of the Tile, which can be represented as a geometry object or XYZ index.

#### `fused.types.Bounds`

This is a `list` of 4 points representing the bounds (extent) of a geometry. The 4 points represent `[xmin, ymin, xmax, ymax]` of the bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    print(bounds)

>>> [-1.52244399, 48.62747869, -1.50004107, 48.64359255]
```

`fused.types.Bounds` is a list of 4 points so it might be helpful to convert it to a `GeoDataFrame` when returning spatial data:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    box = shapely.box(*bounds)
    return gpd.GeoDataFrame(geometry=[box], crs=4326)
```

The `fused` module also comes with many handy utils functions that allow you to quickly access these common operations. For example, you can use the `bounds_to_gdf` utility function to perform the same operation as above. You can also use `estimate_zoom` to estimate the zoom level that matches the bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    utils = fused.load("https://github.com/fusedio/udfs/tree/e74035a1/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    zoom = utils.estimate_zoom(bounds)
    print(zoom)
    return bounds
```

### Legacy types

These types are still currently supported in `fused` though only for legacy reasons and will soon be deprecated.

#### [Legacy] `fused.types.Tile`

This is a geopandas.GeoDataFrame with `x`, `y`, `z`, and `geometry` columns.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Tile=None):
    print(bounds)

>>>      x    y   z                                           geometry
>>> 0  327  790  11  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `fused.types.TileGDF`

This behaves the same as `fused.types.Tile`.

#### [Legacy] `fused.types.ViewportGDF`

This is a geopandas.geodataframe.GeoDataFrame with a `geometry` column corresponding to the Polygon geometry of the current viewport in the Map.
```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.ViewportGDF=None):
    print(bbox)
    return bbox

>>>  geometry
>>>  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `bbox` object

UDFs defined using the legacy keyword `bbox` are automatically now mapped to `bounds`. Please update your code to use `bounds` directly as this alias will be removed in a future release.

## Call HTTP endpoints

A UDF called via an HTTP endpoint is invoked as `Single` or `Tile`, depending on the URL structure.

### Single endpoint

This endpoint structure runs a UDF as a `Single`. See implementation examples with Felt and Google Sheets for vector.

```bash
https://www.fused.io/server/.../run/file?dtype_out_vector=csv
```

   In some cases, `dtype_out_vector=json` may return an error. This can happen when a GeoDataFrame without a `geometry` column is being return or a Pandas DataFrame. You can bypass this by using `dtype_out_vector=geojson`.

### Tile endpoint

This endpoint structure runs a UDF as a `Tile`. The `//` templated path parameters correspond to the Tile's XYZ index, which Tiled web map clients dynamically populate. See implementation examples for Raster Tiles with Felt and DeckGL, and for Vector Tiles with DeckGL and Mapbox.

```bash
https://www.fused.io/server/.../run/tiles///?&dtype_out_vector=csv
```

## Tile UDF behavior in `fused.run()`

UDFs behave different when using `fused.types.Tile` than in any other case. When passing a `gpd.GeoDataFrame`. `shapely.Geometry` to bounds:

```python showLineNumbers
fused.run(tile_udf, bounds=bounds)
```

Or passing `X Y Z`:

```python showLineNumbers
fused.run(tile_udf, x=1, y=2, z=3)
```

The `tile_udf` gets tiled and run on Web mercator XYZ tiles and then combined back together to speed up processing rather than executing a single run. This is in contrast to most other UDFs (either using no `bounds` input at all or using `bounds: fused.types.Bounds`) which run a single run across the given input.

================================================================================

## gee_bigquery
Path: tutorials/Geospatial with Fused/gee_bigquery.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/gee_bigquery/

# Google Earth Engine & BigQuery

## Google Earth Engine

Fused interfaces Google Earth Engine with the Python `earthengine-api` library. This example shows how to load data from GEE datasets into Fused UDFs and read it with xarray.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/gee_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from Google Earth Engine

Create a UDF to load data from a GEE ImageCollection and open it with xarray. Authenticate by passing the key file path to `ee.ServiceAccountCredentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, n=10):

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils

    # Authenticate GEE
    key_path = '/mnt/cache/gee_creds.json'
    credentials = ee.ServiceAccountCredentials("fused-account@fused-gee.iam.gserviceaccount.com", key_path)
    ee.Initialize(opt_url="https://earthengine-highvolume.googleapis.com", credentials=credentials)

    # Generate GEE bounding box for spatial filter
    geom = ee.Geometry.Rectangle(*bounds.total_bounds)
    scale = 1 / 2 ** max(0, bounds.z[0])  # A larger scale will increase your resolution per z but slow down the loading

    # Load data from a GEE ImageCollection
    ic = ee.ImageCollection("MODIS/061/MOD13A2").filter(
        ee.Filter.date("2023-01-01", "2023-06-01")
    )

    # Open with xarray (the `xee` package must be present for engine="ee" to work)
    ds = xarray.open_dataset(ic, engine="ee", geometry=geom, scale=scale).isel(time=0)

    # Transform image color with a utility function
    arr = utils.arr_to_plasma(ds["NDVI"].values.squeeze().T, min_max=(0, 8000))
    return arr

```

## BigQuery

Fused integrates with Google BigQuery with the Python `bigquery` library.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/bq_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from BigQuery

Create a UDF to perform a query on a BigQuery dataset and return the results as a DataFrame or GeoDataFrame. Authenticate by passing the key file path to `service_account.Credentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, geography_column=None):
    from google.cloud import bigquery
    from google.oauth2 import service_account

    # This UDF will only work on runtime with mounted EFS
    key_path = "/mnt/cache/bq_creds.json"

    # Authenticate BigQuery
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )

    # Create a BigQuery client
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # Structure spatial query
    query = f"""
        SELECT * FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`
        LIMIT 10
    """

    if geography_column:
        return client.query(query).to_geodataframe(geography_column=geography_column)
    else:
        return client.query(query).to_dataframe()
```

================================================================================

## index
Path: tutorials/Geospatial with Fused/index.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/

# Geospatial Analysis in Fused

Welcome to geospatial analysis with Fused! This section covers everything you need to know to work with geospatial data in Fused.

<DocCardList className="DocCardList--no-description"/>

================================================================================

## other-integrations
Path: tutorials/Geospatial with Fused/other-integrations.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/other-integrations/

# Other Integrations

This section shows how to integrate Fused UDFs with popular mapping and visualization tools.

## DeckGL

DeckGL is a highly framework to create interactive map visualizations that handle large datasets.

This guide shows how to load data from Fused into DeckGL maps created from a single standalone HTML page.
### Setup

1. First create a UDF and generate an HTTP endpoint.

2. Create an `.html` file following this template. This code creates a DeckGL map then introduces a layer that renders data from the specified Fused endpoint.

```html

```

### H3HexagonLayer

Create an `H3HexagonLayer`.

```js
new H3HexagonLayer(),
```

### Vector Tile Layer

Vector Tile layers are created by placing a `GeoJsonLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a vector layer.

The layer in the sample map comes from Overture Buildings UDF.

```js
new TileLayer(//?dtype_out_vector=geojson",
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new GeoJsonLayer(props, );
  },
});
```

### Raster Tile Layer

Raster Tile layers are created by placing a `BitmapLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a raster layer. The sample layer below was created from the NAIP Tile UDF.

```js
new TileLayer(//?dtype_out_raster=png`,
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new BitmapLayer(props, );
  },
  pickable: true,
});
```

Learn more about DeckGL

## Felt

Felt is a collaborative mapping platform for creating interactive maps. Load Fused data directly via URLs.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `dtype_out_raster=png`
   - Replace path with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?dtype_out_raster=png
```

3. In Felt, click "Upload from URL" and paste the modified URL

### Vector Data

1. Create a UDF that returns vector data
2. Generate a shared URL and modify it:
   - Set `dtype_out_vector=csv` or `dtype_out_vector=parquet`
   - Add UDF parameters as needed

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?dtype_out_vector=csv&param1=value1
```

3. In Felt, click "Upload from URL" and paste the URL

Learn more about Felt

## Kepler

Kepler is an open source tool for visualizing large geospatial datasets. The Fused UDF Builder provides direct integration with Kepler.

### Usage

1. Create a UDF that returns vector data
2. In the UDF Builder, click "Open in Kepler.gl" on the top-right menu
3. Wait for data transfer and click "Open in Kepler.gl" in the bottom-right

This opens your data directly in Kepler for advanced visualization and analysis.

Learn more about Kepler

//?dtype_out_raster=png",
    tile_size=512,
    zoom_offset=-1,
)
m.add_layer(tile_layer)
m
```

### Vector Tiles

```python

m = ipyleaflet.Map(center=(37.7749, -122.4194), zoom=17)

# Add vector tile layer
vector_layer = ipyleaflet.VectorTileLayer(
    url="https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?dtype_out_vector=mvt"
)
m.add_layer(vector_layer)
m
```

Learn more about Leaflet */}

## Mapbox

Mapbox GL JS creates interactive web maps. Load Fused data using tile sources.

### Basic Setup

- Generate a Mapbox token

```html

```

### Vector Tiles

```html

```

### Raster Tiles

```html

```

Learn more about Mapbox GL JS

## QGIS

QGIS is an open source desktop GIS platform. Load Fused data as raster tiles, vector tiles, or vector files.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `dtype_out_raster=png`
   - Replace with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?dtype_out_raster=png
```

3. In QGIS: Right-click "XYZ Tiles" ‚Üí "New Connection"
4. Paste the URL and configure the layer

### Vector Tiles

1. Create a UDF that returns vector tiles
2. Generate a shared URL with `dtype_out_vector=mvt`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?dtype_out_vector=mvt
```

3. In QGIS: Right-click "Vector Tiles" ‚Üí "New Connection"
4. Paste the URL and configure the layer

### Vector Files

1. Create a UDF that returns vector data
2. Generate a shared URL with `dtype_out_vector=geojson`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?dtype_out_vector=geojson
```

3. In QGIS: Layer ‚Üí Add Layer ‚Üí Add Vector Layer
4. Paste the URL as the data source

Learn more about QGIS

## Related 

- Generate HTTP endpoints for your UDFs
- Check the Fused catalog for ready-to-use UDFs

================================================================================

## processing-statistics
Path: tutorials/Geospatial with Fused/processing-statistics.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/processing-statistics/

# Processing & Statistics

### Buffer analysis

```python
@fused.udf
def udf(n_points: int=1000, buffer: float=0.0025):

    from shapely.geometry import Point, Polygon, LineString

    # Create LineString to represent a road
    linestring_columbus = LineString([[-122.4194,37.8065],[-122.4032,37.7954]])
    gdf_road = gpd.GeoDataFrame()

    # Create Points to represent GPS pings
    minx, miny, maxx, maxy = gdf_road.total_bounds
    points = [Point(random.uniform(minx, maxx), random.uniform(miny, maxy)) for _ in range(n_points)]
    gdf_points = gpd.GeoDataFrame()

    # Create a buffer around the road
    buffered_polygon = gdf_road.buffer(buffer)

    # Color the points that fall within the buffered polygon
    points_within = gdf_points[gdf_points.geometry.within(buffered_polygon.unary_union)]
    gdf_points = gdf_points.loc[points_within.index]

    return gdf_points
```

'),
                        'mean_elevation_m': round(mean_elevation, 2) if not np.isnan(mean_elevation) else None
                    })
                except Exception:
                    results.append('),
                        'mean_elevation_m': None
                    })
    
    # Create results GeoDataFrame
    results_gdf = gpd.GeoDataFrame(results, geometry=gdf_stations.geometry, crs=4326)
    
    return results_gdf
``` */}

================================================================================

## read-data
Path: tutorials/Geospatial with Fused/read-data.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/read-data/

# Read Data

Common examples for reading geospatial data in Fused.

## Python Packages

### `geopandas`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/subway_stations.geojson"):

    return gpd.read_file(path)
```

### `shapely`

```python
@fused.udf
def udf():

    from shapely.geometry import Point, Polygon
    
    # Create geometries with shapely
    points = [Point(-122.4, 37.8), Point(-122.3, 37.7)]
    polygon = Polygon([(-122.5, 37.7), (-122.3, 37.7), (-122.3, 37.9), (-122.5, 37.9)])
    
    gdf = gpd.GeoDataFrame(
        ,
        geometry=points + [polygon],
        crs=4326
    )
    
    return gdf
```

### `duckdb`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.parquet"):

    conn = duckdb.connect()
    result = conn.execute(f"""
        SELECT * 
        FROM ''
        WHERE latitude IS NOT NULL
        LIMIT 1000
    """).df()
    
    return result
```

### `rioxarray`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/elevation.tif"):

    # Read raster data with rioxarray
    raster = rxr.open_rasterio(path)
    
    # Convert to DataFrame for display
    df = raster.to_dataframe().reset_index()
    
    return df.head(1000)
```

### `xarray`

```python
@fused.udf
def udf():

    # Download NetCDF data to mount disk for proper reading
    path = fused.download('s3://fused-sample/demo_data/2025_01_01_ERA5_surface.nc','2025_01_01_ERA5_surface.nc')
    ds = xr.open_dataset(path)
    
    # Convert to DataFrame
    df = ds.to_dataframe().reset_index()
    
    return df.head(1000)
```

## Table Formats (Vector)

### GeoJSON (.geojson, .json)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.geojson"):

    return gpd.read_file(path)
```

### Shapefile (.shp + .shx, .dbf, .prj)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_shapefile.shp"):

    return gpd.read_file(path)
```

### GeoPackage (.gpkg)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_geopackage.gpkg"):

    return gpd.read_file(path)
```

### KML/KMZ (.kml, .kmz)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.kml"):

    return gpd.read_file(path)
```

")
    
    # Read specific layer
    return gpd.read_file(path, layer=0)
``` */}

### Parquet (.parquet)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/buildings.parquet"):

    return gpd.read_parquet(path)
```

### CSV with coordinates (.csv)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.csv"):

    from shapely.geometry import Point
    
    # Read CSV
    df = pd.read_csv(path)
    
    # Convert to GeoDataFrame
    gdf = gpd.GeoDataFrame(
        df, 
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs=4326
    )
    
    return gdf
```

### Excel (.xlsx)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.xlsx"):

    from shapely.geometry import Point
    
    # Read Excel file
    df = pd.read_excel(path)
    
    # Convert to GeoDataFrame if coordinates exist
    if 'longitude' in df.columns and 'latitude' in df.columns:
        gdf = gpd.GeoDataFrame(
            df,
            geometry=gpd.points_from_xy(df.longitude, df.latitude),
            crs=4326
        )
        return gdf
    
    return df
```

## Array Formats (Raster)

### GeoTIFF (.tif, .tiff)

```python
@fused.udf
def udf(
    path: str = 's3://fused-sample/demo_data/satellite_imagery/wildfires.tiff'
):

    with rasterio.open(path) as src:
        data = src.read()
        bounds = src.bounds

    return data, bounds
```

### NetCDF (.nc)

```python
@fused.udf
def udf():

    # Download to mount disk for proper NetCDF reading
    path = fused.download('s3://fused-sample/demo_data/climate_data.nc', 'climate_data.nc')
    
    # Open NetCDF dataset
    ds = xr.open_dataset(path)
    
    return ds.to_dataframe().reset_index().head(1000)
```

)
``` */}

### STAC Catalog

#### Earth on AWS

```python
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-77.083, 38.804, -76.969, 38.983],
):

    odc.stac.configure_s3_access(aws_unsigned=True)
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

    # Loading Elevation model
    items = catalog.search(
        collections=["cop-dem-glo-30"], 
        bbox=bounds
    ).item_collection()

    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)

    return xarray_dataset["data"], bounds
```

#### Microsoft Planetary Computer

```python
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-122.463,37.755,-122.376,37.803],
):

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        modifier=planetary_computer.sign_inplace,
    )

    # Loading Elevation model
    items = catalog.search(collections=["cop-dem-glo-30"],bbox=bounds).item_collection()
    
    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)
    
    return xarray_dataset["data"], bounds
```

================================================================================

## visualization
Path: tutorials/Geospatial with Fused/visualization.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/visualization/

# Visualization

## Within Workbench

- Map View
- Layer Styling

## Outside of Workbench

### Make a Map Tile Server in seconds

This example works best in Workbench

```python
@fused.udf
def udf(
   bounds: fused.types.Bounds,
   path: str="s3://fused-asset/data/tiger/state/tl_rd22_us_state 1pct.parquet"
):

   df = gpd.read_parquet(path)
   df = df.cx[bounds[0]:bounds[2], bounds[1]:bounds[3]]
   df['area'] = df['geometry'].area.round(2)
   return df
```

- Switch to using a Tile UDF
- Save your UDF
- Create a new shared token
- Copy the HTTPS endpoint

Edit the tile coordinates to use `//`:
```
https://.../run/tiles///?dtype_out_vector=parquet
```

Connect this anywhere to deliver a map tile server!

================================================================================

## write-data
Path: tutorials/Geospatial with Fused/write-data.mdx
URL: https://docs.fused.io/tutorials/Geospatial with Fused/write-data/

# Write Data

When working with Fused we recommend you save your files in 2 formats: Parquet & Cloud Optimized GeoTIFF (COG).

Read more about why we recommend those formats.

### Table: to parquet

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
    housing['price_per_area'] = round(housing['price'] / housing['area'], 2)
    
    processd_data = housing[['price', 'price_per_area']]

    # Saving to user specific location
    username = fused.api.whoami()['handle']
    output_path = f"s3://fused-users/fused//housing_2024_processed.parquet"
    processd_data.to_parquet(output_path)

    return f"File saved to "
```

### Array: to Cloud Optimized GeoTIFF (COG)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/satellite_imagery/wildfires.tiff"):

    # Read the raster data
    with rasterio.open(path) as src:
        data = src.read()
        profile = src.profile
    
    # Process the data
    processed_data = np.where(data > np.percentile(data, 80), 255, 0).astype(np.uint8)
    
    # Update profile for writing
    profile.update()
    
    # Write to Fused's shared disk (accessible to all UDFs in org)
    username = fused.api.whoami()['handle']
    output_path = f"/mnt/cache/wildfires_processed_.tif"
    
    with rasterio.open(output_path, 'w', **profile) as dst:
        dst.write(processed_data)
    
    return f"File saved to shared disk at "
```

### Geo-partitioning large datasets: `fused.ingest()`

Large geospatial data might not be optimally formatted or partitioned. Fused offers a simple way to ingest your data at scale.

```python
# Get your user handle 
user = fused.api.whoami()['handle']

# Ingest Washington DC Census data
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd:///data/census/partitioned/", # Saving to your Fused bucket
)

job.run_remote()
```

You can tail logs to see how the job is progressing:

```python
fused.api.job_tail_logs("your-job-id")
```

Learn more about Fused data ingestion

================================================================================

## Load & Export Data
Path: tutorials/load_and_save_data.mdx
URL: https://docs.fused.io/tutorials/load_and_save_data/

# Load & Export Data 

Common examples for loading and saving data in Fused.

## Load Data

### `pandas`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.csv"):

    return pd.read_csv(path)
```

### `duckdb`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.parquet"):

    conn = duckdb.connect()
    result = conn.execute(f"""
        SELECT * 
        FROM ''
        LIMIT 10
    """).df()
    
    return result
```

### From other UDFs

```python
@fused.udf
def udf(bounds: fused.types.Bounds):
    overture_udf = fused.load('https://github.com/fusedio/udfs/tree/main/public/Overture_Maps_Example/')
    buildings = fused.run(overture_udf, bounds=bounds, theme='buildings', overture_type='building')
 
    return buildings
```

### Download data to shared Fused mount

```python
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    return str(out_path)
```

Files will be written to `/mount/tmp/`, where any other UDF can then access them.

Read more about `fused.download()` here

## Export Data

### Fused managed storage: `fd://`

```python
df.to_parquet("fd://my-dataset/data.parquet")
```

Read more about about `fd://` S3 Bucket

### Fused mount disk: `/mnt/cache`

```python
df.to_parquet("/mnt/cache/data.parquet")
```

Read more about about `/mnt/cache` mount disk

### AWS S3: `s3://`

```python
df.to_parquet("s3://my-bucket/data.parquet")
```

### Google Cloud Storage: `gcs://`

```python
df.to_parquet("gcs://my-bucket/data.parquet")
```

### Use as API (No file saving required)

You can directly call your UDFs as APIs, removing the need to even save your data at all!

After creating a Shared Token for your UDF, you can change the output format of your HTTPS endpoint:

```bash
https://fused.io/.../run/file?dtype_out_vector=json
```

**Tabular data downloads:**

```bash
?dtype_out_vector=csv          # CSV download
?dtype_out_vector=geojson      # GeoJSON download
?dtype_out_vector=parquet      # Parquet download
?dtype_out_vector=json         # JSON download
?dtype_out_vector=mvt          # Mapbox Vector Tile download
```

**Image data downloads:**

```bash
?dtype_out_raster=png          # PNG image
?dtype_out_raster=tiff         # GeoTIFF download
```

## Integrations

Call your UDFs from other tools after creating a Shared Token for your UDF:

### DuckDB

```bash
select * from read_parquet('https://fused.io/.../run/file?');
```

### Curl

```bash
curl -L -XGET 'https://fused.io/.../run/file?'
```

### Google Sheets

```bash
=importData('https://fused.io/.../run/file?')
```

### Notion

- Use `/embed` block with UDF endpoint: `'https://fused.io/.../run/file?'`

================================================================================

# PYTHON SDK

## fused.api
Path: python-sdk/api-reference/api.mdx
URL: https://docs.fused.io/python-sdk/api-reference/api/

## fused.api.whoami

```python
whoami()
```

Returns information on the currently logged in user

---

## fused.api.delete

```python
delete(path: str, max_deletion_depth: int | Literal['unlimited'] = 3) -> bool
```

Delete the files at the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì Directory or file to delete, like `fd://my-old-table/`
- **max_deletion_depth** (<code>int | Literal['unlimited']</code>) ‚Äì If set (defaults to 3), the maximum depth the operation will recurse to.
  This option is to help avoid accidentally deleting more data that intended.
  Pass `"unlimited"` for unlimited.

**Examples:**

```python
fused.api.delete("fd://bucket-name/deprecated_table/")
```

---

## fused.api.list

```python
list(path: str, *, details: bool = False) -> list[str] | list[ListDetails]
```

List the files at the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì Parent directory URL, like `fd://bucket-name/`
- **details** (<code>bool</code>) ‚Äì If True, return additional metadata about each record.

**Returns:**

- <code>list\[str\] | list\[ListDetails\]</code> ‚Äì A list of paths as URLs, or as metadata objects.

**Examples:**

```python
fused.api.list("fd://bucket-name/")
```

---

## fused.api.get

```python
get(path: str) -> bytes
```

Download the contents at the path to memory.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>bytes</code> ‚Äì bytes of the file

**Examples:**

```python
fused.api.get("fd://bucket-name/file.parquet")
```

---

## fused.api.download

```python
download(path: str, local_path: str | Path) -> None
```

Download the contents at the path to disk.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`
- **local_path** (<code>str | Path</code>) ‚Äì Path to a local file.

---

## fused.api.upload

```python
upload(
    local_path: str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame,
    remote_path: str,
    timeout: float | None = None,
) -> None
```

Upload local file to S3.

**Parameters:**

- **local_path** (<code>str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame</code>) ‚Äì Either a path to a local file (`str`, `Path`), a (Geo)DataFrame
  (which will get uploaded as Parquet file), or the contents to upload.
  Any string will be treated as a Path, if you wish to upload the contents of
  the string, first encode it: `s.encode("utf-8")`
- **remote_path** (<code>str</code>) ‚Äì URL to upload to, like `fd://new-file.txt`
- **timeout** (<code>float | None</code>) ‚Äì Optional timeout in seconds for the upload (will default to `OPTIONS.request_timeout` if not specified).

**Examples:**

To upload a local json file to your Fused-managed S3 bucket:

```py
fused.api.upload("my_file.json", "fd://my_bucket/my_file.json")
```

---

## fused.api.sign_url

```python
sign_url(path: str) -> str
```

Create a signed URL to access the path.

This function may not check that the file represented by the path exists.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>str</code> ‚Äì HTTPS URL to access the file using signed access.

**Examples:**

```python
fused.api.sign_url("fd://bucket-name/table_directory/file.parquet")
```

---

## fused.api.sign_url_prefix

```python
sign_url_prefix(path: str) -> dict[str, str]
```

Create signed URLs to access all blobs under the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a prefix, like `fd://bucket-name/some_directory/`

**Returns:**

- <code>dict\[str, str\]</code> ‚Äì Dictionary mapping from blob store key to signed HTTPS URL.

**Examples:**

```python
fused.api.sign_url_prefix("fd://bucket-name/table_directory/")
```

---

## fused.api.get_udfs

```python
get_udfs(
    n: int = 10,
    *,
    skip: int = 0,
    per_request: int = 25,
    max_requests: int | None = None,
    by: Literal["name", "id", "slug"] = "name",
    whose: Literal["self", "public"] = "self"
) -> dict
```

Fetches a list of UDFs.

**Parameters:**

- **n** (<code>int</code>) ‚Äì The total number of UDFs to fetch. Defaults to 10.
- **skip** (<code>int</code>) ‚Äì The number of UDFs to skip before starting to collect the result set. Defaults to 0.
- **per_request** (<code>int</code>) ‚Äì The number of UDFs to fetch in each API request. Defaults to 25.
- **max_requests** (<code>int | None</code>) ‚Äì The maximum number of API requests to make.
- **by** (<code>Literal['name', 'id', 'slug']</code>) ‚Äì The attribute by which to sort the UDFs. Can be "name", "id", or "slug". Defaults to "name".
- **whose** (<code>Literal['self', 'public']</code>) ‚Äì Specifies whose UDFs to fetch. Can be "self" for the user's own UDFs or "public" for
  UDFs available publicly. Defaults to "self".

**Returns:**

- <code>dict</code> ‚Äì A list of UDFs.

**Examples:**

Fetch UDFs under the user account:

```py
fused.api.get_udfs()
```

---

## fused.api.job_get_logs

```python
job_get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> ‚Äì Log messages for the given job.

---

## fused.api.job_print_logs

```python
job_print_logs(
    job: CoerceableToJobId, since_ms: int | None = None, file: IO | None = None
) -> None
```

Fetch and print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- **file** (<code>IO | None</code>) ‚Äì Where to print logs to. Defaults to sys.stdout.

**Returns:**

- <code>None</code> ‚Äì None

---

## fused.api.job_tail_logs

```python
job_tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = True,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) ‚Äì how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) ‚Äì if true, print out only a sample of logs. Defaults to True.
- **timeout** (<code>float | None</code>) ‚Äì if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

## fused.api.job_get_status

```python
job_get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

## fused.api.job_cancel

```python
job_cancel(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì A new job object.

---

## fused.api.job_get_exec_time

```python
job_get_exec_time(job: CoerceableToJobId) -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns:**

- <code>timedelta</code> ‚Äì Time the job took. If the job is in progress, time from first to last log message is returned.

---

## fused.api.job_wait_for_job

```python
job_wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until this job has finished

**Parameters:**

- **poll_interval_seconds** (<code>float</code>) ‚Äì How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) ‚Äì The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> ‚Äì if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

## fused.api.FusedAPI

```python
FusedAPI(
    *,
    base_url: str | None = None,
    set_global_api: bool = True,
    credentials_needed: bool = True
)
```

API for running jobs in the Fused service.

Create a FusedAPI instance.

**Other Parameters:**

- **base_url** (<code>str | None</code>) ‚Äì The Fused instance to send requests to. Defaults to `https://www.fused.io/server/v1`.
- **set_global_api** (<code>bool</code>) ‚Äì Set this as the global API object. Defaults to True.
- **credentials_needed** (<code>bool</code>) ‚Äì If True, automatically attempt to log in. Defaults to True.

---

### create_udf_access_token

```python
create_udf_access_token(
    udf_email_or_name_or_id: str | None = None,
    /,
    udf_name: str | None = None,
    *,
    udf_email: str | None = None,
    udf_id: str | None = None,
    client_id: str | Ellipsis | None = ...,
    public_read: bool | None = None,
    access_scope: str | None = None,
    cache: bool = True,
    metadata_json: dict[str, Any] | None = None,
    enabled: bool = True,
) -> UdfAccessToken
```

Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.

**Parameters:**

- **udf_email_or_name_or_id** (<code>str | None</code>) ‚Äì A UDF ID, email address (for use with udf_name), or UDF name.
- **udf_name** (<code>str | None</code>) ‚Äì The name of the UDF to create the token for.

**Other Parameters:**

- **udf_email** (<code>str | None</code>) ‚Äì The email of the user owning the UDF, or, if udf_name is None, the name of the UDF.
- **udf_id** (<code>str | None</code>) ‚Äì The backend ID of the UDF to create the token for.
- **client_id** (<code>str | Ellipsis | None</code>) ‚Äì If specified, overrides which realtime environment to run the UDF under.
- **cache** (<code>bool</code>) ‚Äì If True, UDF tiles will be cached.
- **metadata_json** (<code>dict\[str, Any\] | None</code>) ‚Äì Additional metadata to serve as part of the tiles metadata.json.
- **enable** ‚Äì If True, the token can be used.

---

### upload

```python
upload(path: str, data: bytes | BinaryIO, timeout: float | None = None) -> None
```

Upload a binary blob to a cloud location

---

### start_job

```python
start_job(
    config: JobConfig | JobStepConfig,
    *,
    instance_type: WHITELISTED_INSTANCE_TYPES | None = None,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: Sequence[str] | None = ("FUSED_CREDENTIAL_PROVIDER=ec2",),
    image_name: str | None = None
) -> RunResponse
```

Execute an operation

**Parameters:**

- **config** (<code>JobConfig | JobStepConfig</code>) ‚Äì the configuration object to run in the job.

**Other Parameters:**

- **instance_type** (<code>WHITELISTED_INSTANCE_TYPES | None</code>) ‚Äì The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- **region** (<code>str | None</code>) ‚Äì The AWS region in which to run. Defaults to None.
- **disk_size_gb** (<code>int | None</code>) ‚Äì The disk size to specify for the job. Defaults to None.
- **additional_env** (<code>Sequence\[str\] | None</code>) ‚Äì Any additional environment variables to be passed into the job, each in the form KEY=value. Defaults to None.
- **image_name** (<code>str | None</code>) ‚Äì Custom image name to run. Defaults to None for default image.

---

### get_jobs

```python
get_jobs(
    n: int = 5,
    *,
    skip: int = 0,
    per_request: int = 25,
    max_requests: int | None = 1
) -> Jobs
```

Get the job history.

**Parameters:**

- **n** (<code>int</code>) ‚Äì The number of jobs to fetch. Defaults to 5.

**Other Parameters:**

- **skip** (<code>int</code>) ‚Äì Where in the job history to begin. Defaults to 0, which retrieves the most recent job.
- **per_request** (<code>int</code>) ‚Äì Number of jobs per request to fetch. Defaults to 25.
- **max_requests** (<code>int | None</code>) ‚Äì Maximum number of requests to make. May be None to fetch all jobs. Defaults to 1.

**Returns:**

- <code>Jobs</code> ‚Äì The job history.

---

### get_status

```python
get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

### get_logs

```python
get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> ‚Äì Log messages for the given job.

---

### tail_logs

```python
tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = False,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) ‚Äì how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) ‚Äì if true, print out only a sample of logs. Defaults to False.
- **timeout** (<code>float | None</code>) ‚Äì if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

### wait_for_job

```python
wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until the given job has finished

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **poll_interval_seconds** (<code>float</code>) ‚Äì How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) ‚Äì The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> ‚Äì if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

### cancel_job

```python
cancel_job(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì A new job object.

---

### auth_token

```python
auth_token() -> str
```

Returns the current user's Fused environment (team) auth token

---

================================================================================

## fused.core
Path: python-sdk/api-reference/core.mdx
URL: https://docs.fused.io/python-sdk/api-reference/core/

## `run_tile`

```python showLineNumbers
def run_tile(email: str,
             id: Optional[str] = None,
             *,
             x: int,
             y: int,
             z: int,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_shared_tile`

```python showLineNumbers
def run_shared_tile(token: str,
                    *,
                    x: int,
                    y: int,
                    z: int,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    _client_id: Optional[str] = None,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_file`

```python showLineNumbers
def run_file(email: str,
             id: Optional[str] = None,
             *,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF.

---
## `run_shared_file`

```python showLineNumbers
def run_shared_file(token: str,
                    *,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  The response from the server after executing the operation on the file.

**Raises**:

- `Exception` - Describes various exceptions that could occur during the function execution, including but not limited to invalid parameters, network errors, unauthorized access errors, or server-side errors.

This function is designed to access shared operations that require a token for authorization. It requires network access to communicate with the server hosting these operations and may incur data transmission costs or delays depending on the network's performance.

---
## `run_tile_async`

```python showLineNumbers
async def run_tile_async(
        email: str,
        id: Optional[str] = None,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to asynchronously run a UDF on a specific tile defined by its x, y, and z coordinates. It supports customization of the output data types for vector and raster data, and accommodates additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - User's email address. Used to identify the user's saved UDFs. If the ID is not provided, the email is also used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF on the specified tile and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_tile_async`

```python showLineNumbers
async def run_shared_tile_async(
        token: str,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared tile-based UDF using a specific access token.

This function constructs a URL for running an operation on a tile, defined by its x, y, and z coordinates, accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation on the specified tile.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the specified tile and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---
## `run_file_async`

```python showLineNumbers
async def run_file_async(
        email: str,
        id: Optional[str] = None,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a file-based UDF associated with the specific email and ID.

This function constructs a URL to run a UDF on a server, allowing for output data type customization for vector and raster outputs and supporting additional parameters for the UDF execution. If no ID is provided, the user's email is used as the identifier.

**Arguments**:

- `email` _str_ - The user's email address, used to identify the user's saved UDFs. If the ID is not provided, this email will also be used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the function fetches the user's email as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_file_async`

```python showLineNumbers
async def run_shared_file_async(
        token: str,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared file-based UDF using the specific access token.

Constructs a URL to run an operation on a file accessible via a shared token, enabling customization of the output data types for vector and raster data. It accommodates additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the file and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

================================================================================

## fused.ingest
Path: python-sdk/api-reference/job.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/api-reference/job/

## `fused.ingest`

```python showLineNumbers
def ingest(
    input: Union[str, Sequence[str], Path, gpd.GeoDataFrame],
    output: Optional[str] = None,
    *,
    output_metadata: Optional[str] = None,
    schema: Optional[Schema] = None,
    file_suffix: Optional[str] = None,
    load_columns: Optional[Sequence[str]] = None,
    remove_cols: Optional[Sequence[str]] = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: Optional[bool] = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: Union[int, float, None] = None,
    partitioning_maximum_per_chunk: Union[int, float, None] = None,
    partitioning_max_width_ratio: Union[int, float] = 2,
    partitioning_max_height_ratio: Union[int, float] = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: Optional[float] = None,
    subdivide_stop: Optional[float] = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: Optional[Tuple[str, str]] = None,
    gdal_config: Union[GDALOpenConfig, Dict[str, Any], None] = None
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Arguments**:

- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

        ```python showLineNumbers
        fused.ingest(
            ...,
            lonlat_cols=("x", "y")
        )
        ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        ```python showLineNumbers
        fused.ingest(
            ...,
            gdal_config=
            }
        )
        ```

**Returns**:

Configuration object describing the ingestion process. Call `.run_remote` on this object to start a job.

**Examples**:

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).run_remote()
```

---
#### `job.run_remote`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_remote` on this object to start the ingestion job.

```python
def run_remote(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Begin job execution.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_remote()` returns a `RunResponse` object which has the following methods:

#### `get_status`

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

---

#### `print_logs`

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

**Returns**:

  None

---
#### `get_exec_time`

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

---
#### `tail_logs`

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1) -> None
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.

================================================================================

## JobPool
Path: python-sdk/api-reference/jobpool.mdx
URL: https://docs.fused.io/python-sdk/api-reference/jobpool/

## JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### retry

```python
retry()
```

Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

---

### cancel

```python
cancel(wait: bool = True)
```

Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

---

### total_time

```python
total_time(since_retry: bool = False) -> timedelta
```

Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

---

### times

```python
times() -> list[Optional[timedelta]]
```

Time taken for each task.

Incomplete tasks will be reported as None.

---

### done

```python
done() -> bool
```

True if all tasks have finished, regardless of success or failure.

---

### all_succeeded

```python
all_succeeded() -> bool
```

True if all tasks finished with success

---

### any_failed

```python
any_failed() -> bool
```

True if any task finished with an error

---

### any_succeeded

```python
any_succeeded() -> bool
```

True if any task finished with success

---

### arg_df

```python
arg_df()
```

The arguments passed to runs as a DataFrame

---

### status

```python
status()
```

Return a Series indexed by status of task counts

---

### wait

```python
wait()
```

Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### tail

```python
tail(stop_on_exception = False)
```

Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### results

```python
results(return_exceptions = False) -> List[Any]
```

Retrieve all results of the job.

Results are ordered by the order of the args list.

---

### results_now

```python
results_now(return_exceptions = False) -> Dict[int, Any]
```

Retrieve the results that are currently done.

Results are indexed by position in the args list.

---

### df

```python
df(
    status_column: Optional[str] = "status",
    result_column: Optional[str] = "result",
    time_column: Optional[str] = "time",
    logs_column: Optional[str] = "logs",
    exception_column: Optional[str] = None,
    include_exceptions: bool = True,
)
```

Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

---

### get_status_df

```python
get_status_df()
```

---

### get_results_df

```python
get_results_df(ignore_exceptions = False)
```

---

### errors

```python
errors() -> Dict[int, Exception]
```

Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

---

### first_error

```python
first_error() -> Optional[Exception]
```

Retrieve the first (by order of arguments) error result, or None.

---

### logs

```python
logs() -> list[str]
```

Logs for each task.

Incomplete tasks will be reported as None.

---

### first_log

```python
first_log() -> Optional[str]
```

Retrieve the first (by order of arguments) logs, or None.

---

### success

```python
success() -> Dict[int, Any]
```

Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

---

### pending

```python
pending() -> Dict[int, Any]
```

Retrieve the arguments that are currently pending and not yet submitted.

---

### running

```python
running() -> Dict[int, Any]
```

Retrieve the results that are currently running.

---

### cancelled

```python
cancelled() -> Dict[int, Any]
```

Retrieve the arguments that were cancelled and not run.

---

### collect

```python
collect(ignore_exceptions = False, flatten = True)
```

Collect all results into a DataFrame

---

================================================================================

## fused.options
Path: python-sdk/api-reference/options.mdx
URL: https://docs.fused.io/python-sdk/api-reference/options/

## fused.options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

**Examples:**

Change the `request_timeout` option from its default value to 120 seconds:

```py
fused.options.request_timeout = 120
```

## Options

### __dir__

```python
__dir__() -> List[str]
```

### base_url

```python
base_url: str = PROD_DEFAULT_BASE_URL
```

Fused API endpoint

### auth

```python
auth: AuthOptions = Field(default_factory=AuthOptions)
```

Options for authentication.

### show

```python
show: ShowOptions = Field(default_factory=ShowOptions)
```

Options for object reprs and how data are shown for debugging.

### max_workers

```python
max_workers: int = 16
```

Maximum number of threads, when multithreading requests

### request_timeout

```python
request_timeout: Union[Tuple[float, float], float, None] = 120
```

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### request_max_retries

```python
request_max_retries: int = 5
```

Maximum number of retries for API requests

### request_retry_base_delay

```python
request_retry_base_delay: float = 1.0
```

Base delay before retrying a API request in seconds

### realtime_client_id

```python
realtime_client_id: Optional[StrictStr] = None
```

Client ID for realtime service.

### max_recursion_factor

```python
max_recursion_factor: int = 5
```

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

```python
save_user_settings: StrictBool = True
```

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

```python
default_udf_run_engine: Optional[StrictStr] = None
```

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

```python
default_validate_imports: StrictBool = False
```

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

```python
prompt_to_login: StrictBool = False
```

Automatically prompt the user to login when importing Fused.

### no_login

```python
no_login: StrictBool = False
```

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

```python
pyodide_async_requests: StrictBool = False
```

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

```python
cache_directory: Path = Field(default_factory=_cache_directory)
```

The base directory for storing cached results.

### data_directory

```python
data_directory: Path = Field(default_factory=_data_directory)
```

The base directory for storing data results.

### temp_directory

```python
temp_directory: Path = Path(tempfile.gettempdir())
```

The base directory for storing temporary files.

### never_import

```python
never_import: StrictBool = False
```

Never import UDF code when loading UDFs.

### gcs_secret

```python
gcs_secret: str = 'gcs_fused'
```

Secret name for GCS credentials.

### gcs_filename

```python
gcs_filename: str = '/tmp/.gcs.fused'
```

Filename for saving temporary GCS credentials to locally or in rt2 instance

### logging

```python
logging: StrictBool = Field(default=False, validate_default=True)
```

Control logging for Fused

### verbose_udf_runs

```python
verbose_udf_runs: StrictBool = True
```

Whether to print logs from UDF runs by default

### default_run_headers

```python
default_run_headers: Optional[Dict[str, str]] = 

```

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

```python
default_dtype_out_vector: StrictStr = 'parquet'
```

Default transfer type for vector (tabular) data

### default_dtype_out_raster

```python
default_dtype_out_raster: StrictStr = 'tiff'
```

Default transfer type for raster data

### fd_prefix

```python
fd_prefix: Optional[str] = None
```

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

```python
verbose_cached_functions: StrictBool = True
```

Whether to print logs from cache decorated functions by default

### local_engine_cache

```python
local_engine_cache: StrictBool = True
```

Enable UDF cache with local engine

### base_web_url

```python
base_web_url
```

### save

```python
save()
```

Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

================================================================================

## Authentication
Path: python-sdk/authentication.mdx
URL: https://docs.fused.io/python-sdk/authentication/

## Authenticate

Authenticate the Fused Python SDK in a Python Notebook.

Make sure to have the `fused` package installed.

```python showLineNumbers
pip install "fused[all]"
```

To use Fused you need to authenticate. The following will store a credentials file in `~/.fused/credentials`:

</Tabs>

## Log out

Log out the current user. This deletes the credentials saved to disk and resets the global Fused API.

```python showLineNumbers

fused.api.logout()
```

## Get Bearer (Access) token

Get the account's Bearer (sometimes referred to as Access) token.

```python showLineNumbers

fused.api.access_token()
```

This can be helpful when calling UDFs via HTTP requests outside of the Fused Python SDK and Workbench to authenticate with the Fused API.

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

:::

================================================================================

## Batch jobs
Path: python-sdk/batch.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/batch/

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

```python showLineNumbers

@fused.udf
def udf(
    source_s3_path = '',
    destination_s3_path = ''
):

    file_path = fused.download(source_s3_path, source_s3_path.split('/')[-1])

    # Create a temporary directory to extract the files
    s3 = s3fs.S3FileSystem()
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/unzipped_files')

    # Upload each file to the S3 bucket
    for root, dirs, files in os.walk('/tmp/unzipped_files'):
        for file in files:
            file_path = os.path.join(root, file)
            print('file_path', file_path, destination_s3_path)
            s3.put(file_path, destination_s3_path)

    return pd.DataFrame() # UDFs must return a table or raster

```

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_remote()` to make kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

```python showLineNumbers
job = udf(
    source_s3_path = 'https://datadownload-production.s3.amazonaws.com/WCMC_carbon_tonnes_per_ha.zip', 
    destination_s3_path = 's3://fused-users/fused/<YOUR_DIR>/dswid/WCMC_carbon_tonnes_per_ha_10gb/'
)
job_id = job.run_remote()
```

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

```python showLineNumbers
fused.api.job_tail_logs("df335890-4406-4832-bf93-6a3b092e496d")
```

================================================================================

## Changelog
Path: python-sdk/changelog.md
URL: https://docs.fused.io/python-sdk/changelog/

# Changelog

## v1.21.1 (2025-07-14)

**`fused-py`**

- Updated LanceDB to 0.24.1.
- Fixed issues starting batch jobs.

**Workbench**

- AI Assistant now sees all UDFs in Workbench & has access to updated Fused documentation
- App builder now integrates with the new version control page.
- Fixed scrolling on the version control page.
- Fixed saving and layout issues on the Canvas view.

## v1.21.0 (2025-07-11)

**`fused-py`**

New Features:
- Added `fused.api.resolve`.
- Added `fused.api.team_info`.
- IPython magics now load automatically.
- When running a large (batch) job, it is now possible to specify the job's name.
- Upgraded `xarray` to 2025.4.0, DuckDB to 1.3.2, and H3 to 4.3.0.
- The `fd://` filesystem scheme will automatically be registered with `fsspec`.
- UDFs that return HTML will be loaded as `str` objects from `fused.run`.
- UDFs saved on Fused server now have a `catalog_url` property to get the Workbench UDF URL.
- `npy` output format is now supported for numpy (raster) return values.
- Arrow-compatible return values are now accepted.
- `fused.cache` will detect changes to referenced UDFs.
- `fused.run` accepts `verbose` keyword.

Bug Fixes:
- Fixed bugs with `udf.render` on some IPython versions.
- Fixed bugs with `repr`s for access tokens and UDFs.
- Fixed calling `fused.run` with UDF objects and `sync=False`.
- Renamed the UDF class to `Udf`.
- Removed some unused and deprecated code.
- Fixed bugs with `fused.cache` showing as not found, having stale files, or not passing arguments through.
- Removed the `n` keyword argument from `get_udfs` and `get_apps`.
- Fixed bugs with HEAD requests to UDF endpoint.
- `fused.submit` will warn about conflicting arguments.
- `/tmp/` size has been increased in realtime instances.
- `fused.api.list` and related functions supports `/mount` paths.
- Improved the performance of GitHub sync.
- Changed defaults for `JobPool.cancel` and fixed a bug where it would continue to retry.
- Fixed bugs with `JobPool.df` and UDF runs that result in exceptions.
- Fixed encoding URL paths in `fused.api.download` and related functions.

**Workbench**

New Features:
- Added AI editing and AI chat capabilities in the UDF builder.
- Workbench now has a code profiler: each line of code will show its execution runtime
- Added new Canvas dashboard builder mode (experimental).
- Added new Table data view mode.
- GitHub integration has a new page and is no longer beta.
- GitHub integration remembers relevant open PRs better.
- Fused apps uses a newer version of Streamlit and Stlite.
- Added a menu item to take a screenshot in higher-than-screen resolution.
- Added type-to-filter in File Explorer.
- File Explorer can be browsed without logging in.
- File Explorer now shows a summary of the current directory.
- File preview UDFs can now be specified with regular expressions.
- Cmd+Click on shared tokens will now take you to the UDF catalog.

Bug Fixes:
- Results panel shows when memory usage is unknown.
- Share code is more consistent with the selected output format.
- Fixed bugs with read-only app UI.
- The large data warning will now show in File Explorer as well when applicable.
- Fixed bugs with app/UDF catalog layout and sorting.
- Adjusted the UI for visualization settings and parameters in the UDF list.
- Reordered menu items in File Explorer.
- Cursor position will be remembered when switching between UDFs.
- Share tokens that are already shown on the page are no longer redacted.
- Map tooltip can now be scrolled.

## v1.20.1 (2025-06-09)

**`fused-py`**

New Features:
- Added `fused.api.resolve` and `fused.api.team_info`.
- IPython magics will automatically be loaded when importing `fused`.
- `run_remote` (batch jobs) can now accept a job name.

Bug Fixes:
- Fixed the `render` method of UDF objects.
- Fixed access tokens for apps being rendered in IPython.
- Fixed calling `fused.run(udf, sync=False)` with UDF objects.
- Removed some deprecated fields and arguments.

**Workbench**

- Workbench will now show the amount of time taken in UDF functions as a heatmap.
- Memory bars in the Results panel will show when usage is unknown.
- Update how shared token URLs are generated for different output formats.
- Updated stlite to 0.82.0.

## v1.20.0 (2025-06-03)

**`fused-py`**

New Features:
- Running a UDF with engine `local` will cache similarly to how it would when running on `remote` (supporting `cache_max_age` to control the caching).
- Large (batch) jobs will be in a Pending state if they cannot start immediately.
- UDFs can now accept `**kwargs` parameters, which will always be passed in as strings.
- `@fused.cache` has a new `cache_verbose` option. If set to `True` (default), it prints a message when a cached result is returned.
- `@fused.cache` renamed the `reset` parameter to `cache_reset`. The existing `reset` parameter is deprecated.
- Some file listing APIs like `fused.api.list` will work for public buckets when on free accounts.
- `fused.load` accepts `import_globals` (default `True`) for controlling importing UDF globals. Also, when globals cannot be imported, a warning is emitted instead of an exception.

Bug Fixes:
- Clarified login-needed message in Fused Apps.
- Fixed bugs with `@fused.cache` results not being ready.
- Fixed bugs with `@fused.cache` not detecting changes in the cached function.
- Loading a UDF from a file will autodetect the UDF function name.
- Fixed bugs with returning GeoDataFrames that do not contain geometry.
- Fixed calling `to_fused` on an app.

**Workbench**

- A message will appear above the UDF body when parameters are set in the UDF list.
- A lock icon will be shown next to read-only UDFs and Apps in Workbench.
- When pushing UDFs to GitHub, the preview image will be pushed to a public URL so that the README in GitHub is rendered correctly.
- When pushing UDFs to GitHub, Workbench will assign the PR to you if possible.
- It is now possible to delete UDFs and Apps directly from the catalog.
- Added a "Reload Collection" button. This pulls all latest version of UDF currently in your Collection.
- Workbench will minimize more changes from the PRs it creates on GitHub.
- "Open in Kepler.gl" supports H3 (string) data.
- The visibility button for a Fused App will now reset the app.
- A new Reset 3D View button is added to the UDF Builder map, and the keyboard shortcut has been updated to `Cmd+Shift+UpArrow` on MacOS (`Ctrl+Shift_UpArrow` on Windows / Linux).
- Workbench will show the current environment name above the map by default.
- Workbench will remember which UDF was selected when reopening the page.
- Adjusted which UDF mode label is shown when automatically detecting the UDF mode.
- Fixed some bugs with dynamic output mode.
- UI updates for the Pull Changes (history) and Push Changes views, including showing the README file in both views.
- Drag&Drop UDF into Workbench now works on the entire tab
- Added a button to download usage table in the Profile view.
- Fixed some visual bugs with light mode.
- File Explorer will no longer show file opener UDFs saved on your personal catalog, and will clarify when file opener UDFs are from your team catalog.

## v1.19.0 (2025-05-19)

**`fused-py`**

Breaking changes:
- Large (batch) jobs have been updated to pass parameters into the UDF the same way as other UDF runs. For compatibility, if the parameters passed into the job do not correspond with the parameters, a dictionary parameter is passed into the UDF instead. This will be deprecated and removed in a future release.
- The `context` and `bbox` parameters to a UDF are no longer treated as special.
- Python 3.9 support, which was previously deprecated, is now removed. The minimum Python version for the `fused` package is now 3.10.

New Features:
- PyArrow upgraded to version 16.0.
- Ingestion now supports input files without extensions, and filters out files with `.` or `_` prefix.
- `cache=False` is now a shortcut for disabling cache, e.g. `cache_max_age=0`.
- UDFs now support parameters annotated as `shapely.Geometry` or `shapely.Polygon`.
- `fused.load` now support loading UDFs from Github Pull Request URLs.
- Added a timeout parameter for `fused.api.upload`.
- Fused API functions now support `/mount` without `file://` prefix.
- `fused.api.download` now supports downloading files from `/mount`.
- `fused.api.list` now supports listing an individual file under `/mount`.
- `max_deletion_depth` in `fused.api.delete` default changed from 2 to 3.

Bug Fixes
- Fixed pickling UDF objects.
- Fixed UDF equality checks not conforming to Python specifications.
- Many fixes to ensure compatibility between the `fused` module available on PyPI and the `fused` module within the Fused backend.
- Fixed returning `pd.Timestamp` objects.
- Fixed bugs with handling of stdout if the UDF is async.
- Fixed bugs with UDF object `repr` in Jupyter.
- Fixed `@fused.cache` in Fused Apps.
- Fixed "multiple auth mechanisms" error when retrieving job results.
- Fixed deserialization of GeoDataFrame without geometry column.
- Fixed cases where UDFs would be indented when run.
- Various stability and performance updates, including new self-healing capabilities on the Fused backend.

**Workbench**

- Upgraded Streamlit in apps to 1.44.
- Fused Apps is no longer "beta".
- Fused Apps will now highlight syntax errors.
- Fused Apps will now autocomplete the `fused` module correctly.
- "Changes pending" in the map has been renamed to "Running" and now shows the time elapsed without needing to hover over it.
- UDF builder tooltips have been refreshed, it is now possible to click on data on the map to pin the tooltip on screen. Pinned tooltips show how many data records were under the mouse and allow paging through them.
- Workbench can now highlight H3 hexagons on click.
- Workbench can now detect decimal H3 indexes and Cmd+Click works on them.
- `udf://` URLs in Workbench now entirely overwrite parameters of the selected UDF.
- Fixed UDF builder showing partially updated map states for Tile UDFs.
- Collections catalog can now be sorted.
- Fixed a bug where Workbench would not detect newly added utils on UDFs.
- Parameters and Visualization sections are now styled slightly differently to make it easier to pick out your UDFs.
- Renamed Visualization "Surprise me" button to "Preset".
- Fixed a visual bug with the job status.
- Updated the share UDF and share app pages.
- Fixed bugs with UDF or app catalog showing the wrong content.
- Renamed "History" to "Pull Changes" and updated styling of that page.
- Fixed Pull Changes not showing diffs if utils had been added or deleted.
- Fixed Workbench showing the original Github link for forked UDFs.
- Workbench code editors will now remember scroll position.
- Clicking the viewport location label will now copy it.

## v1.18.0 (2025-04-28)

**`fused-py`**

Breaking changes:
- `fused.submit` now raises an error by default, if there is any run erroring

New Features:
- It is now possible set the cache storage location with `@fused.cache(storage=...)`.
- `@fused.cache` can now exclude arguments from the cache key.
- `@fused.cache` uses Pandas' own way of hashing DataFrames.
- Added storage argument to `fused.file_path(storage=...)`.
- Large jobs now pick up AWS credentials more consistently.
- Auth redirect can dynamically select the port when logging in locally.
- `udf.to_fused` will show the diff when UDF name conflicts with an existing UDF.
- `fused.load` can load by the unique UDF ID again.
- UDFs can be run by username and UDF name in addition to email and UDF name. (ex: `fused.run(user@team.com/my_cool_udf)`). 
- Preview images can now be specified in-directory in Github.
- Adjusted UDF caching behavior for performance.

Bug Fixes
- Fixed behavior when loading and running UDFs with code outside of the UDF function.
- Fixed `fused.api.list` being incompatible with some async stacks.
- Fixed a bug where strings inside UDFs would get extra spaces added to them.
- Shared tokens can be created by a team account.
- Fixed a bug that could occur where Fused would try to duplicate index column names of the returned DataFrame.
- Fixed various bugs when a UDF closes `stdout`.
- Fixed a bug where `fused.run` would not return printed messages from a UDF.
- Fixed a bug where `fused.load` would crash on very large strings.
- Fixed various bugs with exporting UDFs from within fused-py.
- Fixed a bug where `partitioning_schema_input` would not be found when ingesting.
- Fixed a bug where UDFs might import incorrectly when several pushes happen in quick succession in a linked Github repo.

**Workbench**

- `File (Viewport)` renamed to `Single (Viewport)`.
- Added `Single (Parameter)` UDF type that behaves like `Single (Viewport)` but does not pass the viewport bounds.
- File Explorer will show a per-user home favourite.
- Your UDF list will now sync across different browser tabs.
- Preference toggles are added to the Command Palette.
- The share page has been redesigned.
- Collections catalog page now shows the UDF in a given Collection by hovering over them.
- When adding a duplicate UDF to Workbench, you will be prompted to duplicate or replace it.
- Memory usage for UDFs can be found in the results panel (once displaying memory usage was been turned on in Preferences)
- Workbench will indicate that UDF runs were cached in all circumstances.
- The public map page is now compatible with any public-readable shared token.
- Added `udf://name?param=value` URL support to Workbench.
- Reduced the size of metadata diffs generated by Workbench when pushing to Github.
- Various performance improvements.
- Fixes for various UI layout bugs.

## v1.17.0 (2025-04-10)

**`fused-py`**

- Team UDFs can be loaded or run by specifying the name "team", as in: `fused.load("team/udf_name")`
- `Udf.to_fused` supports overwriting the UDF when saving.
- Added `fused.api.enable_gcs()` to configure using the Google Cloud Platform secret specified in Fused secret manager.
- `@fused.cache` locking mechanism has changed and will not allow multiple concurrent runs.
- Upgraded DuckDB to v1.2.2.
- Running a saved UDF by token or name will now also show the logs, including print statements and error tracebacks.
- All functions interacting with the Fused server will now retry automatically, by default 3 times.
- Python 3.9 support is deprecated. The next release of `fused` will require Python 3.10+.
- Deprecated `fused_batch` module is removed.

**Workbench**

- Cached UDF runs will show the original logs.
- "Change output parameters" in the Share UDF screen shows all detected parameters.
- Added a copy viewport bounds button in the Results panel.
- Improved the performance of the catalog screen.
- Fixed the job page showing times in inconsistent time zones.

App Builder:
- Deprecated `fused_app` module is removed.

## v1.16.3 (2025-04-03)

**`fused-py`**

- It is now possible to return general `list`s and `tuple`s from UDFs. (Note: a tuple of a raster and bounds will be treated as a raster return type.)

**Workbench**

- Workbench will now prompt you when loading large UDF results that could slow down or overwhelm your browser. The threshold for this prompt is configurable in your Workbench preferences.
- Fixed bugs with loading large UDF results.
- UDF list will show an error if a UDF has an empty name.
- Fixed running some public UDFs in Workbench.

## v1.16.2 (2025-04-01)

**`fused-py`**

- It is now possible to return dictionaries of objects from a UDF, for example a dictionary of a raster numpy array, a DataFrame, and a string.
- Whitespace in a UDF will be considered as changes when determining whether to return cached data. (a UDF with different whitespace will be rerun rather than cached)
- Fixed calling `fused.run` in large jobs.

**Workbench**

- Added experimental AI agent builder.
- Workbench will now prompt you to replace an existing UDF when adding the same UDF (by name) from the catalog.
- Added ability to download & upload an entire collection.
- Fixed saving collections with empty names.

Visualization:
- Added an H3-only visualization preset.
- Fixed a bug where changing TileLayer visualization type could result in a crash.

App Builder:
- Updated the runtime.

## v1.16.0 (2025-03-27)

**`fused-py`**

- The result object of running in `batch` now has `logs_url` property.
- Fixed `fused.submit` raising an error if some run failed.

**Workbench**

- Added a Download UDFs button for downloading an entire collection.
- Results will show a message at the top if UDF execution was cached.
- Non-visible UDFs will have a different highlight color on them in the UDF list.
- Collections will show as modified if the order of UDFs has been changed.
- Fixes for Collections saving the ordering and visibility of UDFs.
- Fixed the Team Jobs page in Workbench crashing in some cases.

**Shared tokens**

- Shared token URLs can be called with an arbitrary (ignored) file extension in the URL.

## v1.15.0 (2025-03-20)

**`fused-py`**

- Loading UDFs now behaves like importing a Python module, and attributes defined on the UDF can be accessed.
- The `fused.submit()` keyword `wait_on_result` has been renamed to `collect`, with a default of `collect=True` returning the collected results (pass `collect=False` to get the JobPool object to inspect individual results).
- New UDFs default to using `fused.types.Bounds`.
- Upgraded `duckdb` to v1.2.1.
- UDFs can now return simple types like `str`, `int`, `float`, `bool`, and so on.
- Files in `/mount/` can be listed through the API.
- UDFs from publicly accessible GitHub repositories can be loaded through `fused.load`.
- `fused.load` now supports loading a UDF from a local .py file or directory
- The `x`, `y` and `z` aren't protected arguments when running a UDF anymore (previously protected to pass X/Y/Z mercantile tiles).

**Workbench**

New:
- Added a new account page and redesigned preferences page.
- You can now customize the code formatter settings (available under Preferences > Editor preferences).
- UDFs can optionally be shared with their code when creating a share token.

General:
- Moved shared token page to bottom left bar, and adjusted the icons.
- The ordering of UDFs in collections is now saved.

App Builder:
- Updated app list UI.
- Fixed bugs with shared apps showing the wrong URL in the browser.

## v1.14.0 (2025-02-25)

v1.14.0 introduces a lot of new changes across `fused-py` and Workbench

**`fused-py`**

- Introducing `fused.submit()` method for multiple job run
- Improvement to UDF caching
    - All UDFs are now cached for 90 days by default
    - Ability to customize the age of cached data & UDFs with the new `cache_max_age` argument when defining UDFs, running UDFs or when caching regular Python functions
- `pandas` & `geopandas` are now optional for running non-spatial UDF locally
- Removed hardcoded `nodata=0` value for serializing raster data

**Workbench**

New:
- Introducing Collections to organize & aggregate UDFs together
- Redesigned "Share" button & page: All the info you need to share your UDFs to your team or the world

General:
- Improvements to Navigation in Command Pallette. Try it out in Workbench by doing `Cmd + K` (`Ctrl + K` on Windows / Linux)
- Autocomplete now works with `Tab` in Code Editor with `Tab`
- Added a Delete Button in the Shared Tokens page (under Account page)
- Ability to upload images for UDF Preview in Settings Page
- Adding ‚ÄúFullscreen‚Äù toggle in Map View
- Improved `colorContinuous` in Visualize Tab
- Allowing users to configure public/team access scopes for share tokens 
- No longer able to edit UDF & App name in read-only mode
- Fixing job loading logs

File Explorer:
- Download directories as `zip`
- Adding favorites to file path input search results 
- Ability to open `.parquet` files with Kepler.gl

## v1.13.0 (2025-01-22)

- Fixed shared UDFs not respecting the Cache Enabled setting.
- Added a cache TTL (time-to-live) setting when running a UDF via a shared token endpoint.
- Tags you or your team have already used will be suggested when editing a UDF's tags.
- Team UDFs will be shown as read-only in Workbench, similar to Public UDFs.
- File Explorer shows deletion in progress.
- File Explorer can accept more S3 URLs, and uses `/mount/` instead of `/mnt/cache`.
- UDF Builder will no longer select a UDF when clicking to hide it.
- Fixed how Push to Github chooses the directory within a repository to push to.
- Fixed the browser location bar in Workbench updating on a delay.
- Fixed writing Shapefile or GPKG files to S3.
- (Beta) New fusedio/apps repository for public Fused Apps.
- Navigating to Team UDFs or Saved UDFs in the UDF Catalog will now prompt for login.
- Fixed the "Select..." environment button in Workbench settings.
- UDF Builder will no longer replace all unaccepted characters with `_` (underscore).
- Fixed loading team UDFs when running a UDF with a shared token.
- Batch jobs that use `print` will now have that output appear in the job logs.
- Apps in the shared token list show an app icon.
- Removed some deprecated batch job options.
- Installed `vega-datasets` package.

## v1.12.0 (2025-01-10)

- (Beta) Added an App catalog in Workbench, and a new type of URL for sharing apps.
- Added `/mount` as an alias for `/mnt/cache`.
- More consistently coerce the type of inputs to UDFs.
- Added more visualization presets to UDF builder in Workbench.
- Fixed an issue where the tab icon in Workbench could unintentionally change.
- Fixed bugs in Workbench File Explorer for `/mnt/cache` when browsing directories with many files.
- Fixed bugs in `fused` Python API not being able to list all files that should be accessible.
- Fixed bugs in the Github integration, command palette, and file explorer in Workbench.
- Fixed bugs in caching some UDF outputs.
- The shareable URL for public and community UDFs will now show in the settings tab for those UDFs.
- UDFs can customize their data return with `Response` objects.

## v1.11.9 (2024-12-19)

- Accounts now have a *handle* assigned to them, which can be used when loading UDFs and pushing to community UDFs
- Account handle can be changed once by the user (for more changes please contact the Fused team.)
- Added a command palette to the Workbench, which can be opened with Cmd-k or Ctrl-k.
- When creating a PR for a community UDF or to update a public UDF, it will be under your account if you log in to Fused with Github.
- Bug fixes for pushing to Github, e.g. when pushing a saved UDF, and for listing the Fused bot account as an author.
- Batch (`run_remote`) jobs can call back to the Fused API.
- Team UDFs can be pinned to the end of the featured list.
- Speed improvements in ingestion.
- Ingestion will detect `.pq` files as Parquet.
- Format code shortcut in Workbench is shown in the keyboard shortcut list and command palette.
- Workbench will hide the map tooltip when dragging the map by default.
- Workbench will now look for a `hexLayer` visualization preset for tabular results that do not contain `geometry`.
- Workbench file explorer can now handle larger lists of files.
- Fix for browsing disk cache (`/mnt/cache`) in Workbench file explorer.
- Teams with multiple realtime instances can now set one as their default.
- Fix for saving UDFs with certain names. Workbench will show more descriptive error messages in more cases for issues saving UDFs.

## v1.11.8 (2024-12-04)

- New File Explorer interface, with support for managing Google Cloud Storage (GCS) and `/mnt/cache` files.
- Workbench will show an error when trying to save a UDF with a duplicate name.
- Fixed a few bugs with Github integration, including the wrong repository being selected by default when creating a PR.
- Updated `fsspec` and `pyogrio` packages.

## v1.11.7 (2024-11-27)

- Decluttered the interface on mobile browsers by default.
- Fixed redo (Cmd-Shift-z or Ctrl-Shift-z) sometimes being bound to the wrong key.
- Tweaked the logic for showing the selected object in Workbench.

## v1.11.6 (2024-11-26)

- Added Format with Black (Alt+Shift+f) to Workbench.
- Fix the CRS of DataFrame's returned by get_chunk_from_table.
- Added a human readable ID to batch jobs.
- Fused will send an email when a batch job finishes.
- Fix for opening larger files in Kepler.gl.
- Fix for accessing UDFs in a team.
- Improved messages for UDF recursion, UDF geometry arguments, and returning geometry columns.
- Adjusted the UDF list styling and behavior in Workbench.
- Fix for secrets in shared tokens.

## v1.11.5 (2024-11-20)

- Show message for keyword arguments in UDFs that are reserved.
- Added reset kernel button.
- Workbench layers apply visualization changes immediately when the map is paused.
- Show the user that started a job for the team jobs list.
- Fix for running nested UDFs with utils modules.
- Fix for returning xarray results from UDFs.
- Fix for listing files from within UDFs.
- Upgraded to GeoPandas v1.

## v1.8.0 (2024-06-25) :package:

- Added Workbench tour for first-time users.
- Undo history is now saved across UDFs and persists through reloads.
- Added autocomplete when writing UDFs in Workbench.
- Added `colorBins`, `colorCategories`, and `colorContinuous` functions to Workbench's Visualize tab.
- Migrated SDK to Pydantic v2 for improved data validation and serialization.
- Fixed a bug causing NumPy dependency conflicts.

## v1.7.0 (2024-06-04) :bird:

- Execution infrastructure updates.
- Update DuckDB package to v1.0.0.
- Improve responsivity of Workbench allotments.
- Crispen Workbench UI.

## v1.6.1 (2024-05-06) :guardsman:

_GitHub integration_

- Updates to team GitHub integration.
- Users are now able to create shared UDF token from a team UDF both in Workbench and Python SDK.

## v1.6.0 (2024-04-30) :checkered_flag:

- The Workbench file explorer now shows UDFs contributed by community members.
- Team admins can now set up a GitHub repository with UDFs that their team members can access from Workbench.

## v1.5.4 (2024-04-15) :telescope:

- Button to open slice of data in Kepler.gl.
- Minor UI design and button placement updates.

## v1.5.3 (2024-04-08) :duck:

- Improved compatibility with DuckDB requesting data from shared UDFs.
- Geocoder in Workbench now supports coordinates and H3 cell IDs.
- GeoDataFrame arguments to UDFs can be passed as bounding boxes.
- The package ibis was upgraded to 8.0.0.
- Utils modules no longer need to import fused.

## v1.5.2 (2024-04-01) :tanabata_tree:

- File browser can now preview images like TIFFs, JPEGs, PNGs, and more.
- Users can now open Parquet files with DuckDB directly from the file browser.

## v1.5.0 (2024-03-25) :open_file_folder:

- The upload view in Workbench now shows a file browser.
- Users can now preview files in the file browser using a default UDF.

## v1.4.1 (2024-03-19) :speech_balloon:

- UDFs now support typed function annotations.
- Introduced special types  `fused.types.TileXYZ`, `fused.types.TileGDF`, `fused.types.Bbox`.
- Workbench now autodetects Tile or File outputs based on typing.
- Added button to Workbench to autodetect UDF parameters based on typing.

## v1.1.1 (2024-01-17) :dizzy:

- Renamed `fused.utils.run_realtime` and `fused.utils.run_realtime_xyz` to `fused.utils.run_file` amd `fused.utils.run_tile`.
- Removed `fused.utils.run_once`.

## v1.1.0 (2024-01-08) :rocket:

- Added functions to run the UDFs realtime.

## v1.1.0-rc2 (2023-12-11) :bug:

- Added `fused.utils.get_chunk_from_table`.
- Fixed bugs in loading and saving UDFs with custom metadata and headers.

## v1.1.0-rc0 (2023-11-29) :cloud:

- Added cloud load and save UDFs.
- `target_num_files` is replaced by `target_num_chunks` in the ingest API.
- Standardize how a decorator's headers are preprocesses to set `source_code` key.
- Fixed a bug loading UDFs from a job.

## v1.0.3 (2023-11-7) :sweat_drops:

_Getting chunks_

- Added `fused.utils.get_chunks_metadata` to get the metadata GeoDataFrame for a table.
- `run_local` now passes a copy of the input data into the UDF, to avoid accidentally persisting state between runs.
- `instance_type` is now shown in more places for running jobs.
- Fixed a bug where `render()`ing UDFs could get cut off.
- Fixed a bug with defining a UDF that contained an internal `@contextmanager`.

## v1.0.2 (2023-10-26) :up:

_Uploading files_

- Added `fused.upload` for uploading files to Fused storage.
- Added a warning for UDF parameter names that can cause issues.
- Fixed some dependency validation checks incorrectly failing on built-in modules.

## v1.0.1 (2023-10-19) :ant:

- Added `ignore_chunk_error` flag to jobs.
- Added warning when sidecar table names are specified but no matching table URL is provided.
- Fixed reading chunks when sidecars are requested but no sidecar file is present.
- Upgraded a dependency that was blocking installation on Colab.

## v1.0.0 (2023-10-13) :ship:

_Shipping dependencies_

- Added `image_name` to `run_remote` for customizing the set of dependencies used.
- Added `fused.delete` for deleting files or tables.
- Renamed `output_main` and `output_fused` to `output` and `output_metadata` respectively in ingestion jobs.
- Adjusted the default instance type for `run_remote`.
- Fixed `get_dataframe` sometimes failing.
- Improved tab completion for `fused.options` and added a repr.
- Fixed a bug where more version migration messages were printed.
- Fixed a bug when saving `fused.options`.

================================================================================

## index
Path: python-sdk/index.mdx
URL: https://docs.fused.io/python-sdk/

# Python SDK

> The latest version of `fused-py` is <FusedVersionLive />.

## Documentation overview

    Installing `fused` is required if you're running `fused` on your end (locally or in a development environment). If you're working in Workbench UDF Builder or App Builder `fused` is already installed for you.

1. Set up a Python environment:

We're using `venv` but you could use `conda` or any other environment manager in Python. 

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install the `fused` package:

You can only install the base package, though we recommend still adding the optional dependencies:
```bash
pip install fused
```

Or install with optional dependencies:
```bash
# For raster data processing
pip install "fused[raster]"

# For vector data processing
pip install "fused[vector]"

# Install all optional dependencies
pip install "fused[all]"
```

</details>

### Authenticate

The first time you use Fused you'll need to authenticate.

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the URL in your browser to authenticate.

## Basic API usage

Some basic examples of how to use `fused` to run UDFs:

#### Hello World UDF

```python

@fused.udf
def udf(x: int = 1):
    return f"Hello world "

fused.run(udf)
```

```bash
>> Hello world 2
```

#### Simple data UDF

```python

@fused.udf
def udf(x: int = 3):

    return pd.DataFrame()

fused.run(udf)
```

```bash
>>   | x |
|---|---|
| 0 | 0 |
| 1 | 1 |
| 2 | 2 |
```

###

================================================================================

## Top-Level Functions
Path: python-sdk/top-level-functions.mdx
URL: https://docs.fused.io/python-sdk/top-level-functions/

## @fused.udf

```python
udf(
    fn: Optional[Callable] = None,
    *,
    schema: Union[Schema, Dict, None] = None,
    name: Optional[str] = None,
    cache_max_age: Optional[str] = None,
    default_parameters: Optional[Dict[str, Any]] = None,
    headers: Optional[Sequence[Union[str, Header]]] = None
) -> Callable[..., Udf]
```

A decorator that transforms a function into a Fused UDF.

**Parameters:**

- **schema** (<code>Union[Schema, Dict, None]</code>) ‚Äì The schema for the DataFrame returned by the UDF. The schema may be either
  a string (in the form `"field_name:DataType field_name2:DataType"`, or as JSON),
  as a Python dictionary representing the schema, or a `Schema` model object.

  Defaults to None, in which case a schema must be evaluated by calling `run_local`
  for a job to be able to write output. The return value of `run_local` will also
  indicate how to include the schema in the decorator so `run_local` does not need
  to be run again.

- **name** (<code>Optional[str]</code>) ‚Äì The name of the UDF object. Defaults to the name of the function.

- **cache_max_age** (<code>Optional[str]</code>) ‚Äì The maximum age when returning a result from the cache.

- **default_parameters** (<code>Optional\[Dict[str, Any]\]</code>) ‚Äì Parameters to embed in the UDF object, separately from the arguments
  list of the function. Defaults to None for empty parameters.

- **headers** (<code>Optional\[Sequence\[Union[str, Header]\]\]</code>) ‚Äì A list of files to include as modules when running the UDF. For example,
  when specifying `headers=['my_header.py']`, inside the UDF function it may be
  referenced as:

  ```py

  my_header.my_function()
  ```

  Defaults to None for no headers.

**Returns:**

- <code>Callable\..., [Udf\]</code> ‚Äì A callable that represents the transformed UDF. This callable can be used
- <code>Callable\..., [Udf\]</code> ‚Äì within GeoPandas workflows to apply the defined operation on geospatial data.

**Examples:**

To create a simple UDF that calls a utility function to calculate the area of geometries in a GeoDataFrame:

```py
@fused.udf
def udf(bbox, table_path="s3://fused-asset/infra/building_msft_us"):
    ...
    gdf = table_to_tile(bbox, table=table_path)
    return gdf
```

---

## @fused.cache

```python
cache(
    func: Callable[..., Any] | None = None,
    cache_max_age: str | int = DEFAULT_CACHE_MAX_AGE,
    path: str = "tmp",
    concurrent_lock_timeout: str | int = 120,
    cache_reset: bool | None = None,
    storage: StorageStr = "auto",
    cache_key_exclude: Iterable[str] = None,
    cache_verbose: bool | None = None,
    reset: bool | None = None,
) -> Callable[..., Any]
```

Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function
to cache its return values. The cache behavior can be customized through
keyword arguments.

**Parameters:**

- **func** (<code>Callable</code>) ‚Äì The function to be decorated. If None, this
  returns a partial decorator with the passed keyword arguments.
- **cache_max_age** (<code>str | int</code>) ‚Äì A string with a numbered component and units. Supported units are seconds (s), minutes (m), hours (h), and
  days (d) (e.g. "48h", "10s", etc.).
- **path** (<code>str</code>) ‚Äì Folder to append to the configured cache directory.
- **concurrent_lock_timeout** (<code>str | int</code>) ‚Äì Max amount of time in seconds for subsequent concurrent calls to wait for a previous
  concurrent call to finish execution and to write the cache file.
- **cache_reset** (<code>bool | None</code>) ‚Äì Ignore `cache_max_age` and overwrite cached result.
- **storage** (<code>StorageStr</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.
- **cache_key_exclude** (<code>Iterable[str]</code>) ‚Äì An iterable of parameter names to exclude from the cache key calculation. Useful for
  arguments that do not affect the result of the function and could cause unintended cache expiry (e.g.
  database connection objects)
- **cache_verbose** (<code>bool | None</code>) ‚Äì Print a message when a cached result is returned

Returns:
Callable: A decorator that, when applied to a function, caches its
return values according to the specified keyword arguments.

**Examples:**

Use the `@cache` decorator to cache the return value of a function in a custom path.

```py
@cache(path="/tmp/custom_path/")
def expensive_function():
    # Function implementation goes here
    return result
```

If the output of a cached function changes, for example if remote data is modified,
it can be reset by running the function with the `cache_reset` keyword argument. Afterward,
the argument can be cleared.

```py
@cache(path="/tmp/custom_path/", cache_reset=True)
def expensive_function():
    # Function implementation goes here
    return result
```

---

## fused.load

```python
load(
    url_or_udf: Union[str, Path],
    /,
    *,
    cache_key: Any = None,
    import_globals: bool = True,
) -> AnyBaseUdf
```

Loads a UDF from various sources including GitHub URLs,
and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused
platform-specific identifier composed of an email and UDF name. It intelligently
determines the source type based on the format of the input and retrieves the UDF
accordingly.

**Parameters:**

- **url_or_udf** (<code>Union[str, Path]</code>) ‚Äì A string representing the location of the UDF, or the raw code of the UDF.
  The location can be a GitHub URL starting with "https://github.com",
  a Fused platform-specific identifier in the format "email/udf_name",
  or a local file path pointing to a Python file.
- **cache_key** (<code>Any</code>) ‚Äì An optional key used for caching the loaded UDF. If provided, the function
  will attempt to load the UDF from cache using this key before attempting to
  load it from the specified source. Defaults to None, indicating no caching.
- **import_globals** (<code>bool</code>) ‚Äì Expose the globals defined in the UDF's context as attributes on the UDF object (default True).
  This requires executing the code of the UDF. To globally configure this behavior, use `fused.options.never_import`.

**Returns:**

- **AnyBaseUdf** (<code>AnyBaseUdf</code>) ‚Äì An instance of the loaded UDF.

**Raises:**

- <code>ValueError</code> ‚Äì If the URL or Fused platform-specific identifier format is incorrect or
  cannot be parsed.
- <code>Exception</code> ‚Äì For errors related to network issues, file access permissions, or other
  unforeseen errors during the loading process.

**Examples:**

Load a UDF from a GitHub URL:

```py
udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
```

Load a UDF using a Fused platform-specific identifier:

```py
udf = fused.load("username@fused.io/REM_with_HyRiver")
```

---

## fused.run

```python
run(
    udf: Union[str, None, UdfJobStepConfig, Udf, UdfAccessToken] = None,
    *args: Union[str, None, UdfJobStepConfig, Udf, UdfAccessToken],
    x: Optional[int] = None,
    y: Optional[int] = None,
    z: Optional[int] = None,
    sync: bool = True,
    engine: Optional[Literal["remote", "local"]] = None,
    type: Optional[Literal["tile", "file"]] = None,
    max_retry: int = 0,
    cache_max_age: Optional[str] = None,
    cache: bool = True,
    parameters: Optional[Dict[str, Any]] = None,
    _return_response: Optional[bool] = False,
    _ignore_unknown_arguments: bool = False,
    **kw_parameters: bool
) -> Union[
    ResultType,
    Coroutine[ResultType, None, None],
    UdfEvaluationResult,
    Coroutine[UdfEvaluationResult, None, None],
]
```

Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote),
with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and
allows for both synchronous and asynchronous execution. It dynamically determines the execution
path based on the provided parameters.

**Parameters:**

- **udf** (<code>str, Udf or UdfJobStepConfig</code>) ‚Äì the UDF to execute.
  The UDF can be specified in several ways:
  - A string representing a UDF name or UDF shared token.
  - A UDF object.
  - A UdfJobStepConfig object for detailed execution configuration.
- **x, y, z** (<code>int</code>) ‚Äì Tile coordinates for tile-based UDF execution.
- **sync** (<code>bool</code>) ‚Äì If True, execute the UDF synchronously. If False, execute asynchronously.
- **engine** (<code>Optional\[Literal['remote', 'local']\]</code>) ‚Äì The execution engine to use ('remote' or 'local').
- **type** (<code>Optional\[Literal['tile', 'file']\]</code>) ‚Äì The type of UDF execution ('tile' or 'file').
- **max_retry** (<code>int</code>) ‚Äì The maximum number of retries to attempt if the UDF fails.
  By default does not retry.
- **cache_max_age** (<code>Optional[str]</code>) ‚Äì The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. ‚Äú48h‚Äù, ‚Äú10s‚Äù, etc.).
  Default is `None` so a UDF run with `fused.run()` will follow `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) ‚Äì Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **parameters** (<code>Optional\[Dict[str, Any]\]</code>) ‚Äì Additional parameters to pass to the UDF.
- \*\***kw_parameters** ‚Äì Additional parameters to pass to the UDF.

**Raises:**

- <code>ValueError</code> ‚Äì If the UDF is not specified or is specified in more than one way.
- <code>TypeError</code> ‚Äì If the first parameter is not of an expected type.
- <code>Warning</code> ‚Äì Various warnings are issued for ignored parameters based on the execution path chosen.

**Returns:**

- <code>Union\[ResultType, Coroutine\[ResultType, None, None\], UdfEvaluationResult, Coroutine\[UdfEvaluationResult, None, None\]\]</code> ‚Äì The result of the UDF execution, which varies based on the UDF and execution path.

**Examples:**

Run a UDF saved in the Fused system:

```py
fused.run("username@fused.io/my_udf_name")
```

Run a UDF saved in GitHub:

```py
loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

Run a UDF saved in a local directory:

```py
loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

This function dynamically determines the execution path and parameters based on the inputs.
It is designed to be flexible and support various UDF execution scenarios.
</details>

---

## fused.submit

```python
submit(
    udf,
    arg_list,
    /,
    *,
    engine: Optional[Literal["remote", "local"]] = "remote",
    max_workers: Optional[int] = None,
    max_retry: int = 2,
    debug_mode: bool = False,
    collect: bool = True,
    cache_max_age: Optional[str] = None,
    cache: bool = True,
    ignore_exceptions: bool = False,
    flatten: bool = True,
    _before_run: Optional[float] = None,
    _before_submit: Optional[float] = 0.01,
    **kwargs: Optional[float],
) -> Union[JobPool, ResultType, pd.DataFrame]
```

Executes a user-defined function (UDF) multiple times for a list of input
parameters, and return immediately a "lazy" JobPool object allowing
to inspect the jobs and wait on the results.

See `fused.run` for more details on the UDF execution.

**Parameters:**

- **udf** ‚Äì the UDF to execute.
  See `fused.run` for more details on how to specify the UDF.
- **arg_list** ‚Äì a list of input parameters for the UDF. Can be specified as:
  - a list of values for parametrizing over a single parameter, i.e.
    the first parameter of the UDF
  - a list of dictionaries for parametrizing over multiple parameters
  - A DataFrame for parametrizing over multiple parameters where each
    row is a set of parameters
- **engine** (<code>Optional\[Literal['remote', 'local']\]</code>) ‚Äì The execution engine to use. Defaults to 'remote'.
- **max_workers** (<code>Optional[int]</code>) ‚Äì The maximum number of workers to use. Defaults to 32.
- **max_retry** (<code>int</code>) ‚Äì The maximum number of retries for failed jobs. Defaults to 2.
- **debug_mode** (<code>bool</code>) ‚Äì If True, executes only the first item in arg_list directly using
  `fused.run()`, useful for debugging UDF execution. Default is False.
- **collect** (<code>bool</code>) ‚Äì If True, waits for all jobs to complete and returns the collected DataFrame
  containing the results. If False, returns a JobPool object, which is non-blocking
  and allows you to inspect the individual results and logs.
  Default is True.
- **cache_max_age** (<code>Optional[str]</code>) ‚Äì The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d)
  (e.g. ‚Äú48h‚Äù, ‚Äú10s‚Äù, etc.).
  Default is `None` so a UDF run with `fused.run()` will follow
  `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) ‚Äì Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **ignore_exceptions** (<code>bool</code>) ‚Äì Set to True to ignore exceptions when collecting results.
  Runs that result in exceptions will be silently ignored. Defaults to False.
- **flatten** (<code>bool</code>) ‚Äì Set to True to receive a DataFrame of results, without nesting of a
  `results` column, when collecting results. When False, results will be nested
  in a `results` column. If the UDF does not return a DataFrame (e.g. a string
  instead,) results will be nested in a `results` column regardless of this setting.
  Defaults to True.
- \*\***kwargs** ‚Äì Additional (constant) keyword arguments to pass to the UDF.

**Returns:**

- <code>Union\[JobPool, ResultType, DataFrame\]</code> ‚Äì JobPool

**Examples:**

Run a UDF multiple times for the values 0 to 9 passed to as the first
positional argument of the UDF:

```py
df = fused.submit("username@fused.io/my_udf_name", range(10))
```

Being explicit about the parameter name:

```py
df = fused.submit(udf, [dict(n=i) for i in range(10)])
```

Get the pool of ongoing tasks:

```py
pool = fused.submit(udf, [dict(n=i) for i in range(10)], collect=False)
```

---

## fused.download

```python
download(url: str, file_path: str, storage: StorageStr = 'auto') -> str
```

Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

üí° Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

üí° Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.

**Parameters:**

- **url** (<code>str</code>) ‚Äì The URL to download.
- **file_path** (<code>str</code>) ‚Äì The local path where to save the file.
- **storage** (<code>StorageStr</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> ‚Äì The function downloads the file only on the first execution, and returns the file path.

**Examples:**

```python
@fused.udf
def geodataframe_from_geojson():

    url = "s3://sample_bucket/my_geojson.zip"
    path = fused.core.download(url, "tmp/my_geojson.zip")
    gdf = gpd.read_file(path)
    return gdf
```

---

## fused.ingest

```python
ingest(
    input: str | Path | Sequence[str | Path] | gpd.GeoDataFrame,
    output: str | None = None,
    *,
    output_metadata: str | None = None,
    schema: Schema | None = None,
    file_suffix: str | None = None,
    load_columns: Sequence[str] | None = None,
    remove_cols: Sequence[str] | None = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: bool | None = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: int | float | None = None,
    partitioning_maximum_per_chunk: int | float | None = None,
    partitioning_max_width_ratio: int | float = 2,
    partitioning_max_height_ratio: int | float = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: float | None = None,
    subdivide_stop: float | None = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: tuple[str, str] | None = None,
    partitioning_schema_input: str | pd.DataFrame | None = None,
    gdal_config: GDALOpenConfig | dict[str, Any] | None = None
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Parameters:**

- **input** (<code>str | Path | Sequence[str | Path] | gpd.GeoDataFrame</code>) ‚Äì A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.

- **output** (<code>str | None</code>) ‚Äì Location on S3 to write the `main` table to.

- **output_metadata** (<code>str | None</code>) ‚Äì Location on S3 to write the `fused` table to.

- **schema** (<code>Schema | None</code>) ‚Äì Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.

- **file_suffix** (<code>str | None</code>) ‚Äì filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.

- **load_columns** (<code>Sequence[str] | None</code>) ‚Äì Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.

- **remove_cols** (<code>Sequence[str] | None</code>) ‚Äì The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.

- **explode_geometries** (<code>bool</code>) ‚Äì Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.

- **drop_out_of_bounds** (<code>bool | None</code>) ‚Äì Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.

- **partitioning_method** (<code>Literal['area', 'length', 'coords', 'rows']</code>) ‚Äì The method to use for grouping rows into partitions. Defaults to `"rows"`.

  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.

- **partitioning_maximum_per_file** (<code>int | float | None</code>) ‚Äì Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/10th the total area of all geometries. Defaults to `None`.

- **partitioning_maximum_per_chunk** (<code>int | float | None</code>) ‚Äì Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/100th the total area of all geometries. Defaults to `None`.

- **partitioning_max_width_ratio** (<code>int | float</code>) ‚Äì The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.

- **partitioning_max_height_ratio** (<code>int | float</code>) ‚Äì The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.

- **partitioning_force_utm** (<code>Literal['file', 'chunk', None]</code>) ‚Äì Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".

- **partitioning_split_method** (<code>Literal['mean', 'median']</code>) ‚Äì How to split one partition into children. Defaults to `"mean"` (this may change in the future).

  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.

- **subdivide_method** (<code>Literal['area', None]</code>) ‚Äì The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).

- **subdivide_start** (<code>float | None</code>) ‚Äì The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.

- **subdivide_stop** (<code>float | None</code>) ‚Äì The value below which geometries will not be subdivided into smaller parts, according to `subdivide_method`. Recommended to be equal to subdivide_start. If `None`, geometries will be subdivided up to a recursion depth of 100 or until the subdivided geometry is rectangular.

- **split_identical_centroids** (<code>bool</code>) ‚Äì If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".

- **target_num_chunks** (<code>int</code>) ‚Äì The target for the number of files if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files generated can be higher or lower than this number, depending on the spatial distribution of the data itself.

- **lonlat_cols** (<code>tuple[str, str] | None</code>) ‚Äì Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

  ```py
  fused.ingest(
      ...,
      lonlat_cols=("x", "y")
  )
  ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- **gdal_config** (<code>GDALOpenConfig | dict[str, Any] | None</code>) ‚Äì Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

  ```py
  fused.ingest(
      ...,
      gdal_config=
      }
  )
  ```

**Returns:**

- <code>GeospatialPartitionJobStepConfig</code> ‚Äì Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

**Examples:**

For example, to ingest the California Census dataset for the year 2022:

```py
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).execute()
```

---

#### `job.run_remote`

```python showLineNumbers
def run_remote(output_table: Optional[str] = ...,
    instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
    *,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: List[str] | None = None,
    image_name: Optional[str] = None,
    ignore_no_udf: bool = False,
    ignore_no_output: bool = False,
    validate_imports: Optional[bool] = None,
    validate_inputs: bool = True,
    overwrite: Optional[bool] = None) -> RunResponse
```

Begin execution of the ingestion job by calling `run_remote` on the job object.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

#### Monitor and manage job

Calling `run_remote` returns a `RunResponse` object with helper methods.

```python showLineNumbers
# Declare ingest job
job = fused.ingest(
  input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
  output="s3://fused-sample/census/ca_bg_2022/main/"
)

# Start ingest job
job_id = job.run_remote()
```

Fetch the job status.

```python showLineNumbers
job_id.get_status()
```

Fetch and print the job's logs.

```python showLineNumbers
job_id.print_logs()
```

Determine the job's execution time.

```python showLineNumbers
job_id.get_exec_time()
```

Continuously print the job's logs.

```python showLineNumbers
job_id.tail_logs()
```

Cancel the job.

```python showLineNumbers
job_id.cancel()
```

---

## fused.file_path

```python
file_path(
    file_path: str, mkdir: bool = True, storage: StorageStr = "auto"
) -> str
```

Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF.
It takes a relative file_path, creates the corresponding directory structure,
and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files,
such as when writing intermediary files to disk when processing large datasets.
`file_path` ensures that necessary directories exist.
The directory is kept for 12h.

**Parameters:**

- **file_path** (<code>str</code>) ‚Äì The relative file path to locate.
- **mkdir** (<code>bool</code>) ‚Äì If True, create the directory if it doesn't already exist. Defaults to True.
- **storage** (<code>StorageStr</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> ‚Äì The located file path.

---

## fused.get_chunks_metadata

```python
get_chunks_metadata(url: str) -> gpd.GeoDataFrame
```

Returns a GeoDataFrame with each chunk in the table as a row.

**Parameters:**

- **url** (<code>str</code>) ‚Äì URL of the table.

---

## fused.get_chunk_from_table

```python
get_chunk_from_table(
    url: str,
    file_id: Union[str, int, None],
    chunk_id: Optional[int],
    *,
    columns: Optional[Iterable[str]] = None
) -> gpd.GeoDataFrame
```

Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

**Parameters:**

- **url** (<code>str</code>) ‚Äì URL of the table.
- **file_id** (<code>Union[str, int, None]</code>) ‚Äì File ID to read.
- **chunk_id** (<code>Optional[int]</code>) ‚Äì Chunk ID to read.

**Other Parameters:**

- **columns** (<code>Optional\[Iterable\[str\]\]</code>) ‚Äì Read only the specified columns.

---

================================================================================

# WORKBENCH

## Account
Path: workbench/account.mdx
URL: https://docs.fused.io/workbench/account/

On the Account page, you can edit your Username, view basic account information, and check your usage in the Usage section. You can also access the Server, Auth, and Instance tabs to view authentication settings and instance details.

 <ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/account_page_edit2.mp4" width="100%" />

================================================================================

## Adding a Map to your App
Path: workbench/app-builder/add-a-map.mdx
URL: https://docs.fused.io/workbench/app-builder/add-a-map/

For many geospatial applications you will want to add a map to your Fused App, especially if your UDF returns a Map `Tile`.

This section shows a few examples of how you can do that. While we do recommend you use `pydeck` (the Python implementation of deck.gl) for its versatility, you can use other options like `folium`

    Note that you need to install dependencies with `micropip` inside your Fused app. More on this here.

## Pydeck

Create a pydeck GeoJsonLayer that plots a simple GeoDataFrame.

You can find this app right here and test it for yourself!

Here's what this would look like:

[Image: ImgStdout]

```python showLineNumbers
# installing pydeck & geopandas inside Fused app

await micropip.install(['pydeck', 'geopandas'])

st.write("# Hello World! üëã")
st.write("Here's a simple example of a Fused app plotting NYC metro stations")

DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
gdf = gpd.read_file(DATASET)
# We buffer the points to make them more visible on our map
gdf.geometry = gdf.geometry.buffer(0.001)

# Creating an empty pydeck element
deck = st.empty()

# Initiating pydeck element with view over NYC
view_state = pdk.ViewState(
    latitude=40.73,
    longitude=-73.96,
    zoom=10,
    pitch=0
)

# Creating a GeoJSON layer with our GeoDataFrame
geojson_layer = pdk.Layer(
    'GeoJsonLayer',
    gdf,
)

updated_deck = pdk.Deck(
    layers=[geojson_layer],
    initial_view_state=view_state,
    map_style='mapbox://styles/mapbox/light-v9'
)

deck.pydeck_chart(updated_deck)
```

Read more about how to use Pydeck on their official documentation.

    This example shows how to plot a `GeoDataFrame` directly, but you could swap this out for a UDF that returns a `GeoDataFrame` too:

    ```python showLineNumbers
    # DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
    # gdf = gpd.read_file(DATASET)
    # highlight-next-line
    gdf = fused.run("YOUR_UDF_RETURNING_A_GDF")
    ```

    Read more about `fused.run` in the dedicated section

## Folium

Create a streamlit-folium `TileLayer` that calls a UDF HTTP endpoint.

```python showLineNumbers

from streamlit_folium import st_folium

m = folium.Map(location=[22.5, -115], zoom_start=4)
url_raster = 'https://www.fused.io/server/v1/realtime-shared/fsh_3QYQiMYzgyV18rUBdrOEpO/run/tiles///?dtype_out_raster=png'
folium.raster_layers.TileLayer(tiles=url_raster, attr='fu', interactive=True,).add_to(m)
st_folium(m)
```

================================================================================

## App Builder
Path: workbench/app-builder/app-builder.mdx
URL: https://docs.fused.io/workbench/app-builder/app-builder/

Learn everything there is about Fused Apps

<DocCardList className="DocCardList--no-description"/>

================================================================================

## App Builder Overview
Path: workbench/app-builder/app-overview.mdx
URL: https://docs.fused.io/workbench/app-builder/app-overview/

# App Builder Overview

The App Builder is an IDE to transform User Defined Functions (UDFs) into interactive, shareable apps.

Data scientists often need to make analytics interactive and accessible to broader audiences. However, building traditional React apps with maps and widgets can be impractical, especially considering prototypes might be discarded. Additionally, frontend frameworks are not well-suited for transforming data or handling large datasets.

With this in mind, the App Builder enables users to build and run apps with serverless Streamlit, an open source framework to deliver dynamic data apps with just a few lines of Python. These are some of its capabilities to keep in mind:

- Build apps
- Install dependencies
- Troubleshoot
- Call UDFs and cache responses
- Share live apps

\
Try running the code snippets below to acquaint yourself with the App Builder.

```python showLineNumbers

st.write("Hello, *Fused!* :rocket:")
```

## Dependencies

To set Python packages for your app, only packages compatible with Pyodide are supported. Please get in touch if you need help with a specific package.

You may also choose to install dependencies at runtime to reduce start-up time. Use micropip to install packages at runtime. 

```python showLineNumbers

await micropip.install(["geopandas", "mercantile"])
```

## Write UDFs

You may define UDFs in the App Builder's code editor and invoke them with `fused.run`. This snippet creates a UDF that returns a `DataFrame` with a column of zeros with a length determined by a slider widget.

```python showLineNumbers

count = st.slider("Count", 1, 10, 4)

@fused.udf
def udf(count: int = 1):

    return pd.DataFrame()

df = fused.run(udf, count=count)
st.write(df)
```

You may also run the UDF on a remote worker by setting `engine='remote'` in the `fused.run` call.

```python showLineNumbers
df = fused.run(udf, count=count, engine='remote')
```

## Call UDFs

Apps may call UDFs and load their output into memory. This enables them to run resource-intensive operations and use libraries unsupported by Pyodide. These snippets illustrate a few ways to call UDFs.

### With `fused.run` (beta)

Call a UDF by its shared token with `fused.run` and pass parameters from a slider.

```python showLineNumbers

threshold = st.slider("Count filter", 0, 400, 25)
df = fused.run('UDF_DuckDB_H3_SF', count=threshold)
```

### HTTP endpoints

Call UDF HTTP endpoints with the requests library and pass parameters from a dropdown selectbox.

```python showLineNumbers

city = st.selectbox("Select city", ("Boston", "Paris", "New York"))
url = f"https://www.fused.io/server/v1/realtime-shared/fsh_2wEv0k8Xu2grl4vTVRlGVk/run/file?dtype_out_vector=geojson&city="
response = requests.get(url)
st.json(response.json())
```

Render the raster response of UDFs as images.

```python showLineNumbers

st.image('https://www.fused.io/server/v1/realtime-shared/fsh_7Yuq2R1Ru1x5hgEEfNDF5t/run/tiles/11/583/787?dtype_out_raster=png')
```

## Caching

It can be helpful to cache the response of UDF calls. To cache a function in Streamlit, decorate it with `@st.cache_data`.

```python showLineNumbers

@st.cache_data
def cached_output():
    return fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9')

df = cached_output()

st.write(fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9'))
```

## Share

The App Builder settings menu includes options to generate a URL to share the app or embed it with an `<iframe>`.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/app_builder_share_edit5.mp4" width="100%" />

### Shareable links
The app is saved to Fused and referenced by a token, such as `https://www.fused.io/app/fsh_7hVSIymGijZ53YGmEs2EIM`.

## Troubleshoot

Click "Rerun" on the top-right menu of the App view in case things aren't working as expected.

[Image: Rerun App]

================================================================================

## File Explorer
Path: workbench/file-explorer.mdx
URL: https://docs.fused.io/workbench/file-explorer/

The File Explorer provides a streamlined interface to browse, preview, and open files in cloud object storage and the mounted disk. Fused supports Amazon S3 and Google Cloud Storage, with more integrations coming soon.

When working with data, it can be time-consuming to track down datasets, request access, download gigabytes of data, and write boilerplate code to read files. The File Explorer simplifies this process by enabling users to easily browse any object storage bucket, visualize file contents without writing code, and quickly create User Defined Functions (UDFs) in the UDF Builder with templates for specific file types.

- Browse object storage buckets and list their files
- Quickly preview files
- Create a new UDF from a template to open the file
- Connect an S3 or GCS bucket

## Preview

The Explorer displays a bucket's directories and objects as folders. Each listed file shows metadata such as file size and path, along with utilities to download or delete the file, copy its path, generate a signed URL, and create a UDF to open it.

Click on a file to preview its content. If the file has a spatial component, it will be displayed on the map, allowing you to zoom and pan to explore the rendered data. For images or other file types, Fused will make a best-effort to render and display the content.

## Create UDF

Create Fused UDFs using templates for common file types. Double-click on a file to create a new templated UDF that reads the file, or find additional readers in the file's kebab menu. Parquet tables show an "Open Table" button to open them at the directory level.

## Template UDFs

Template UDFs are available for common file types (like `CSV`, `Parquet`) and tools (like `DuckDB` and `GeoPandas`). See the latest template UDFs in the UDFs repo.

Supported file types for vector tables include `parquet`, `JSON`, `CSV`, `excel`, `zip`, and `KML`. For raster files `GeoTIFF` and `NetCDF` are supported. If you need a file type that isn't supported, request it on the Fused Discord channel or contribute a template to the community.

## Upload / Download / Edit 

File Explorer also allows you to:
- Upload files (drag & drop)
- Download files
- Create new directories

In File Explorer directories will not be saved if they do not have content in them.

## Connect your own bucket

Alternatively, use this Fused app to automatically structure the policy for you.

The bucket must enable the following CORS settings to allow uploading files from Fused.

### Google Cloud Storage (GCS)

To connect a Google Cloud Storage bucket to your Fused environment, you'll need to follow these steps:

1. Create a Service Account in GCS
Set up a Google Cloud service account with permissions to read, write, and list from the GCS bucket. See the Google Cloud documentation for instructions to:
- Create a Service Account
- Set permissions for the Service Account

2. Download the JSON Key File
Download the JSON Key file associated with the Service Account. This file contains credentials that Fused will use to access the GCS bucket.

3. Set the JSON Key as a Secret
Set the JSON Key as a secret in the secrets management UI. The secret must be named `gcs_fused`.

You then need to write these credentials to a JSON file and pass them to Google as:

```python
@fused.udf
def udf():
    from google.cloud import storage

    # get GCP secrets
    with open("/tmp/gcs_key.json", "w") as f:
        f.write(fused.secrets["gcs_fused"])
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcs_key.json"

    # your code here
```

================================================================================

## Overview
Path: workbench/overview.mdx
Status: UNLISTED
URL: https://docs.fused.io/workbench/overview/

Workbench is a browser-based platform to build with data. It offers developers immediate access to their data and instant feedback as they open files, write code to transform data, and build apps.

Users browse datasets in the File Explorer and open them as UDFs, convert them into analytics scripts in the UDF Builder, share them in the UDF Catalog, and finally turn them into shareable apps with the App Builder. These all revolve around and interact with User Defined Functions (UDFs) - Python functions used to read files, process data, and power apps.

Workbench supports building end-to-end analytics workflows in minutes, but every one of its components is fully replaceable. File Explorer can be swapped out with upstream database connections (like BigQuery, GEE, and others), and the App Builder can be replaced with downstream third-party apps (like DeckGL, Retool, and others).

But Workbench is more than just a platform to work with data ‚Äî it's a portal to a community where creators share their UDFs, apps, and expertise to help each other improve their work.

Continue reading the documentation or get started with Workbench at fused.io/workbench.

================================================================================

## Preferences
Path: workbench/preferences.mdx
URL: https://docs.fused.io/workbench/preferences/

You can access the Preferences page from Workbench by clicking on Preferences in the bottom-left corner.

This page contains general user preferences, experimental features, theme selection, data settings, and other customization options.

[Image: Preferences button location]

This page is roughly split in 2 sections:

[Image: preferences page sections]

- **Environment**: Setting up kernels, Secret management and environment variables. See below for details.
- **General Preferences**: Such as theme, file formats, etc. These are also searchable from Command Palette.

## Kernel

Selecting a kernel in Workbench sets the environment for your session, providing access to Python libraries at runtime. Contact the Fused team through Slack, Discord or email to set up new kernels.

[Image: kernel]

## Secrets management

Store and manage secrets that are securely encrypted in the backend.

[Image: secrets management]

Add secrets directly in the UI by clicking "+ Add new secret"

You can then retrieve secrets from your UDF:

```python showLineNumbers

fused.secrets["my_secret"]
```

You can also list all available secrets with:

```python showLineNumbers
dir(fused.secrets)
```

## Environment variables

[Image: env variables]

Environment variables in Fused are pre-configured by the Fused team. Contact the Fused team to set new ones. 

These variables can be accessed in the usual way through Python's os module:

```python showLineNumbers

os.environ["ENV_VAR_NAME"]
```

================================================================================

## Code Editor
Path: workbench/udf-builder/code-editor.mdx
URL: https://docs.fused.io/workbench/udf-builder/code-editor/

The Code Editor is where developers write UDFs using standard Python libraries and installed dependencies. The Editor tab contain functionality to organize code, create HTTP endpoints, and configure the UDF.

[Image: Code Editor]

## Editor

The editor contains the UDF's function declaration. Whenever code is updated, Fused automatically runs the function named `udf` that is decorated with `@fused.udf` and returns the output. Other UDFs declared in the editor are ignored unless referenced by the main `udf` function.

## Default Parameters hierarchy

Just like any other Python function, UDFs can have default parameters. In Workbench, these can be set in 2 different location though:
1. In the UDF Layer (the left side of the code editor)
2. In Python directly

The hierarchy is as follows:
1. Workbench UDF Layer UI
2. In-Python parameter (when defining the function)
3. HTTP Request / Calling UDF with shared token

[Image: Default parameters hierarchy]

Let's start with a simple UDF:
```python
@fused.udf
def udf(my_default_param: float = 1.5):
    
    print(f"")
    return my_default_param
```

In Workbench, without defining else we would get `1.5` back:

[Image: default python params]

Now we can set default parameters in the UDF layer UI:

### Rationale

We decide to **prioritize the UI-based parameters** over Python so that non technical users can interact with a UDF in Map View without having to interact with Python at all:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/editing_parameters_in_ui2.mp4" width="100%" />

Regardless of the defaults defined in Python, the UI-based parameters will always take precedence.

## Debug

The code editor highlights errors in the code and shows error logs to help debug.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/debug_code_editor_edit3.mp4" width="100%" />

## Profiler

UDF Builder comes with a built-in profiler that can be used to analyze the performance of a UDF.

This gives you the line-by-line execution time in the UDF.

Note:
- Values are only available for the current run of the UDF.
- Running the same UDF twice might lead to different runtimes especially if you call cached functions or cached UDFs.
- When a line is called multiple times the profiler shows the sum time of all calls & number of hits:

[Image: multiple hits profiler]

## Save a UDFs

UDFs show an asterisk (`*`) next to their name when changes have been made since the last save. Clicking the "Save" icon saves the present state of the UDF under your account's UDFs.

If the "Save" icon appears greyed out, it means you're viewing a read-only version of the UDF. Make a copy to create a new version than can be modified and saved.

[Image: Save Icon]

## Utils Module

A Fused UDF can import Python objects from its accompanying utils Module, defined in the Utils Tab's code editor. You can import functions from it in your UDF with `from utils import my_function`.

Here is an example in the Public Overture_Maps_example UDF:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    release: str = "2025-01-22-0",
):
    from utils import get_overture

    gdf = get_overture(
        bounds=bounds,
        release=release,
    )
    return gdf
```
[Image: UDF Builder Utils]

### Auto, Tile, and Single (Viewport, Parameter)

On UDF Builder, UDFs can explicitly be set to run as Tile or File - or autoselect between the two if the `bounds` object is typed.

[Image: File]

================================================================================

## Collections
Path: workbench/udf-builder/collections.mdx
URL: https://docs.fused.io/workbench/udf-builder/collections/

A Collection is a way for users to organize their UDFs into different projects. This allows you to use Workbench for completely different, unrelated projects and organise your UDFs in a cleaner way. 

Collection is still in Beta so you need to enable it under "Preferences -> Enable UDF Collections [Beta]" to access it

[Image: Enable UDF Collections]

You can:
- Save all current open UDFs into a Collection (after giving it a name)
- Upload / Download Collections via Workbench (Click the 3 dots next to Collection name)
- Delete Collections (this doesn't delete the UDFs inside, only the Collection)

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/collections_edit3.mp4" width="100%" />

We are actively working on expanding Collection features!

### Examples of Collections in use

- See how we recommend using Collections as part of our Best Practices

================================================================================

## Map
Path: workbench/udf-builder/map.mdx
URL: https://docs.fused.io/workbench/udf-builder/map/

As developers edit UDFs in the Code Editor and explore data, they can receive immediate visual feedback on how the code's transformations affect the data.

## Geospatial data

Fused will render `gpd.GeoDataFrame`, `gpd.GeoSeries`, and `shapely geometry` UDF outputs as geometries on the map if their CRS is `EPSG:4326`. If the CRS differs, Fused will make a best-effort to project and render the geometries correctly.

To render array (raster) objects on the map, they must be `uint8` and define their spatial extent. Objects like `xarray.DataArray` already contain spatial metadata. The spatial extent of arrays without spatial metadata, like `numpy.ndarray`, can be specified with a geometry object or an array bounds as `[xmin, ymin, xmax, ymax]`. If the bounds are not present, they default to `(-180, -90, 180, 90)`.

```python showLineNumbers
return np.array([[‚Ä¶], [‚Ä¶]]), [xmin, ymin, xmax, ymax]
```

For UDFs that return map Tiles, Fused runs the UDF for only the Tiles in the viewport. This enables efficient analysis on a fraction of a dataset.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/map_overture.mp4" width="100%" />

### Map controls

The map can be panned by dragging the viewport, zoomed in and out, and rotated with `Cmd` + `Click` + drag on MacOS (`Ctrl` + `Click` + drag on Windows / Linux)

The top of the map has controls to interact with the viewport. These include an address search bar, a basemap selector, a screenshot button, a fullscreen toggle, and a dropdown to freeze, resume, or reset UDF execution.

You can change the basemap by setting it in the map style settings, located at the top right of the UDF Builder map. Currently, light, dark, satellite, and blank basemaps are supported.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/mapcontrols_edit.mp4" width="100%" />

## Debug

Clicking a rendered feature enters debug mode. To exit, press `Escape` or click the `X` in the map tooltip header."

When data renders successfully on the map, clicking or hovering on it shows attributes for selected pixels or geometries. When data doesn't render, clicking errored tiles shows an error code, and the full error details can be copied as JSON. Additional debugging information can be found in the Results pane.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/debugclick3.mp4" width="100%" />

================================================================================

## Navigation
Path: workbench/udf-builder/navigation.mdx
URL: https://docs.fused.io/workbench/udf-builder/navigation/

The Navigation pane on the left side of the UDF Builder displays the UDFs available to the user. UDFs are listed as layers. Their order corresponds to their stacking order on the map. Each layer includes icons to delete, reorder, and toggle the visibility of UDFs. The selected layer becomes the "active" layer. Its code appears in the Code Editor and its output in the Results pane.

## Zoom to Layer

"Zoom to layer" allows you to quickly zoom the map to the location of the relevant UDF layer. You would use it when you want to focus the map on a specific area or features defined by the layer, especially when the UDF has a spatial component or a default view state.
UDFs that return an object with a spatial component or have a default view state show a "zoom to layer". Clicking it zooms the map to the relevant location.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/zoom_to_layer_edit3.mp4" width="100%" />

## Layer Visibility

The layer toggle feature allows users to show or hide specific visualizations on the map. These layers provide enhanced map insights, making spatial data easier to understand and interpret.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/layers_edit3.mp4" width="100%" />

## UDF Options

The UDF list includes several interactive options to manage and organize functions efficiently.

### Drag and Drop

Users can reorder UDFs by dragging them up and down within the list. To do this, they need to hover over the UDF name to see the drag handle.

### Close/Delete

If the UDF is saved, a "Close UDF" button is available to exit the editor. If the UDF is unsaved and still being edited, a "Delete UDF" button replaces it, allowing users to discard the changes.

[Image: udfOpt]

## Share

The Share button in Workbench provides access to all options to share the UDF.

[Image: Share UDF]

### Share snippets

UDFs saved in the UDF Builder can be called with HTTP endpoints using the public UDF name and/or tokens.

The "Shared token" section shows snippets to run the UDF using a public token. This allows any application to invoke the UDF without authentication - including `cURL` calls, Lonboard, Leaflet, Mapbox, Google Sheets, DuckDB, the Fused App Builder, and Python applications with `fused.run`.

### Private snippets

Private snippets are now located under the "Shared token" section in the "Share" view. The "Private snippets" section shows snippets that can only be called by services authenticated with a private token. These include `Links` and `Python snippets`.

### Metadata

#### Image preview

[Image: image preview]

#### Tags

UDF tags can be set to help with discoverability in the UDF Catalog.

[Image: Image tags]

#### Description

UDFs can be documented using Markdown with a brief description of their purpose, code, and associated datasets. The description appears in the UDF profile and `README.md` file.

## Toolbar

The toolbar under the UDF name includes buttons to Share, Save and Download. Additional actions like Duplicate, Delete, Pull changes, and Push changes are available via the three-dot menu on the right.

### GitHub

Organizations with the GitHub Integration enabled can push UDFs to a GitHub repository as a Pull Request or restore a prior version of a UDF from the commit history.

### Download

Clicking "Download" downloads a `.zip` file with the UDF code, module, and configuration.

[Image: File]

### Default parameter values

UDFs by default run with the parameters specified in their function declaration. Predefined default parameter values take precedence and appearing within the Parameters section on the left side of the screen, under the selected UDF's options.

<ReactPlayer playsinline= className="video__player" loop= playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/default_parameters_edit3.mp4" width="100%" />

================================================================================

## Results
Path: workbench/udf-builder/results.mdx
URL: https://docs.fused.io/workbench/udf-builder/results/

Exploring and analyzing data involves scanning logs, previewing intermediary outputs, and debugging errors.

The Results section of the UDF Builder dynamically displays information related to UDF execution, including visual output, debug info, and request details available via the toolbar.

### Toolbar

The toolbar at the top of the Results pane shows the latitude and longitude of the viewport center and includes a menu button for displaying debug info and copying request details.

[Image: resultstoolbar]

================================================================================

## UDF Builder
Path: workbench/udf-builder/udf-builder.mdx
URL: https://docs.fused.io/workbench/udf-builder/udf-builder/

The UDF Builder is an IDE to write User Defined Functions (UDFs) and visualize their output.

Data analysis is inherently iterative. It involves loading data, writing code, executing the code, and visualizing results. As developers write and debug their code in the UDF Builder, Fused gives them immediate feedback by automatically re-running the UDF and showing the output. Developers can write UDFs in the Code Editor, preview the output on the Map, and style the visualization. To help debug, the Results pane displays logs and errors generated by the UDF.

- Write UDFs in the Code Editor
- Manage UDFs as map layers in the Navigation pane
- Preview the UDF's output on the Map and edit the Layer Styling its visualization
- View the UDF's logs in the Results pane

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/docs_udfbuilderv2.mp4" width="100%" />

================================================================================

## Layer Styling
Path: workbench/udf-builder/viz-styling.mdx
URL: https://docs.fused.io/workbench/udf-builder/viz-styling/

# Layer Styling: Visualization Tab

The UDF builder displays data from the UDF into the map view. You can change the visual representation of a UDF's output is configured under the "Visualization" tab:

You'll notice a few differences to the Editor:
- The Visualization tab isn't written in Python, rather this is a JSON file
- There are a few defaults namely `TileLayer`, `rasterLayer` and `vectorLayer`
- "Preset" button. Try it out for yourself, see what happens! (you can always `Ctrl + Z` to go back if you don't like it)

[Image: File]

You can explore this example right here for yourself. Click on the "UDF Builder" icon on the left to open the code editor:

  ```json
  ,
    "vectorLayer": ,
      // highlight-next-line
      "getFillColor": ,
      // highlight-next-line
      "getElevation": ,
      "elevationScale": 10
    }
  }
  ```

## Vector `GeoJsonLayer`

The visualization of the output of a UDF that returns a `DataFrame` or `GeoDataFrame` can be configured dynamically based on column values. Attributes of the `vectorLayer` can be set to use either hardcoded values or column values, such as:
- Line color (`getLineColor`) and line width (`getLineWidth`)
- Elevation (`getElevation`) with `extruded` set to true
- `lineWidthUnits` helps maintain visual consistency across zoom levels when set to `pixels`

[Image: File]

  ```json
  ,
    "vectorLayer": ,
      "getFillColor": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]"
    }
  }
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

#### Based on a property value

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
  }
  ```

Alternatively, to support a default color when a value is absent.

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
    }
  }
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": 
    }
  }
  ```

Note that unexpected behaviors may arise if too many domains are used.

#### Using `colorContinuous`

To set the color with colorContinuous, use the `attr` property to specify the table column for the values, and the `colors` property to define the desired color palette.

  ```json
  ,
  "hexLayer": ,
   "getFillColor": ,
    "getElevation": ,
    "elevationScale": 10
  }
}
  ```

## Raster `BitmapLayer`

Raster layers can be set to display a tooltip on hover by setting the `pickable` property to `true`. See DeckGL documentation.

[Image: Crops]

  ```json
  ,
    "rasterLayer": 
  }
  ```

  ```json
  ,
    "rasterLayer": ,
    "vectorLayer": ,
      "getFillColor": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]"
    },
    "loadingLayer": ,
    "errorLayer": 
  }
  ```

  Let's take the example of a UDF that returns a `GeoDataFrame` with `hex` values:
  
  ```python showLineNumbers
  @fused.udf()
  def udf(
      bounds: fused.types.Bounds = None,
  ):  
      # get_hex() is a non-important function for this demo that gives us US counties
      df_hex = get_hex(gdf, hex_res)
      df_hex['state_id'] = [id[:2] for id in df_hex["GEOID"]]
      
      return df_hex 
  ```

  And our visualization JSON looks like this:
  ```json
    
    }
  }
  ```

  You should make sure:
  1. `hexLayer > getFillColor > attr` is set to a column that exists in the `GeoDataFrame` (in this case `state_id`)
  2. Make sure your `attr` column is in either `int` or `float` type, not in `str`. In this case we should cast `state_id` to `int`:

  ```python  showLineNumbers
  @fused.udf()
  def udf(
      bounds: fused.types.Bounds = None,
  ):
      df_hex = get_hex(gdf, hex_res)
      df_hex['state_id'] = [id[:2] for id in df_hex["GEOID"]]
      df_hex['state_id'] = df_hex['state_id'].astype(int)
      
      return df_hex 
  ```

  3. Making sure your values are within the correct domain (`hexLayer > getFillColor > domain`). In our case, we're showing US States, so the domain should be `[0, 50]`.

</details>

================================================================================

## UDF Catalog
Path: workbench/udf-catalog.mdx
URL: https://docs.fused.io/workbench/udf-catalog/

The UDF Catalog offers a searchable collection of User Defined Functions (UDFs) that can be imported into the UDF Builder for editing. It facilitates sharing and discovering UDFs within teams and the broader Fused community.

Data teams frequently encounter silos when looking to share and reuse code snippets & data across different projects. Traditional notebooks pose challenges in version control, sharing individual utility functions, managing dependencies, and reproducing results in new environments. Additionally, assets often end up disconnected from the code that generated them. UDFs on the other hand encapsulate modules, outputs, and code, which addresses issuess related to reproducibility and lineage.

- Create a new UDF
- Search the catalog across UDF categories
- View the detailed profile for each UDF
- Add any UDF from its Github URL
- Delete a UDF you created yourself
- Upload UDFs from JSON ot ZIP files

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/udf_catalog_edit4.mp4" width="100%" />

## Add a UDF

Create a new UDF by clicking the top-right "New UDF" button. This opens a UDF with a base template.
Alternatively, you can use the "Upload UDF" button to import an existing UDF from your local machine.

[Image: UDF Catalog]

## Search

Open the UDF Catalog by Clicking "Add UDF". Search UDFs by name, sort them, and toggle between gallery and list views. Click UDF cards to view their profiles or add them to the UDF Builder.

[Image: Add UDF Button]

### UDF categories

- **All UDFs:** View all available UDFs in one place, including all categories listed below.
- **Public UDFs:** Verified and accessible to all users
- **Community UDFs:** Shared by the community and accessible to all users
- **Team UDFs:** Shared privately within a team in a GitHub repo
- **Saved UDFs:** Private UDFs in the user's account

## Add from GitHub URL

If GitHub integration is enabled on your Workbench, you can paste the link of any UDF from GitHub to open it. This allows you to open a UDF from any branch or revision from GitHub.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/import_udf_from_github_edit2.mp4" width="100%" />

## UDF profile

Select a UDF to view its profile which includes the `README.md` description, last update date, author, link to its code on GitHub, and code preview.

## Contribute to Fused

Fused welcomes your skills and enthusiasm in support of the geospatial community!

There are numerous opportunities to get involved, from contributing code to engaging the community on Discord, LinkedIn, and other social media platforms.

### Where to start?

UDFs can be easily re-used, so before you build something new or share yours with the community, check the UDF Catalog to see if someone has already built something you might benefit from! Here are a few examples of Public & Community UDFs:

- Opening Overture Building Datasets to browse the latest building dataset from Overture's open data on Source Coop
- Exploring Sentinel 2 satellite imagery by using Microsoft's Planetary Computer or AWS Open Data Program
- Computing height for buildings in the US using the ALOS 30m Digital Surface Model

### Publish a UDF to a GitHub repository

Once you write a UDF, you can use the Push to GitHub button in Fused Workbench to publish or update your UDF in a GitHub repository. You will be able to select the repository (e.g., `fusedudf`, `community`, or `public`) and automatically create a pull request on GitHub. Read how to enable the GitHub integration.

To add a UDF to the community repository, select community from the dropdown before creating a pull request.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/push_udf_to_github_edit3.mp4" width="100%" />

Note that once the UDF is committed to the main branch of the repo, you can expect to wait up to 3 minutes for it to deploy and appear in the UDF Catalog.

================================================================================

# USE CASES

## Climate Dashboard
Path: use-cases/climate-dashboard.mdx
URL: https://docs.fused.io/use-cases/climate-dashboard/

# Building a Climate dashboard

We're going to build an interactive dashboard of global temperature data, after processing 1TB of data in a few minutes!

### Install `fused`

```python
pip install "fused[all]"
```

Read more about installing Fused here.

In a notebook:

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the link to authenticate.

Read more about authenticating in Fused.

Each file being about 140MB a quick back of the envelope calculation gives us:
```python
recent_days = [day for day in available_days if day.split('datestr=')[1][:7] in recent_months]
len(recent_days) * 140 / 1000 # size in GB of files we'll process
```

```bash
1005.62
```
</details>

Fused allows us to run a UDF in parallel. So we'll process 1 month of data across hundreds of jobs:

```python
results = fused.submit(
  udf, 
  recent_months, 
  max_workers=250, 
  collect=False
)
```

See a progress bar of jobs running:

```python
results.wait()
```

See how long all the jobs took:

```python
results.total_time()
```

```bash
>>> datetime.timedelta(seconds=40, ...)
```

**We just processed 20 years of worldwide global data, over 1TB in 40s!!**

All we need to do now is aggregate the data by month:

```python
@fused.udf(cache_max_age='0s')
def udf():

    monthlys = fused.api.list(fused.file_path(f"monthly_climate/"))
    file_list = "', '".join(monthlys)
    
    result = duckdb.sql(f"""
       SELECT 
           LEFT(datestr, 7) as month,
           ROUND(AVG(daily_mean_temp), 2) as monthly_mean_temp
       FROM read_parquet([''])
       GROUP BY month
       ORDER BY month
    """).df()

    return result
```

Instead of running this locally, we'll open it in Workbench, Fused's web-based IDE:

```python
# Save to Fused
udf.to_fused("monthly_mean_temp")

# Load again to get the Workbench URL
loaded_udf = fused.load("monthly_mean_temp")
```

Return `loaded_udf` in a notebook and you'll get a URL that takes you to Workbench:

```python
loaded_udf
```

Click on the link to open the UDF in Workbench. Click "+ Add to UDF Builder" 

[Image: Monthly temperature aggregation in Workbench]

================================================================================

## Dark Vessel Detection
Path: use-cases/dark-vessel-detection.mdx
URL: https://docs.fused.io/use-cases/dark-vessel-detection/

_A complete example show casing how to use Fused to ingest data into a geo-partitioned, cloud friendly format, process images & vectors and use UDFs to produce an analysis_

### Requirements
- Access to Fused
- Access to a Jupyter Notebook
- Installing `fused` with `[all]` dependencies (mainly to have `pandas` & `geopandas`):

```python showLineNumbers
pip install "fused[all]"
```

## 1. The problem: Detecting illegal boats

Monitoring what happens at sea isn't the easiest task. Shores are outfitted with radars and each ship has a transponder to publicly broadcast their location (using Automatic Identification System, AIS), but ships sometimes want to hide their location when taking part in illegal activities.

Global Fishing Watch has reported on "dark vessels" comparing Sentinel 1 radar images to public AIS data and matching the two to compare where boats report being versus where they _actually_ are.

In this example, we're going to showcase a basic implementation of a similar analysis to identify _potential_ dark vessels, all in Fused.

[Image: Dark Vessel Detection workflow]

Here's the result of our analysis, running in real time in Fused:

 className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/others/dark-vessel-detection/output_analysis_S1_ais.mp4" width="100%" /> */}

     For the nerds out there, we're using the Ground Range Detected product, not the Radiometrically Terrain Corrected because we're looking at boats in the middle of the ocean, so terrain shouldn't be any issue.
    - This dataset is available as Cloud Optimized GeoTiff through a STAC Catalog, meaning we can directly use this data as is.

</Tabs>

`merged_gdf` now returns a column called `S1_acquisition_time` with the time the Sentinel 1 image was taken.

If we save and call this UDF with a token we can call it from anywhere, from a Jupyter Notebook or from another UDF. Let's create a new UDF in Workbench:

```python  showLineNumbers
# This is a new UDF
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: int="2024-09-03/2024-09-10",
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    return s1_detections
```

This prints out:

```bash
Found 16 Unique Sentinel 1 detections
Sentinel 1 image was acquired at : 2024-09-04 00:19:09
```

We can now create another UDF that will take this `s1_acquisition_month_day_hour_min` date + a bounding box in input and returns all the AIS points in that time + area.

We're going to leverage code from the community for this part, namely reading the AIS data from a geo-partitioned GeoParquet. Fused allows us to easily re-use any code we want and freeze it to a specific commit so it doesn't break our pipelines (read more about this here)

We can use this bit of code called `table_to_tile` which will either load the AIS data or the bounding box depending on our zoom level to keep our UDF fast & responsive.

    You could write a GeoParquet reader from scratch or call a UDF that you have that already does this, you don't have to use this option. But we want to show you how you can re-use bits of code from others here.

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    s1_acquisition_month_day_hour_min:str = '2024-09-04T00:19:09.175874',
    time_delta_in_hours: float = 0.1, # by default 6min (60min * 0.1)
    min_zoom_to_load_data: int = 14,
    ais_table_path: str = "s3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09", # This is the location where we had ingested our geo-partitioned AIS data
    ):
    """Reading AIS data from Fused partitioned AIS data from NOAA (data only available in US)"""

    from datetime import datetime, timedelta

    # Load the utils module
    local_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = local_utils.estimate_zoom(bounds)

    sentinel1_time = pd.to_datetime(s1_acquisition_month_day_hour_min)
    time_delta_in_hours = timedelta(hours=time_delta_in_hours)

    month_date = sentinel1_time.strftime('%Y_%m')
    monthly_ais_table = f"prod_/"
    print(f"")

    @fused.cache
    def getting_ais_from_s3(bounds, monthly_table):
        return local_utils.table_to_tile(
            bounds,
            table=monthly_ais_table,
            use_columns=None,
            min_zoom=min_zoom_to_load_data
        )

    ais_df = getting_ais_from_s3(bounds, monthly_ais_table)

    if ais_df.shape[0] == 0:
        print("No AIS data within this bounds & timeframe. Change bounds or timeframe")
        return ais_df

    if zoom > min_zoom_to_load_data:
        print(f"Zoom is  | Only showing bounds")
        return ais_df

    print(f"")
    ais_df['datetime'] = pd.to_datetime(ais_df['BaseDateTime'])
    mask = (ais_df['datetime'] >= sentinel1_time - time_delta_in_hours) & (ais_df['datetime'] <= sentinel1_time + time_delta_in_hours)
    filtered_ais_df = ais_df[mask]
    print(f'')
    return filtered_ais_df
```

In workbench UDF builder we can now see the output of both of our UDF:

[Image: Notebook run remote print]

We can now see that 1 of these boats doesn't have an associated AIS point (in red).

    You can change the styling of your layers in the Visualize tab to make them look like the screenshot above

Now all we need to do is merge these 2 datasets together and keep all the boats that don't match an AIS point.

## 7. Merging the 2 datasets together

We can expand the UDF we had started in section 6. to call our AIS UDF by passing a bounding box + `s1_acquisition_month_day_hour_min`.

We'll get the AIS data and join it with the Sentinel 1 detected boats by using geopandas `sjoin_nearest` to get the nearest distance of each boat to an AIS point.

Any point with the closest AIS point >100m from the Sentinel 1 boat will be considered a potentiel "dark vessel".

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: str="2024-09-03/2024-09-10",
    ais_search_distance_in_meters: int=10,
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    @fused.cache()
    def get_ais_from_s1_date(s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds):
        return fused.run("fsh_FI1FTq2CVK9sEiX0Uqakv", s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds)

    ais_gdf = get_ais_from_s1_date()

    # Making sure both have the same CRS
    s1_detections.set_crs(ais_gdf.crs, inplace=True)

    # Buffering AIS points to leverage spatial join
    ais_gdf['geometry'] = ais_gdf.geometry.buffer(0.005)

    joined = s1_detections.to_crs(s1_detections.estimate_utm_crs()).sjoin_nearest(
        ais_gdf.to_crs(s1_detections.estimate_utm_crs()),
        how="inner", # Using left, i.e. s1 as keys
        distance_col='distance_in_meters',
    )

    # Dark vessels will be unique S1 points that don't have an AIS point within 10m
    potential_dark_vessels = joined[joined['distance_in_meters'] > ais_search_distance_in_meters]
    print(f"Found  potential dark vessels")

    # back to EPSG:4326
    potential_dark_vessels.to_crs(s1_detections.crs, inplace=True)
    return potential_dark_vessels
```

And now we have a UDF that takes a `time_of_interest` and bounding box and returns potential dark vessel:

[Image: potential dark vessel detection]

## Limitations & Next steps

This is a simple analysis, that makes a lot of relatively naive assumptions (for ex: all bright spots in SAR are boats for example, which only works in open water and not near the shore or around solid structures like ocean wind mills or oil rigs). There's a lot of ways in which it could be improved but provides a good starting point.

This could be improved in a few ways:
- Masking out any shore or known areas with static infrastructure (to limit potential false positives around coastal wind mill farms)

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/others/dark-vessel-detection/false_positive_wind_mills.mp4" width="80%" />
Example of the Block Island Wind Farm in Rhode Island showing up as false positive "potential dark vessel": Wind mills appear as bright spots but don't have any AIS data associated to them
(`bounds=[-71.08534386325134,41.06338103121641,-70.89011235861962,41.15718153299125]` & `s1_acquisition_month_day_hour_min = "2024-09-03T22:43:33"`)

- Using a more sophisticated algorithm to detect boats. The current algorithm is naive, doesn't return bounding boxes but rather general shapes.
- Return more information derived from AIS data: Sometimes boats go dark for a certain period of time only, making it possible to tie a boat that was dark for a certain time to a known ship when it does have it's AIS on.
- Running this over all the coast of the continental US and/or over a whole year. This would be closer to the Global Fishing Watch dark vessel detection project.

If you want to take this a bit further check out:
- Running UDFs at scale with `run_remote()`. You could use this to run this pipeline over a larger area or over a much longer time series (or both!) to find out more potentiel dark vessels
- More about Fused core concepts like chosing between running UDFs based on Tile or File

================================================================================

## Exploring Maxar Open Data
Path: use-cases/exploring_maxar_data.mdx
URL: https://docs.fused.io/use-cases/exploring_maxar_data/

_A guide showing how to use Fused to get all of Maxar's Open Data from all the available STAC catalogs and explore the imagery_

### Requirements
- Access to Fused

## Summary 

Working with Fused UDFs also give you the option to easily use functions defined in other UDFs. In practice this means we've created a `common` public UDF that contains some functions we've found useful when working with any type of data.

You can explore it yourself by directly reading the code in Github. If you see any functions you'd like to use, we strongly recommend you use `fused.load()` and pass the latest commit hash at the time you want to use it:

```python
commit_hash = "39d93ca" # Latest commit hash of https://github.com/fusedio/udfs/blob/main/public/common/utils.py at time of writing
common = fused.load(f"https://github.com/fusedio/udfs/tree//public/common/").utils
```

Each Maxar Event itself contains multiple collections. We created a simple function that loops over all the available `UNIQUE_ID/collection.json`, reads them an appends them into a single GeoDataFrame:

Looking at the `WildFires-LosAngeles-Jan-2025/collections.json` file:
```json
,

    ],
    "extent": ,
        "temporal": 
    },
    "title": "Los Angeles Wildfires 2025",
    "description": "Driven by strong Santa Ana winds, multiple wildfires are burning in the Los Angeles, California, area. More than 40,000 acres and more than 12,300 structures have burned; at least 19 people have died.",
    "license": "CC-BY-NC-4.0"
}
```

So we create `stac_to_gdf_maxar` to:
- Loop over all the `UNIQUE_ID/collection.json` files
- Read each `collection.json` file
- Extract the metadata & extent of each collection 
- Convert the `extent` into a GeoDataFrame
- Concat all into a single GeoDataFrame

Once again, you can directly read the code in Github to see exactly how we do this

</details>

You can easily rename your UDFs in Workbench. Rename this UDF to `Maxar_Open_Data_STAC_single_catalog` so we can call it later directly by name.

Make sure to save your UDF with `Cmd + S` (or `Ctrl + S` on Windows / Linux) or in the Workbench UI for these changes to take effect.

[Image: Renaming UDF]

And here we get all the images for the Los Angeles Wildfires 2025 event:

[Image: Single collection Maxar STAC]

## Aggregating all available data

### Getting all `events`

To be able to explore all of Maxar's Open Data Program we now need to run this specific UDF over all the available events. 

We'll do this in 2 steps:
- Fetch all the event names 
- Use `fused.submit()` to fetch all the STAC Collections for each event name

```python showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    print(f"")

    return collections
```

Let's break this UDF down:
- We're using `@fused.cache` to cache the Catalog request so our UDF doesn't need to do this request each time we execute it. It prevents being rate limited and doing the same request over and over against the same endpoint
- We're returning a list (`collections`) so if you run this in Workbench you'll notice nothing shows up on the map! That's also why we print the first 5 rows. 

Read through the Best Practices for more handy tips on how to write efficient and easy to debug UDFs

This UDF returns a list of all the available event names currently accessible through Maxar's Open STAC Catalog:

```python 
>>> print(f"")
['BayofBengal-Cyclone-Mocha-May-23', 'Belize-Wildfires-June24', 'Brazil-Flooding-May24', 'Cyclone-Chido-Dec15', 'Earthquake-Myanmar-March-2025']
```

[Image: Maxar STAC Events]

### Preparing `fused.submit()` to run in parallel

We're going to use `fused.submit()` to run our first UDF in parallel. To do this we need a few things:
- Prepare our inputs (in this case the name of all the `events`). We recommend doing this as a dataframe as it's simple to read & work with
- Pass our first UDF to `fused.submit()`

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        debug_mode=True # Using debug to run just the 1st event at first
    )
    print(f"")

    return dfs_out
```

Let's unpack this:
- We're calling the UDF called `"Maxar_Open_Data_STAC_single_catalog"` that we renamed earlier over `collections_df`. At the time of writing this example this represents 46 events¬ß
- We use `fused.submit(..., debug_mode = True)` to run only the 1st value from `collections_df`. This allows us to test that our `fused.submit()` job is written correctly. 

`fused.submit()` allows you to run a single over a list / dataframe of inputs in parallel. Under the hood Fused spins up many realtime instances (see technical docs for details) that will each run the given UDF (in this case `"Maxar_Open_Data_STAC_single_catalog"`) all at the same time.

This is a powerful way to scale a process with just a single function call.

Read the dedicated Docs section on `fused.submit()` for more

[Image: Maxar submit debug mode]

### Getting all Maxar open data

Once we're confident that our `fused.submit()` job setup is correct, we can remove `debug_mode=True` (it's set to `False` by default) and run our UDF across all events.

We can also increase the number of `max_workers`, as we have 46 events and the default `max_workers` is set to 32. We can ask Fused server to thus spin up more instances for us so this parallel job is even faster:

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        max_workers=50, # Increasing the number of max_workers as we have more than events than the default value
    )
    print(f"")

    return dfs_out
```

After a few seconds, we get back a `GeoDataFrame` containing all the Maxar open data STAC catalogs:

[Image: Maxar submit all STACs]

This allows us to do a few different things:
- Explore _all_ of the available Maxar Open Data on a map directly. This helps us see what data Maxar has available that might be of interest, to compare image quality across areas for example. 
- Offer a wide variety of high resolution imagery to query against. For example retrieving as much the cloud free imagery as possible

## Choosing 1 Event to display

With access to all the images from Maxar, we can navigate the map and choose any image we'd like to display. Let's select one and display it in Workbench.

First we can use the Results Tab to find the URL of an image we'd like to display:

<ReactPlayer 
    playsinline= 
    className="video__player" 
    playing= 
    muted= 
    controls height="100%" 
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/others/maxar_stac/maxar_choosing_image.mp4"
    width="100%" 
/>

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = True
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        utils = fused.load("https://github.com/fusedio/udfs/tree/5432edc/public/common/").utils
        tiles = utils.get_tiles(bounds)
    
        arr = utils.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

Unpacking this UDF:
- This UDF takes :
    - a `bounds` object. This allows us to pass the current Workbench Map Viewport to our UDF
    - `path` represents the path on S3 to one of the images we want to display
    - `chip_len`: The size of the chip size we'd like our image to display in

These images can be loaded using `bounds` and Tile mode because Maxar has provided these images as Cloud Optimized Geotiffs. This allows us to leverage their tiles & overviews and only load the data we need as we pan around the map

We can check this by reading the metadata in CLI with `gdalinfo` and see that each band has `Block`, meaning is tiled:

```bash  
gdalinfo /vsis3/maxar-opendata/events/Cyclone-Chido-Dec15/ard/38/300200022120/2024-06-11/104001009713BA00-visual.tif

>>>
Driver: GTiff/GeoTIFF

...

Band 1 Block=512x512 Type=Byte, ColorInterp=Red
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 2 Block=512x512 Type=Byte, ColorInterp=Green
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 3 Block=512x512 Type=Byte, ColorInterp=Blue
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
```

Running the above UDF we can for now return the extent of the image:

[Image: maxar return img extent]

This allows us to introduce 2 concepts in Workbench:
- 1. Zoom to layer
- 2. Tile / File modes

### 1. Setting a default view in Workbench

After getting the extent of our image, we're going to Zoom to layer and set this view as the default view:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/others/maxar_stac/zoom_to_layer_default_view.mp4" width="100%" />

This allows us to to any change we want to this UDF or pan anywhere on the map and always be able to zoom back to this default view!

### 2. Displaying the image

Now we can edit our UDF to return the image by changing `display_extent` to `False`:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = False
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        utils = fused.load("https://github.com/fusedio/udfs/tree/5432edc/public/common/").utils
        tiles = utils.get_tiles(bounds)
    
        arr = utils.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

This change returns the image instead of the extent, but it returns the image all at once, because Workbench is set to "File" mode by default

[Image: File mode array return]

If you reproduce this yourself and pan around the map you'll notice:
- We see the whole image but with a relatively low resolution.
- Nothing changes as we pan around the map (resolution doesn't change)

We can change this by setting Workbench to "Tile" mode:

<ReactPlayer 
    playsinline= 
    className="video__player" 
    playing= 
    muted= 
    controls height="100%" 
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/tutorials/others/maxar_stac/workbench_image_switching_to_tile_video.mp4"
    width="100%" 
/>

Under the hood, switching to "Tile" mode tells Workbench to run this UDF not only 1 times, but by breaking it up into Mercantile tiles. This is why you see the image being broken up into a grid of tiles.

These different modes don't change _what_ code is being executed, as our `udf` didn't change. It's only changing what geospatial parameters are being passed:
- "File" mode passes the `bounds` of the current viewport (run 1 time¬ß)
- "Tile" mode passes the `bounds` of the current viewport broken up into a grid of tiles (run 1 per each tile)

## Next steps

We've shown you how to:
- Use `fused.submit()` to run a UDF in parallel
- Use `@fused.cache` to cache requests to reduce costs and improve performance
- Use Workbench to display images in different modes

If you want to go a bit further you could:
- Explore the Best Practices to make the most of UDFs or learn handy tips to use Workbench as its fullest
- Go more in depth with "File" & "Tile" modes in Workbench
- Dive into the different ways Fused allows you to use caching to improve performance

================================================================================

## Zonal stats with Fused: 10 minute guide
Path: use-cases/zonal-stats.mdx
URL: https://docs.fused.io/use-cases/zonal-stats/

_A step-by-step guide for data scientists._

### Requirements

- access to Fused
- access to a Jupyter Notebook
- the following Python packages installed locally:
  - `fused`
  - `pandas`
  - `geopandas`

## 1. Using Fused for a Zonal Statistics Example

In this guide, we'll estimate how much alfalfa grows in zones defined by polygon geometries.
This will show you how to:
- Bring in your data
- Write a UDF to process the data
- Run the UDF remotely & in parallel
- Create an app that shows your results and can be shared with anyone

```mermaid
---
title: Overview of Zonal Stats
---
graph LR
    A("Continuous Field<br/>(raster image)") --> D
    B("Areas of Interest<br/>(vector polygons)") --> D
    D --> E(Areas of Interest<br/>w/ Attributes)
```

## 2. Bring in your data

You'll first upload your own vector table with `fused.ingest`. This spatially partitions the table and writes it in your specified S3 bucket as a GeoParquet file. You'll then calculate zonal stats over a raster array of alfalfa crops in the USDA Cropland Data Layer (CDL) dataset.

This example shows how to geo partition polygons of Census Block Groups for Utah, which is a Parquet table with a `geometry` column. You can follow along with this file or any vector table you'd like. Read about other supported formats in Ingest your own data.

First, set up a local Python environment, install the latest Fused SDK with `pip install fused`, and authenticate.

Now, write the following script to geo partition your data. Pass the URL of the table to `fused.ingest`. When you kick off an ingest job with `run_remote`, Fused spins up a server to geo partition your table and writes the output to the path specified by the `output` parameter. In the codeblock below, `fd://tl_2023_49_bg/` is the base path to your account's S3 bucket.

```python showLineNumbers

  job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER2023/BG/tl_2023_49_bg.zip",
    output="fd://tl_2023_49_bg/"
  )
  job_id = job.run_remote()
  ```

After running the preceeding code, open fused.io/jobs to view the job status and logs.

Once the job is complete, you can preview the output dataset in the File Explorer.

You can also ingest data without installing anything by using this Fused App.

For the next step you can use the path to the data you just ingested or, if you prefer, this public sample table: `s3://fused-asset/data/tl_2023_49_bg/`.

[Image: Zonal Stats Parquet Files in Workbench File Explorer]

## 3. Write a UDF to process the data

To see the data as we process it, we will write a UDF in the Fused Workbench. As you write code in the UDF Builder you'll see how visualization results, logs, and errors show up immediately.

To write a UDF simply wrap a Python function with the decorator `@fused.udf`.

The first parameter of this UDF, `bounds`. It is reserved for Fused to pass a `GeoDataFrame` which the UDF may use to spatially filter datasets, and usually corresponds to a web map tile. This enables Fused to run the UDF for each tile in the viewport to distribute processing across multiple workers.

The `year` parameter is used to structure the S3 path of the CDL GeoTiff which the utility function `read_tiff` reads for the area defined by `bounds`. The `crop_id` parameter 36 corresponds to alfalfa the CDL colormap, which the UDF uses to mask the raster array.

Fused lets you import utility Modules from other UDFs with `fused.load`. Their code lives in the public UDFs repo.

- `read_tiff` loads an array of the CDL dataset for the specified `bounds` extent and `year`
- `table_to_tile` loads the table you geo partitioned for the specified `bounds` extent
- `geom_stats` calculates zonal statistics by aggregating the `arr` variable over the geometries specified by the `gdf`

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    year: int = 2020,
    crop_id: int = 36
):

    # Convert bounds to tile
    utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = utils.estimate_zoom(bounds)
    tile = utils.get_tiles(bounds, zoom=zoom)

    # Load CDLS data
    arr = utils.read_tiff(
      tile,
      input_tiff_path=f"s3://fused-asset/data/cdls/_30m_cdls.tif"
    )

    # Mask for crop
    arr = np.where(np.isin(arr, [crop_id]), 1, 0)

    # Load polygons
    gdf = utils.table_to_tile(
      bounds,
      table='s3://fused-asset/data/tl_2023_49_bg/',
      min_zoom=5,
      use_columns=['NAMELSAD', 'GEOID', 'MTFCC', 'FUNCSTAT', 'geometry']
    )
    gdf.crs = 4326

    # Calculate zonal stats
    return utils.geom_stats(gdf, arr)
```

Try running the UDF in the UDF Builder and visually inspect the output on the map. See what happens when you change `year`. Try introducing print statements such as `print(arr)` and `print(gdf)` to show logs in the console.

[Image: Zonal Stats Map]

You might receive a timeout error in the `Results` tab. Try zooming into Utah on the map where the zonal areas are highlighted, to reduce the size of the tile being passed into the UDF from the map.

```json showLineNumbers
,
  "rasterLayer": ,
  "vectorLayer": ,
    "getLineColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```

</div>

\
Click "Copy shareable link" to share the app with others!

## 6. Conclusion and next steps

We've shown how you can use Fused to develop a distributed Python workflow to power an app. Through a simple sequence of steps we loaded data, wrote analytics code, and created an app to interact with the data. With a single click you went from experimental development code to a live application.

We hope this overview gives you a glimpse of what you can build with Fused. You can continue to learn how to read data, process data, and integrate with other applications.

Find inspiration for your next project, ask questions, or share your work with the Fused community.

- GitHub
- Discord
- LinkedIn
- Twitter

================================================================================

# BLOG POSTS

## Fused is SOC2 Type 1 Compliant!
Path: blog/2025-07-08-soc2_type1/index.mdx
URL: https://docs.fused.io/blog/2025-07-08-soc2_type1/

# Fused is now SOC 2 Type 1 compliant!

We're excited to announce that Fused has achieved SOC 2 Type 1 compliance. This is a significant milestone for us, as it demonstrates our commitment to security and transparency.

At Fused, we want to allow all data teams to get work done quickly. We want to make sure to do this on top of a secure and compliant platform.

Through the comprehensive auditing process overseen by Insight Assurance, we've demonstrated our adherence
to the stringent requirements outlined by the SOC 2 Type 1 standard, reinforcing our dedication to safeguarding
sensitive data and maintaining operational resilience.

We're actively pursuing SOC 2 Type 2 certification, building on our existing Type 1 compliance.

================================================================================

## Notes from EO Summit 2025
Path: blog/2025-06-19-eo_summit/index.mdx
URL: https://docs.fused.io/blog/2025-06-19-eo_summit/

# Notes from EO Summit 2025

Last week, we were at EO Summit, Fused came out of beta, we officially launched our new site and are open for business! 

Our mission is to help data teams get stuff done quickly, which is relevant for the people we talked to at EO Summit.

[Image: Fused at EO Summit]

Here are some of our takeaways:

### 1. AI makes writing code simpler, but executing it at scale is still hard

A lot of us are going through a bit of an existential crisis, while realising that damn, yep, AI can help us write code to glue datasets together quickly. Knowing the intricacies of a Python library isn‚Äôt a competitive advantage for individuals and companies to build the best analytics anymore. 

That‚Äôs all well and good when working on a small, one-off project. But conferences like EO Summit keep showing that there‚Äôs more data than ever before. Archives of imagery, 3D point clouds and any dataset keeps growing in scale, resolution & time backlog. Problems aren‚Äôt always local, nor limited to a small time & place. 

AI is changing what it means to build software & products, there‚Äôs not doubt about that. But executing it at scale is still a dark art. 

### 2. So much data, yet we still struggle to get things done in a timely manner

At another conference, the Cloud Native Geopaptial last month Brianna Pag√°n shared her story of the Los Angeles fires taking down her home, and while having so much data available, so little was actually accessible in a helpful manner quickly. This topic came again at this conference, wildfires being the prime example of the complexity of making timely and updated use of data in times of need despite having so much.

A few months ago, our own Milind Soni made a quick interactive dashboard in a few hours as a proof of concept of what rapid tools could look like for wildfire updates using Fused.

These conversations are directly going into our internal product development discussion; as we write this, we're developing tools to build these dashboard for rapid iteration. Stay tuned for more on that soon! 

### 3. The gap between data & applications

Nadine Alameh, at the head of the Taylor Geospatial Institute & former CEO of the OGC, the consortium in charge of standards for all things geospatial, mentioned the lack of companies in the middle solving problems between data providers and companies building analytics products. This is something Aravind, head of Terrawatch -who organised this conference- has been saying for a while.  

That‚Äôs why we were excited about being at EO Summit, we think we can make a difference tackling these problems!

[Image: Talking to attendees]

We‚Äôre building tools that make getting data from all types of places & formats together, execute code (however it was written) at any scale with a few lines of code and sharing it all to whomever needs it, be it interactive dashboard, CSVs or tile servers. 

If you‚Äôd like to learn more about Fused, book some time with Max, our Developer Advocate right here to get a demo!

================================================================================

## Scaling Environmental Insights with Fused and H3
Path: blog/2025-05-27-environmental-insights/index.mdx
URL: https://docs.fused.io/blog/2025-05-27-environmental-insights/

# Scaling Environmental Insights with Fused and H3

Farmers and analysts face a familiar challenge: weather and crop data is fragmented, slow to process, and hard to act on.

We worked with Emma Quirk (Senior Data Analyst) and Majid Alivand (Senior Data & Analytics Manager) to showcase how Fused can help bring all these datasets together. In this webinar they give an overview of the industry challenges interfacing backend data analytics with frontend data consumption. Emma walks through the notebook she used to model climate and irrigation patterns for vineyards.

<ReactPlayer
    playsinline=
    className="video__player"
    muted=
    playing=
    justifyContent="center"
    width="100%"
    controls
    style=}
    url="https://youtu.be/ESokph_0660"
/>

<br />

## Addressing the Challenge

Building useful & actionable weather models for environmental insights requires bringing datasets from:
- Different sources
- Different resolutions
- Different formats

To address these challenges, Emma & Majid used Fused + H3 to bring together datasets like GridMET climate, CDL crops, LANID irrigation, and gSSURGO soils by converting them from raster to H3 to build unified parquet files.

By building UDFs to manipulate each dataset, the team can iterate on their analysis in seconds while H3 allows them to more easily combine all the datasets together.

## Why H3?

- Harmonized format across datasets and regions
- Scalable queries at multiple resolutions
- Compact storage parquets with improved spatial performance
- No reprojection needed for global analyses
- Equal neighbors for clean spatial logic

## Try it out for yourself

Try out the notebook from the presentation for yourself:
- Colab Notebook (if you already have a Fused account & environment)

üöß Coming Soon, under development üöß:
- Colab Notebook (if you don't have a Fused account yet)

Join our waitlist to get access to Fused.

================================================================================

## Launching Fused Apps
Path: blog/2025-05-20-launching-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-launching-fused-apps/

# Launching Fused Apps

Today we're launching Fused Apps, allowing you to write Python in the browser, creating, editing & sharing interactive apps 

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/Introducing_fused_apps_intro.mov"
/>

Watch the announcement video: 

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="30vh" width="100%" url="https://youtu.be/yjiNxqAcPdw" alignItems='center'/>

## üêç Write Python in the browser

Fused Apps are one of the the fastest way to write Python, directly in the browser. 
- No environment setup, just start writing Python
- Create interactive apps using Streamlit 
- Save & rename your Apps

## üîó Share your apps with anyone

As soon as you save your Fused App, you can create a shareable link for anyone to open

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>

## üîí Standalone & Private, running in your browser 

Fused Apps are built on top of Pyodide, Streamlit & Stlite but offer many features:
- Github Integration for team collaboration
- Run light-weight LLMs locally in your browser with libraries like `transformers.js.py`

[Image: CDL Bunnies]

## üìö Access a Catalog of existing Fused Apps to get started

We've curated a catalog of existing Fused Apps to get you started, including:
- üåΩ Explore different crops around the US
- üõ∞Ô∏è Visualize the trend of objects sent to Space
- üå≤ Pan a map of Forest statistics per global municipality

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/App_catalog_browsing.mp4"
/>

You can try out Fused Apps for yourself directly in Fused Workbench and read our dedicated Docs page

## ‚öôÔ∏è How we built Fused Apps: Technical Details

We wrote a dedicated technical blog post going into details of how Fused Apps are built:
- How we originally built Fused Apps
- The challenges of Python in the browser
- Our approach to building a product

## Join our webinar on May 22nd!

On Thursday May 22nd we'll be hosting a webinar showcasing how our customers use Fused

- Join on LinkedIn

[Image: Webinar thumbnail]

================================================================================

## Inside Fused Apps: Python in The Browser
Path: blog/2025-05-20-inside-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-inside-fused-apps/

# Inside Fused Apps: Python in The Browser

This is a technical deep dive into how we built **Fused Apps**, a way to build a Python-based workflow in your browser, that you can save and share with someone. You can read the full product announcement here.

At Fused, we‚Äôre building tools to help data scientists work more efficiently: we want to give them the ability to work on any dataset, create an analysis and scale it to the whole world with just a few lines of code. But data scientists don‚Äôt work in a vacuum, and analysis aren‚Äôt (always) done because people are simply curious about a topic. 

We built Fused Apps in the spirit of allowing a single person to do all the work themselves; and in this first episode of *Inside Fused*, a series of blogposts about how we‚Äôre building Fused, we want to take you behind the curtain to show how Fused Apps is built. 

### Fused Apps at a glance

Fused provides both a Python package to run User Defined Functions (UDFs), and Workbench, a browser-based IDE to write, execute, visualize them as well as create Fused Apps to make interactive frontends. 

Here‚Äôs what Fused Apps look like:

[Image: Fused apps preview]

_Fused Apps. From left to right: the list of apps that have been loaded in Workbench, the app code editor, and the running app itself._

The App code editor & renderer allow users to write their own Python code using Streamlit to build a frontend entirely in Python, a language most data scientists already work with.

We want data scientists to be able to go from ‚Äúhey, that‚Äôs a cool idea‚Äù to ‚Äúhere‚Äôs what it looks like‚Äù without tech getting in their way. Especially in a world where LLMs make writing code simpler, the bottleneck becomes the speed at which data scientists can execute & ship code, not write it.

Fused Apps offers a way for data scientists to orchestrate their entire workflow using Python, without having to worry about backends, scaling, or clusters. Fused Apps complement our UDF builder, which offers a way to build data processing and backend functions, by offering an end-to-end workflow. At Fused, we use this for ingesting and managing datasets, managing resources we make available to our UDFs, and finalizing analyses.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/Vision_Pro_cdl_exploring_sped_up.mp4"
/>

_Fused Apps work on *any* device, as long as there's a browser!_

### How we built it

In short, Fused Apps tie together a frontend application for users to write Python code with Streamlit, a backend that saves & serves these apps and provides shareable links, and a product experience tying all of this together (error handling, autocomplete suggestions, async UI functionalities, etc.)

Fused Apps are built on top of Stlite & Streamlit. Streamlit being a library allowing you to write a frontend application, entirely in Python, and Stlite an in-browser version of Streamlit. This allows people familiar with Python but not so much frontend development (like data scientists) to have something rendered on screen in HTML, but only with writing Python. 

Stlite provides the ability to run apps in the browser, and while Stlite Sharing does support URL-encoded mechanisms for app sharing, the URL is the same as the code, preventing users from *updating* said code; there‚Äôs also no such thing as app catalogs, etc. So we ended up using Stlite as the engine and built a product experience around that. 

The frontend for building a Python-in-the-browser application already exists, with Streamlit & Stlite. However the backend had to be built from the ground up.

Originally, we didn‚Äôt even have a way for people to save their apps! Our internal workflow while developing & testing this was to have a Slack channel where we pasted our app links in to be able to find them later on

[Image: Early days Slack sharing links]

This is at the the core of our development philosophy: Build a prototype, use it a bunch, find the pain points, fix them, build more. In this case, saving & tracking apps was the next piece to build.

This is how we added functionality like Github integration, sharing options, and an app catalog. All these were only added after we got some traction internally and from early customers.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>
_Sharing Fused Apps is just a couple of clicks now_

### Python, in the browser

The next piece of this puzzle is to realise that running Python locally on your machine and in the browser is quite different. We want to make the experience for the user as seamless as possible: we‚Äôre building products for data scientists to build everything in Python.

We use Pyodide to run Python in WASM. This enables the same Python language to be used in a new environment ‚Äì the browser. The browser environment is key because it gives us a way to safely ship applications to users.

Safely shipping software to people wasn‚Äôt always the simplest. Previous efforts via Flash and Java applets enabled a generation of rich web content. The promise of Java applets was run anywhere ‚Äì that the same code could run on anyone‚Äôs computer. These technologies died out mainly because of security model problems. (Java lives on in Blu-ray disks.)

At the time of writing this, in mid 2025, browser applications are considered well sandboxed (with the possible exceptions of RowHammer/Spectre type issues), browser applications open when you want them, and go away completely when you dismiss them, all the while performing more and more complex tasks. Browser applications handle video encoding/decoding for video conferencing, graphics rendering for games, maps, and design, and more frequently, for programming. Companies like Figma are building complex, professional grade applications for the browser first.

From a development perspective, the browser becomes an operating system. It becomes less important to know what exact hardware & underlying operating system the users have, and we develop applications for the browsers‚Äô APIs. Those applications are inherently portable to new devices, because the heavy lifting is porting the browser to these new devices. 

### Building a Product

There are other projects out there that allow building hosted Stlite applications, however these miss some of the features that people expect from a well rounded product. Here are few things we‚Äôve added to Fused Apps:
- Package Handling (Every Fused Apps comes with `fused` and `pyarrow` pre-installed)
- Error handling 
- Naming & saving of Apps
- Github integration for teams
- Creating shareable links allowing users to send their apps to anyone with a browser

Beyond the technical challenges we want to build a product that helps data scientists build, iterate and ship faster. A data scientist can build an analysis that takes in different parameters and make an interactive graph for their project manager to test out directly all without needing support

A big part of this is the philosophy of how we build things at Fused: We‚Äôre a team of engineers & data scientists. A lot of the features we‚Äôre listing above here come directly from our own usage of Fused Apps while making real applications with some of our customers.

We care about how fast it takes for a new user to click on a Fused App and start running it, or what the experience of saving & sharing an app looks like. 

Every Fused App comes with some packages pre-installed, such as `fused` and `pyarrow`, which are helpful for data scientists. (Streamlit comes with a number of other common packages like Pandas and NumPy already.) But this comes at the expense of loading time, as each new app is a self-contained application which requires downloading all the required packages. This is leading us to spend time optimizing the initial loading time of Pyodide.

### What can we expect from Pyodide

Fused Apps, and any implementation of Python in the browser, isn‚Äôt as customizable as a fully local setup. Packages with native code that are not prebuilt for WASM will not work. At the time of writing, some popular packages like Pandas & Numpy are built and supported, but others aren‚Äôt. The backbone of the map processing pipeline, GDAL, for example, isn‚Äôt currently supported. This isn‚Äôt from lack of popularity, but reflects the complexity of building GDAL.

Some packages will need architecture updates. As another example people currently call into ffmpegfrom Python using the CLI. The WASM environment does not have a CLI concept, and this would need to be replaced with library calls. Other packages which are not well suited to this architecture (such as Torch) might have alternatives developed because the wasm ecosystem is so attractive a development target.

#### LLMs in the Browser

An example of this is transformers.js.py. This library (developed by Yuichiro Tachibana, also behind Stlite) needed more than one level of architectural adaptation to get running in the browser, but once adapted it brought the capability of running lightweight LLMs. This allows us to ship applications that run small LLMs models, running locally in the browser! 

We made a Fused App exploring the USDA‚Äôs Crop Data Layer (CDL) dataset, a dataset of different crop types in the US. Instead of showing all 130+ categories, we can just take a text input, run a similarity analysis on the fly against all the categories and find the closest CDL crop type! 

[Image: CDL Bunnies]

_You too can find out what Bunnies prefer by trying this Fused App here_

#### Compatibility & load times

This is a fast changing ecosystem though, for example we‚Äôve made a small contribution of a build of the H3 package specifically because we wanted to have H3 supported in Python in the browser. The list of supported packages built in Pyodide is growing quickly. 

Moving large amounts of data in and out of the browser is slow, so data-intensive libraries like Torch or Dask are not well-suited for this. We solve this by bringing in the compute performance & flexibility of Fused UDFs, which run native (non-WASM) Python in a cloud environment. Fused UDFs run code much closer to the data, improving performance, and are not limited by what packages are available for Pyodide, improving flexibility.

Install times for Pyodide are relatively long from a user point of view, as the application isn‚Äôt stored locally and installed each time it's opened. This will most likely keep improving over time as the technology is developed further. For example Pyodide wants to add memory snapshotting to help with this, but it isn‚Äôt stable yet.

Even when the application is stored locally, we found we needed to reinitialize Pyodide in some cases. There is no concept of switching virtual environments in Pyodide, since running a new script reuses the same Python VM. When switching between apps in Fused, we reinitialize Pyodide in order to prevent packages from one app interfering with another. If we didn‚Äôt do this, a user might accidentally rely on a package installed by App A in developing App B, which would then not work when sharing App B.

We also needed to be careful with when the app can run, since the app has access to the user‚Äôs browser context. We chose to give users the chance to inspect the app‚Äôs code before running it if it would have access to their Fused account.

Code written in Stlite and Pyodide looks almost the same as regular Python, but there are slight differences. Many of these come in the form of adapting synchronous and asynchronous code. Stlite for example allows for top-level `await` because the browser‚Äôs event loop is being used. This can be tricky to work with because regular advice for working around asynchronous code in Python does not work with Pyodide.

[Image: CDL loading async model & logging]
_Async-aware UI allows us to provide improved feedback as an app loads a light weight LLM model or data to display_

In order to create a good product experience, we added our own syntax checking and linting on top of Pyodide. 

[Image: Error handling]

_Lots of small features go a long way to delighting users with a smooth experience._

### Delivering at near-zero cost

Fused Apps are part of the free tier of Fused. You can use Fused Apps without logging in at all, and with a free login you can save and share your apps. We want to take a moment here to explain why this is naturally a free offering and will continue to be free.

As a browser-based application, all of the code execution and data transfer happens in the user's browser. This means that we do not need to sandbox code execution or pay for cloud compute resources to run anyone‚Äôs Fused Apps. 

Where we do incur costs are in the shared control plane layer of Fused. Technically, the control place layer doesn‚Äôt see much difference between an app and a UDF. As a result, the incremental cost of serving an app user is very low and it is easy for us to offer that for free. The core offering of Fused is the backend serverless execution of code, which is our paid product.

### Try it out for yourself 

Don‚Äôt take our word for it, give Fused Apps a try for yourself! As we mentioned, Fused Apps are free and don‚Äôt require login. You can check out our catalog of public Apps in Fused Workbench.

Recently we announced re-partitioning the Crop Data Layer dataset into H3 hexagons for anyone to use and hosted the resulting dataset on Source Cooperative.

Alongside this we created a public Fused App allowing you to explore any crop for the 2024 dataset

### We're hiring: Help us build the future of data science workflows!

We firmly believe data scientists need tools that give them the independence to do their work rather than asking for support to scale their analysis or share their results. 

We need smart people to help us build all of this. We are hiring for:

- Deep knowledge of Python & Pyodide
- Opinionated thinking in building the future of data science pipelines
- People wanting to join a fast moving startup and build things

If you‚Äôd like to join the team, **send us your info here**!

[Image: The team]

_Join the team!_

================================================================================

## How Sylvera uses Fused to prototype and power DeckGL applications
Path: blog/2025-05-16-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2025-05-16-danieljahn/

**TL;DR Sylvera quickly builds and tests new app features by serving data to DeckGL applications using Fused HTTP endpoints.**

At its core, Sylvera rates carbon projects. Our ratings are powered by several earth observation and geospatial analysis data products. From climate risk data, and deforestation indicators, to biomass-predicting ML models, a wealth of data goes into generating a single-letter rating.

```javascript showLineNumbers
const BOUNDS_AFRICA: [number, number, number, number] = [
  -25.35, -46.95, 51.35, 37.35,
];
const UDF_H_AOI_FILE_CALL = "FUSED_UDF";

function createTileLayer(id: string, data: string[]): TileLayer<ImageBitmap>  = props;

      return [
        new BitmapLayer(otherProps, ),
      ];
    },
  });
}

function createBitmapLayer(
  id: string,
  image: string,
  bounds: [number, number, number, number]
) );
}

const createFusedFileLayer = (
  layerId: string,
  fusedId: string,
  bounds: [number, number, number, number],
  year: number
) => -$`;
  const param = `year=$`;
  const imageUrl = `https://www.fused.io/server/v1/realtime-shared/$/run/file?dtype_out_raster=png&dtype_out_vector=csv&$`;

  const layer = createBitmapLayer(key, imageUrl, bounds);

  return layer;
};

const createBasemapLayer = () => //.png",
  ]);
};

  const bounds = BOUNDS_AFRICA;
  const year = 2000;

  const basemap = createBasemapLayer();
  const fused = createFusedFileLayer(
    "fused",
    UDF_H_AOI_FILE_CALL,
    bounds,
    year
  );

  return (

  );
}

  createRoot(container).render(<App />);
}
```
</details>

## Conclusion and future work

The application we built isn't yet fully featured to be put in front of users ‚Äì but that's the point. We were not aiming for a finished product yet. Instead, we achieved rapid iteration that enabled us to gather relevant stakeholder feedback.

The speed we could reach wouldn't have been possible without Fused's development platform. Fused unifies three traditionally separate stages ‚Äî prototyping, scaling, and visualization‚Äîinto a single seamless solution. Thanks to this, Fused was an indispensable tool for product iteration.

In the future, we would like to explore the coming integration with Zarr stores. Being able to not only visualize the results but also to immediately persist them into a Zarr store will be a game-changing capability for anyone who uses Zarr as the persistence layer.

================================================================================

## Repartitioning Crop Data Layer & US Census into H3 hexagons
Path: blog/2025-05-06-cdl-census-hex/index.mdx
URL: https://docs.fused.io/blog/2025-05-06-cdl-census-hex/

# Repartitioning Crop Data Layer & US Census into H3 hexagons

Fused has repartitioned the USDA Crop Data Layer and US Census into H3 hexagons.

These 2 datasets are both available on Source Cooperative for anyone to use, free of charge!

[Image: Source coop CDL hex]

### What are these datasets?

We've taken 2 popular datasets and made them available in H3 Hexagons, this provides a number of benefits:
- Much faster ability to aggregate data at different scales
- A simple way to join datasets together (as they, and any other H3 tiled dataset are using the same grid)

You can read more about H3 Indexes on the dedicated page.

We're providing:

1. Crop Data Layer (from USDA CroplandCros)

- Available for `[2012, 2014, 2016, 2018, 2020, 2022, 2024]`
- Available in `hex7` and `hex8` resolutions

2. US Census (from data.census.gov)

- Available for `2020`
- Available in `hex7` and `hex8` resolutions

### Using these datasets

These datasets are completely free for anyone to use, with or without Fused. To showcase what's possible we have made a public Fused UDF anyone can use without an account to explore these.

[Image: CDL from source udf]

You can try it here without any account.

================================================================================

## Fused featured in Maxar TED Talk
Path: blog/2025-04-08-TED/index.mdx
URL: https://docs.fused.io/blog/2025-04-08-TED/

# Fused featured in Maxar TED Talk

[Image: Fused Powering Maxar]

Fused is excited to be featured in a TED Talk today by Peter Wilczynski, CPO at Maxar, which explored a multi-source site monitoring scenario of Vancouver's greening initiatives over the past five years powered by the Fused platform.

[Image: Fused Maxar Workbench]

Site monitoring scenarios are at their most effective when data from disparate sources is combined‚Äîfrom satellite imagery and map data to municipal and demographic data. Analytics today requires bringing all these different sources of data together: from raw pixels to contextual intelligence ‚Äî in real-time, seamlessly, and at scale.

This is an iterative process, each dataset being in its own format & hosted on different cloud providers, updated independently and on different schedules.

Fused is built by data scientists, engineers & open source contributors who know the reality of building pipelines with breaking changes in libraries, new file formats that require rethinking how data is ingested and business requirements that constantly change.

We firmly believe we need tools that help us bring as many datasets together as possible, especially in a world where LLMs and AI can increasingly talk directly to data.

If you‚Äôd like to try out an instance of the Fused workbench with pre-loaded data from Maxar later this month, which Peter demoed on screen, sign up for the waitlist on fused.maxar.com to get access and see for yourself this is more than a pre-recorded tech demo!

================================================================================

## Announcing Fused AI Builder
Path: blog/2025-04-01-announcing-ai-builder/index.mdx
URL: https://docs.fused.io/blog/2025-04-01-announcing-ai-builder/

# Announcing Fused AI Builder

**Fused AI Builder let's you create LLM Agents that can directly call & execute deployed Python code through Fused UDFs!**

### Give LLMs the ability the talk to your data & code

We're launching Fused AI Builder, a simple way for you to give an LLM access to any UDF you want

### Data, Python & LLMs all in one

You can now use LLM + Fused UDFs to do a whole host of tasks:

</Tabs>

================================================================================

## Announcing Fused 2.0
Path: blog/2025-02-25-fused/index.mdx
URL: https://docs.fused.io/blog/2025-02-25-fused/

# Announcing Fused 2.0

**Fused 2.0 is our biggest update to date, with changes across Workbench & `fused-py`!**

### A New UDF Editor in Workbench

[Image: New Fused Workbench]

#### Introducing Collections: A simple way to organize your UDFs

Collections now allows you to organize your UDFs together as you work on different projects, save them together and keep your editor focused on the project at hand.

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/load_collections_new.mp4" width="100%" />

Collection is still in early access so you need to enable it under "Preferences -> Enable UDF Collections [Beta]" to access it

#### Redesigned UDF Editor

UDF Editor now comes with a host of new features & redesigned UI:
- Adding a new Full Screen Map View
- The Visualize Tab is now under the UDF expanded parameters, allowing you to hide your Code Editor and just focus on tweaking the visualization of your data
- Split screen "Editor" & "Module" on top of each other: Keeping your code clean in the main "Editor" Tab is now easier by moving functions under the "Module" tab.

#### A New Share Page

We've moved all the tools & information you need to easily share your UDFs into a dedicated page (and button). You can easily:
- Create a token to share your UDFs 
- Edit the Description, Tags & Image preview of your UDFs

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/share_udf.mp4" width="100%" />

### Changes to `fused-py`

Our Python library `fused-py` is getting some updates to make processing data in Python simpler, from a small one time task to processing huge datasets

#### Simplifying how Fused handles geometries

- Moving away from Fused having many different `fused.types` to only having 2 simple types: `fused.types.Bounds` and `fused.types.Tile`.
- Replacing `bbox` object with `bounds`: a more generic term to pass geometries to UDFs

#### `fused.submit()` for simple, multi job runs

We're introducing `fused.submit()` as a simple way to run many UDFs all at once in parallel

[Image: fused.submit() demo]

This can significantly speed up parallel tasks like:
- Fetching lots of individual data points from an API
- Scaling small processing steps to lots of data points

#### Improved caching

Under the hood we've significantly improved how Fused caches results of recurring UDFs & cached functions + we've introduced new tools for developers to control caching:
- A default 90 days cache time for all UDFs
- Editing the cache duration with the new `cache_max_age` argument

Read more about Caching

### Full Fused 2.0 Changelog

Read our Changelog to see every change happening with Fused 2.0

================================================================================

## Enhance your data with GERS IDs
Path: blog/2025-02-18-jennings/index.mdx
URL: https://docs.fused.io/blog/2025-02-18-jennings/

**TL;DR Enriching your spatial data with GERS IDs can make it interoperable across the data ecosystem. You can use Fused UDFs to create custom HTTP endpoints to enrich your data with GERS.**

The Global Entity Reference System (GERS) is a framework that structures, encodes, and matches map data to a shared universal reference within Overture. GERS helps organizations identify and reference their own datasets with standard identifiers to Overture data to help unify datasets.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gers_sheets.mp4" width="100%" />

\
In this blog post we show how to create simple endpoints with Fused UDFs to enrich a dataset with GERS IDs. We'll use the Overture Building footprints to first enrich a polygon with GERS IDs then another one to look-up metadata for a specified GERS ID.

## The benefit of GERS

When third-party dataset is spatially matched to an Overture feature it's "enriched" with that feature's GERS ID and becomes "GERS-enabled". This makes it easy to associate it by ID to any other GERS-enabled dataset.

```
                            gers_id             buliding_name
0  08b2a1072534cfff020018b8a6efde22  James A. Farley Building
1  08b2a100d2cb6fff02000821de8bdff1      Pennsylvania Station
```

For example, a municipal government with a dataset of building footprints for local offices, coffee shops, and museums could match those entities to a GERS ID. This would enable the government to easily join its data other "GERS-enabled" datasets to enrich them with additional information such as insurance data, historical property values, restaurant reviews, fire risk, or rooftop solar potential.

## Create a UDF to enrich a polygon with GERS IDs

We can create a UDF that takes in a polygon and returns a GERS ID. This UDF will spatially match the polygon to Overture Buildings and return the GERS ID of the building that intersects the polygon. This is useful for enriching a dataset with GERS IDs.

Users can design and preview the workflow interactively, allowing them to test assumptions and visualize their effect. Parameters can be adjusted, and the output can be previewed before running the UDF on the entire dataset.

```python showLineNumbers

aoi = json.dumps(,"geometry":}]})

@fused.udf
def udf(bbox: fused.types.TileGDF=aoi):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Convert bbox to GeoDataFrame
    if isinstance(bbox, str):
        bbox = gpd.GeoDataFrame.from_features(json.loads(bbox))

    # 2. Load Overture Buildings that intersect the given bbox centroid
    gdf = utils.get_overture(bbox=bbox.geometry.centroid, overture_type='building', min_zoom=10)

    # How many Overture buildings fall within the bbox centroid?
    print("Buildings in centroid: ", len(gdf))

    # 3. Rule to set only one GERS on the input polygon
    bbox['id'] = gdf.id.values[0]

    return bbox
```

### Create an HTTP endpoint

With Fused, it's easy to turn your UDF into an HTTP endpoint. This enables you to run the UDF it programmatically via HTTP requests to integrate the functionality into various workflows and applications.

This endpoint runs a public UDF with the code above. You can call it with a geojson of a single polygon in the bbox query parameter and it will return a geojson with the polygon and an assigned GERS ID.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Enrich/run/file?dtype_out_vector=geojson&bbox=,"geometry":}]}
```

## Create a UDF to look-up metadata for a GERS IDs

We can also create a sample UDF to do a reverse operation: look-up a Building and its attributes by passing a GERS id. A user will be able to pass a GERS id and the UDF will look-up the building and return its geometry along with attributes about the building.

To do this, we create a UDF that takes in a `gers_id` parameter. Because the first 16 digits of GERS correspond to an H3 cell, we can use the ID to create a polygon to spatially filter the dataset. It'll bring up any buildings that intersect the H3 cell. Once we have the building, we can easily work with its geometry object and attributes using GeoPandas.

```python showLineNumbers
@fused.udf
def udf(gers_id: str='08b2a100d2cb6fff02000821de8bdff1'):

    from shapely.geometry import Polygon

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. H3 from GERS
    h3_index = gers_id[:16]
    print('h3_index', h3_index)

    # 2. Polygon from H3
    bounds = Polygon([coord[::-1] for coord in h3.cell_to_boundary(h3_index)])
    bbox = gpd.GeoDataFrame()

    # 3. Load Overture Buildings
    gdf = utils.get_overture(bbox=bbox, overture_type='building', min_zoom=10)

    # 4. Subselect building
    gdf = gdf[gdf['id'] == gers_id]

    # 5. De-struct the names column
    normalized_df = pd.json_normalize(gdf['names'])
    gdf = gdf.reset_index(drop=True).join(normalized_df)

    return gdf[['id', 'primary', 'subtype', 'class', 'geometry']]
```

### Create an HTTP endpoint

Here's how you can create and use an HTTP endpoint for your GERS building lookup UDF.

This endpoint returns a CSV table of the building's GERS ID, primary name, subtype, class, and geometry. You can use this endpoint to enrich your dataset with GERS IDs by calling it with a GERS ID query parameter.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Lookup/run/file?08b2a100d2cb6fff02000821de8bdff1&dtype_out_vector=csv
```

For example, you could call this endpoint from a Google Sheet to enrich a dataset with GERS IDs. This sample Google Sheet returns the enriches the "primary" name column for any given Building GERS. Just drag the formula to apply it to any row below. It works by calling the "GERS lookup" endpoint with a GERS ID query parameter.

================================================================================

## We're partnering with Overture to make their Data easily accessible with Fused
Path: blog/2025-02-11-overture/index.mdx
URL: https://docs.fused.io/blog/2025-02-11-overture/

**_TL;DR: We've made it easier to work with Overture data by leveraging Fused._**

Fused has been working with the team at The Overture Maps Foundation to enable direct access to their data through Fused UDFs. We are excited to share that the Overture docs now show examples on how to see how to integrate any Overture data into your workflows using Fused.

[Image: Alt]

Overture Maps aims to provide foundational building blocks of data across various themes designed to be broadly applicable across industries. Our goal at Fused is to make it easy to work with Overture data and adopt standards (such as GERS). To this end, we are creating easy abstractions to access data, tools to perform foundational operations such as conflation and enrichment, and example workflows to inspire and help you understand how to leverage this data.

One of the key usecases for Overture + Fused is enriching datasets with Overture Maps data. This tutorial showcases 2 simple Python workflows that do this by performing a spatial join with Overture Buildings. This example of a simple enrichment operation will help you understand how to work with Overture data in your own workflows.

To follow along, check out the:
- Overture Maps Docs "Getting Data" Page
- Overture Maps Docs "Examples" Page
- Overture Maps Example UDF
- Overture + NSI UDF

## Overview

The Overture Buildings dataset is dividen into themes. Two key themes are:
- Buildings is composed of building footprints represented as polygons
- Places is composed of business establishment locations and associated metadata, represented with point coordinates.

We'll first show how you can load Overture data by reusing an existing Fused UDF, then write a User Defined Function (UDF) with custom logic to perform enrichment with a spatial join. You'll be able to run the resulting UDF for any custom area of interest (AOI).

## Step 1: Load data with the Overture Maps UDF

Fused has a catalog of pre-made UDFs you can easily copy and repurpose for your own data analysis workflows. In the catalog, you'll find the Fused Overture UDF which enables you to quickly load Overture data from any of the themes for an area of interest (AOI). You can run the UDF with `fused.run` and specify an AOI to load data for using the bbox parameter. You may also pass optional parameters to select between Overture releases, themes, and columns - that way you can fetch only the data you need. In this example, we can specify the 'building' theme by setting the `overture_type` parameter.

```python showLineNumbers

bbox = gpd.GeoDataFrame(
    geometry=[shapely.box(-73.9847, 40.7666, -73.9810, 40.7694)],
    crs=4326
)

fused.run('UDF_Overture_Maps_Example', bbox=bbox, overture_type='building')
```

The output should look like this:

```python showLineNumbers

        id	                                geometry	                                        class   ...
24134	08b2a100d65a6fff0200b45ce7e2b99b	POLYGON ((-73.98552 40.76736, -73.98557 40.767...	apartments  ...
24135	08b2a1008b259fff02007917db1c32d3	POLYGON ((-73.98441 40.76703, -73.98431 40.767...	apartments  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	apartments  ...
24179	08b2a100d6516fff02006dc174022a7e	POLYGON ((-73.98346 40.76623, -73.98327 40.766...	commercial  ...
24180	08b2a1008b248fff0200315983940aa8	POLYGON ((-73.98407 40.76749, -73.98402 40.767...	None    ...

```

By browsing the UDF's catalog page, you can see its code and even copy it to run it interactively on the Fused Workbench. You'll notice that the UDF uses the `get_overture` helper function to read from spatially partitioned parquets of the overture data releases, hosted in a Source Cooperative S3 bucket. The source code of the helper function is fully open and hosted on GitHub here.

Here's a simplified version to show the core of what's going on in `get_overture`. It constructs the table path on S3 and then uses the table_to_tile helper function from Fused to load data that falls within the specified bounding box. This approach allows you to efficiently perform spatial queries on a large dataset and load only the records within the given area.

```python showLineNumbers
# Structure the table path with input parameters
table_path = f"s3://us-west-2.opendata.source.coop/fused/overture//theme=/type="

# Load the data within the bounding box
df = utils.table_to_tile(bbox, table=part_path)
```

## Step 2: Write a Custom UDF to join Places with Buildings
The example above shows how to run an existing UDF to load Overture data, but it's likely you want to write your own data transformations. You can borrow `get_overture` to load data into your own UDFs. As an example, here is a UDF to load Overture Buildings polygons and Overture Places points. The UDF perform a spatial join between them to determine which points fall within each building.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Buildings
    gdf_buildings = utils.get_overture(bbox=bbox, theme='buildings')

    # 2. Load Places
    gdf_places = utils.get_overture(bbox=bbox, theme='places')

    # 3. Create a column with the Buliding Name
    gdf_buildings['primary_name'] = gdf_buildings['names'].apply(lambda x: x.get('primary') if isinstance(x, dict) else None)

    # 4. Spatial join between Places and Buildings
    gdf_joined = gdf_places.sjoin(gdf_buildings[['geometry', 'primary_name']])[['id', 'names', 'primary_name', 'geometry']]

    return gdf_joined
```

To run this UDF, you simply call it with your AOI. Fused will execute the code with the given parameter then return the UDF's output.

```python showLineNumbers

bbox = gpd.GeoDataFrame(geometry=[shapely.box(-73.9847, 40.7666, -73.9810, 40.7694)], crs=4326)

fused.run(udf, bbox=bbox)
````

This will return a GeoDataFrame with the geometry of the Place, the `primary_name` of the building it falls within, and other attributes as defined in the UDF's return statement.

The output should look like this:

```python showLineNumbers
        id	                                names                                                   primary_name	    geometry
3883	08f2a100d65160860308e7269804dcb7	 className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/join_places.mp4" width="100%" />

## Step 3: Write a Custom UDF to join Buildings with the NSI dataset

As a second example, you can also enrich the Overture Buildings dataset with metadata data from the National Structure Inventory (NSI) API. The NSI API offers point data on buildings in the U.S. that is relevant to hazard analyses.

This UDF loads the Overture and NSI datasets, performs a spatial join to enrich the building polygons with hazard metadata, and returns the enriched GeoDataFrame. It can be used within a larger analysis workflow to enrich building polygons to calculate risk indices. You can read more about performing spatial operations to enrich Overture Buildings with NSI in our geospatial processing guide.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Overture Buildings
    gdf_overture = utils.get_overture(bbox=bbox)

    # 2. Load NSI from API
    response = requests.post(
        url="https://nsi.sec.usace.army.mil/nsiapi/structures?fmt=fc",
        json=bbox.__geo_interface__,
    )

    # 3. Create NSI gdf
    gdf = gpd.GeoDataFrame.from_features(response.json()["features"])

    # 4. Join Overture and NSI
    cols = ["id","geometry","metric","ground_elv_m","height","num_floors","num_story"]
    join = gdf_overture.sjoin(gdf, how='left')
    join["metric"] = join.apply(lambda row: row.height if pd.notnull(row.height) else row.num_story*3, axis=1)
    return join[cols]

```

The output should look like this:

```python showLineNumbers

	    id	                                geometry	                                        val_struct      med_yr_blt  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	348190.820	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
```

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/overture_nsi_2025.mp4" width="100%" />

## Conclusion

In this short tutorial, we outlined how you can integrate Overture data into your workflows using Fused. We saw how you can use Fused to load the data, write a custom Python workflow, and run it for a custom AOI. We hope the these foundational pieces help you see how you can unlock the full potential of Overture Maps and start creating your own workflows to enrich your own datasets.

================================================================================

## How Pilot Fiber creates internal tools to support telecom operations
Path: blog/2025-01-23-kyle/index.mdx
URL: https://docs.fused.io/blog/2025-01-23-kyle/

**TL;DR Pilot Fiber creates apps with Fused to quickly identify and resolve service interruptions for its New York City customers.**

Pilot Fiber is a commercial Internet Service Provider primarily in New York City. Our primary value proposition in competing with national-scale ISPs is our commitment to customer experience‚Äìwe are fast and flexible in responding to customer needs in all aspects of the business:

- During the sales process, we aim to answer customer questions quickly and accurately, including technical details and routing.
- When designing our deployment into new buildings, we equip our engineers with as much detail as possible before they arrive on-site to maximize the efficiency of time spent with building engineers.
- When an incident interrupts a service we will immediately jump into action to address the cause and restore connectivity.

We use Fused to support all of these areas, and this post focuses on the last one: incident management.
- What happens when a service interruption occurs?
- How do we identify the likely location of an issue and get to a solution as quickly as possible?

### The Problem: Why Speed Matters

One of the most common ways a customer's service can be impacted is through damage to the physical fiber cables connecting them back to a data center and the internet. Almost all fiber optic cables in Manhattan run through a shared manhole-and-duct system beneath the streets. As such, road construction or work by other providers in a manhole has the potential to damage the equipment of multiple providers. When that damage occurs, it is first come, first served to get your network repaired and customers back online. Field teams from multiple providers can't work in the same manhole simultaneously, so being onsite first and ready to repair can mean a difference of hours in customer downtime.

Because of this, Pilot uses an active fiber monitoring system across our network. Sophisticated devices in our data centers are constantly shooting light down the fibers in our network looking for potential damage. Those devices return a reflectance signature from the fiber and compare it with a reference "snapshot" created when that fiber was initially installed in a building.

When an anomaly is registered, it immediately fires an alert giving a fiber route and distance to the potential problem (i.e. "There is unexpected light loss on the fiber serving 1234 5th Avenue at a distance of 2.351 kilometers from the data center."). When this happens, our engineering and support teams analyze the data within minutes to determine the issue's exact location and, if necessary, get crews headed to the site to begin repairs.

## The Process: Fused As The Glue-Layer

Historically, this analysis has required the attention of an Outside Plant engineer with access to specialized software and network knowledge, regardless of the time of day or day of the week. This bottleneck is not ideal when time is of the essence, even at 3 a.m. So today, we are creating a more sophisticated future using Fused to make this information accessible to more support team members and make our response times even faster.

Using Fused as a back-end glue layer, we built a web app allowing users to select a route and distance and calculate where the system has registered the fault. We also created a simple user interface that provides the user a view of nearby network infrastructure and automatically generates the reports field crews would need to complete repairs based on that nearby infrastructure.

The workflow requires a series of calls to UDFs that act as intermediaries to a Postgres/PostGIS database, which in turn is sourcing data from other internal sources. This structure allows us to easily keep the business logic organized at the UDF layer while limiting the scope of data access and security via Postgres and internal processes maintaining the sync.

The basic process is seen below:

_Workflow diagram._

Two separate flows are initiated when the user inputs a route and a distance to process. One retrieves the selected route to load onto the Mapbox-based map within the app, while the second kicks off a processing chain to analyze the fault information. This chain utilizes UDFs that assist in isolating the location of the fault and relevant nearby infrastructure and adding elements to the map display to assist the user in visualizing what may be occurring.

_UDF to find fault location._

If you consider the cables you see strung along utility poles, they are not perfectly straight: they can sag, bend, go up and down, and have coils of extra cable along the way. The same is true of our cables under the streets. All of those variations add distance to the run, which needs to be considered when determining where a fault is likely to be located.

Taking those variables into account, we use Fused to apply GeoPandas and PostGIS spatial functionality to assess where the fault is most likely to be located. After calculating that location, the tool loads splice cases along that route that point to where problems are most likely to have occurred, and slack loops built into the route to make the user aware of nearby capacity that could enable faster repair of more significant damage. We next determine which splice cases are closest to the likely damage point and if any of those are within 150m of the automated distance calculation. These manholes would be the first locations our field teams would be sent to investigate.

Once we have determined the relevant nearby splice cases, we use another UDF to build a CSV that reproduces what a splice report ## The Impact of Fused

The impact of Fused across this process is many-fold:
- The ability to easily work with data across several systems.
- Centralizing business logic to the UDFs involved eliminates the tendency for this logic to be spread across client-side processes, server-side processes, and possibly the database itself.
- Modularization of operations into UDFs. For example, the UDF that generates the splice reports for this process can easily be reused in any other method that requires the same functionality.
- The effortless ability for the same UDF to simultaneously serve as a modular processing unit where needed in one workflow and a map service for display in another.
- Using Python and standard libraries enables developers who may not be spatial data experts to read, understand, and modify UDFs as necessary.

## Conclusion

Pilot's success in the market is largely based on our flexibility and responsiveness to customer needs, which is never more important than when physical damage to the network is impacting their service. Within this scenario, Fused is providing a critical layer enabling us to offer more non-technical users access to data in multiple systems through a simple UI that will result in repair teams moving to restore service to our customers as quickly as possible.

Fused is an ideal product for Pilot Fiber in that we can increasingly make highly technical information available in a usable format to additional teams throughout the company in support of our drive to be fast, flexible, and accurate in delivering service to our customers in all aspects of the business.

================================================================================

## How Fused Powers BlackPrint's Acquisition Intelligence Platform
Path: blog/2025-01-22-blackprint/index.mdx
URL: https://docs.fused.io/blog/2025-01-22-blackprint/

**TL;DR BlackPrint streamlines and transforms fragmented real estate data into actionable insights across Latin America.**

BlackPrint Technologies began its journey as a satellite mapping service designed to help municipalities modernize their property registries. Recognizing a greater opportunity, we transitioned into the private sector to address a significant gap in the commercial real estate market: the need for accessible, actionable data. Today, our platform empowers professionals with precise acquisition intelligence, transforming how decisions are made across Mexico and, soon, all of Latin America.

In this blog post, we will explore how BlackPrint built the backend of its intelligence platform with Fused to provide a comprehensive view of the commercial real estate market in Mexico.

## The Problem: Challenges in Real Estate Data
Real estate professionals face fragmented data sources and manual processes. This complexity limits access to critical metrics such as zoning regulations, demographic patterns, and traffic trends, making site selection decisions both high-stakes and prone to errors. BlackPrint recognized this challenge and set out to create an all-in-one platform, transforming disjointed data into actionable intelligence to empower professionals across the commercial real estate landscape in Latin America.
Our Solution: The BlackPrint Approach

BlackPrint's platform offers a comprehensive suite of tools and insights, including property and zoning data, demographic and socioeconomic analytics, and detailed foot and vehicle traffic analysis. Underpinning this solution is a staggering volume of geospatial data sourced from diverse datasets such as cadastral records, demographic studies, and traffic sensors. This vast amount of information is meticulously analyzed to deliver actionable insights, all while being presented through an intuitive and user-friendly interface. By simplifying complex data into accessible formats, BlackPrint empowers users‚Äîregardless of their technical expertise‚Äîto navigate and leverage insights with ease. Building such a robust platform required not only expertise but also the right tools. That's where Fused came in.

## Leveraging Fused: A Partnership for Efficiency

Fused became an indispensable partner in turning our vision for BlackPrint into reality. Its end-to-end cloud platform allowed us to move from concept to MVP significantly faster by simplifying our data processing and data delivery workflows. Before using Fused, we faced daunting challenges, such as processing and visually inspecting terabytes of traffic data and massive datasets from the Overture Maps Foundation for points of interest.

These tasks, which previously required extensive time and resources, became streamlined and efficient with Fused. Its ability to process geospatial datasets via HTTP-accessible Python functions made the development process seamless, enabling us to focus on creating an intuitive, high-performance platform. With Fused, we could deliver real-time insights and user-friendly visualizations at a scale that was unimaginable only a few years ago.

## Empowering Professionals with Unprecedented Insights

BlackPrint's platform is transforming real estate decision-making by delivering unprecedented insights to professionals. For example, a retailer in Mexico City utilized our tools to identify a high-traffic site with optimal customer reach, saving weeks of manual analysis and significantly improving accuracy.

Our platform's effectiveness is measured through key metrics like time saved, improved precision, and enhanced ROI for our clients. Users have reported up to a 30% increase in efficiency when planning site expansions or evaluating investments.

Looking ahead, BlackPrint's vision extends beyond Mexico, aiming to revolutionize acquisition intelligence across Latin America. With partners like Fused, we continue to innovate, making geospatial analytics scalable, intuitive, and accessible to professionals at every level.

## Conclusion

BlackPrint Technologies is on a mission to redefine real estate intelligence by delivering actionable insights that drive smarter, faster decisions. This journey has been significantly accelerated thanks to Fused, whose serverless data delivery empowered us to process and serve complex geospatial data at an unprecedented scale. Together, we are shaping a future where real estate professionals can access intuitive, data-driven tools that simplify their workflows and enhance their outcomes. Join us in transforming the industry‚Äîvisit blackprint.ai to see how we are revolutionizing real estate decision-making.

================================================================================

## Hot-spot analysis for invasive species using Overture Maps
Path: blog/2025-01-21-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2025-01-21-elizabeth/

**TL;DR Elizabeth Rosenbloom creates hotspot maps to identify key areas where Arundo donax is likely to spread, streamlining analysis to improve invasive species mitigation.**

In 2020 while working in Silicon Valley for the county of Santa Clara Valley, I became obsessed with improving monitoring and prevention efforts surrounding Arundo donax. The search and mitigation process this invasive plant species, Arundo donax, was a Sisophisian struggle that had been subject to the same procedures year in and year out, with no progress on beating the spread. To improve the efficacy and efficiency in battling against this notorious weed, I decided to build a tool that would identify the key areas for mitigation - both for the frequency of propagation (occurrence) and for spread potential.

In this blog post I show how I used Fused to create a map of key hotspots where Arundo donax is likely to spread based on built-environment factors derived from Overture Maps data.

To follow along, you check out the UDF associated with the blog post:
- Invasive_Species_Hotspot UDF

## Introduction

In 2020, the problem with managing Arundo was that many agency employees considered this to be a hopeless pursuit, given the exorbitant amount of time cleaning, layering, and calculating the weighted analysis would take. Despite our vast ArcGIS library of tools, engineers, hydrologists, and GIS managers all warned me that I was going to drive myself crazy trying to get the enterprise software to successfully run my analysis. My only regret in building the tool back then was that I didn't have a tool like Fused to expedite the data pulling, processing, and calculating - as it would have saved me from the very lunacy I was warned about.

The obsession with the grass species, Arundo donax, began with an insight to the positive feedback loops created by increased flooding and the spread of invasive species due to climate change. Arundo donax is one of the most invasive plant species worldwide. In addition to destroying biodiversity and disrupting habitats for native species, this large grass also contributes to significant flooding patterns. As weather events become more severe and biodiversity declines, these changes create compounded consequences in our changing climate.

## Challenges with Hot-spot Analysis

Hot-spot analysis tools using weighted sums can significantly increase accuracy in targeting key areas for prevention. To build a hot-spot analysis for Arundo donax, the following variables need to be scored according to their degree of influence: distance to nutrient loading sources (such as a golf course), distance to a riparian buffer (creek or river), distance to a water-flow disruptor (such as a bridge), and size of the stream.

The key pain points of running a weighted sum on traditional GIS software include:
- Slow calculations: "State-of-the-art" software like ArcGIS can take several hours to calculate weighted sums. Furthermore, running weighted sums on large datasets/geographic areas can be nearly impossible given exhaustive RAM and GPU demands, so analyses over 1000 sq miles often require a user to split analyses into different geographic regions.
- Program crashes: beyond the significant wait time required for typical weighted sum calculations, users of prominent GIS software often experience output delays as a result of runtime errors and other issues spurring a program crash.
- Data transformations, cleaning, and standardization: most users of traditional GIS software will find they need to start from scratch when compiling data for a hot-spot analysis or weighted sum. Sometimes, standard base layers like slope and aspect will be searchable on the local software basis. Still, often, lengthy transformations are required to make the layers compatible with the final overlay calculation.

## How Fused Changed My Workflow

Using cloud-based systems like Fused can significantly increase calculation speeds, program resilience, and access to public Cloud Native datasets.

After encountering the Fused and learning about how I could improve the speed and geographic spread of site suitability analyses, I wanted to put it to the test by expanding on a previous analysis I did in 2021 using ArcGIS. The original 2021 analysis took several months of data collection, interviews with other local agencies, and extensive data cleaning, standardization, and transformations. I experienced all the aforementioned pain points of hot-spot analysis/weighted sum calculations and more.

Flash forward to today, where I am compiling global data sets, layering them, and deriving statistical insights within 1% of the time that it took me using ArcGIS. Running buffer analyses, weighting variables, and procuring data has taken a fraction of the time for GLOBAL data - and if you remember from before, the previous analysis from 2021 took months for a county-wide calculation and final product.

The most satisfying aspect of my new hot-spot analysis application wasn't just the expansive end product; the process was more seamless and engaging than I had imagined.

## Workflow Design

I created a UDF with a simple model to identify hotspots susceptible to arundo. The model uses a weighted sum of several base Overture data classes:

- Golf Courses
- Bridges
- Water bodies (rivers, streams, etc.)

The UDF performs the following steps:
1. Create GeoDataFrames from the Overture maps dataset using get_overture
2. Generate an H3 score based on buffers around each feature
3. Aggregate the H3 scores to create a weighted sum

## Key Takeaways
Given the complexity of procuring, layering, and interweaving data along with significant wait times and resource consumption, many governmental agencies, non-profits, and even private corporations struggle to run spatial analyses such as site-suitability and hot-spot tools. Insights and tools can be created by improving the speed and efficacy of operations such as weighted sums and fuzzy analysis across spatial layers with UDFs.

Site-suitability and hot-spot analysis go beyond species detection. By simplifying the approach to these types of tools, we can more quickly and accurately detect climate-vulnerable zones, prioritize habitat restoration, and create models to build resilient communities. Industries such as real estate development, retail, and logistics can more quickly understand the variables that affect their businesses by using cloud-based systems like Fused, which can easily manage large datasets.

================================================================================

## Calculating Fire Ratings with Overture Buildings and Places
Path: blog/2025-01-20-amico/index.mdx
URL: https://docs.fused.io/blog/2025-01-20-amico/

**TL;DR Chris Amico shows how to combine Overture Maps data with fire perimeters to analyze wildfire impact on buildings and businesses.**

As communities continue to rebuild and recover from the devastation caused by natural disasters such as wildfires, the question remains: How can we quantify what was lost, especially in the built environment? With the ability to analyze detailed building footprints and overlay fire boundaries, we can begin to answer this by providing a rough estimate of the damage and identifying which structures were impacted by the flames.

In this blog post, Chris Amico shows how by leveraging data such as Overture Building footprints and fire progression maps, we can gain insight into the extent of fire risk. This enables news agencies to derive insights such as count of shops or homes exposed or even assess the capacity of highway routes for residents to evacuate before a prospect fire reaches them.

To follow along, you check out the UDFs and app associated with the blog post:

- Fire Proximity GERS_Lookup UDF
- Fire Proximity Building_Score UDF
- Fire Proximity Buffer UDF
- Google Sheet that enriches the "Fire Risk" column for any given Building GERS
- App: H3 rollups within water buffer

### Introduction

Fused simplifies the process of spatially joining datasets with Overture Maps data, such as integrating with fire-related datasets.

For this example, we use the Inter Agency Fire Perimeter Historical dataset (published by the National Interagency Fire Center (NIFC)) which includes historical fire perimeters up to 2024. Joining fire perimeters with Overture Buildings and Places data enables us to highlight service gaps or identify regions that may require immediate response.

This demo will first select buildings within a buffer zone to determine which fire perimeter they fall within. Then, it will perform an H3 stats roll-up of business categories from Overture Places, counting the number and types of businesses that fall within each distance range. This will involve rolling up Overture Places business categories into H3 hexagons.

## The Workflow

These UDFs return Overture Buildings and Places within a buffer distance from a fire. They offers a simple way to determine the scope of possible fire damage and quickly assess the number of businesses, homes, and other significant structures within the affected area. By adjusting the buffer or selecting fire extent based on dates, users can fine-tune their analysis to gain deeper insights into how far the fire's reach extends and what establishments were most at risk.

### b. Fire Proximity Building Score

Next, we load the Overture Buildings dataset and spatially join it with the fire buffer zones. This workflow categorizes buildings based on their proximity to the fire, helping us assess which structures are most at risk.

1. Load the NIFC fire perimeter data
2. Create buffer zones around the fire perimeters
3. Load Overture Buildings
4. Spatially join buildings within the buffer zones to categorize them by proximity to the fire

### c. Overture Places Rollup by H3

Finally, we perform a spatial aggregation by calculating the H3 index for the centroids of Overture Places within the fire buffer. This allows us to roll up business categories into H3 hexagons, enabling a holistic overview of business distribution in relation to the fire perimeter.

1. Load the NIFC fire perimeter data
2. Load Overture Places
3. Determine the H3 for the centroid of each building
4. Normalize the 'categories' column into individual columns
5. Roll-up categories by H3, create categories primary set

## Conclusion and Next Steps

This kind of analysis helps understand not only the immediate impact but also in planning for future fire preparedness and recovery efforts. In this post, we saw how Overture and Fire-extent data can help us estimate the extent of the damage, from individual buildings to entire neighborhoods.

Organizations looking to integrate these types of perspectives into their workflows could create apps or API services that deliver derivative products, such as GERS lookups to categorize "fire risk" based on buffer proximity.

They could also use Fused HTTP endpoints from the UDF to return a CSV with the GERS and "Fire Risk" score for buildings within a defined bounding box, as specified by a query parameter. Additionally, they could also use the HTTP endpoints to automatically enrich the "Fire Risk" column of an arbitrary dataset for any given Building GERS. Users could apply it to any row of their table, with the functionality powered by a "GERS lookup" endpoint using a GERS ID query parameter.

================================================================================

## Characterize cities with embeddings of Overture Place categories
Path: blog/2025-01-16-maribel/index.mdx
URL: https://docs.fused.io/blog/2025-01-16-maribel/

**TL;DR Maribel Hernandez shows how to create clusters of business categories using Overture Places data.**

Maribel Hernandez is a computer scientist and researcher at CINVESTAV, a multidisciplinary academic institution in Mexico. She specializes in graph theory in the field of computational genomics and complex networks. In this blog post, Maribel shows how she characterizes cities based on the distribution of businesses by rolling-up business categories by H3.

As someone who works with urban networks, Maribel's focus is on exploring how cities function and how their design impacts inclusivity. The core question driving this analysis is: Does the city have an even distribution of business services? Or are there shortages such as food deserts or unequal access to health facilities?

To follow along, you can clone and run the associated:
- Colab Notebook
- Overture Places Embedding Clusters UDF

## Introduction

Consider these scenarios:
- How connected are areas dominated by upper-class establishments to the broader city fabric?
- Are health services distributed with equal access from low-income neighborhoods?
- Do certain metropolis have better accessibility and service availability compared to others?

Take, for example, a comparison between 3 key cities in Mexico: Mexico City, Le√≥n, and M√©rida. Are neighborhoods in these cities equally served by essential services such as healthcare, food, and transportation? Could some areas be considered food deserts, while others enjoy easy access to all services?

_Preview of UDF on Workbench._

## Descriptive analysis

When analyzing the urban fabric of cities like Mexico City, Leon, and Merida, we can obsere distinct patterns of service distribution that manifest as rings around the city center.

_Mexico City._

### a. The heart of the city
Especially in Leon and Merida, the central area tends to form a cohesive, dense cluster of services. The core zone houses a variety of businesses including health services, retail, and food outlets, which are generally well-connected and easily accessible. Intuitively, the central cluster can be thought of as the "heart" of the city, serving as the primary hub for commerce, social interaction, and access to essential services.

_Leon._

### b. Islands of Services
Beyond the center, we notice smaller clusters of services scattered across the city, often in the form of "islands." These islands represent pockets of neighborhoods that, while not part of the dense city center, offer an array of unique services. These can serve to highlight the phenomenon of emergent neighborhoods within a greater whole.

_Merida._

### c. Peripheral ring
The periphery of these cities forms another distinct pattern. These outer regions also tend to cluster together in similar service categories, forming a ring that surrounds the central core. It's possible services in these peripheral areas are often more limited in scope and may reflect a focus on residential and less commercial needs, or reflect lower-income neighborhoods with scarcer access to key services.

## Conclusion
This ring-like pattern of service distribution suggests a common trend where the core of the city is highly accessible, while the periphery often lacks the same diversity and depth of business services. The islands of services in between can be seen as attempts to bridge this gap, but they are not always as effective in meeting the needs of the population on the periphery.

These spatial patterns offer valuable insights into the inclusivity of urban networks, highlighting areas that may need attention to ensure strategic business placement and guarantee equitable access to essential services.

## Future work
- Create Network: Create a bipartite network between place categories and H3 indices, using weights as counts for each category.
- Tie in Demographic Data: Integrate INEGI sociodemographic data at the census block group level to understand how services align with population needs.
- Access by Neighborhood: Use origin-destination (OD) movement networks to evaluate service accessibility by neighborhood of residence.
- Segregation Analysis: Identify zones with low visit rates or difficult accessibility. These zones may represent areas of segregation or neglect in the urban network.

================================================================================

## Streamlining the design of parcel delivery routes with H3
Path: blog/2025-01-09-antonius/index.mdx
URL: https://docs.fused.io/blog/2025-01-09-antonius/

**TL;DR GLS uses Fused to create internal tooling to optimize routing for parcel delivery operations.**

In the parcel delivery business, geospatial analyses are crucial to answer questions about daily operations. Do delivery drivers visit the same regions each day, letting them know their areas intimately? Or is there a high volatility of the regions? And of course, how do we optimize the routes of multiple drivers servicing the same region?

Those are the questions Antonius is working on at GLS Studio, an innovation lab by GLS (General Logistics Systems) which is an international parcel delivery service provider.

In this blog post, Antonius highlights how he uses Fused to create stable delivery areas for single-day and multi-day aggregates.

## The Challenge Designing Delivery Areas

While the planned areas of driver tours to delivery packages can be rather well-defined, an evaluation of the areas actually served by drivers is equally important and might not be as easy. It can be used for guiding delivery drivers to become more efficient while also enabling managers to identify planned areas that are suboptimal due to built environment features like rivers or big highways.

Creating delivery areas out of single geospatial data points is challenging. A na√Øve approach would be to use convex polygons, but this causes multiple issues:
- A single data point can have a high influence on the shape and area of the polygon
- A tour consisting of multiple not connected sub-areas is hard to detect and correctly display
- This limits the area to what is covered by historic data which leaves a gap in new target regions
- Calculations using polygons are computationally expensive, hindering ad-hoc changes to the selected time span or the calculation parameters

Building new and sometimes experimental features means that the database is often not optimized for the use case. Data needs to be joined between multiple tables and even between multiple data sources. Therefore, fetching all data can be slow, highly limiting the usefulness of having an experimental feature to play around with.

## Solving the Challenge with H3 and Fused UDFs

### Dynamic Delivery Areas with H3

The solution to the first problem came by using H3 cells instead of polygons. By assigning cells to the drivers based on their historic deliveries to the cell, driver areas result automatically. Using H3 cells across different resolutions also allows us to represent the differences between urban and rural areas which see different parcel volumes. While there exists one "base resolution" to ensure non-overlapping and complete areas, the logical hierarchy among H3 cells can be used to calculate on lower resolutions for rural areas that see fewer deliveries, speeding up the computation and ensuring a broader coverage of those areas beyond the historical data points.

On the other hand, disputed H3 cells can be broken down to a higher resolution and assigned to different drivers or, when the "base resolution" has been reached, assigned to the driver delivering most parcels to the cell. As H3 cells have clearly defined neighborhoods, areas can easily be extended beyond their historical limits when desired, covering the empty space around to include a new parcel that falls outside of historically served area boundaries.

_Fused app to show dynamic delivery areas at different H3 resolutions._

### Streamlining workflows with Fused UDFs and Caching

Fused UDFs helped us solve problems around the latency of querying and calculations. When a user looks at an area for a day, they are probably interested in the same area on some of the previous and following days as well, right? So why not pre-calculate that already?

Using Fused, it is simply a matter of fire-and-forget to trigger the UDF with parameters for some previous and following days which are then already running to cache. So when the user views an adjacent day, the data will already be there. And more broadly, when it is possible to limit the number of parameter combinations in an experimental feature to a manageable amount, this fire-and-have-it-cached approach is not limited to caching data from previous and following days, but can also be used for a range of other cases.

_Sample workflow with Fused UDFs._

## Conclusion

When developing new features that are not yet supported by the current data infrastructure, Fused UDFs enabled us to easily test things without having to change the underlying infrastructure in advance. The UDFs are easily shareable and adjustable, allowing testing by multiple people without having to run code locally while automatically making sure everyone is using the same code that is hosted in the UDF. And because we can easily call UDFs with HTTP endpoints, when we have verified the feasibility of a feature, it's easy to integrate into our product.

================================================================================

## The Strength in Weak Data Part 3: Prepping the Model Dataset
Path: blog/2024-12-12-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-12-12-kristin/

**TL;DR Kristin shares a UDF to create training data for a corn yield prediction model using Zonal Statistics.**

Now, suppose we want to do this where corn is grown in the midwest US. Here is what the states that grew corn in 2018:

Within these states, we have **1,333 counties**. Assuming each is similar in size to my home county of Lyon County, we can calculate:

1,333 counties √ó 20,000 data points = **26 million data points**

That's **20,000 times** the statistical power! üéâ

Let me say that louder for the people in the back: **26,000,000 vs. 1,333 data points**

And that's just for one time period. If we run a model on 2‚Äì4 time periods, we're looking at nearly **100 million data points**. Now, building a model on 100 million data points isn't trivial, but at Fused, this process becomes a breeze.

## Building the Training Dataset

I'm aiming to predict corn yield based on my SIF readings from early May, late May, early June, and late June. So, we need to build out a table with this structure:

| County  | Year | Yield (bushels per acre) | Area of Corn (acres) | Area of County (acres) | SIF Value-201605a (early May) | SIF Value-201605a (late May) |
| --- | --- | --- | --- | --- | --- | --- |
| 17015 | 2016 | 205 | 124,145 | 3,032,0383 | .15 | .65 |
|  |  |  |  |  |  |  |

To quickly validate this table against a map, I'll build out my workflow in Fused using Python and query the table with SQL. In the past, working with these two languages would have required complex tooling, storing data in a warehouse, and roughly **five hours** to run. With Fused, I can simply reference the a GeoDataFrame object and query in SQL with DuckDB all within the same UDF‚Äîtaking just **five seconds**!

Here's what it looks like:

## Splitting the Data

But we're not stopping there! To ensure our model is both robust and unbiased, we need to carefully split our data. Enter Walk-Forward Cross-Validation ‚Äî a game-changer for time series data. Think of it like planting seeds each season and harvesting them before the next planting. By always training on past data and testing on future data, we respect the natural flow of time. This method is perfect for corn yields because, just like how last year's harvest influences this year's, our model benefits from understanding those temporal dependencies. Plus, it prevents any sneaky data leakage, ensuring our predictions are based solely on what's known up to that point.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/krv3.mp4" width="100%" />

## Conclusion and Next Steps

Keep going or end here?

By progressively expanding our training set, each fold builds on the last, capturing more nuanced patterns and trends. To bring this to life, I'm leveraging TimeSeriesSplit from **sklearn**, seamlessly integrating it into our workflow. This tool simplifies the process, allowing us to focus on what truly matters‚Äîunlocking the full potential of our 100 million data points for actionable predictions.

And there you have it! We've taken weak, infrequent data and transformed it into a powerhouse dataset ready to drive smarter decisions in the $21 billion corn commodities market. Stay tuned for the next part of our journey, where we'll dive into building and fine-tuning our predictive models. Until then, keep cultivating those insights! üåΩ

================================================================================

## Streamlining Infrastructure Risk Analysis with Fused
Path: blog/2024-12-11-jacob/index.mdx
URL: https://docs.fused.io/blog/2024-12-11-jacob/

**TL;DR Jacob at VIDA uses Fused to streamline processing and rendering of CMIP6 climate risk models, improving data sharing and sanity checks.**

================================================================================

## Map Overture Buildings and Foursquare Places with Leafmap
Path: blog/2024-12-10-qiusheng/index.mdx
URL: https://docs.fused.io/blog/2024-12-10-qiusheng/

**TL;DR Dr. Qiusheng walks through how you can call Fused UDFs to load data into leafmap maps using Jupyter Notebooks.**

- Google Colab Notebook Walkthrough
- Leafmap Docs

## Calling Fused UDFs to load data

You first use leafmap to create a bounding box over an area of interest (AOI) `user_aoi` and create a GeoDataFrame `gdf_aoi` with it. Then, you can run the Overture Maps UDF, passing the AOI as a parameter to define the area to fetch data for.

================================================================================

## From query to map: Exploring GeoParquet Overture Maps with Ibis, DuckDB, and Fused
Path: blog/2024-12-06-naty/index.mdx
URL: https://docs.fused.io/blog/2024-12-06-naty/

**TL;DR Naty shares a UDF to use Ibis with DuckDB's spatial extension to query and explore Overture Maps data.**

Naty is a Senior Software Engineer and a contributor to Ibis, the portable Python dataframe library. One of her main contributions was enabling the DuckDB spatial extension for Ibis in 2023.

In this blog post, she shows us how to leverage the spatial extension in DuckDB with Ibis to query Overture data. Ibis works by compiling Python expressions into SQL, you write Python dataframe-like code, and Ibis takes care of the SQL. Thanks to Ibis integration with Pandas and GeoPandas, you only need to do `to_pandas()` to get your expression as a GeoDataFrame.

}
  title="Overture H3 Skyline"
/> */}

We first establish a connection to a DuckDB database, in this particular case we have an in-memory connection. Then, we do `read_parquet` and we receive a table expression. Since our result, `t`, is a table expression, we can now run queries against the file using Ibis expressions. In this example, to start, we filter by some infrastructure subtypes (pedestrian, and water), select only a few columns, and limit our "search" to a bounding box `bbox`. Notice that this `bbox` is the Fused bounding box, not the overture maps one.

We then rename the column `class` to avoid conflicts with the deferred operator, and finally filter the expression by specific infrastructure classes like toilets, ATMs, drinking water, and other useful information. Up to this point, we only have a table expression, Ibis has a deferred execution model. It builds up expressions based on what you ask it to do and then executes those expressions on request.

To show an example of an aggregate, we executed and printed the `value_counts()` as a Pandas DataFrame. Ibis can execute the table against the DuckDB backend, and return it as a Pandas DataFrame or a GeoPandas GeoDataFrame (if `geometry` column is present), by only doing `to_pandas()`.

### Conclusion

The synergy between Ibis, DuckDB, and Fused has redefined the ease of querying and visualizing geospatial data. These frameworks provide an intuitive and powerful toolkit, enabling users to express geospatial queries, perform efficient transformations, and access high-performance analytics with minimal setup.

By leveraging this stack, interacting with vast geospatial datasets like Overture Maps becomes straightforward, efficient, and accessible.

### Resources

If you want to learn more about Ibis geospatial capabilities, check some of the geospatial blog posts here.

You might also find these resources useful as you dive into Ibis, DuckDB, and Overture:

- Overture Maps Data Repo
- Ibis Docs
- DuckDB spatial extension
- DuckDB spatial functions docs
- Ibis Zulip Chat

================================================================================

## Creating an app to model road mobility networks in Lima, Peru
Path: blog/2024-12-05-claudio/index.mdx
URL: https://docs.fused.io/blog/2024-12-05-claudio/

**TL;DR Claudio used Fused to create an app to model road mobility networks in Lima, Peru, using GeoPandas, and OSMnx.**

On December 2023, I visited the Institute for Metropolitan Planning (IMP) in Lima. The director had invited me to share some of my geospatial analysis projects from my master's studies and explore potential collaborations. Around that time, Lima's mayor had announced a bold infrastructure initiative: building 60 flyover bridges to ease traffic congestion in one of the most gridlocked cities in Latin America.

1. Extracting the Road Network: Using OSMnx, I extracted road networks within a defined Area of Interest (AOI).
2. Enriching Data: Each road segment was assigned speed and travel time values.
3. Defining Population Data: A 1km¬≤ grid with population density and zoning data was loaded into a GeoPandas GeoDataFrame.
4. Setting Simulation Parameters:
    - Population size: Derived from density data.
    - Trips per person: Assumed at 2 trips/day (commute to and from work).
    - Origins and Destinations: Residential zones were assigned as homes and commercial zones as workplaces.
    - Trip Schedules: Normal probability distributions were used for departure (6-8 AM) and return times (5-7 PM).

With these parameters, the simulation sampled "home" and "work" nodes, calculated start times, and determined the shortest paths between origins and destinations. Async UDF calls made the process parallelized and efficient. The final output was a GeoDataFrame with:

- Start Time (Unix timestamp)
- Trip Type ("home" or "work")
- Path (list of coordinates)
- Timestamps (for each coordinate)

## Future Plans

This project is far from over. Here are the features I aim to add to make it a valuable tool for urban planners, especially in resource-constrained settings like Lima:

1. Larger AOI Support: Handle bigger datasets and simulate more trips.
2. Multimodal Routing: Incorporate walking, biking, driving, and public transit options, akin to OSRM profiles.
3. Custom Infrastructure: Allow users to model new transit infrastructure within the OSM road network.
4. Mobility Metrics: Provide detailed metrics (e.g., travel times, congestion levels) for each simulation.

With these enhancements, this tool could empower city stakeholders to make data-driven decisions on critical urban interventions‚Äîwhether it's building flyovers or optimizing public transit routes. The ultimate goal? Improving mobility for over 11 million residents in Lima and beyond.

You can try out the public UDF here
}>
<Iframe
  id="claudio"
  code=
  height="600px"
  useResizer=
/>
</div> */}

================================================================================

## Beyond RGB: Interactive Exploration of NEON's Hyperspectral Data
Path: blog/2024-12-03-guillermo/index.mdx
URL: https://docs.fused.io/blog/2024-12-03-guillermo/

**TL;DR Guillermo used Fused to build an interactive tool for exploring NEON hyperspectral data, making large-scale geospatial analysis more accessible and actionable for researchers.**

_Source: NEON Imaging Spectrometer._

The real challenge, however, isn't collecting this rich data - it's making it accessible and actionable for researchers. This is where Fused enters the picture. Diving into their documentation and gallery of click-and-run examples, I found myself inspired by the platform's potential. By combining elements from various examples, I began building my first User Defined Function (UDF), eventually discovering the App Builder - a feature that would prove crucial in creating an interactive interface for hyperspectral data exploration. Having worked extensively with Google Earth Engine (GEE) and NEON data, the recent announcement of NEON AOP's availability through GEE presented the perfect opportunity to test Fused's capabilities. My goal was simple yet powerful: create a user-friendly application that could tap into this wealth of hyperspectral data and make it instantly accessible to researchers.

## Looking Ahead

Looking ahead, I'm already planning the next phase of this app. I just want to add functionality that tracks sampling locations and allows users to The experience of building this demo app has reinforced my belief in the importance of platforms like Fused in the geospatial community. They serve as crucial bridges between massive datasets and practical applications, eliminating infrastructure headaches and letting researchers focus on what matters most - the science.

For those interested in exploring NEON's hyperspectral data or learning more about this application, feel free to connect with me or try the app for yourself here.

The future of remote sensing analysis lies in making powerful data more accessible, and I'm excited to be working in this field.

================================================================================

## How DigitalTwinSim Models Wireless Networks with DuckDB, Ibis, and Fused
Path: blog/2024-11-26-sameer/index.mdx
URL: https://docs.fused.io/blog/2024-11-26-sameer/

**TL;DR DigitalTwinSim uses Fused with Ibis and DuckDB to model high-resolution wireless networks.**

Sameer, co-founder of DigitalTwinSim, leads the development of advanced geospatial analysis tools to support the telecom industry in strategic network planning. DigitalTwinSim specializes in using high-resolution data to optimize the placement of network towers ensuring reliable, high-speed connectivity.

In this blog post, Sameer shares how he leverages Ibis with a DuckDB backend, and Fused to model wireless networks at high resolution. This approach enables him to quickly generate network coverage models for his clients. He explains and shares a Fused UDF that processes data in an H3 grid to evaluate optimal locations for network towers.

# Fused for Interactive Processing With Instant Visualization

Here, tools like Fused have become essential. Fused allows us to filter and visualize raw output data in a more interactive way, which we can also share with clients to illustrate network design and coverage areas.

To set up the UDF in Fused, we uploaded our data as a Hive-partitioned Parquet folder and created a UDF in Ibis to generate visualizations on demand based on zoom level and area of interest. At higher zoom levels, we compute the parent H3 index and aggregate data to show broader coverage areas; at lower zoom levels, we display individual H3 indices. The H3 polygons are generated and colored dynamically based on the data in the Parquet folder, allowing us to interactively filter data and share visualizations with clients.

Click here to launch the UDF in Fused Workbench.

# Conclusion

As network demands grow and requirements for high-speed internet access become more stringent, accurate, high-resolution modeling is essential for effective planning and deployment.

DigitalTwinSim's integration of tools like DuckDB and Fused, alongside Ibis and H3 grids, enables us to tackle the challenges of processing, analyzing, and visualizing massive datasets. By leveraging DuckDB's powerful data aggregation capabilities, we can manage and analyze high-resolution data efficiently, irrespective of memory constraints. Meanwhile, Fused empowers us to deliver interactive, client-ready visualizations, allowing stakeholders to better understand network coverage and performance.

================================================================================

## The Fastest Way to Download Foursquare's new POI Dataset
Path: blog/2024-11-21-foursquare-poi/index.mdx
URL: https://docs.fused.io/blog/2024-11-21-foursquare-poi/

**TL;DR The Fused Team made Foursquare's open dataset of 100M global places accessible via GeoParquet files which you can access via a UDF.**

Foursquare just released an open dataset of over 100M global places of interest.

We at Fused have partitioned these points into easily accessible GeoParquet files, and hosted them on Source Cooperative

On top of that, we've build a publicly available User Defined Function (UDF) that anyone can use to easily look at & download to GeoJSON, all from the browser

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="30vh" url="https://youtu.be/ubRnHvPuY40" width="100%" />

\
**Try it out for yourself!**

You don't need to login or create an account to easily access the Foursquare POI points

- Join our waitlist to get full access to UDFs that you can edit in your browser
- Get familiar with Fused by checkout out our Quickstart docs
- Follow us on LinkedIn to keep up with updates
- Read this more in-depth look at the whole dataset from our community member Mark Litwintschik

================================================================================

## How I Got Started Making Maps with Python and SQL
Path: blog/2024-11-04-kent/index.mdx
URL: https://docs.fused.io/blog/2024-11-04-kent/

**TL;DR Stephen Kent shares his journey making maps with Fused using Python and SQL.**

I am a self taught developer and data enthusiast. I first came across the spatial data community when I saw a Matt Forrest video on LinkedIn where he demonstrated how to visualize buildings from the Vida Combined Building Footprints dataset with DuckDB. Immediately I thought, what if you could see all the buildings in a country, say, Egypt? I set out to do just that and made this map with DuckDB and Datashader.

_Buildings in Egypt._

Find Stephen's UDF code here:
- Five Minutes Away in Bushwick Brooklyn UDF

## Starting with Fused

The day after I posted that image on LinkedIn, in April, 2024, I had a call with Plinio Guzman of Fused. I told him what I had been up to, and he was enthusiastic and confident that Fused would fit my needs. One key feature he mentioned was the live development. While I was developing that Egypt map, I had to start the ETL to the final product over and over until I got it looking the way I wanted.

So I got started right away. I found Fused User Defined Functions (UDFs) like Overture Maps and the S2 Explorer and traveled all over the world looking for stunning images. It was thrilling to fly from New York to Tokyo and see the results render instantly.

_Exploring the world with Sentinel 2._

I then began to change the components of these UDFs to see different Overture types, but at this point I was hesitant to build my own UDF from scratch.

## Instant UDFs

That was until Fused launched its File Explorer. With one click, it was now possible for me to create a UDF from providers like Source Cooperative and visualize with numerous presets like DuckDB or GeoPandas. With this new feature, I recreated my Egypt map with the same Vida dataset, this time using DuckDB with the H3 extension. It was liberating, I came to realize the components were simpler than I thought.

## Local Tests

I used DuckDB with the H3 extension without Fused to query Overture Maps for countries and continents all locally in a Jupyter notebook. The benefit with the H3 extension is that if you set up the query right you can aggregate larger than in memory datasets at ease from your notebook.

_Road Density in Africa._

And made this Egypt building map with H3, how does it compare with the Datashader version up top?

_Egypt Building Density with H3._

## Fused and Overture Maps

In August, Fused announced a tighter partnership with Overture Maps Foundation and that came with even more Overture features. Like with Source Cooperative I could now instantly generate UDF of buildings, places, land use, or roads, etc by joining parquet files (and more). I proceeded to use the framework of that UDF to join all kinds of data.

_Proximity analysis between Road Networks and Hospitals in Paris._

## Joining H3 with GeoJSON

One day I was looking at the DuckDB_H3_Example, and I was struck ‚Äî what if I joined those cells with Overture Buildings? I learned how to use the DuckDB H3 extension from all of the example UDFs on Fused. So I called that UDF in an Overture UDF and used GeoPandas to join the two. The result is the map below. The color of the buildings comes from the count of corresponding Yellow Cab pickups. There are millions of points in this TLC parquet file, and H3 helped me to aggregate to thousands for an easier spatial join.

_Overture Buildings joined with H3 Yellow Cab pickups in New York City._

I made this particular map with Kepler.gl, with two clicks from the Workbench. I could have also exported the data to tools like Felt and Mapbox. You can find the code I used to recreate this map here.

## App Builder

I just started working on the Fused App Builder, and made a dashboard to view and interact with NYC‚Äôs 311 call data as a 3D H3 heatmap. Anyone using it can set the date range and resolution to change the display. Very fast and easy to use.

## Community

There's so much exciting data science happening on Fused. Check out Kevin Lacaille's post on ML-less global vegetation segmentation at scale. And Christopher Kyed's Analyzing traffic speeds from 100 billion drive records, that is the kind of project I would love to work on.

I continuously find inspiration as I browse community UDFs. Here's a join of H3 heatmaps with Overture types. This is a heatmap of connectors (intersections) joined with segments (roads) in London. The darker colors have more intersections. I am looking to incorporate traffic counts.

_Road density in London._

## Conclusion

I have been using Fused for several months but it feels like I am just getting started. It seems like the only real limit here is what I can dream up.

_This is a cross-post of Stephen Kent's Medium Article published October 14, 2024._

================================================================================

## Discovering NYC Chronotypes with Fused
Path: blog/2024-10-30-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2024-10-30-elizabeth/

**TL;DR Elizabeth Cultrone analyzed NYC Taxi pickup data to identify neighborhood boundaries based on activity patterns. She created a UDF to implement H3 binning and similarity metrics.**

Neighborhoods within a city have consistent characteristics but often have ill-defined boundaries. Some neighborhoods are more similar than others even though they're not nearby. Understanding these local boundaries and the demographics, dynamics and behaviors of different areas affects a wide range of business applications, including advertising, site selection, business analytics, and many more.

_Highlighting natural catchment area boundaries around Koreatown._

## Statistical Analysis

In the App Builder, we created graphs to summarize the similarity values shown in the map. Histograms of the pickups across the most and least similar hexes to each location confirm that the distributions are different for each. We can also explore the cumulative count of hexes to help determine an optimal threshold for similarity values, depending on the application.

_Comparison between most and least similar hexes of two AOIs._

## Conclusion

The Fused UDF Builder makes developing and iterating on these analyses swift and convenient, with no need to jump between different environments for developing vs viewing the results. And although the taxi dataset is small, the Fused Tiling functionality offer the possibility of developing similar analyses with larger datasets. With more data and richer features this proof-of-concept could be expanded to discover more robust, fully-defined neighborhoods, allowing us to develop data-driven approaches to local geography.

================================================================================

## Earth-scale AI pipelines for Earth Observation (Part 1: Data Curation)
Path: blog/2024-10-29-durkin/index.mdx
URL: https://docs.fused.io/blog/2024-10-29-durkin/

**TL;DR Fused simplifies how Earth Observation data is processed to curate training data for AI models. Gabriel Durkin shows a Streamlit app he created to train and run land use and crop detection models.**

Later, as part of the workflow, I will demonstrate spatial querying that will allow us to choose custom regions on which to train a model - simply by outlining polygons on a map.

## Article structure
This is the first of a 3 part series that will show how Fused can be used in querying, processing/ETL, visualization, and inference tasks across stages of the model development lifecycle:
1. **Data curation:** Prepare and generate training images and masks.
2. **Model training:** Feed curated data into a state-of-the-art multiclass segmentation model.
3. **Model Inference:** Establish generalization limits by evaluating model performance on user-selected holdout regions from different locations and timeframes.

## The role of Fused

Fused is a platform that simplifies the engineering challenges involved in building data workflows for Earth Observation (EO) analysis. Its key features for curating training data include:
- **Python User Defined Functions (UDFs):** UDFs define transformations on data and can easily be called with parameters and across different areas of interest.
- **Remote Accessibility:** UDFs can be called from anywhere via HTTP requests, delivering data exactly where it's needed without the need to store or transfer large datasets.
- **Parallel Execution:** UDFs can run in parallel, processing thousands of data chips per minute for efficient scaling.
- **UDF Workbench:** The UDF Builder provides instant feedback during algorithm development, allowing users to visually inspect the resulting data chips on a dynamic map. Any changes are immediately deployed upon saving.

The synergy of these Fused components enables researchers to dedicate more time to experimentation and analysis rather than data engineering.

> "Ultimately we want Data Scientists to be able to deliver autonomously ‚Äî without operational reliance on a dedicated engineering team, especially given the unwieldy scale and volume of earth observation data."

## Problem overview

An example workflow addresses the challenge of developing a model to categorize and quantify agricultural land use across the continental U.S. with multispectral satellite imagery. This task involves complex multi-class segmentation with several key challenges:

- **Image variability:** Satellite images can vary in resolution, quality, brightness, and cloud cover. Crop reflectivity fluctuates across regions and growing seasons, impacting data selection and algorithm design.
- **Engineering limitations:** Traditional approaches restrict the number of iterations researchers can perform to design and tune a model.
- **Increasing data complexity:** The growing number of spectral bands and satellite sources available requires a systematic approach to selecting an index combination.

By automating engineering processes such as image chipping and source harmonization (time, space, and projection) to prepare training data, researchers can have more iteration cycles as they define spectral indices and band combinations that generalize images.

## Input datasets

Our example model will use 3 bands of Sentinel 2 satellite imagery as input to our ML model to predict a CDL crop mask value - the target layer. We will leverage a Fused "Cube Factory" app we built to generate the chips or "datacubes" that contains both inputs and target. Thinking bigger, there are multiple fused assets which can take the role of input or target. A strength of the fused Cube Factory app is the implicit matching of data in both spatial and temporal dimensions. A further goal of this blog is to demonstrate how easily custom apps like this one can be created and shared on the Fused App Builder.

- Sentinel 2 offers 13-bands of 10 meter resolution imagery with broad temporal and spatial coverage. We chose the Glacier index as the Pseudo Red Channel and Channels 8 (NIR) and 11 as Green and Blue, respectively (image above).
- Land Use Land Cover (LULC) dataset provides a global land cover classification
- Digital Elevation Model (DEM) dataset provides terrain elevation
- Sentinel 1 offers HV and VV polarization bands of 10 meter resolution - active imaging data derived from SAR with broad temporal and spatial coverage and a 6 day cadence.
- The USDA Cropland Data Layer (CDL) dataset maps individual pixels of 30m resolution to hundreds of crops and land types like 'soybean' and 'corn', or even 'pumpkins' and 'cherries'. Below you can see a hyperspectral image of central california, and the dominant crop and land classes in the associated CDL image below.

An additional flexibility offered by the app is the variety of Fused assets that can be called as either inputs or targets. For the viewport visualization we set the left side of the split map as the input layers and the right side as the targets.

Returning to our chosen target, the CDL dataset, it contains up to 256 named crop classes (see: CDL_names_codes_colors), we grouped these together into 10 superclasses to simplify our task.

1. Background/Missing BG
2. Wheat WH
3. Corn CO
4. Soybeans SB
5. Grass/shrub/open lands OL
6. Forest and wetlands FW
7. Open water OW
8. Developed DV
9. Other agricultural (catch-all) OA
10. Other/barren OT

```json

```

## Data curation workflows

The Mask2Former model we will employ for this exercise is optimized for 3 channel RGB images, so we'll use Fused UDFs to create a 4 layer datacube, three inputs from Sentinel 2 and one CDL layer as target. Then, we'll call them across a set of tiles to create the training dataset.

For this example, I selected 3 bands to predict with: glacier index (red-green normalized difference index) is on top, and bands 8 and 11 from Sentinel palette in the middle. The CDL mask at the base of the datacube has pixel values that correspond to major crop classes.

## Create datacube

The first 3 channels of the datacube encode Sentinel 2 bands as RGB false color channels, and the fourth channel encodes the CDL mask are associated with the following 3 UDFs:

1. Choose Right and Left map layers from the Fused asset collection.

2. Choose a Survey Period and location using the left side pop-out menu.

3. If using Sentinel 2 - select 3 of the available 13 bands or one of 6 normalized difference indices.

4. If using digital elevation model as input or target (left or right map side), choose whether terrain, or gradient modes, choose scale parameters, and choose a colormap for visualization.
5. Draw polygons on the map.

## Model training

To train a model on our datacube we downloaded the Mask2Former model from Hugging Face and fine-tuned it on the 10,000 datacubes of 240x240 pixels (resulted in ~1GB of training data). This can be done locally on a GPU machine, or in the cloud on colab or paperspace gradient (now digital ocean). In our next post we will demonstrate how to train ML models natively on the Fused platform.

### Model Hosting

The final model is available on a dedicated Inference endpoint hosted by Hugging Face, and accessed via a API call through a custom Fused UDF. It is available in the Cube Factory app for inspection as the Mask2Former ML layer. This allows a qualitative side by side comparison with the CDL layer. The model was trained on July imaged crops in the midwest among other areas, here is how it performs vs ground truth near Jacksonville Illinois. The left side shows the target layer and the right side shows the model prediction.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gabe_slider_HD.mp4" width="100%" />

## Conclusion

This post showed how Fused makes it easy to generate training datasets. It gave a practical example of how with Fused UDFs the data loading, data preprocessing, and data loading happen behind an HTTP endpoint, enabling easy data retrieval from anywhere. There's no need to pre-compute, store preprocessed data, or manually generate tiles, as Fused dynamically generates data for the specified tiles using simple HTTP calls.

This approach offers several advantages to build customized training datasets. Since data generation is on demand and gets cached, a data scientist can quickly iterate, adjusting spatial or temporal parameters without worrying about managing storage or running jobs to generate datasets. The flexibility to load data that is needed, when it's needed, accelerates experimentation and refinement of models.

To replicate this work for your own AOIs, you can try out the Cube Factory app yourself or run the underlying UDFs on your own Fused Workbench environment. Please feel free to contact me on LinkedIn.

## References
- [1] L. L. Zhang, S. Dhaka, et al., Building a Crop Segmentation Machine Learning Model with Planet Data and Amazon SageMaker Geospatial Capabilities (2023)
- [2] S. Hamdani, Supervised Wheat Classification Using PyTorch's TorchGeo ‚Äî Combining Satellite Imagery and Python (2023)
- [3] Sentinel-2 Mission Overview (2023)
- [4] Digital Elevation Model (DEM) Data (2023)
- [5] ESA WorldCover 10m 2020 V100 (2023)

## Additional Resources
- USDA Cropland Data Layer Methodology
- Sentinel 2 Spectral Band Indices
- Fused Documentation

================================================================================

## DuckDB, Fused, and your data warehouse
Path: blog/2024-10-24-stefano/index.mdx
URL: https://docs.fused.io/blog/2024-10-24-stefano/

**TL;DR GLS Studio uses Fused to optimize Snowflake queries. This enables route planning in their ParcelPlanner app with H3-partitioned geospatial data served to a Honeycomb Maps frontend.**

We then created two key UDFs within Fused. These work in tandem to handle authentication, caching, and efficient data retrieval for our DuckDB-powered map:

- **The "Public" UDF (Hammer):** This UDF isn't cached and serves as the entry point. It handles authentication and collects the full date range requested by the customer.
- **The "Private" UDF (Nail):** This cached UDF takes a single date and returns the necessary data for that specific day.

The "Hammer" UDF spawns up to 1,000 asynchronous Fused workers, each running an instance of the "Nail" UDF to fetch data for an individual date, as specified with a parameter. Once the data is retrieved, it is stitched together into a single GeoPandas DataFrame, ready for use.

With this approach, historical data only needs to be read once from Snowflake. For the present date, which is subject to updates, we handle caching differently and apply a one-hour cache to optimise performance.

## Conclusion

In the end, Fused allowed us to integrate our Honeycomb maps directly with Snowflake, handling caching and security concerns. This approach saved us significant backend development and data engineering work‚Äîall with just a few dozen lines of Python.

================================================================================

## The Strength in Weak Data Part 2: Zonal Statistics
Path: blog/2024-10-22-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-10-22-kristin/

**TL;DR Kristin created a UDF to mask cropland areas using USDA data and run a Zonal Statistics workflow for corn yield predictions.**

Farming isn't static‚Äîcorn fields rotate with soybeans or cover crops yearly, adding noise to our data. Here's a 25 km¬≤ area:

This block includes not only farmland but also trees, towns, and water bodies. Our challenge is to isolate the specific areas where corn is grown to enhance the precision of our analysis. Enter Fused, which has a Public CDLs UDF that reads the USDA Cropland Data Layer, letting me specify the year and crop type to pinpoint corn accurately.

## Masking crop areas with a UDF

To tackle this, I created a Fused UDF that loads the USDA Cropland Data Layer for a specified year and crop type to identify corn-growing regions. I then used corn-growing regions to mask a Solar Induced Fluorescence raster. Finally, I calculate its mean values for each county.

Now for the fun part:

1. **SIF Data:** Display SIF for a specific month from a NetCDF file.
2. **Corn Areas:** Map corn cultivation that year from a GeoTIFF file of the Cropland Data Layer (CDL) data product.
3. **Precision Clipping:** Clip layers to show SIF values only where corn grows.
4. **Zonal Statistics:** Aggregate the SIF that incides on corn crops for each county.

You can see the UDF code here and even clone it to your Fused workspace.

**Voila!** From one county's weak data to creating summary statistics for the county. This provides the ingredients to boost the prediction strength and reduce noise in the prediction model I want to build.

## Scaling Up

Applying this to **400 Midwest counties** transforms our dataset from 400 points to **60 million**. The results?

- **Enhanced statistical power:** More data = stronger, more reliable predictions.
- **Improved accuracy:** Predictions are more closely aligned with actual outcomes.

Here is how the data compares on a map.

## Why It's simple with Fused

With Fused, working with rasters and vectors is straightforward. This blog post showed how I'm turning weak, unreliable data into a powerhouse of insights effortlessly.

## Ready to transform?

Curious to see the magic? Interact with the UDF in the Fused UDF Builder and elevate your data from weak to strong. Harness your data's full potential and make impactful decisions!

Feel free to reach out if you have any questions.

================================================================================

## Blazing Fast Geospatial SQL in DuckDB
Path: blog/2024-10-17-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-10-17-isaac/

In this video from the FOSS4G 2024 conference, Isaac Brodsky, CTO and co-founder of Fused, shows the power of combining H3 with DuckDB to enhance geospatial data analysis.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://www.youtube.com/watch?v=hckjt7gfP20" width="100%" />

<br />

As an example, Isaac shows a Fused User Defined Function (UDF) that joins the Overture Places dataset and the Natura 2000 biodiversity areas dataset, achieving significant reductions in file size and query execution times. He showcases how the integration allows for efficient data exploration, filtering, and real-time queries, emphasizing the power of DuckDB's H3 extension and spatial extension.

Isaac explains how H3 simplifies geospatial analytics by offering common spatial index to join datasets and enables efficient storage & processing by converting spatial features into 64-bit integers. Additionally, DuckDB enables developers to conveniently transition between Python and SQL. He also highlights how DuckDB can simplify data processing architectures by querying data in real-time from object storage systems such as S3.

================================================================================

## Analyzing traffic speeds from 100 billion drive records
Path: blog/2024-09-25-pacific/index.mdx
URL: https://docs.fused.io/blog/2024-09-25-pacific/

**TL;DR Pacific Spatial Solutions uses Fused to streamline data workflows and feature engineering to predict national traffic risk in Japan.**

_Drive recorder data points from a single day in a specific part of downtown Tokyo._

## Moving to Fused
Specifically, what we are trying to achieve is to map the speed values from the drive recorder data points to their nearest road. Using a traditional "nearest neighbor" approach would not be feasible, as we would need to measure the distance between billions of points and thousands of roads.
With our current cloud service provider we therefore had to rely on "clustering", so that data points that are close location wise would be close in memory too. This definitely increases performance, but adds some randomness to the processing time and cost because depending on where the area of interest lies in memory, you might have to search through all of your data to find it. As a result, to keep cost and processing time reasonable, we had to limit the nearest neighbor search area using a very small buffer. This was the only way to make our analysis with a dataset of this magnitude feasible.

_Nationwide drive recorder data points and their Fused spatial partitions._

### UDF Design
1. Use `bbox` to load GPS points and roads in the viewport.
2. Structure `DataFrame`s with the GPS points, road krings and road geometry.
3. For each point identify the road with the closest kring cell within a certain k distance, and map the speed to it.
4. Aggregate all of the speed values.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF=None,
    base_path: str = '...'
):
    from utils import df_to_gdf, list_s3, run_pool, get_GPS_road_data

    # Load ingested GPS and road data
    L = list_s3(f'/GPS_hex/')
    df_GPS, df_road_hex, df_road_geom = get_GPS_road_data(bbox, L)

    # Nearest neighbor calculation
    df_final = df_GPS.merge(df_road_hex, on='hexk')
    df_final['distance'] = (df_final['k']+0.5)*k
    df_final['cnt'] = 1
    dfg = df_final.groupby('segment_id')[['cnt', 'speed', 'distance']].sum().reset_index()
    dfg['speed'] = dfg['speed']/dfg['cnt']
    dfg['distance'] = dfg['distance']/dfg['cnt']

    # Introduce geometry to roads
    df = df_road_geom.merge(dfg)
    df['width_metric'] = df['cnt']**0.5/5
    return df.sjoin(bbox[['geometry']])
```

We now have our result which is a DataFrame representing the road network within our bbox. All the roads have their respective aggregated speed, distance and metric values as well as how many points were used for the aggregation. This result can easily be enriched by bringing in more columns from the base data such as the timestamp. This would make it possible to create hourly speed pattern analysis or maybe even a time series visualization.

For demonstration purposes, the video above shows this UDF running on a fraction of the ingested dataset.

_UDF result of Osaka Japan. Line width shows point density. Brighter yellow colors indicate high speed and darker purple colors low speed._

## Conclusion

By leveraging the spatial partitioning that Fused does during ingestion and the flexibility of the h3 library, we have created a method to reliably map our drive recorder points to their nearest segment.

The natural next step will be to scale our analysis using multiple machines and run on all of our data. To achieve this, we would iterate over each of the chunks that fused produced when ingesting our road data, instead of the bbox. This will ensure that our calculations are only run once for each of our roads. The modification can be achieved fairly easily in Fused and we are very excited to see how well Fused will be able to perform in this case.

================================================================================

## Creating cloud-free composite HLS imagery with Fused
Path: blog/2024-09-24-marie/index.mdx
URL: https://docs.fused.io/blog/2024-09-24-marie/

**TL;DR Pachama partnered with Fused to generate cloud-free HLS image composites, improving tropical forest monitoring and canopy height mapping for carbon conservation projects.**

_Example composites highlight how the HLS-L30 product alone can have gaps when attempting to make a seasonal composite, as fewer cloud-free observations._

This blog post explores how Pachama's engineering team partnered with Fused to generate cloud-free seasonal composites using Harmonized Landsat Sentinel-2 (HLS) data, enabling higher quality optical imagery and better canopy height map creating ML model performance.

## Obstacles to create a cloud-free HLS image composite

The HLS dataset is an exciting development put forward by NASA's Satellite Needs Working Group. It provides consistent surface reflectance data with global observations every 2-3 days at a 30-meter resolution. The dataset harmonizes data from Landsats 8 & 9 with the European Space Agency's Sentinel-2A & 2B satellites such that the results are high quality, standardized, and able to be combined [2].

The HLS dataset consists of scene-level harmonized data, and does not create any cloud-free composite images by default. A significant amount of compute power is needed to process and combine this data, which contains multiple petabytes of data. Iteration on the compositing algorithm is also essential to quickly experiment and refine the process.

_Example of HLS image for a region in Brazil with clouds._

One common solution to this problem is to use Google Earth Engine (GEE). However, only the Landsat portion of this dataset (HLS-L30) is available on GEE. Without the Sentinel-2 portion of this dataset (HLS-S30), we do not get a 2-3 day temporal resolution that is required for cloud-free imagery in frequently cloudy areas.

## With Fused

Pachama turned to Fused to create scalable workflows for quickly iterating on a compositing algorithm. Fused's UDF model allowed Pachama to design algorithms that parallelize image processing, generate cloud-free composites, and run these workflows at scale.

### Pachama's UDF workflow

Here's the workflow we created with a Fused User Defined Function (UDF) to generate cloud-free composite HLS imagery.

### 1. Write a UDF to load imagery

This sample UDF loads data for the Landsat and Sentinel2 data products. It queries for a specific date range and does a first pass at filtering out images with too many clouds.

```python showLineNumber
# To Get your username and password, Please visit https://urs.earthdata.nasa.gov
@fused.udf
def udf(
    bbox: fused.types.TileGDF,
    mask_url: str,
    band_url: str,
    username="<INSERT USERNAME>",
    password="<INSERT PASSWORD>",
    env="earthdata",
):

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils
    # Authenticate
    aws_session = utils.earth_session(cred=)
    cred = 
    overview_level = max(0, 12 - bbox.z[0])

    # Read band data
    band_arr = utils.read_tiff(
        bbox,
        band_url,
        overview_level=overview_level,
        cred=cred,
    )

    # Read and apply cloud mask
    mask_arr = utils.read_tiff(
        bbox,
        mask_url,
        overview_level=overview_level,
        cred=cred,
    )
    cloud_mask = (mask_arr & 0b00000010) >> 1
    band_arr = np.where(cloud_mask == 1, np.nan, band_arr)

    # Filter nan's and convert to RGB values
    band_arr = np.where(band_arr == -9999, np.nan, band_arr)
    band_arr = band_arr / 10
    band_arr += 1 # workaround for uint8 and nan values
    band_arr = band_arr.astype("uint8")

    return np.array(band_arr)
```

### 2. Call the UDF asynchronously

This UDF queries the LP DAAC STAC catalog for data that matches the time and location of interest. This UDF then calls the previous one in parallel asynchronously to fetch each cloud-free image in parallel. It then combines the outputs, taking the median of each band to create a cloud-free composite.

```python showLineNumber

@fused.udf
async def udf(
    bbox: fused.types.TileGDF,
    date_range="2023-05/2023-06"
):

    from collections import defaultdict

    RGB_BANDS = ["B04", "B03", "B02"]
    F_MASK_BAND = "Fmask"

    # Query STAC catalog
    band_urls = get_band_urls(bbox, date_range)

    # Call the image loading/masking UDF in parallel
    tasks = defaultdict(list)
    for band in RGB_BANDS:
        for mask_url, band_url in zip(band_urls[F_MASK_BAND], band_urls[band]):
            arr_task = fused.run(
                "<INSERT UDF TOKEN>",
                bbox=bbox,
                sync=False,
                parameters=)
            tasks[band].append(arr_task)

    # Combine each band
    rgb = []
    for band in RGB_BANDS:
        task_results = await asyncio.gather(*tasks[band])
        composite_values = []

        # Convert back to format with nan's
        for arr in task_results:
            arr = arr.image.values.astype("uint8")
            arr = np.where(arr == 0, np.nan, arr)
            arr += 1
            composite_values.append(arr)

        # Take median of the composite values
        band_composite = np.nanmedian(composite_values, axis=0)
        band_composite = band_composite.astype("uint8")
        rgb.append(band_composite)

    return np.array(rgb)
```

The UDF above generates a cloud-free composite image and gives Pachama control and transparency over the image inputs.

_Example of cloud-free HLS image composite for the same region in Brazil._

## Benefits of using Fused

The best part is that Pachama's Data Science team can design UDF while looking at a specific area, and to run it for a different region by simply changing the input bounding box (bbox). This flexibility allows Pachama to create individual image tiles for any location worldwide. They can easily experiment and generate composites for different date ranges by adjusting the input parameters.

- Easy parallelization with simple Python function calls, no need to manage clusters
- Iterate on both UDFs in the same code editor with the UDF Builder
- Instant feedback during algorithm development, no need to wait for pipelines to run
- Invoke UDF and load its data into a Jupyter Notebook with `fused.run` for downstream analysis

## Conclusion

Thanks to Fused, Pachama's scientists and engineers can quickly iterate and experiment with different algorithms to optimize their image composites. Scaling the algorithm to apply to a larger area also becomes trivial by using Fused. Pachama can more efficiently improve transparency into forest carbon projects through better data and better insights, faster.

## References

- [1] On the Advantages of Using Harmonized Landsat Sentinel-2 Data for Monitoring Environmental Change
- [2] An Update on NASA's Harmonized Landsat and Sentinel-2 Project
- [3] An initial evaluation of carbon proxies for dynamic reforestation baselines

================================================================================

## The Strength in Weak Data Part 1: Navigating the NetCDF
Path: blog/2024-09-23-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-09-23-kristin/

**TL;DR Fused streamlined Kristin's workflow to integrate CSV and NetCDF data directly from S3.**

The resolution differences are huge‚Äîgoing from 30 square meters up to 5 billion! Traditional tools would have you pulling your hair out, but Fused lets you turn this "weak" data into something powerful.

## Actual Variable: Handling the Data Mismatch

When dealing with data that doesn't quite match up‚Äîlike trying to combine different resolutions‚Äîyou need to align everything to the coarsest resolution. In this case, that's the county level.

Here's how I tackled it: I grabbed a CSV file of county ANSI codes along with my actual variable data. Using Fused's Fused's File Explorer, I plotted the data easily. Just a quick visit to the File Explorer S3 bucket, a double-click on the file, and the entire map rendered instantly.

Remember the days of wrestling with shapefile resolutions? No more. I edited the UDF to pull my actual data CSV straight from my S3 bucket in under 30 seconds. Boom.

## Predictor Variable: Navigating the NetCDF

Now, let's get into the predictor variable‚Äîa NetCDF file from 5 degrees off the equator, covering around 25 square kilometers. NetCDF files can be a bit tricky to work with due to their complex formats, but Fused's utility modules make it easier. I imported some key functions directly into my UDF to clip the array, convert it into an image, and add a colormap.

```python showLineNumber
@fused.udf
def udf(bbox: fused.types.TileGDF=None, path: str='s3://fused-asset/misc/kristin/sif_ann_201508b.nc'):
    xy_cols=['lon','lat']
    utils = fused.load("https://github.com/fusedio/udfs/tree/057a273/public/common/").utils
    # Get the data array using the constructed path
    da = utils.get_da(path, coarsen_factor=3, variable_index=0, xy_cols=xy_cols)
    # Clip the array based on the bounding box
    arr_aoi = utils.clip_arr(da.values,
                       bounds_aoi=bbox.total_bounds,
                       bounds_total=utils.get_da_bounds(da, xy_cols=xy_cols))
    # Convert the array to an image with the specified colormap
    img = (arr_aoi*255).astype('uint8')
    return utils.arr_to_plasma(arr_aoi, min_max=(0, 1), colormap="rainbow", include_opacity=False, reverse=True)
```
Once I saved the UDF and created an HTTP endpoint, I visualized the data interactively in the App Builder.

## The Variable That is Going to Make this Weak Data Strong

Okay, I have prepped my actual and predictor variables. Now, I will focus on how to fuse the geometries together using the variable that is going to make this Weak Data Strong (30 square meters). For that, stay tuned for Part 2, where I'll dive into the techniques for aligning and merging these spatial layers into a cohesive analysis. See you in the next installment!

================================================================================

## Enrich your dataset with GERS and create a Tile server
Path: blog/2024-09-19-overture/index.mdx
URL: https://docs.fused.io/blog/2024-09-19-overture/

**TL;DR Fused enables on-the-fly enrichment of Overture datasets using simple spatial joins.**

================================================================================

## The App That Finds Your City's Rainfall Twin Globally
Path: blog/2024-09-17-milindsoni/index.mdx
URL: https://docs.fused.io/blog/2024-09-17-milindsoni/

**TL;DR Milind analyzes global precipitation patterns using H3 indexing, cosine similarity, and Earth Engine data to create an interactive rainfall comparison app.**

## How It Works

Our UDF utilizes the following key components:

1. Earth Engine API: To fetch global precipitation data
2. H3 Index: For efficient spatial indexing
3. DuckDB: For fast query execution on geospatial data
4. Cosine Similarity: To compare rainfall vectors

## The Workflow

1. **Data Aggregation with DuckDB**: The data retrieval process is streamlined using Fused and Xarray:
   - **Fused and Earth Engine**: Fused simplifies access to Google Earth Engine's vast catalog. It provides a more intuitive and faster interface with a much better file manager for working with spatial data compared to the Earth Engine platform itself.

   - **Xarray Integration**: We use Xarray to work with our multi-dimensional rainfall data. It allows for easy handling of labeled arrays and datasets, particularly useful for time-series climate data.

2. **Data Aggregation with DuckDB**: After retrieving the raw data, we use DuckDB to efficiently aggregate it. This involves:
   - Grouping the data by H3 hexagon and month
   - Calculating the average monthly rainfall for each hexagon
   - Creating 12-element vectors representing annual rainfall patterns for each location

3. **Cosine Similarity Calculation**: Finally, we use cosine similarity to compare these rainfall vectors. This allows us to quantify how similar the rainfall pattern of one location is to another, or a reference pattern.

4. **Converting UDF to an app with Fused App Builder**: To make the rainfall similarity comparison UDF accessible and interactive, I used the Fused App Builder to help quickly build an app from the UDF that I just created. Every data scientists favourite prototyping tool is Streamlit which helps to build frontends in Python quickly and that's what the app builder brings to you! Convenience of Streamlit with the Power of Fused.

## The App Builder

If you are familiar with Streamlit, it is super convenient to build UI from just Python code. Folium maps helped me build interactive maps where I can draw areas to compare with and I could also write a custom HTML-based iframe to integrate Mapbox GL within the app itself, the snippets of which again are available in the Fused documentation.

1. **Interactive Folium Map**

I implemented a Streamlit Folium based map that allows users to select a location of interest.

2. **Plotly Charts**

A bar chart displays monthly rainfall data for the selected location in the folium map after querying the UDF and passing the GeoJSON shape as a parameter in the UDF,

3. **Iframe Integration**

- The hex-similarity map shows global rainfall pattern similarities.

### Calling the UDF within the App

Just one line of code to call my UDFs within the app to

- Fetch the historical rainfall data from Google Earth Engine for the marked area.
- Aggregate rainfall vectors
- Calculate the similarities of the location with the vectors in the bounding box in the iframe

It was as easy as `fused_app.run("fsh_****")`

### Performance and Optimization

Fused and Streamlit already have excellent caching mechanisms which helped me cache large amounts of data and information prior to the usage so that the next time the app loads, the computations are much faster! I can compare the rainfall patterns of any two locations on the Earth in seconds with a few lines of code. How cool is that!

> Building scalable Geospatial Applications have never been so quick and easy!

================================================================================

## Six ways to use Fused
Path: blog/2024-09-12-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2024-09-12-danieljahn/

**TL;DR: Fused is a versatile platform that serves as a code catalog, a parallel data processing engine, an app creation tool, a serverless HTTP endpoint generator, and an IDE.**

*Example from How Pachama creates maps on-the-fly with Fused*

## 5. Geospatial Streamlit

Streamlit is a Python library that helps you create and
deploy web apps
with a few lines of code.

Streamlit is also the best first-time-user experience I've had with a library.
Without prior experience, I could immediately go from a Python script straight to an interactive web app.

With Fused's App Builder, any UDF can be turned into an interactive Streamlit app.
Fused also automatically serves the app for you.
While the app itself runs in the browser using Pyodide, it can call any Fused UDF, processing the data using the Fused engine.

  height="800px"
  useResizer=
  requirements=
/> */}

## 6. Geospatial-first IDE

Of the six, this is the most aspirational use case.
It's also potentially the most impactful.

Fused provides the Workbench, a great web-based IDE.
Working with it started changing how I think of developing geospatial applications.

[Image: ]

Today, there are two worlds.

- On one side, the software engineer uses test-driven-development to develop well-designed code in quick iterations.

- On the other side, the data scientist develops code directly against real data using notebooks and visualizations.

Fused can bring these worlds together. Simply annotating your function as `@fused.udf` gives you the ability to immediately visualize the results with real data, over any geographic region.
Fused Workbench does this, but you could equally develop in VSCode and switch to QGIS to immediately inspect the results.

By developing your code as a web of stateless UDFs and utilizing `@fused.cache`, you gain the ability to develop automatically cached pipelines whose results can be inspected in tools like Felt or served with an HTTP endpoint without any added work.

Often the greatest cost of data pipelines is developer time.
Fused has the potential to tighten the development feedback loop and catch errors early, reducing the time needed to develop robust data pipelines.

## Conclusion

This article gave six concrete examples of how you can use Fused today.

However, the possibilities of Fused are not limited to these examples. With its powerful execution engine, visual IDE, growing host of integrations, and just-copy-the-link app deployment, Fused is generic enough to enable use cases not even the team behind it has thought of.

I'm excited about the future of Fused. I wouldn't be surprised to see it become a ubiquitous tool in the geospatial world.

================================================================================

## AI for object detection on 50cm imagery
Path: blog/2024-09-05-dl4eo/index.mdx
URL: https://docs.fused.io/blog/2024-09-05-dl4eo/

**TL;DR Jeff Faudi used Fused for real-time object detection on 50cm satellite imagery, displaying results as an interactive web map.**

To display this image on the web, you typically need to project it in Web Mercator projection with gdal and cut it into 256x256 pixels tiles that will be displayed nicely by web-mapping applications such as GoogleMaps, OpenLayers, Mapbox, MapLibre, Leaftlet or Deck.gl.

Until recently, I would have done this physically and generated thousands of tiles. Now, we will do this almost magically with Fused.

## Creating a UDF

Basically, I just have to write the piece of code that generate the content of a tile and Fused takes care of running the code and providing the urls to share the layer in any application. The Python function that I have to write is called a UDF and it has at least one parameter which contains the bounding box (bbox) on which I need to generate the tile.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF = None,
    chip_len: int = 256):

    from utils import read_geotiff_rgb_3857

    geotiff_file = 's3://fused-users/dl4eo/my_image.tif'
    return read_geotiff_rgb_3857(bbox, geotiff_file, output_shape=(chip_len, chip_len))
```

First, it is worth noting that we extract all content from a GeoTIFF image (ideally a COG i.e. Cloud Optimized GeoTIFF) which contains the bands and geometric information about the satellite image. This GeoTIFF is stored anywhere on the cloud. Here, it is stored in the AWS S3 bucket provided by Fused. Also, note that the function returns an array for raster tiles but could return a GeoJSON for vector tiles.

We use the bounding box of the tile provided as a parameter, convert it from lat/long to Web Mercator (EPSG:3857), get the corresponding bounding box in the original image, and project it in Web Mercator projection in the destination array with the correct desired tile size (typically 256x256 pixels).

The Fused UDF Builder enables one to view the result and logs while coding.

## Implementing aircraft detection

Now, if we want to display a real-time aircraft detection layer, we could replicate the previous step: send the resulting image extract to the API and display a vector layer. However, we must avoid applying deep learning algorithms to images that might have been zoomed. These algorithms are typically trained at a specific resolution, and the Web Mercator projection does not preserve size.

_https://en.wikipedia.org/wiki/Mercator_projection_

We read the content of the Pleiades image in its original projection (either the raw geometry or a transverse mercator projection in which the central meridian would pass through the center of the image). In this case, the resolution is guaranteed to be the correct native resolution of the image.

The UDF gets the Pleiades image in the correct projection, then calls the prediction API, and finally returns the predictions in a GeoDataFrame which will be dynamically rendered on the map. For performance, we have added the @fused.cache decorators which make the function automatically cache results for identical parameters. The predictions are returned in pixels in the source image and then converted into lat/long so they render on a map. Then, when we look at the result in the workbench, we get some issues at the border of the tiles.

The reason is that if an aircraft is on the tile border, it will be detected partially on the lower tile and potentially on the upper tile. The two bounding boxes might not align perfectly so we cannot merge them. The solution here is to extract a image larger than the tile: if the center of the predicted box is inside the tile we keep it, if it is outside we discard it. We usually use a margin that is the upper size of the objects we are trying to detect i.e. 100 meters for aircrafts. After these little improvements, the result is much nicer

## Building a web app

Now that everything is running fine in the workbench, it is time to extract the layers and include them in a webpage. Fused provides an easy way to integrate layers in external applications via HTTP requests. You just need to go to Settings, click Share and copy the provided URL.

Then, you can integrate this URL as the tile source in any mapping application. I am not diving into that here, but you can read how to do this in the DeckGL Fused docs. You can check the code source of the demonstration below. Here is the extract of the JavaScript Deck.gl code where the URL is integrated.

And here it is: the final working demonstration!

## Conclusion

Huge thanks to the amazing team at Fused for their incredible support, and to my former colleagues at Airbus for providing the stunning Pleiades image. I think that this application turned out to be very sleek and powerful. If the underlying satellite image changes, the AI layer gets automatically recomputed on the fly.

I'd love to hear your thoughts!

_This article was originally published in LinkedIn on June 20th 2024._

================================================================================

## Summarizing building energy ratings
Path: blog/2024-09-03-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-09-03-isaac/

In this video tutorial, I show a complete data app workflow in Fused. Starting with exploring the data in Fused, the tutorial walks through developing a UDF to serve the data, and then a Fused App to share results.

With Fused, this whole workflow takes just minutes from beginning to end. Fused helps me visualize the data at every step, iterate on my analytical logic, and finally publish a dashboard.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://youtu.be/crakOM4Pytg?start=180" width="100%" />

================================================================================

## ML-less global vegetation segmentation at scale
Path: blog/2024-08-29-kevin/index.mdx
URL: https://docs.fused.io/blog/2024-08-29-kevin/

**TL;DR Kevin used Fused to create a global vegetation segmentation layer without machine learning, displaying results as an interactive web map.**

================================================================================

## How Pachama creates maps on-the-fly with Fused
Path: blog/2024-08-27-pachama/index.mdx
URL: https://docs.fused.io/blog/2024-08-27-pachama/

**TL;DR Pachama uses Fused to create maps on-the-fly for their sustainability platform.**

Pachama recently built the Land Suitability Tool within their Reforestation Partner Portal to revolutionize how project developers assess the restoration potential of prospective project sites. In this portal, organizations and landowners looking to start a reforestation project define an Area Of Interest (AOI) by drawing or uploading a polygon, then estimate the land's eligibility based on data layers derived from environment models that take into account country-level data about land cover, vegetation history, and natural risks. For example, a project may look to derive credits from carbon sequestration through native reforestation and ally with local communities that earn an income as stewards of the land.

One of Pachama's challenges was making preprocessed data available for user-defined AOIs that aren't known ahead of time. This would require generating and storing data for entire countries, which is expensive given that a preprocessing step is billed for each square kilometer.

Furthermore, the process involved transferring data between backend and frontend teams, each with different requirements. This resulted in converting datasets between formats, workflows with complex infrastructure, long-running jobs, and slow turn-around times.

## The Solution: Serverless Tile Generation with Fused

To overcome these challenges, Pachama turned to Fused to generate maps on the fly with serverless API endpoints. Fused now provides them an elegant way to write custom workflows to crunch data with Python and serve it behind tile endpoints that natively integrate with map tile layers. This makes it possible to process and visualize any dataset with manageable operation costs.

> **"Fused has been critical in our product lifecycle. The speed at which we were able to iterate based on new requirements is unrivaled."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

The ability to trigger a UDF that generates a vector directly from a Zarr file was a game-changer for Pachama's ability to close the gap between their analytics and their end-users. This innovation has made the team more productive and enabled them to streamline complex tasks that were previously cumbersome and impractical.

> **"Fused takes DevOps out of our hands to focus on our core mission, building technology to restore nature."**
>
> **Marie Hoeger, Staff Software Engineer @ Pachama**

The Land Suitability tool covers the contiguous USA, Brazil, Mexico, Argentina, Guatemala, Panama, Paraguay, Colombian Amazon, and the Peruvian Amazon. Pachama plans to expand to more regions around the world. It processes a variety of datasets including Pachama's proprietary canopy height map. Pachama generates regional maps of average top-of-canopy height using a combination of lidar from GEDI and a suite of satellite observations at varying spatial scales, including optical and radar imagery, topography, and climate data. Fused's on-the-fly tiling simplifies the workflows to generate and load the data into the user-facing app.

By combining analytical and visualization capabilities, Fused enables powerful and productive workflows. Instead of pre-computing tiles for entire datasets, Pachama now generates tiles dynamically only for user-defined AOIs, reducing system complexity and cost.

Here's a minimalist example of how Pachama uses a Fused User Defined Function (UDF) to generate a vector from a raster file in COG format:

```python
@fused.udf
def udf(bbox: fused.types.TileGDF=None):

    from utils import raster_to_vector

    table_path = "s3://pachama-fused-data/dataset.tiff"
    gdf = raster_to_vector(table_path, bbox)
    return gdf
```

This UDF can be called via HTTP request with the following URL structure:

```
https://www.fused.io/server/v1/realtime-shared/fsh_1gcTv/run/tiles///?dtype_out_vector=mvt
```

## Key Features

The Fused automatically provisions an endpoint for each of Pachama's UDFs. The prospecting application then loads the endpoint into a Mapbox application, which consumes the output in MVT format as defined by the `dtype_out_vector` parameter.

- HTTP Endpoints work with slippy maps, which is standard across map tiling applications.
- Map clients call the endpoint for each tile in the viewport, passing values for z, x, and y. Fused then runs the UDF, passing a GeoDataframe with the Tile coordinates.
- The UDF code spatially filters the referenced dataset, processes the fraction of data, and returns it to the client app as the response of the HTTP call in the format specified via a query parameter. This avoids the need to pre-compute data or manage files.

## The Result: Simplify Data Workflows By 50%
Fused and its UDF environment revolutionize how Pachama renders tile-based maps by leveraging analytical tools: cloud-optimized data formats, the flexibility of Python for spatial operations, and the scalability of serverless. Engineers at Pachama used to see a gap between the analytical data formats (e.g. COGs & GeoParquets) and visualization data formats (MVT, PMTiles, XYZ Tiles). Fused closed the gap and let them retire a major piece of the pipeline.

> **"Fused replaced 4 steps of the pipeline with a single Fused UDF."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

## Future innovation for Pachama
Looking ahead, Pachama aims to expand this powerful tool worldwide, catalyzing high-integrity reforestation projects in the regions that need it the most. With Fused's infrastructure underpinning its platform, Pachama can stay focused on making powerful science and analytics accessible to everyone through intuitive visual interfaces.

Read about Pachama's mission and learn how they use technology to evaluate forest carbon projects to assess carbon projects.

================================================================================

## Geospatial workflows of any size
Path: blog/2024-04-22-webinar/index.mdx
URL: https://docs.fused.io/blog/2024-04-22-webinar/

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://www.youtube.com/watch?v=SztyQoBtfh0" width="100%" />

Isaac Brodsky, the CTO of Fused, delved into the power of Fused during a LinkedIn live session with Matt Forrest. They discussed the contrast of Python vs. SQL for data analytics, the advantages of serverless geospatial processing, and showcased a live demo of the UDF Builder. During the demo, Isaac created a User Defined Function visualize Overture building footprints that are within a certain proximity of water.

You can re-watch the webinar on LinkedIn, YouTube, or below.

================================================================================

## DuckDB + Fused: Fly beyond the serverless horizon
Path: blog/2024-04-09-duckdb/index.mdx
URL: https://docs.fused.io/blog/2024-04-09-duckdb/

**TL;DR Fused extends DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.**

The combination of Fused serverless operations and DuckDB offers blazing fast data processing. Fused embraced Python to create serverless User Defined Functions (UDFs). Now, with the help of DuckDB, Fused enables developers to leverage the ease and familiarity of SQL in these functions ‚Ää- ‚Ääwithout compromising performance and parallelism.

This blog explains how Fused User-Defined Functions (UDFs) can extend DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.

The blog post illustrates three complimentary implementations:
1. Run DuckDB in a Fused UDF
2. Call Fused UDFs from DuckDB
3. Integrate DuckDB in applications using Fused

## The evolution of the data processing landscape
For companies with bottom lines that depend on time to insight, the data landscape is driven by the need to process increasing data volumes and make operations easier to express. This section discusses how Fused and DuckDB can address these needs within the context of the latest wave of the data processing ecosystem.

### Increasing data¬†volumes
When the size of data required for an operation is larger than memory, it becomes a bottleneck. In the early 2010's, the effort to process increasing volumes of data created MapReduce, Hadoop, and Spark to help companies scale out clusters. The complexity of managing clusters gave way to managed services like Databricks and Snowflake, but their high cost and inefficient data transfer with Python (by now a staple of data science) still left parts of the market unaddressed.

Many technologies emerged to attempt to address latent gaps, but it was DuckDB that surged around 2020 as a fast, easy to use, and cost effective solution to process large volumes of data with SQL while reducing the switching cost of having to learn new frameworks. At around the same time, serverless solutions to address the scale out problem started to gain traction.

Now, as AI training and inference require ever more data, the speed of processing and the speed of development become critical bottlenecks. DuckDB and serverless processing together enable new applications. DuckDB gives workflows an in-process performant SQL engine with:

- Fast processing of large datasets through larger than memory processing with a vectorized query engine.
- Zero-copy interoperability with Python, thanks to formats like Apache Arrow.
- Portability and unprecedented developer experience with easy set-up and without the need to maintain a database server.
- Extensibility thanks to an ecosystem of plugins and extensions (C++), scalar Python UDFs, and WebAssembly compatibility.

DuckDB's modularity in data interchange and query execution makes it an ideal choice for serverless architectures. The combination of DuckDB and serverless has unique advantages:

- Fast and cheap data access thanks to cloud optimized data formats that enable retrieving part of the file (e.g. Parquet for tabular data, Cloud Optimized GeoTIFF for imagery.)
- Scalability, distributed compute without managing infrastructure and without expense when code is not running.
- Easy to share results and create integrations by triggering jobs and loading data via simple HTTP calls.

## Python + SQL¬†synergy
Python is the lingua franca of data science and AI. It's an imperative language‚Ää-‚Ääwhich means it's easy to write complex logic without sacrificing readability, and interface a broader range of data formats‚Ää-‚Ääenabling operations inaccessible to SQL like calling API clients, fine-grained analytic calculations, and processing arrays and rasters. The Python ecosystem recently adopted Rust to write high performance, memory safe modules. However, Python historically struggled with concurrency and managing the memory of distributed clusters, which hindered its ability to process large datasets.

Declarative languages like SQL offer simple syntax to define data manipulations for performant query engines, but they lack explicit control flow and are limited to select data structures.

Two approaches to intertwine SQL and Python emerged, each with particular tradeoffs in portability and efficiency:
- **SQL queries in Python.** These tend to sacrifice data transfer efficiency between runtimes or require specialized, complicated data warehousing.
- **Python UDFs within SQL.** These tend to incur performance costs and require maintaining a Python runtime within the DBMS.

These are offered, to different extents, by tools like Databricks, BigQuery, and Postgres.
- **Databricks** offers a notebook environment, familiar to the data scientist, that enables workflows to transition between Python and SQL‚Ää-‚Ääbut requires specialized data warehousing, complicated cluster management, and lacks debuggability.
- **BigQuery** UDFs bring an imperative language to SQL engine‚Ää-‚Ääbut it's constrained to Javascript which lacks Python's powerful data operations and libraries.
- **Postgres** and other databases can bring SQL to a Python runtime with connector libraries such as Psycopg2 and SQLAlchemy‚Ää-‚Ääbut this pattern has the infrastructure overhead of needing to run a separate database server.

However versatile, DuckDB is founded on SQL and still needs to rely on Python and plugins for expressibility. But its support for Python UDFs and plugins is yet to mature.
- DuckDB only supports scalar Python UDFs.
- Constrained to the capabilities of the local runtime process.
- There's no seamless way to share Python UDFs across databases or runtimes.
- Plugins are difficult to write and deploy.

## Fused + DuckDB¬†synergy
Fused is a framework to author and run serverless operations. Every Fused UDF is an HTTP API that can be called to run and load data from any application that can make HTTP requests. Integrating UDFs into workflows is as easy as passing the endpoint as a string. Spreadsheets, web maps, ETL pipelines, and DuckDB can all load data from HTTP API endpoints, and dynamically parametrize calls with query parameters.

- Eliminates the need to provision, manage, and scale instances‚Ää-‚Ääwhich is what caused the initial break away from the Map Reduce, Hadoop, and Spark era. Its just-in-time backend scales from zero to cluster as quickly as needed.
- UDFs can call UDFs‚Ää-‚Ääwhich results in blazing fast execution by running thousands of parallel jobs -without worrying about orchestration.
- Pay only when code runs, and run from anywhere‚Ää-‚Ääwhich speaks to market segments unaddressed by managed platforms like Databricks and Snowflake.
- Natively runs on a standard Python interpreter‚Ää-‚Ääso it seamlessly runs DuckDB while keeping Python's expressibility and ecosystem of libraries.
- Dovetails with cloud-native data formats. Their atomic data loading and compressed formats make for reduced data transfer between local processes and third party cloud warehouses.

Fused and DuckDB together reduce architectural complexity and make it easy to have cutting-edge analytic processing in any application. Together, they eliminate the need for cumbersome distributed query engines which are slow to start-up and are overkill for smaller datasets.

Fused UDFs are easy to share and can run from anywhere. The examples in this post are available as community UDFs you can find on the open source Github repo and run them in any Python environment with the Fused SDK.

## Example patterns

This section shows and discusses three powerful patterns at the intersection of Fused and DuckDB.
### 1. Run DuckDB in a Fused¬†UDF

DuckDB parallelizes its own operations under the hood thanks to its columnar vectorized query engine that provides compelling performance for querying using SQL. However, there can still be bottlenecks in operations upstream or downstream of DuckDB. To resolve this, Fused UDFs easily run DuckDB and create a seamless experience between Python and SQL.

See the full example in our documentation.

### 2. Call Fused UDFs from¬†DuckDB

Any database that supports querying data via HTTP can call and load data from Fused UDF endpoints using common formats like Parquet or CSV. This means that DuckDB can dispatch operations to Fused that otherwise would be too complex or impossible to express with SQL, or would be unsupported in the local runtime.

In this example, a Fused UDF returns a table where each record is a polygon generated from the contour of a raster provided by the Copernicus Digital Elevation Model as a Cloud Optimized GeoTIFF. DuckDB can easily trigger a UDF and load its output with this simple query, which specifies that the UDF endpoint returns a Parquet file.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sql.gif" alt="overture" width="600"/>

This pattern enables DuckDB to address use cases and data formats that it doesn't natively support or would otherwise see high data transfer cost, such as raster operations, API calls, and control flow logic.

See the full example in our documentation or open it in this [DuckDB shell%0ALIMIT-10~).

### 3. Integrate DuckDB in applications using¬†Fused

Fused is the glue layer between DuckDB and apps. This enables seamless integrations that trigger Fused UDFs and load their results with simple parameterized HTTP calls.

DuckDB is an embedded database engine and doesn't have built-in capability to share results other than writing out files. As a corollary of the preceding example, it's possible to query and transform data with DuckDB and seamlessly integrate the results of queries into any workflow or app.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sheets.gif" alt="overture" width="600"/>

To try this example simply make a copy of this Google Sheets spreadsheet (File > Make a copy) and click, and modify the parameters in B2:4 to trigger the Fused UDF endpoint and load data.

See the full example in our documentation.

## Conclusion

While the pendulum of the data landscape swung from distributed compute to single-node, Fused's serverless operations swing the conversation back with a simple and cost-efficient scale-out.

This blog post discussed how gaps in the modern data stack can be addressed by integrating Fused and DuckDB, two emerging data processing tools. The intersection between DuckDB's portable SQL and Fused's scalable python operations creates a stack that is:
Flexible due to the seamless interaction of Python and SQL.
Scalable, simple, and cost efficient.

Easy for data scientists to create, and easy for non-coders to consume.

DuckDB is an early example of how Fused integrates with the modern data stack. We're eager to share the growing list of compelling integrations over the following months.

We would like to extend our thanks to Wes McKinney and Michael Driscoll for reviewing drafts of this post before it went out.

## Get started with¬†Fused

Want to get involved?

- If you'd like to take Fused for a spin, please sign up for the Private Beta waitlist.
Give back to the community by contributing a UDF.
- You can also join the conversation by becoming a member of the Fused Discord community. We are always happy to hear your thoughts.
- Does taking serverless operations to the next level sound exciting to you? Fused is hiring! Shoot us a note at `sina@fused.io`.

================================================================================

## Fused redefines geospatial with instant maps
Path: blog/2024-03-06-pressrelease/index.mdx
URL: https://docs.fused.io/blog/2024-03-06-pressrelease/

Fused is a modern geospatial toolkit for companies to code, scale, and ship geospatial workflows of any size.

This week we are unveiling Fused, a toolkit to enable interoperability between all geospatial datasets and tools in the modern data stack. Fused is the glue layer that integrates data platforms with data tools via a managed serverless API.

## Overview

Co-founders Sina Kashuk and Isaac Brodsky met while working at Uber. They co-founded Unfolded to commercialize the popular open source geospatial visualization projects Kepler.gl, Deck.gl, and H3. Unfolded was acquired by Foursquare in June, 2021.

Fused has raised $1 million in pre-seed funding from Fontinalis Partners, Wes McKinney, Michael Driscoll, Jason Richman, and angels from Uber, Airbnb, DoorDash, and others.

Fused delivers serverless geospatial operations at any point of the stack ‚Äî with a simple HTTP call. This is like when users pull-up information from map apps, but with custom and transparent logic. This shields developers from hours of burdensome engineering, enabling businesses to serve their customers with timely insights, faster.

Teams supercharge their favorite IDEs, tools, and frameworks with Fused. They build with the Python SDK, preview on the browser with Fused Workbench, and run in their stack via the Hosted API.

_Fused ecosystem and product line._

## The problem

We're now in a moment where large-scale geospatial datasets are migrating to open cloud-enabled formats. However, we have personally seen how it can be challenging to utilize this data at scale. At the same time, there has been a rise in Earth observation imagery, which will only accelerate as we monitor climate change and as satellites continue scaling, enabling. However, the sheer volume of data, complexity of operations, and fragmentation of tooling holds back how we process and present that data that is critical for making informed decisions about critical company operations.

Today, data scientists and analysts manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. The size of data limits the possible depth of insight of last-mile analytics and the speed at which they can be delivered ‚Äî leaving problems unaddressed. Moreover, data scientists handoff algorithms to data engineers who then translate code to work with orchestrators that run on distributed compute systems maintained by an entire devops team. An analyst needs to navigate a sea of buzzwords like CRS, GDAL, Spark clusters, geo-partitions, raster and vector joins, zonal stats, and census blocks ‚Äî just to prepare for the analysis they actually want to do.

## The solution

Today, a leading global media company animates atmospheric rivers to report weather news ‚Äî 36x speed improvement, from hours to minutes. An EV company uses Fused to optimize its EV charging station network planning capabilities ‚Äî blending data at an unprecedented coverage and detail. A carbon offset company creates custom deforestation basemaps with better operational efficiency ‚Äî closing the analysis loop for stakeholder reports.

Fused empowers teams in these companies to seamlessly layer weather, infrastructure, road, and deforestation data; while transforming it with custom Python code to create apps for real-time decision making. Fused simplifies workflows so small teams can deliver novel business-critical insights where it wasn't possible before.

Read more: Founder's Blog Post

## Vibrant community

As a founding tenet, Fused promotes open source, transparency, and collaboration. To this end, data scientists and app builders engage the Fused community on GitHub and Discord to find, reuse, and share verified code snippets that they can bring into their workflows.

Community UDF of hydrology model by Taher Chegini
In sum, Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when for any geospatial analysis you would have to send it to your GIS person to analyze it and get it back in 3 weeks, if you get lucky (and forget about integrating that with any other tools). Fused is built to be the interoperable glue between geospatial data systems, and we're excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join the journey to break away from old geospatial infrastructure. Let's revolutionize geospatial technology together! fused.io. üåéüöÄ

Join the journey

- Read the announcement on Tech Crunch
- Follow us on Twitter/X
- Follow us on LinkedIn
- Star our GitHub repo
- Join the conversation on Discord
- Read Fused's founding principles

================================================================================

## Founder's blog post: why Fused?
Path: blog/2024-03-01-welcome/index.mdx
URL: https://docs.fused.io/blog/2024-03-01-welcome/

Fused enables interoperability between datasets and tools in the modern data stack. It's a glue layer to integrate data platforms with data tools via a managed serverless API.

## Current limitations with data processing

Today, there is a fragmented ecosystem around scalable geospatial data processing. Python geospatial libraries like GeoPandas, Shapely, and Rasterio make it easy to do small jobs but are single-threaded and operate entirely in-memory. For bigger jobs, there are Python parallel processing tools like Dask that require complex installations and are liable to memory pressure errors. Spark-based tools like Apache Sedona and RasterFrames have a steep learning curve and are hard to debug and orchestrate. Postgres and its geospatial extension PostGIS operate on larger-than-memory datasets but are hard to scale larger than the disk of one machine, aren‚Äôt designed for OLAP workloads, and can be hard to administer. Cloud data warehouses like Databricks and Snowflake are monolithic systems that tend to bring lock-in and pricing that is hard to anticipate.

Spatial SQL is a great way to run scalable operations on tables with vector data - but falls short on raster data and does not have native access to libraries for the finesse operations of data science. Geospatial data science teams largely use Python and would prefer to use it both in development and in production - but tooling fragmentation forces them to juggle languages and frameworks. The present paradigm accepts the inefficiencies of complexity as a necessary evil because there hasn‚Äôt been a better way to work with both raster and vector data at scale. Data teams have an unaddressed need for a friendly Python API that scales. To increase development velocity it‚Äôs convenient for most code to run in Python, moving only computationally heavy code into specialized frameworks - as efficiently as possible. Additionally, scaling Python from local development to massive cloud workloads calls for efficient parallelization.

## Seizing the moment

The last several years have seen a commoditization of modular building blocks of OLAP systems and increased adoption of geospatial cloud-native data formats. With the convergence and popularity of columnar memory formats like Apache Arrow and Apache Parquet, easy-to-use columnar OLAP databases like DuckDB, and broader adoption of geospatial cloud-native data formats like Cloud-Optimized GeoTIFF and GeoParquet, we believe there‚Äôs a window for a serverless geospatial OLAP engine. Moreover, serverless computing has emerged as a prominent trend, delegating infrastructure management and dynamically scaling resources in response to demand, leading to heightened flexibility and cost efficiency. Leveraging serverless cloud infrastructure like AWS Lambda, Azure Functions, Google Cloud Functions, or Cloudflare Workers enables event-driven processing closer to the data source.

Parquet files have become the standard file format for columnar data and have helped to commoditize the decoupling of storage and compute by enabling queries directly on object storage like AWS S3. GeoParquet ‚Äì a specification for storing point, line, and polygon geometries in Parquet ‚Äì has seen recent momentum as a fast storage format for geospatial vector data and has started to be integrated into industry-standard tools like GDAL. Moreover, with spatial partitioning, operations can be broken down into small independent parts that execute simultaneously in multiple processes. For geospatial array data like satellite imagery, Cloud-Optimized GeoTIFF ‚Äì an extension to GeoTIFF that enables chunked access via HTTP range requests ‚Äì has taken hold as the standard way to store geospatial image data, with petabytes publicly available from AWS‚Äô open data program and buy-in from major vendors like USGS and Planet.

Apache Arrow has become the universal in-memory columnar data format for columnar, analytic data because its language-independent specification enables easier movement of data between languages and frameworks. Moreover, GeoArrow ‚Äì an incubating specification for storing geospatial data in Arrow ‚Äì gives us a way to move geospatial data from Python to compiled code for free, and will likely serve as the foundation for an ecosystem of large-data geospatial tools. Already in the frontend, deck.gl can use GeoArrow-style data buffers to visualize millions of coordinates with no serialization costs.

As a result of all these trends, smaller data can be transferred to and processed on serverless cloud services in ways that are not possible ever before. Public clouds enable event-driven compute services that automatically scale, which makes for simple infrastructure and dependency management. Managed offerings reduce the complexities of data pipelines enabling geospatial workloads of any size to run on demand ‚Äì to empower users with the ability to go from code to map, instantly.

## Why Fused?

Fused instantly converts user‚Äôs Python code to workflows and maps in Jupyter notebooks, low-code web apps, the Fused Workbench web-app, ETL pipelines, or any tool that consumes HTTP API endpoints. Fused lets developers run real-time serverless operations at any scale and build responsive maps, dashboards, and reports. Developers develop in production and run on any scale data without infrastructure friction using serverless parallel computing powered by advanced caching of geo-partitioned data. This enables bringing interoperable workflows, apps, and maps to the user's preferred stack and avoiding vendor lock-in.

With Fused, users find, reuse, and share User Defined Functions (UDFs) in the Fused vibrant community. Fused UDFs are building blocks of serverless geospatial operations that integrate across the stack - with Planetary Computer, Google Earth Engine, Big Query, Snowflake, DuckDB, and more. They load datasets from the cloud ecosystem such as NASA, NOAA, US Census, and Overture. Fused serverless API turns these UDFs into live HTTP endpoints that load their output into any tools.

Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when you manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. Fused is built to be the interoperable glue between geospatial data systems, and we‚Äôre excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join us in our journey to break from old geospatial infrastructure. Let's revolutionize geospatial technology together! üåéüöÄ

- The Fused Founding Team

================================================================================


---

Generated automatically from Fused documentation. Last updated: 2025-07-15
Total sections: 5
