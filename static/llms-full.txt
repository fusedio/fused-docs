# Fused Documentation - Complete Reference

> Fused is an end-to-end cloud platform for data analytics, built around User Defined Functions (UDFs): Python functions that can be run via HTTPS requests from anywhere, without any install required.

This comprehensive reference contains the complete text of all Fused documentation, including all API methods, examples, tutorials, and guides.

================================================================================

# CORE CONCEPTS

## Call UDFs asynchronously
Path: core-concepts/async.mdx
URL: https://docs.fused.io/core-concepts/async

A UDF can be called asynchronously using the async/await syntax. A common implementation is to call a UDF multiple times in parallel with different parameters then combine the results.

    Setting `sync=False` in `fused.run` is intended for asynchronous calls when running in the cloud with `engine='remote'`. The parameter has no effect if the UDF is ran in the local environment with `engine='local'`.

To illustrate this concept, let's create a simple UDF and save it as `udf_to_run_async` in the workbench:

```python showLineNumbers
@fused.udf
def udf(date: str='2020-01-01'):

    time.sleep(2)
    return pd.DataFrame()
```
    We can not pass a UDF object directly to `fused.run`. Asynchronous execution is only supported for saved UDFs specifed by name or token.

We can now invoke the UDF asynchronously for each date in the `dates` list and concatenate the results:

```python showLineNumbers
async def parent_fn():

    # Parameter to loop through
    dates = ['2020-01-01', '2021-01-01', '2022-01-01', '2023-01-01']

    # Invoke the UDF as coroutines
    promises_dfs = []
    for date in dates:
        df = fused.run("udf_to_run_async", date=date, engine='remote', sync=False)
        promises_dfs.append(df)

    # Run concurrently and collect the results
    dfs = await asyncio.gather(*promises_dfs)
    return pd.concat(dfs)
```

nest_asyncio might be required to run UDFs async from Jupyter Notebooks.
```python showLineNumbers
!pip install nest-asyncio -q

nest_asyncio.apply()
```

================================================================================

## Build with LLMs
Path: core-concepts/best-practices/build_with_llms.mdx
URL: https://docs.fused.io/core-concepts/best-practices/build-with-llms

# Building with LLMs: Getting things done, fast.

_Our mission at Fused is to help you get things done, fast. LLMs can help you write User Defined Functions, then leverage Fused to get feedback on your execution and make analytics faster_

This setup helps you setup Cursor code editor in a way that helps you write & debug UDFs fast.

By the end of this guide you'll have:
- A custom Cursor setup that allows you to ask LLMs to write UDFs for you
- Cursor testing + debugging UDFs based on Fused best practices
- Gives you links to open UDFs in Workbench directly

## Requirements

- A Fused Account with an engine
- A Cursor account
- Locally installed Cursor (update to latest version)

## Setup

_If you get blocked, check out some of the common issues in the Getting unblocked section at the end of this guide._

#### 1. Install `fused` locally. 

This is to help Cursor write UDFs, save them to Workbench for you as well as test & debug them. 

```python
pip install "fused[all]" # this installs optional dependencies especially helpful for geospatial operations
```

You also need to be logged in to your Fused account for Cursor to be able to save UDFs to Workbench for you. You can test this with:

```python

fused.api.whoami()

>>>

```

#### 2. (Optional but recommended) Clone the `udfs` public repo

At Fused we want to build things quickly so we made a common set of functions that we often re-use
You can pass this to Cursor to help it write UDFs without having to re-invent the wheel.

```bash
git clone https://github.com/fusedio/udfs.git
cd udfs/
```

#### 3. Start Cursor locally

Either click on it in your applications or:

```bash
cursor .
```

#### 4. Give Cursor knowledge about Fused

Cursor & all AI tools are evolving so quickly we recommend you update Cursor to the latest version. 
Some of the exact layouts might change by the time you read this.

On Mac you can update Cursor by clicking the top left application and selecting "Check for Updates"

[Image: Update Cursor]

If you get errors when trying to run fused commands in Cursor, it's likely because Cursor is not using the right Python environment. Here are a few things to try:

1. Make sure you have the right Python environment activated. You can do this by:
   - Opening the command palette (Cmd/Ctrl + Shift + P)
   - Searching for "Python: Select Interpreter"
   - Selecting the environment where you installed fused

2. If that doesn't work pass what you need to activate the correct environment to Fused as rule:
- Go to Cursor settings -> "Rules" -> "+ Add Rule"
- Give it instructions, for example:

```md
Always run:
`source /Users/user/miniforge3/etc/profile.d/conda.sh` and then `conda activate fused` to use the latest fused packages
```

3. Try restarting Cursor after activating the right environment

You might need to tinker a bit to make sure you've got the proper environment activated. If nothing else, ask Cursor to help you!

This seems to be a bug if you have `zsh` and `powerlevel10k`. 

At time of writing this isn't fully solved but forums recommend editing your `.zshrc` file to remove the `powerlevel10k` plugin.

Cursor & AI tools are evolving so quickly we recommend you update Cursor to the latest version as some of the exact layouts keep changing.

On Mac you can update Cursor by clicking the top left application and selecting "Check for Updates"

[Image: Update Cursor]

You might not be logged into your Fused Account. Try:

```python

fused.api.whoami()

>>>

```

Cursor might also have not been able to save the UDF to Workbench. Sometimes just ask it again! It can also give you the name of the UDF it wrote, which you can then find in your Saved UDFs in Workbench.

</details>

### Share your findings with us!

We're always looking to improve our best practices and make it easier to get things done with LLMs. 

If you have any feedback, let us know on Discord!

================================================================================

## Best Practices
Path: core-concepts/best-practices/index.mdx
URL: https://docs.fused.io/core-concepts/best-practices

# Best Practices

Get the most out of Fused by following these proven best practices for building efficient UDFs, organizing your workflows, and optimizing performance.

## Documentation overview

<DocCardList className="DocCardList--no-description"/>

================================================================================

## Making the most out of UDFs
Path: core-concepts/best-practices/udf-best-practices.mdx
URL: https://docs.fused.io/core-concepts/best-practices/udf-best-practices

# Build & Running UDFs

_An opinionated guide to making the most out of Fused UDFs_

Fused UDFs are Python functions that run on serverless compute and can be called from anywhere with `fused.run(udf)`. This guide is a resource meant to help you on your way to making the most out of UDFs.

## A short reminder: The anatomy of a UDF

```python showLineNumbers
@fused.udf
def udf(my_awesome_input: int = 1):

    return pd.DataFrame()
```

Each UDF has a few specific elements:
- The `@fused.udf` decorator
- Arguments -ideally typed-
- Imports _inside_ the function
- Some logic
- A supported `return` object

All of this is explained in the "Write UDF" section in much more details.

You can then run UDFs from _anywhere_ with `fused.run(udf)`. These are still Python functions, giving you a lot of flexibility on what oyu can do, but we have some recommendations for keeping them fast & efficient.

## Writing efficient UDFs

### Keep things small

The main benefit of Fused UDFs is how responsive they are. They achieve this by running on Python serverless compute. They can time out, so the best way to keep workflows fast is to keep them small:

- Break pipelines into single-task UDFs
- Leverage `fused.run()` to chain UDFs together
- Or run small tasks in parallel

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):

        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):

        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ```python showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):

        df = fused.run(load_data_udf, data_path=data_path)
        processed_df = fused.run(process_data_udf, df=df)

        return processed_df
    ```

    This is a breakdown of what happens when you run a UDF with `fused.run()` and why we recommend you keep your UDFs at the 30s-45min mark:

    [Image: UDF Design Guidelines]

    Let's imagine we have a `fetch_single_data` that loads data from an API for a large amount of inputs `input_data=[0,1,2,3,4,5,7,8,9]`:

    ```python showLineNumbers
    @fused.udf
    def fetch_single_data(single_input: int):

        # Considering this as our API call, sleeping to simulate the time it takes to get the data
        time.sleep(3)

        # This is a small dataframe, but we could also save to a file and return the file path
        return pd.DataFrame("]})
    ```
    If we were to run this UDF in Workbench we would only be able to run it for 1 input at a time, so we could edit our UDF to loop over the inputs:
    ```python showLineNumbers
    @fused.udf
    def fetch_data(inputs: list):

        fetched_data = []
        for i in inputs:
            # Considering this as our API call
            time.sleep(3)
            fetched_data.append(f"processed_")

        return pd.DataFrame()
    ```
    However, running this UDF with `fused.run(fetch_data, inputs=input_data)` will take longer as we add inputs, we could even quickly go over the 120s limit. We still do want to fetch data across all our inputs which is where `fused.submit()` comes in:

    Going back to our original UDF, we can now run it with `fused.submit()` to run it in parallel:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(input_data):

        results = fused.submit(
            fetch_single_data,
            input_data,
            engine='local', # This ensures the UDF is run in our local server rather than spinning up new instances.
        )
        fetched_data = results.collect_df()

        return fetched_data
    ```

    This is of course a simplified example, but it shows how you can use `fused.submit()` to run a UDF in parallel for each input.

    This now runs a lot faster by running the `fetch_single_data` UDF in parallel for each input.

        The example here blocks the main thread until all the `fused.submit()` calls have finished. This means you might have to wait longer in Workbench for the results to show up.
    
    Comparison of both approaches in Workbench:

    Running with `fused.run()`, 30.89s:
    [Image: 10 inputs fused.run]

    Running with `fused.submit()`, 5.3s:
    [Image: 10 inputs fused.submit]

    Re-using the example from keeping things small:

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down AND cache the calls:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):

        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):

        # Some complicated processing logic to create df_processed
        # ...
        return processed_df
    ```

    ```python  showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):

        @fused.cache
        def load_data(data_path):
            return fused.run(load_data_udf, data_path=data_path)

        @fused.cache
        def process_data(df):
            return fused.run(process_data_udf, df=df)

        df = load_data(data_path)
        processed_df = process_data(df)

        return processed_df
    ```

    ```python  showLineNumbers
    @fused.udf
    def udf():

        beginning_time = time.time()

        # long processing step #1
        time.sleep(5)
        end_process_1 = time.time()
        process_time_1 = round(
            end_process_1 - beginning_time, 2
        )
        print(f"")

        # short processing step
        time.sleep(0.2)
        process_time_2 = round(
            time.time() - end_process_1, 2
        )
        print(f"")

        return
    ```

    Would give us:

    ```
    >>> process_time_1=5.0
    >>> process_time_2=0.2
    ```
</details>

### Test out your UDFs before running them in parallel

When using `fused.submit()` to run a UDF in parallel you can use the `debug_mode` to run the 1st argument directly to test if your UDF is working as expected:

```python showLineNumbers
job = fused.submit(udf, inputs, debug_mode=True)
```

This runs `fused.run(udf, inputs[0])` and returns the results. It allows you to quickly test out `udf` before running it on a large number of inputs.

Since UDFs are cached by default, using `fused.submit(udf, inputs, debug_mode=True)` means Fused won't run the 1st input again, as you'll have just run it! (unless `udf` is set with `cache_max_age=0`)

### Join the Discord for support

We host & run a Discord server where you can ask any questions! We or the community will do our best to help you out!

[Image: Discord]

================================================================================

## Workbench Best Practices
Path: core-concepts/best-practices/workbench-best-practices.mdx
URL: https://docs.fused.io/core-concepts/best-practices/workbench-best-practices

# Workbench best practices

_Tips & Tricks for making Fused Workbench work for you_

[Image: Workbench Overview]

Workbench is a web-IDE built to make working with Fused UDFs even faster! 

## Experimenting with UDFs, fast

In UDF Builder you have access to a Code Editor that runs your UDFs and outputs results directly on the Map View for you. As soon as you make changes they show up in Map View! 

### üí° Leverage all the UDF Best Practices

While this page is for Workbench, it builds on top of all the Best Practices that make your UDF fast & efficient. So if you haven't yet, take a look at our dedicated UDF Best Practices.

### Use `return` to quickly explore data

Your UDF will stop at the first `return` it sees, which you can use to your advantage to return an intermediate result and explore it directly on the map:

    In this example, we're using the Overture Maps Example UDF but not sure exactly what our `bounds` object looks like.

    The easiest way to check is simply to return it inside our UDF before any other logic:

### Format your code for more visibility

You can hit `Opt + Shift + F` (or `Alt + Shift + F` on Windows/Linux) to format your code with a smaller line-length. This comes in handy if you don't want to scroll left and right to read your code, at the expense of having a bit more up and down scrolling to do.

## üó∫Ô∏è Visualizing results

### Visual Debugging Techniques

- **Elevation as a debugging tool**: Use 3D elevation (`extruded: true`) to add an extra dimension to your analysis
- **Multi-dimensional visualization**: Combine opacity, color, and height to encode different aspects of your data
- **Color by data attributes**: Map colors to categorical attributes and height to quantitative metrics

### Optimizing Color Use for Analysis

- **Color categories for types**: Use distinct color schemes like `TealRose` to clearly distinguish between categories
- **Sequential colors for metrics**: Use color gradients for representing continuous values like ratios or densities
- **Diverging color schemes**: Highlight values above and below an interesting mid-point in quantitative data - the middle color is assigned to the critical value with contrasting colors on either end
- **Visibility first**: Choose colors that maintain visibility over your basemap (darker colors for light basemaps, lighter colors for dark basemaps)

For a complete reference of available color schemes, check out CARTO color schemes which are implemented in DeckGL and available in Fused. These include categorical schemes like `Bold` and `Pastel`, sequential schemes like `BluYl` and `Sunset`, and diverging schemes like `TealRose` and `Tropic`.

## Navigating Workbench

### Using Keyboard Shortcuts: Command Palette

Workbench has built-in keyboard shortcuts & quick navigation features: Hit `Cmd + K` (or `Ctrl + K` on Windows / Linux) to bring up Command Palette or use the search bar in the header for quick access:

[Image: Command Palette]

Without lifting your hands from the keyboard you can:
- Open a New UDF
- Search the Docs, directly in Workbench!
- See some of the most helpful Keyboard Shortcuts

You'll find a more extended list of Keyboard Shortcuts in the command palette.

[Image: Preferences - Keyboard Shortcuts]

### Quickly jump from UDF Builder to File Explorer

UDF Builder & File Explorer work well together, so we've made easy to jump from one to the other

- In UDF Builder, `Cmd + Click` on a `s3://...` path will open it directly in File Explorer
- In File Explorer double clicking on a file will prompt Fused to do its best at guessing which Catalog UDF to use to load this file in Code Editor

## Organising your work

### Renaming UDFs

You can easily rename UDFs by clicking on the UDF name in the header and hitting `Enter`

Your team can load your own UDFs by calling it with a team udf name so be sure to give it an explicit name!

### Using tags

You can add tags to your UDF in the Share page. This gives yet another way to find & search your UDFs. We recommend giving tags according to:
- Type of data the UDF works with (e.g. `satellite image`, `elevation model`, `population`)
- Type of analysis the UDF does (e.g. `zonal stats`, `building footprint extraction`, `flood mapping`)
- Type of file the UDF loads (e.g. `vector`, `raster`, `point cloud`)

### Using Canvas

You can use Canvas to organise your UDFs into different projects. This allows you to:
- Have multiple unrelated projects in Workbench
- Be able to share a set of UDFs at once with your team (by downloading a Canvas & sending it to your team mates)

## Troubleshooting

If things feel a bit off, for example your UDF output looks suspicious here are a few things you can do:
- Manually rerun the UDF with `Shift + Enter`
- Check how much RAM your tab is using (in Chrome can easily do so by hovering the tab). Sometimes too much data is brought in to your browser and while we do our best to manage it properly it can get out of hand. A good old tab refresh goes a long way

<ReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/refresh_edit3.mp4" width="100%" />

================================================================================

## Caching
Path: core-concepts/cache.mdx
URL: https://docs.fused.io/core-concepts/cache

# Caching

_This pages explains how caching makes Fused more responsive & some best practices for making the best use of it_

## Caching Basics

The goal of Fused is to make developing & running code faster for data scientists. This is done by using efficient file formats and making UDFs simple to run. On top of those, Fused relies heavily on caching to make recurring calls much faster.

At a high level, caching is storing the output of a function run with some input so we can directly access the result next time that function is called with the same input, rather than re-computing it to save time & processing cost.

[Image: Function + Input run]

_The first run of a [Function + Input] is processed, but the next time that same combination is called, the result is retrieved much faster_

As soon as either the function or the inputs change however, the output needs to be processed (as the result of this new combination has not been computed before)

[Image: Different Function + Input run]

Fused uses a few different types of cache, but they all work in this same manner

## Caching any Python function: `@fused.cache`

### Locally

Any Python function, either inside a UDF or even locally on your machine can be cached using the `@fused.cache` decorator around it:

```python  showLineNumbers
# This works locally on your machine

from datetime import datetime

@fused.cache(cache_max_age='30s')
def telling_time():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return current_time

telling_time()
```

[Image: Fused_cache_function_locally]

As seen in the debug logs, your cached data will be saved under `/tmp/cached_data/tmp/` locally.

Similar to how this works with `fused.run()`, you can overwrite `cache_max_age` when executing your function directly:

```python showLineNumbers
telling_time(cache_max_age="0s") # Overwrite cache duration to be 0s, i.e. no caching
```

### Inside a UDF

This also works inside a UDF by passing `@fused.cache` decorator around any function:

```python  showLineNumbers
@fused.udf
def udf():

    @fused.cache
    def load_data(i):
        # Do heavy processing here
        return pd.DataFrame()

    df_first = load_data(i=1)
    df_first_repeat = load_data(i=1)
    df_second = load_data(i=2)

    return pd.concat([df_first, df_first_repeat, df_second])
```

Under the hood:
- The first time Fused sees the function code and parameters, Fused runs the function and stores the return value in a cache.
    - This is what happens in our example above, line 10: `load_data(i=1)`
- The next time the function is called with the same parameters and code, Fused skips running the function and returns the cached value
    - Example above: line 11, `df_first_repeat` is the same call as `df_first` so the function is simply retrieved from cache, not computed
- As soon as the function _or_ the input changes, Fused re-computes the function
    - Example above: line 12 as `i=2`, which is different from the previous calls

**Implementation Details**

A function cached with `@fused.cache` is:
- Cached for 12h by default (can be changed with `cache_max_age`)
- Stored as pickle file on `mount/`

### Benchmark: With / without `@fused.cache`

Using `@fused.cache` is mostly helpful to cache functions that have long, repetitive calls like for example loading data from slow file formats.

Here are 2 simple UDFs to demonstrate the impact:
- `without_cache_loading_udf` -> Doesn't use cache
- `with_cache_loading_udf` -> Caches the loading of a CSV

```python  showLineNumbers
@fused.udf
def without_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    # @fused.cache
    def load_ais_data(ais_path: str):

        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

and the same:
```python  showLineNumbers
@fused.udf
def with_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    @fused.cache
    def load_ais_data(ais_path: str):

        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

Comparing the 2:

[Image: Caching benchmark]

### Best Practices: `@fused.cache`

Caching a local function or inside a UDF works best for:
- Loading data from slow formats (CSV, Shapefile)
- Repetitive operations that can take a long amount of processing

However, be wary of relying on `@fused.cache` to load very large (>10Gb) datasets as cache is only stored for a few hours by default and is over-written each time you change the cached function or inputs.

Look into ingesting your data in partitioned cloud native formats if you're working with large datasets.

The line between when to ingest your data or use `@fused.cache` to load data inside a UDF is a bit blurry. Check this section for more

### Example use cases

You can look at some real-world use cases in some of our Examples:
- Caching a STAC catalog request when fetching Sentinel 1 radar satellite image in our Dark Vessel Detection example
- Read about Jeff Faudi's use of `@fused.cache` in running an ML-inference model for aircraft detection. (See the public UDF for yourself)

## Caching a UDF

While `@fused.cache` allows you to cache functions locally or _inside_ UDFs, UDFs ran with `fused.run()` are cached by default on Fused server.

You can create a token for your UDF in Python by first saving your UDF to Fused server:

```python showLineNumbers
@fused.udf
def slow_caching_udf():

    time.sleep(5)
    
    return pd.DataFrame()

fused.run(slow_caching_udf)
```

We can demonstrate this caching with a UDF that has a `time.sleep(5)` in it. Running this same UDF twice:

[Image: Cached_fused_run_udf]

This means that UDFs that are repeatably called with `fused.run()` become much more responsive. Do remember once again that UDFs are recomputed each time either anything in the UDF function or the inputs change!

**Implementation Details**

Cached UDF are:
- Stored for 90d by default (see Python SDK for more details)
- Stored on S3
- You can overwrite the cache age by passing `cache_max_age` either when defining the UDF with `@fused.udf(cache_max_age)` or when running the UDF with `fused.run(udf, cache_max_age)`

Note that UDFs work similarly to regular Python functions, default arguments are evaluated when defining the function, not when calling it.

For example:
```python

@fused.udf
def abc(d: str = datetime.datetime.now().strftime('%Y-%m-%D')):
    print("with default", d)
```

and
```python
@fused.udf
def abc(d: str = None):

    d = datetime.datetime.now().strftime('%Y-%m-%D')
    print("with default", d)
```

Will both be cached similarly (i.e. calling either of these functions 2 times consecutively will return the cached results).

## Advanced

### Caching & `bounds`

Pass `bounds` to make the output unique to each Tile.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def fn(bounds):
        # convert bounds to tile
        common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
        zoom = common.estimate_zoom(bounds)
        tile = common.get_tiles(bounds, zoom=zoom)
        return tile

    return fn(bounds)
```

Note that this means that if you're running your Tile UDF in Workbench, every time you pan around on the map you will cache a new file

For this reason, it's recommend to keep cache for tasks that _aren't_ dependent on your `bounds` when possible, for example:

```python  showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    # convert bounds to tile
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    zoom = common.estimate_zoom(bounds)
    tile = common.get_tiles(bounds, zoom=zoom)

    # Loading of our slow data does not depend on bounds so can be cached even if we pan around
    gdf = loading_slow_geodataframe()
    gdf_in_bounds = gdf[gdf.geometry.within(tile.iloc[0].geometry)]

    return gdf_in_bounds
```

### Defining your cache lifetime: `cache_max_age`

You can define how long to keep your cache data for with `cache_max_age`. Valid time units include:
- Seconds (`s`)
- Minutes (`m`)
- Hours (`h`)
- Days (`d`)

Examples: `24h` (24 hours), `30m` (30 minutes), `10s` (10 seconds)

**Cache Behavior:** UDF executions are cached by default. To bypass caching and ensure fresh results, pass `cache_max_age="0s"` in your `fused.run()` call.

```python showLineNumbers
@fused.udf
def udf():

    @fused.cache(
        cache_max_age="24h" # Your cache will stay available for 24h
    )
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    return gdf
```

This also works with `@fused.udf()` & `fused.run()`:
```python showLineNumbers
@fused.udf(cache_max_age="24h") # This UDF will be cached for 24h after its initial run
def udf(path):

    gdf = gpd.read_file(path)

    return gdf
```

This UDF will be cached from the moment it's executed with `fused.run(udf)` for as long as is defined in `cache_max_age`:

```python showLineNumbers
fused.run(udf)
```

If you run `fused.run(udf)` again with no changes to `udf`, then for the next 24h `fused.run(udf)` will return a cached result. This is both faster & cheaper (saving on compute) while giving you control over how long to keep your cache for.

You can also overwrite the `cache_max_age` defined in `udf` when running your UDF:

```python showLineNumbers
fused.run(udf, cache_max_age="12h")
```

`udf` results will now only be cached for `12h`, even if `udf` was defined with a `cache_max_age` of `24h`:

The age of your cache is defined as follows:
- By default a UDF is cached for 90 days.
- If `@fused.udf(cache_max_age)` is defined, this new cache age overwrites the default.
- If `fused.run(udf, cache_max_age)` is passed, then this cache age takes priority over default & `@fused.udf(cache_max_age)`

### Resetting cache: `cache_reset`

Sometimes you might want to reset your cache, when for example:
- Running a UDF with unknown `cache_max_age`, and you want to make sure you're getting fresh results
- Having a `try / except` block to reset cache if your default UDF with cache fails.

You can easily do this by passing `cache_reset=True`:

```python showLineNumbers
fused.run(udf, cache_reset=True)
```

This also works in combination with `@fused.cache`:

```python showLineNumbers
@fused.cache(cache_reset=True)
def my_function():
    ...
    return gdf
```

Defining a UDF that simply returns the current time:
```python showLineNumbers

@fused.udf
def udf():
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

Running this a first time:
```python showLineNumbers
fused.run(udf)
```

Returns:
```bash
2025-06-06 10:00:00
```

Running this a second time:
```python showLineNumbers
fused.run(udf)
```

Returns a cached result:
```bash
Cached UDF result returned.
2025-06-06 10:00:00
```

This is because UDFs are cached by default even without passing any `cache_max_age` argument

We can break this cache by passing `cache_reset=True`:

```python showLineNumbers
fused.run(udf, cache_reset=True)
```

Returns:
```bash
2025-06-06 10:00:12
```

</details>

 showLineNumbers
@fused.cache(
    storage="local"
)
def local_function_load(data_path):
    ...
    return gdf

gdf = local_function_load()
```

    Read more about this in the Python SDK page on `@fused.cache`

================================================================================

## Download
Path: core-concepts/content-management/download.mdx
URL: https://docs.fused.io/core-concepts/content-management/download

# Download

Download remote files to the local system to make them available to UDFs across runs. Files are written to a disk shared across all UDFs in an organization.

## `fused.download`

Download any file to: `/mount/tmp/` which any other UDF can then access.

```python showLineNumbers
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    out_path = fused.download(url=url, file_path='out.zip')
    print(out_path)
```

The `download` function sets a lock to ensure the download happens only once, in case the UDF is called concurrently.

================================================================================

## Environment variables
Path: core-concepts/content-management/environment-variables.mdx
URL: https://docs.fused.io/core-concepts/content-management/environment-variables

Save constants to an `.env` file to make them available to UDFs as environment variables. You should use the secrets manager for sensitive information like API keys.

First, run a File UDF that sets variables in an `.env` file in the `/mnt/cache/` directory.

```py
@fused.udf
def udf():
    env_vars = """
    MY_ENV_VAR=123
    """

    # Path to .env file in disk file system
    env_file_path = '/mnt/cache/.env'

    # Write the environment variables to the .env file
    with open(env_file_path, 'w') as file:
        file.write(env_vars)
```

Now, any UDF can load the values from `.env` as environment variables with the `load_dotenv` and access them with os.getenv.

```py
@fused.udf
def udf():
    from dotenv import load_dotenv

    # Load environment variable
    env_file_path = '/mnt/cache/.env'
    load_dotenv(env_file_path, override=True)

    # Access environment variable
    print(f"Updated MY_ENV_VAR: ")
```

================================================================================

## File systems
Path: core-concepts/content-management/file-system.mdx
URL: https://docs.fused.io/core-concepts/content-management/file-system

Fused provides two file systems to make files accessible to all UDFs: an S3 bucket and a disk. Access is scoped at the organization level.

## `fd://` S3 bucket

Fused provisions a private S3 bucket namespace for your organization. It's ideal for large-scale, cloud-native, or globally accessible datasets, such as ingested tables, GeoTIFFs, and files that need to be read outside of Fused.

Use the File explorer to browse the bucket and see its full path.

[Image: file explorer]

Fused utility functions may reference it with the `fd://` alias.

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="fd://census/ca_bg_2022/",
).run_batch()
```

## `/mnt/cache` disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and `fused.download` write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system. For example, to list files in the directory:

```python showLineNumbers
@fused.udf
def udf():

    for each in os.listdir('/mnt/cache/'):
        print(each)
```

### Troubleshooting

If you encounter the following error, it means `/mnt/cache` is not yet configured for your environment. To resolve this issue, please contact the Fused team to enable it.

```text
Error: No such file or directory: '/mnt/cache/'
```

================================================================================

## GitHub Integration
Path: core-concepts/content-management/git.mdx
URL: https://docs.fused.io/core-concepts/content-management/git

Github integration allows you to:
- Commit your code to Fused public & community repositories for anyone to use!
- Saving & Versioning of UDFs
- Collaboration across teams (for Professional users)

## Connecting your own repositories

<Tag color="#3399ff">Professional and Enterprise</Tag> _Integrating private repositories is accessible to organizations with a Fused Professional & Enterprise subscription (see Plans)._

Fused Github integration plugs directly into your own repositories.

### Configuring Github integration

1. Create a new Github repository. There's no enforced repo structure because Fused scans the entire repo for UDFs, although the Public UDFs repo may serve as guideline.

2. Install the Fused GitHub app for your organization. Navigate to this GitHub URL, click "Configure", and select the GitHub organization that contains the repository.

[Image: install fused github]

3. Scope the app to the target repository. It's recommended to select only the specific repo.

[Image: install fused github 2]

4. Once the above is complete, please reach out to the Fused team to finish setup:
- Provide Fused team with your Github path & name: `some-organization/some-repo`. (Example: `fusedio/udfs`)

5. Confirm the integration is enabled by checking that repo UDFs appear under the "Team UDFs" tab in the UDF Catalog.

You can see all the repositories you have access to in the Versions Tab:

[Image: Github Versions Tab]

================================================================================

## Data Ingestion
Path: core-concepts/generic-data-ingestion.mdx
URL: https://docs.fused.io/core-concepts/generic-data-ingestion

# Data Ingestion

Fused supports ingesting various data types into your workflows.

See the comprehensive Geospatial Data Ingestion guide with detailed tutorials and examples.

### `ingest_nongeospatial`

Ingest dataframes without a spatial component using `fused.ingest_nongeospatial`.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = fused.ingest_nongeospatial(
    input=df,
    output="s3://sample-bucket/file.parquet",
).run_batch()
```

================================================================================

## üê≥ On-prem
Path: core-concepts/onprem.mdx
URL: https://docs.fused.io/core-concepts/onprem

Fused offers an on-prem version of the application in a Docker container. The container runs in your computing environment (such as AWS, GCP, or Azure) and your data stays under your control.

The container image is currently distributed via a private release. Email `info@fused.io` for access.

## Fused On-Prem Docker Installation Guide

[Image: On prem]

_Diagram of the System Architecture_

### 1. Install Docker

Follow these steps to install Docker on a bare-metal environment:

Step 1: Update System Packages

Ensure your system is up-to-date:
```bash
sudo apt update && sudo apt upgrade -y
```

Step 2: Start & Enable Docker
```bash
sudo apt install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable docker
sudo systemctl start docker
```

Step 3: Add Docker Permission to local user (after this command is run, the shell session must be restarted)
```bash
sudo usermod -G docker $(whoami)
```

Step 4: Configure Artifact Registry
```bash
gcloud auth configure-docker us-west1-docker.pkg.dev
```

### 2. Install Dependencies and Create Virtual Environment

Step 1: Install pip
```bash
sudo apt install python3-pip python3.11-venv
```

Step 2: Create virtual environment
```bash
python3 -m venv venv
```

Step 3: Activate virtual environment
```bash
source venv/bin/activate
```

Step 4: Install Fused and dependencies
```bash
pip install pandas ipython https://fused-magic.s3.us-west-2.amazonaws.com/fused-1.14.1.dev2%2B2c8d59a-py3-none-any.whl
```

### 3. Configure Fused on the Docker container

Run the following in a Python environment within the container to configure the on-prem profile. The Fused team will provide values specific to your account via secure communication.

```python showLineNumbers
# Run this locally - not in Workbench

fused.options.base_url = "***"
fused.options.auth.client_id = "***"
fused.options.auth.client_secret = "***"
fused.options.auth.audience = "***"
fused.options.auth.oauth_token_url = "***"
fused.options.auth.authorize_url = "***"

fused.options.save()
```

The code above only needs to be run once. After this is complete, Fused will use the local configuration for future batch jobs.

If Fused has already been configured for batch jobs, you may need to remove the local `~/.fused` directory before running the above code.

### 4. Authenticate an individual Fused user account

Step 1: Start a Python shell
```bash
python
```
Step 2: Obtain credentials URL

```python showLineNumbers
# Run this locally - not in Workbench

credentials = fused.api.NotebookCredentials()
credentials.url
```

Step 3:
Go to the credentials URL from the prior step in a web browser. Copy the code that is generated and paste into Python.
```python showLineNumbers
# Run this locally - not in Workbench
credentials.finalize(code="xxxxxxxxxxxxxxx")
```

### 5. Create Google Cloud service account key and add to Fused

Step 1:
In Google Cloud Console, go to `IAM & Admin > Service Accounts`. Select the service account you want to use, click on the three dots on the right, and select `Manage Keys`. Choose JSON and download the key.

Step 2:
Login to the Fused workbench environment settings. Click `Add new secret`. For name use `gcs_fused` and for value paste the contents of the JSON key file.

### 6. Run Fused API: Test UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

```python showLineNumbers
# Run this locally in notebook - not in Workbench
@fused.udf
def udf(datestr=0):

  loguru.logger.info(f'hello world ')
```

Step 2: Rename this UDF to "hello_world_udf" & Save

[Image: Hello World UDF]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF from Python

```python showLineNumbers
# Run this locally - not in Workbench

fused.api.FusedAPI()

my_udf = fused.load("hello_world_udf") # Make sure this is the same name as the UDF you saved
job = my_udf(arg_list=[1, 2])
fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1"
  ]
)

job_status = job.run_batch()
job_status.run_and_tail_output()
```

Optionally, to mount a filestore volume to the node that runs the job, add the following to the `additional_docker_args`. This assumes that filestore is mounted at `/mnt/cache` on the host machine.
```python showLineNumbers
# Run this locally - not in Workbench
additional_docker_args=["-v", "/mnt/cache:/mnt/cache"]
```

### 7. Run Fused API: Example with ETL Ingest UDF

Now that we've tested a simple UDF we can move to a more useful UDF

Step 1: Open Fused Workbench, create a "New UDF" and copy this UDF to Workbench:

You'll need a GCS Bucket to save this to, pass it to `bucket_name` in the UDF definition for now

```python  showLineNumbers
@fused.udf
def udf(datestr: str='2001-01-03', res:int=15, var='t2m', row_group_size:int=20_000, bucket_name:str):

  path_in=f'https://storage.googleapis.com/gcp-public-data-arco-era5/raw/date-variable-single_level//2m_temperature/surface.nc'
  path_out=f"gs:///data/era5/t2m/datestr=/0.parquet"

  if len(fused.api.list(path_out))>0:
    df = pd.DataFrame([])
    print("Already exists")
    return None

  def get_data(path_in, path_out):
    path = fused.download(path_in, path_in)
    xds = xarray.open_dataset(path)
    df = xds[var].to_dataframe().unstack(0)
    df.columns = df.columns.droplevel(0)
    df['hex'] = df.index.map(lambda x:h3.api.basic_int.latlng_to_cell(x[0],x[1],res))
    df = df.set_index('hex').sort_index()
    df.columns=[f'hour' for hr in range(24)]
    df['daily_min'] = df.iloc[:,:24].values.min(axis=1)
    df['daily_max'] = df.iloc[:,:24].values.max(axis=1)
    df['daily_mean'] = df.iloc[:,:24].values.mean(axis=1)
    return df

  df = get_data(path_in, path_out)

  memory_buffer = io.BytesIO()
  table = pa.Table.from_pandas(df)
  pq.write_table(table, memory_buffer, row_group_size=row_group_size, compression='zstd', write_statistics=True)
  memory_buffer.seek(0)

  gcs = gcsfs.GCSFileSystem(token=json.loads(fused.secrets['gcs_fused']))
  with gcs.open(path_out, "wb") as f:
    f.write(memory_buffer.getvalue())

  print(df.shape)
  return None
```

Step 2: Rename this UDF to "ETL_Ingest"

[Image: Ingest ETL in workbench]

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF

```python showLineNumbers
# Run this locally - not in Workbench

fused.api.FusedAPI()

udf = fused.load("ETL_ingest")
start_datestr='2020-02-01'; end_datestr='2020-03-01';
arg_list = pd.date_range(start=start_datestr, end=end_datestr).strftime('%Y-%m-%d').tolist()
job = udf(arg_list=arg_list)

fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1", "-v", "./.fused:/root/.fused"
  ]
)

job_status = job.run_batch()
job_status.run_and_tail_output()
```

## Commands

### `run-config`

`run-config` runs the user's jobs. The job configuration can be specified either on the command line, as a local file path, or as an S3/GCS path. In all cases the job configuration is loaded as JSON.

```
Options:
  --config-from-gcs FILE_NAME   Job step configuration, as a GCS path
  --config-from-s3 FILE_NAME    Job step configuration, as a S3 path
  --config-from-file FILE_NAME  Job step configuration, as a file name the
                                application can load (i.e. mounted within the
                                container)
  -c, --config JSON             Job configuration to run, as JSON
  --help                        Show this message and exit.
```

### `version`

Prints the container version and exits.

## Environment Variables

The on-prem container can be configured with the followin environment variables.

- `FUSED_AUTH_TOKEN`: Fused token for the licensed user or team. When using the FusedDockerAPI, this token is automatically retrieved.
- `FUSED_DATA_DIRECTORY`: The path to an existing directory to be used for storing temporary files. This can be the location a larger volume is mounted inside the container. Defaults to Python's temporary directory.
- `FUSED_GCP`: If "true", enable GCP specific features. Defaults to false.
- `FUSED_AWS`: If "true", enable AWS specific features. Defaults to false.
- `FUSED_AWS_REGION`: The current AWS region.
- `FUSED_LOG_MIN_LEVEL`: Only logs with this level of severity or higher will be emitted. Defaults to "DEBUG".
- `FUSED_LOG_SERIALIZE`: If "true", logs will be written in serialized, JSON form. Defaults to false.
- `FUSED_LOG_AWS_LOG_GROUP_NAME`: The CloudWatch Log Group to emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_LOG_AWS_LOG_STREAM_NAME`: The CloudWatch Log Stream to create and emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_PROCESS_CONCURRENCY`: The level of process concurrency to use. Defaults to the number of CPU cores.
- `FUSED_CREDENTIAL_PROVIDER`: Where to obtain AWS credentials from. One of "default" (default to ec2 on AWS, or none otherwise), "none", "ec2" (use the EC2 instance metadata), or "earthdata" (use EarthData credentials in `FUSED_EARTHDATALOGIN_USERNAME` and `FUSED_EARTHDATALOGIN_PASSWORD`).
- `FUSED_EARTHDATALOGIN_USERNAME`: Username when using earthdata credential provider, above.
- `FUSED_EARTHDATALOGIN_PASSWORD`: Password when using earthdata credential provider, above.
- `FUSED_IGNORE_ERRORS`: If "true", continue processing even if some computations throw errors. Defaults to false.
- `FUSED_DISK_SPACE_GB`: Maximum disk space available to the job, e.g. for temporary files on disk, in gigabytes.

## Connecting an encrypted S3 bucket

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket. The following steps are required to connect an encrypted S3 bucket:

- Configure KMS policy
```json
,
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

- Configure S3 bucket policy
```json
,
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3        "arn:aws:s3      ]
    }
  ]
}
```

================================================================================

## dependencies
Path: core-concepts/run-udfs/dependencies.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/dependencies

# Dependencies

To keep things simple, Fused maintains a single runtime image. This means that any UDF you run will be executed with these dependencies by default

## UDF Dependencies

The Python packages are listed below and can also be found in this public `.txt` file.

Get in touch to have a package added to the list of dependencies or to learn about private runtime images for your organization.

## [BETA] Install your own dependencies

The simplest way to add your own library is to run the dedicated Package Management Fused app. This app allows you to create a different environment and add any module you'd like

You need to make sure you have access to Fused Apps to be able to run this.

[Image: Beta package management app]

You'll then need to import the environment path in your UDF:

```python showLineNumbers
@fused.udf
def udf():

  sys.path.append(f"/mount/envs/demo_env/lib/python3.11/site-packages/")

  # Logic using your dedicated package
  return
```

================================================================================

## Run UDFs
Path: core-concepts/run-udfs/index.mdx
URL: https://docs.fused.io/core-concepts/run-udfs

# Running UDFs

There are 2 main ways to run UDFs:

1. Realtime - the default way for quick execution with small resource requirements
2. Batch Jobs - for longer execution time or higher resource requirements

## Documentation overview

<DocCardList className="DocCardList--no-description"/>

================================================================================

## Batch Jobs
Path: core-concepts/run-udfs/large_jobs.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/run_large

Some UDFs require more resources than a few Gb of RAM or take more than a few seconds to run. For these cases, you can run them as jobs instead.

### Defining Batch Jobs

Batch jobs are for running UDFs that:
- Take longer than 120s to run
- Require large resources (more than a few Gb of RAM)

To run these we use higher-latency instances than regular UDF runs, but with the ability to specify RAM, CPU count & storage depending on the needs.

This is useful for example when running ingestion jobs which can require a lot of RAM & storage

### A Simple UDF to demonstrate

We'll use the same UDF as in the running multiple UDFs section:

```python showLineNumbers
@fused.udf
def udf(val):

    return pd.DataFrame()
```

As mentioned in the Run UDFs section, to call it 1 time we can use `fused.run()`:

```python showLineNumbers
# Run this locally in notebook - not in Workbench (no need to do fused.run() in Workbench)
fused.run(udf, val=1)
```

## Running `job.run_batch()` from notebook

Running a UDF as a job from a notebook is done in 2 steps:
1. Creating a job with the UDF to run and passing input parameters to run the UDF over
2. Send the job to a remote instance and (optionally) defining the instance arguments

```python showLineNumbers
# Run this locally in notebook - not in Workbench
job = udf(arg_list=[0,1,2,3,4])
job.run_batch()
```

### Passing UDF parameters with `arg_list`

**Single Parameter**

As mentioned above to pass UDF arguments to a remote job, use `arg_list` to specify a `list` of inputs to run your job over:

```python showLineNumbers
# Run this locally in notebook - not in Workbench
job = udf(arg_list=[0,1,2,3,4])
job.run_batch()
```

**Multiple Parameters**

Currently `arg_list` only supports giving 1 input variables to each UDF. We can work around this by aggregating multiple variables into a `dict` and having a UDF take a `dict` as input:

```python showLineNumbers
@fused.udf
def udf(variables: dict = ):

    # Retrieving each variables from the dictionary
    val1 = variables['val1']
    val2 = variables['val2']

    # Some simple boilerplate logic
    output_value = int(val1)*int(val2)
    df = pd.DataFrame(data=)

    # Saving output to shared file location to access results later
    # `/mnt/cache` is the shared file location for all UDF runs & jobs
    df.to_csv(f"/mnt/cache/demo_multiple_arg_list_run/output_.csv")

    return df
```

  You do need to type the input variable for this to work. If we had defined `variables` without typing it as a `dict`:
  ```python showLineNumbers
  @fused.udf
  def udf(variables = ):
    # Notice the lack of `variables: dict = `
    ...
    return df
  ```
  our remote job run would have fail as Fused server has no way of knowing what to expect from `variables`

We can then call this UDF as a remote job by passing a list of dictionaries to `arg_list`:
```python showLineNumbers
# Run this locally in notebook - not in Workbench
job = udf(arg_list=[, ])
job.run_batch()
```

We can confirm this worked by viewing our results by browsing File Explorer:

[Image: Run remote multi arg]

### `run_batch` instance arguments

With `job.run_batch()` you also have access to a few other arguments to make your remote job fit your needs:

- `instance_type`: Decide which type of machine to run your job on (see below for which ones we support)
- `disk_size_gb`: The amount of disk space in Gb allocated to your instance (between `16` and `999` Gb)

For example if you want a job with 16 vCPUs, 64Gb of RAM and 100Gb of storage you can call:
```python showLineNumbers
# Run this locally in notebook - not in Workbench
job.run_batch(instance_type="m5.4xlarge", disk_size_gb=100)
```

### Currently supported `instance_type`

  Fused batch job `instance_type` support instances from both Amazon Web Services (AWS) and Google Cloud Platform (GCP).

  We provide standard aliases for those who don't know

  ```python showLineNumbers
  supported_instance_types = [
    "m5.large",
    "m5.xlarge",
    "m5.2xlarge",
    "m5.4xlarge",
    "m5.8xlarge",
    "m5.12xlarge",
    "m5.16xlarge",
    "r5.large",
    "r5.xlarge",
    "r5.2xlarge",
    "r5.4xlarge",
    "r5.8xlarge",
    "r5.12xlarge",
    "r5.16xlarge",
  ]
  ```

## Running a job from workbench

To run a job from workbench, you can simply add an `instance_type` parameter and run the UDF as usual from the udf builder page:

```python showLineNumbers
@fused.udf(instance_type='small')
def udf(name: str = "world"):

    df = pd.DataFrame()
    print(f"")
    return "Done"
```

[Image: Running a job from workbench]

  UDFs with `instance_type` paramter will not be run automatically as other UDFs. You will need to manually run them with `Shift+Enter` and a confirmation modal.

## Accessing job logs

While your job is running you can access monitor & manage it with the following:

```python showLineNumbers
# Run this locally in notebook - not in Workbench

# View the job status
job.status

# Follow the job logs
job.tail_logs()

# Get the job logs
job.print_logs()

# Cancel the job
job.cancel()
```

These logs can also be accessed:

    Running `job.run_batch()` in a notebook gives you a clickable link:

    [Image: run remote notebook]

    Under the "Jobs" tab, on the bottom left of Workbench:

    Each job leads to an email summary with logs upon completion:

    [Image: run remote email]

    A common use case for offline jobs is as a "pre-ingestion" process. You can find a real-life example of this in our dark vessel detection example

    Here all we're returning is a status information in a pandas dataframe, but the our data in unzipped, read and saved to S3:

    ```python showLineNumbers

    @fused.udf()
    def read_ais_from_noaa_udf(datestr='2023_03_29'):

        url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler//AIS_.zip'
        # This is our local mount file path,
        path=f'/mount/AIS_demo//'
        daily_ais_parquet = f'/.parquet'

        # Download ZIP file to mounted disk
        r=requests.get(url)
        if r.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
                with z.open(f'AIS_.csv') as f:
                    df = pd.read_csv(f)

                    # highlight-next-line
                    df.to_parquet(daily_ais_parquet)
            return pd.DataFrame()
        else:
            return pd.DataFrame(']})
    ```

    Data written to `/mount/` can be accessed by any other instance used by anyone on your team so it can be used by any other UDF you run after.

          You can use File Explorer to easily see your outputs! In this case of the above example typing `efs://AIS_.csv` (and replacing `datestr` with your date) will show the results directly in File Explorer!
    
</details>

### Jobs trade-offs

- Takes a few seconds to startup machine
- Can run as long as needed

## Example use cases

You can explore examples of how we're using `run_batch()` in some of our other examples:
- Scaling an ingestion job of AIS point data
- Ingesting a large amount of cropland data for zonal statistics.

================================================================================

## Realtime
Path: core-concepts/run-udfs/run.mdx
URL: https://docs.fused.io/core-concepts/run-udfs/run-small-udfs

UDFs really shine once you start calling them from anywhere. Realtime is the default way to run UDFs. You can run them in 2 main ways:
1.  `fused.run()` in Python. All you need is the `fused` Python package installed
    - Useful when wanting to run a UDF as part of another pipeline, inside another UDF or anywhere in Python / code.
2.  **HTTPS call** from *anywhere*
    - Useful when you want to call a UDF outside of Python. For example, receiving dataframes into Google Sheets or plotting points and images in a Felt map

### Defining Realtime UDF runs

Realtime UDF runs are defined as any job being:
- Less than 120s to execute
- Using less than a few Gb of RAM to run

These run in "real-time" with no start-up time, so are quick to run, but with limited resources and time-out if taking too long.

## `fused.run()`

`fused.run()` is the simplest & most common way to execute a UDF from any Python script or notebook.

The simplest way to call a public UDF is using a public UDF name and calling it as: `UDF_` + name. Let's take this UDF that returns the location of the Eiffel Tower in a `GeoDataFrame` as an example:

```python showLineNumbers
fused.run("UDF_Single_point_Eiffel_Tower")
```

[Image: Simple UDF fused.run() returning a geodataframe]

There are a few other ways to run a UDF:

- By name from your account
- By public UDF name
- Using a token
- Using a `udf` object
- From Github URL
- From git commit hash (most recommended for teams)

### _Name_ (from your account)

_When to use: When calling a UDF you made, from your own account._

You can call any UDFs you have made simply by referencing it by name (given when you save a UDF).

(Note: This requires authentication)

[Image: Hello World UDF]

This UDF can then be run in a notebook locally (granted that you have authenticated):

```python showLineNumbers
fused.run("Hello_World_bbox")
```

[Image: Running Hello World UDF]

### _Name_ (from your teammate's account)

_When to use: When calling a UDF someone on your team made, from your own account._

Similarly, you can reference by name and run any UDFs under your teammates' accounts. Simply prefix the UDF name with the person's email address, separated by a `/`.

```python showLineNumbers
fused.run("teammate@fused.io/Hello_World_bbox")
```

Note that both your and your teammate's accounts must belong to the same organization.

### _Public UDF Name_

_When to run: Whenever you want to run a public UDF for free from anywhere_

Any UDF saved in the public UDF repo can be run for free.

Reference them by prefixing their name with `UDF_`. For example, the public UDF `Get_Isochrone` is run with `UDF_Get_Isochrone`:

```python showLineNumbers
fused.run('UDF_Get_Isochrone')
```

### _Token_

_When to use: Whenever you want someone to be able to execute a UDF but might not want to share the code with them._

You can get the token from a UDF either in Workbench (Save your UDF then click "Share") or by returning the token in Python.

Here's a toy UDF that we want others to be able to run, but we don't want them to see the code:

```python showLineNumbers
# This example is from a notebook

@fused.udf()
def my_super_duper_private_udf(my_udf_input):

    # This code is so private I don't want anyone to be able to read it
    return pd.DataFrame()
```

We then need to save this UDF to Fused server to make it accessible from anywhere.

```python showLineNumbers
# This example is from a notebook
my_super_duper_private_udf.to_fused()
```

`my_udf.to_fused()` saves your UDF to your personal user UDFs. These are private to you and your team. You can create a token that anyone (even outside your team) can use to run your UDF but by default these UDFs are private.

We can create a token for this `my_super_duper_private_udf` and share it:

    `fused.submit(udf)` runs all the UDF calls in parallel, making it a helpful tool to run multiple UDFs all at once.

    We can demonstrate this by adding a simple `time.sleep(1)` in our original UDF:
    ```python  showLineNumbers
    @fused.udf
    def udf(val):

        time.sleep(1)
        return pd.DataFrame()
    ```

    In a notebook, we can time how long each cell takes to execute with the `%%time` magic command

    ```python showLineNumbers
    # In a jupyter notebook
    %%time
    fused.run(udf, val=1)
    ```

    [Image: Singe run]

    This takes 2s: A few ms of overhead to send the UDF to Fused server & run + 1s of `time.sleep(1)`

    Now using `fused.submit()` to run this over 50 UDFs:

    [Image: 30 runs]

    This takes a few more seconds, but not 100s. `fused.submit()` is a helpful way to scale a single UDF to many inputs in a timely manner.

</details>

### Wall time vs CPU time

There are 2 different runtime calculations for `fused.submit()` jobs:

**Wall time**: Total actual time taken to run the jobs:

```python showLineNumbers
results.total_time()
```

The exact calculation is:
```python showLineNumbers
round(results.total_time().seconds + results.total_time().microseconds / 10e6, 2)
```

```bash
# Total time you had to wait
>>> 11.05
```

**CPU time**: Sum of all the CPU time taken by the jobs

This is the time that counts towards your bill.

```python showLineNumbers
round(sum([t.seconds for t in results.times()]) + sum([t.microseconds for t in results.times()]) / 10e6, 2)
```

```bash
# Total CPU time
>>> 21.78
```

### Example use cases

`fused.submit()` is used in many places across our docs, here are some examples:

- üìà Processing 20Tb of data in minutes: Making a Climate Dashboard of 20 years of data
- ‚õ¥Ô∏è Retrieving 30 days of AIS boat transponder data around the United States to detect illegal fishing
- üõ∞Ô∏è Retrieving all of Maxar's Open Data STAC Catalogs across every events they have imagery for.

üí° Check the Best Practices for more on when to use `submit()` and when to use other methods.

## HTTPS requests

You can call Saved UDFs via HTTPs calls, effecitvaly turning your data in an API!

The same can be done with the Fused Python SDK.

#### Shared token

When you save a UDF for the first time, it by default creates a shared token for you, something like:

```
fsh_**********h4t
```

This is a unique token that is used to identify your UDF and call it via HTTPS requests

Here is what a UDF endpoint looks like:

```
https://www.fused.io/server/v1/realtime-shared/******/run/file?format=png
```

Manage your account's shared tokens in here.

### Passing parameters

You can also pass parameters to your UDF via the URL by adding query parameters (`&param=value`) to the URL.

For example a UDF that takes `lat` and `lon` as parameters:

```python showLineNumbers
@fused.udf
def udf(lat, lon):
    return pd.DataFrame()
```

Could be called with:

```
https://www.fused.io/...&lat=37.7749&lon=-121.4194
```

### Tiling 

You can integrate your UDF as a vector or raster tile server y adding `tiles` path parameter, followed by templated `///` path parameters.

You need to make sure:
- Your UDF is set to Tile mode
- You properly pass `///` instead of default value
- Depending on your server type, you might need to add the `format` parameter. For example QGIS requires you to use `format=mvt` for vector tile servers

```
https://www.fused.io/server/v1/realtime-shared/******/run/tiles///?format=png
```

See our integration guides for examples.

### Private token

Calling UDFs with Bearer authentication requires an account's private token. The URL structure to run UDFs with the private token varies slightly, as the URL specifies the UDF's name and the owner's user account.

```bash
curl -XGET "https://app.fused.io/server/v1/realtime/fused/api/v1/run/udf/saved/user@fused.io/caltrain_live_location?format=png" -H "Authorization: Bearer $ACCESS_TOKEN"
```

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

### Serialization format

The `format` parameter defines the output data type for the serialized result of the UDF.

- The supported types for vector tables are `parquet`, `geojson`, `json`, `feather`, `csv`, `mvt`, `html`, `excel`, and `xml`.
- For raster array: `png`, `gif`, `jpg`, `jpeg`, `webp`, `tif`, and `tiff`.
- For simple Python objects like strings: `json`, `html`

```
https://www.fused.io/server/v1/realtime-shared/****/run/file?format=png
```

Read how to structure HTTPS endpoints to call the UDF as a Map Tile & File.

### Calling UDFs without Python SDK 

You can also run a whole UDF directly in the HTTP request without needing Python when making the call

You first do need to retrieve your bearer token (this requires Python):
```python showLineNumbers
# Figure out your bearer token:
from fused._auth import AUTHORIZATION
AUTHORIZATION.credentials.access_token
```

This returns something like:
```bash
'eyJ4K9pL2Mx...'
```

Next we need to do a POST request to the Fused server endpoint: `'https://www.fused.io/server/v1/realtime/basic-tier/api/v1/run/udf'`

You'll need the client_id:

**Without an environment (Basic Tier)**

Simply use `basic-tier` as your client_id name.

**With environment**

1. In Workbench, go to "Preferences"
2. Write down the "Kernel" name you see displayed, in this case `fused`:

[Image: Finding your kernel name in Workbench]

Here is an example call with a simple "Hello World" UDF using the `basic-tier` account:

```bash
curl -X 'POST' \
  'https://www.fused.io/server/v1/realtime/basic-tier/api/v1/run/udf' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer <BEARER_TOKEN_GOES_HERE>' \
  -H 'Content-Type: application/json' \
  -d ',\"metadata\":,\"code\":\"@fused.udf\\ndef udf(name: str = \\\"world\\\"):\\n    import pandas as pd\\n\\n    return pd.DataFrame()\\n\",\"headers\":[]}}","dtype_in":"json","format":"geojson,png","cache":true}'
```

This:
- Runs the following UDF directly being passing it through the HTTP request:
```python showLineNumbers
@fused.udf
def udf(name: str = "world"):

    return pd.DataFrame()
```
- On the `basic-tier` account (or whatever account you are using)
If you're using a subscribed account, you'll need to adjust the `basic-tier` to fit your environment:

```bash
https://www.fused.io/server/v1/realtime/<your-client-id>/api/v1/run/udf
```

## Caching responses

If a UDF's cache is enabled, its endpoints cache outputs for each combination of code and parameters. The first call runs and caches the UDF, and subsequent calls return cached data.

================================================================================

## Why UDFs
Path: core-concepts/why.mdx
URL: https://docs.fused.io/core-concepts/why

At Fused, our mission is to help get things done, fast. We want every team to be able to get from **Analytics to Action** as quickly as they can. 

We also deeply believe AI is changing the way we work. So here's Notebook LLM telling you all about Fused in 5min:

## Our core believes

We believe every Analytics team should have the tools to:
- üåç Answer the big picture problems first;
- üí° Iterate on their analysis when new data & algorithms becomes available;
- üèÉ Ship a first version rather than getting it perfect;
- üìä Visualize & report their work to anyone in their team.

## User Defined Functions

A lot of the tools Analytics teams have today slow the process down:
- Python dependency management gets in the way of getting work done.
- A lot of the scientific Python tooling focused more on getting the result down to 10 decimals places rather than answering the big picture

That's why we built Fused around User Defined Functions, UDFs. 

[Image: udf pipeline]

UDFs are the DNA of analytics. They are Python functions that can be called from anywhere:
- üêç No environment setup: Just start writing Python immediately.
- üîó Shareable as HTTPS endpoints in 2 clicks: Ship your work to the rest of the team
- üîÑ Iterable: Edit your code, Save, and see the results downstream immediately.
- üöÄ Scales with your hardware requirements: From running a subset of data to analyzing the entire world.

## UDFs are the DNA of analytics

Making every process of your Analytics a UDF makes it faster:

- **Data needs to be ingested constantly**: UDFs can be edited as datasets change & evolve. They get updated when you save them. 
- **New algorithms come and go**: UDFs allow you to iterate on existing data and swap out just what you need.
- **Reporting & Visualization evolve**: UDFs can take your analysis and render it in dynamic ways.

## From your laptop to the World

Look, we know that many Analytics project start in a notebook on a laptop. 

- üíª Start by running UDFs locally, then in 2 lines of code scale to datasets the size of the world
- üåç UDFs can be called from anywhere: From a notebook, a frontend application or integration platforms
- üîÄ Work locally or in Workbench, our browser based IDE interchangeably  

## Efficiently Scaling

Because Fused is built 
- ‚òÅÔ∏è Serverless computing: Only pay for the processing you actually use
- ‚ö°Ô∏è Caching makes recurring calls faster & cheaper

## Get started using UDFs right now

Check out:
- ‚ö°Ô∏è The Quickstart guide: Learn how to use UDFs in 5 minutes
- üìö UDF Core Concepts: Everything you need to know about building & using UDFs
- üéì Our Examples: Real world examples of how to use UDFs

================================================================================

## Write UDFs
Path: core-concepts/write.mdx
URL: https://docs.fused.io/core-concepts/write

[Image: udf anatomy]

Follow these steps to write a User Defined Function (UDF).

- Decorate a function with `@fused.udf`
- Declare the function logic
- Optionally cache parts of the function
- Set typed parameters to dynamically run based on inputs
- Import utility modules to keep your code organized
- Return a vector table or raster
- Save the UDF

## `@fused.udf` decorator

First decorate a Python function with `@fused.udf` to tell Fused to treat it as a UDF.

## Function declaration

Next, structure the UDF's code. Declare import statements within the function body, express operations to load and transform data, and define a return statement. This UDF is called `udf` and returns a `pd.DataFrame` object.

```python showLineNumbers
@fused.udf # <- Fused decorator
# highlight-start
def udf(name: str = "Fused"): # <- Function declaration

    return pd.DataFrame(!']})
# highlight-end
```
The UDF Builder in Workbench imports the `fused` module automatically. To write UDFs outside of Workbench, install the Fused Python SDK with `pip install fused` and import it with `import fused`.

Placing import statements within a UDF function body (known as "local imports") is not a common Python practice, but there are specific reasons to do this when constructing UDFs. UDFs are distributed to servers as a self-contained units, and each unit needs to import all modules it needs for its execution. UDFs may be executed across many servers (10s, 100s, 1000s), and any time lost to importing unused modules will be multiplied.

An exception to this convention is for modules used for function annotation, which need to be imported outside of the function being annotated.

## `@fused.cache` decorator

Use the @fused.cache decorator to persist a function's output across runs so UDFs run faster.

```python showLineNumbers
@fused.udf # <- Fused decorator
def udf(bounds: fused.types.Bounds = None, name: str = "Fused"):

    # highlight-start
    @fused.cache # <- Cache decorator
    def structure_output(name):
        return pd.DataFrame(!']})
    # highlight-end

    df = structure_output(name)
    return df
```

## Typed parameters

UDFs resolve input parameters to the types specified in their function annotations.
This example shows the `bounds` parameter typed as `fused.types.Bounds`
and `name` as a string.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, # <- Typed parameters
    name: str = "Fused"
):
```

To write UDFs that run successfully as both `File` and `Tile`, set `bounds` as the first parameter, with `None` as its default value. This enables the UDF to be invoked successfully both as `File` (when `bounds` isn't passed) and as `Tile`. For example:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    ...
    return ...
```

### Supported types

Fused supports the native Python types `int`, `float`, `bool`, `list`, `dict`, and `list`. Parameters without a specified type are handled as strings by default.

The UDF Builder runs the UDF as a Map Tile if the first parameter is typed as `fused.types.Bounds`.

### `pd.DataFrame` as JSON

Pass tables and geometries as serialized UDF parameters in HTTPS calls. Serialized JSON and GeoJSON parameters can be casted as a `pd.DataFrame` or `gpd.GeoDataFrame`. Note that while Fused requires import statements to be declared within the UDF signature, libraries used for typing must be imported at the top of the file.

```python showLineNumbers

@fused.udf
def udf(
    gdf: gpd.GeoDataFrame = None,
    df: pd.DataFrame = None
):
```

## Reserved parameters

When running a UDF with `fused.run`, it's possible to specify the map tile Fused will use to structure the `bounds` object by using the following reserved parameters.

### With `x`, `y`, `z` parameters

```python showLineNumbers
fused.run("UDF_Overture_Maps_Example", x=5241, y=12662, z=15)
```

### Passing a `GeoDataFrame`
```python showLineNumbers

bounds = gpd.GeoDataFrame.from_features(,"geometry":,"id":1}]})
fused.run("UDF_Overture_Maps_Example", bounds=bounds)
```

### Passing a bounding box list

You can also pass a list of 4 points representing `[min_x, min_y, max_x, max_y]`

```python showLineNumbers
fused.run('UDF_Overture_Maps_Example', bounds=[-122.349, 37.781, -122.341, 37.818])
```

### Import functions from other UDFs

UDFs can import functions from other UDFs with `fused.load` in the UDFs GitHub repo or private GitHub repos. Here the commit SHA `05ba2ab` pins the UDF to specific commit for version control.

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
```

## `return` object

UDFs can return the following objects:

- Tables: `pd.DataFrame`, `pd.Series`, `gpd.GeoDataFrame`,  `gpd.GeoSeries`, and `shapely geometry`.
- Arrays: `numpy.ndarray`, `xarray.Dataset`, `xarray.DataArray`, and `io.BytesIO`. Fused Workbench only supports the rendering of `uint8` arrays. Rasters without spatial metadata should indicate their tile bounds.
- Simple Python objects: `str`, `int`, `float`, `bool`.
- Dictionaries: `dict`. Useful to return dictionaries of raster numpy array for example.

## Save UDFs

UDFs exported from the UDF Builder or saved locally are formatted as a `.zip` file containing associated files with the UDFs code, `utils` Module, metadata, and `README.md`.

```
‚îî‚îÄ‚îÄ Sample_UDF
    ‚îú‚îÄ‚îÄ README.MD       # Description and metadata
    ‚îú‚îÄ‚îÄ Sample_UDF.py   # UDF code
    ‚îú‚îÄ‚îÄ meta.json       # Fused metadata
    ‚îî‚îÄ‚îÄ utils.py        # `utils` Module
```

### In Python: `.to_fused()`

When outside of Workbench, save UDF to your local filesystem with `my_udf.to_directory('Sample_UDF')` and to the Fused cloud with `my_udf.to_fused()`.

This will allow you to access your UDF using a token, from a Github commit or directly importing it in Workbench from the Github URL

### In Workbench: Saving through Github

You can also save your UDFs directly through GitHub as personal, team or community UDF. Check out the Contribute to Fused to see more.

## Update tags and metadata

Modify the UDF's metadata to manage custom tags that persist across the local filesystem, the Fused Cloud, and your team's GitHub repo.

```python showLineNumbers
# Assumging my_udf was loaded or created above
my_udf.metadata['my_company:tags']=['tag_1', 'tag_2']

# Push to Fused
my_udf.to_fused()

# You can reload your UDF and see the updated metadata
fused.load('my_udf').metadata
```

## Debug UDFs

#### UDF builder

A common approach to debug UDFs is to show intermediate results in the UDF Builder runtime panel with `print` statements.

#### HTTPS requests

When using HTTPS requests, any error messages are included in the `X-Fused-Metadata` response header. These messages can be used to debug. To inspect the header on a browser, open the Developer Tools network tab.

[Image: network]

================================================================================

# GEOSPATIAL WITH FUSED

## best-practices
Path: tutorials/Geospatial with Fused/best-practices.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/best-practices

# Best Practices

### Explore data in "Map View"

While you might be tempted to explore a specific row of a `GeoDataFrame` by filtering it and printing it, sometimes it is faster to just click on it in Map View. Once selected, use the tooltip copy icon to copy the object as JSON, which you can then inspect elsewhere.

### Tilt Map view to explore 3D datasets

Map View gives you a top-level view of the world by default. But hold `Cmd` (or `Ctrl` on Windows / Mac) to tilt the view!

[Image: Tilted View]

üèòÔ∏è This can be really helpful to explore 3D datasets like in this DSM Zonal Stats UDF.

You can reset the view by running the "Reset 3D view to top-down" shortcut from Command Palette
:::

================================================================================

## Single / Tile Map UDFs
Path: tutorials/Geospatial with Fused/filetile.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/filetile

# Working with Spatial Data: Single vs Tile

There are 2 main ways to work with spatial data in Fused:

## Single UDF

Single UDFs are the default behavior in Fused. The UDF is run 1 time with the current viewport bounds.

[Image: Single UDF Mode]

Characteristics:
- 1 Single UDF is called on the entire Map Viewport
- Panning Map does NOT re-run the UDF
- "Results" Tab shows the output of the unique UDF called

### Single HTTPS Call

A Single UDF is called like any other UDF via a HTTPS request:

```bash
https://www.fused.io/.../run/file?format=csv
```

Or via the Fused Python SDK:

```python showLineNumbers
fused.run("single_udf")
```

There are 2 types of Spatial Single UDFs:

### Single (Viewport)

Will prioritize the current MAP VIEWPORT bounds, regardless of the `bounds` parameter.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = [-74.38,40.32,-73.31,41.29]):
    print(bounds) # This will be the current MAP VIEWPORT bounds, regardless of the `bounds` parameter
    return bounds
```

### Single (Parameter)

Will use the `bounds` parameter, regardless of the current MAP VIEWPORT bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = [-74.38,40.32,-73.31,41.29]):
    print(bounds) # This will be the `bounds` parameter, regardless of the current MAP VIEWPORT bounds
    return bounds
```

### Example UDF

- Get Isochrone
    - Returns an isochrone (how far a person cal walk / drive) polygon for a given point

## Tile UDF

Tile UDFs tile the users viewport into a grid of smaller tiles and running a UDF for each.

[Image: Tile UDF Mode]

Characteristics:
- Multiple Tile UDFs are called to cover the current Map Viewport
- Panning Map re-runs the UDF for each tile
- "Results" Tab shows the output of the _latest_ tile called

### Tile HTTPS Call

A Tile UDF is called by passing Web mercator XYZ tiles through the HTTPS request

**Two ways to call the HTTPS endpoint:**

**1. Dynamic tile server (for map applications)**

```bash
https://www.fused.io/.../run/tiles///?format=png
```
Use this template URL in mapping libraries `//` gets automatically replaced with tile coordinates as users pan and zoom.

Example use cases:
- QGIS
- Felt
- DeckGL
- Mapbox

**2. Specific tile call**
```bash
# Calls a single tile - San Francisco center at zoom 13
https://www.fused.io/.../run/tiles/13/1316/3169?format=png
```
Use this to fetch data for one specific tile location.

Or via the Fused Python SDK:

```python showLineNumbers
# With bounds parameter
fused.run("tile_udf", bounds=bounds)
```

```python showLineNumbers
# With x, y, z parameters
fused.run("tile_udf", x=1, y=2, z=3)
```

### Example UDF

- Overture Maps Example
    - Manipulate _all_ the buildings from the Overture Maps dataset by leveraging Fused ingestion
- Landsat Tile Example
    - Returns NDVI tile computed on the fly from Landsat data

## Comparing Single and Tile UDFs

| Feature | Single UDF | Tile UDF |
|---------|------------|-----------|
| UDF Calls | 1 | Multiple |
| Parameters | `bounds` (Optional) | `bounds` or `x`, `y`, `z` |
| Map rendering | Static | Dynamic |
| HTTP Call | Single Call | Tile Server calls or Single Call (specific tile) |
| Runtime Tab display | Unique output of the UDF called | Output of the _latest_ tile called |
| Use Case | Small static data | Calling large dataset through tile server |

## `bounds`

`bounds` is simply a list of 4 coordinates representing the bounds of a geometry. The 4 coordinates represent `[xmin, ymin, xmax, ymax]` of the bounds.

In Fused it is defined with the `fused.types.Bounds` type:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    print(bounds)

>>> [-1.52244399, 48.62747869, -1.50004107, 48.64359255]
```

### `bounds` to `GeoDataFrame`

Converting `bounds` (list of 4 coordinates) to a `GeoDataFrame`:

</Tabs>

### Legacy types

Legacy types that might still appear in older UDFs.

#### [Legacy] `fused.types.Tile`

This is a geopandas.GeoDataFrame with `x`, `y`, `z`, and `geometry` columns.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Tile=None):
    print(bounds)

>>>      x    y   z                                           geometry
>>> 0  327  790  11  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `fused.types.TileGDF`

This behaves the same as `fused.types.Tile`.

#### [Legacy] `fused.types.ViewportGDF`

This is a geopandas.geodataframe.GeoDataFrame with a `geometry` column corresponding to the Polygon geometry of the current viewport in the Map.
```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.ViewportGDF=None):
    print(bbox)
    return bbox

>>>  geometry
>>>  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `bbox` object

UDFs defined using the legacy keyword `bbox` are automatically now mapped to `bounds`. Please update your code to use `bounds` directly as this alias will be removed in a future release.

================================================================================

## gee_bigquery
Path: tutorials/Geospatial with Fused/gee_bigquery.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/gee_bigquery

# Google Earth Engine & BigQuery

## Google Earth Engine

Fused interfaces Google Earth Engine with the Python `earthengine-api` library. This example shows how to load data from GEE datasets into Fused UDFs and read it with xarray.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/gee_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from Google Earth Engine

Create a UDF to load data from a GEE ImageCollection and open it with xarray. Authenticate by passing the key file path to `ee.ServiceAccountCredentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, n=10):

    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")

    # Authenticate GEE
    key_path = '/mnt/cache/gee_creds.json'
    credentials = ee.ServiceAccountCredentials("fused-account@fused-gee.iam.gserviceaccount.com", key_path)
    ee.Initialize(opt_url="https://earthengine-highvolume.googleapis.com", credentials=credentials)

    # Generate GEE bounding box for spatial filter
    geom = ee.Geometry.Rectangle(*bounds.total_bounds)
    scale = 1 / 2 ** max(0, bounds.z[0])  # A larger scale will increase your resolution per z but slow down the loading

    # Load data from a GEE ImageCollection
    ic = ee.ImageCollection("MODIS/061/MOD13A2").filter(
        ee.Filter.date("2023-01-01", "2023-06-01")
    )

    # Open with xarray (the `xee` package must be present for engine="ee" to work)
    ds = xarray.open_dataset(ic, engine="ee", geometry=geom, scale=scale).isel(time=0)

    # Transform image color with a utility function
    arr = common.arr_to_plasma(ds["NDVI"].values.squeeze().T, min_max=(0, 8000))
    return arr

```

## BigQuery - Option 1: Credentials file

Fused integrates with Google BigQuery with the Python `bigquery` library.

### 1. Authenticate with a Google Service Account

Create a UDF to set your Google Service Account credentials in your Fused runtime disk in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    # Google Key as JSON
    data = 

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/bq_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

### 2. Load data from BigQuery

Create a UDF to perform a query on a BigQuery dataset and return the results as a DataFrame or GeoDataFrame. Authenticate by passing the key file path to `service_account.Credentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, geography_column=None):
    from google.cloud import bigquery
    from google.oauth2 import service_account

    # This UDF will only work on runtime with mounted EFS
    key_path = "/mnt/cache/bq_creds.json"

    # Authenticate BigQuery
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )

    # Create a BigQuery client
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # Structure spatial query
    query = f"""
        SELECT * FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`
        LIMIT 10
    """

    if geography_column:
        return client.query(query).to_geodataframe(geography_column=geography_column)
    else:
        return client.query(query).to_dataframe()
```

## Big Query - Option 2: Secrets

If you already have a `gcs_secret` in Fused secrets, you can use it to access your GCP secrets. Otherwise you can simply create new secrets in the Fused secrets manager with:
- `GS_ACCESS_KEY_ID`
- `GS_SECRET_ACCESS_KEY`

You can for example use this to access the Github Activity Data

```python showLineNumbers
@fused.udf
def udf(repo_name: str = "athasdev/athas"):

    # This is not required if your account already has the `gcs_secret` in Fused secrets
    os.environ['GS_ACCESS_KEY_ID'] = fused.secrets["GS_ACCESS_KEY_ID"]
    os.environ['GS_SECRET_ACCESS_KEY'] = fused.secrets["GS_SECRET_ACCESS_KEY"]
    
    from google.cloud import bigquery
    # Initialize BigQuery client
    client = bigquery.Client()
    
    # Get total stars 
    total_query = f"""
        SELECT 
            repo.name as repository,
            COUNT(*) as total_stars
        FROM `githubarchive.day.202508*`
        WHERE type = 'WatchEvent' 
 --           AND repo.name = ''
        GROUP BY repository
    """
    
    total_query = f""" SELECT  * FROM `githubarchive.day.202508*` limit 10"""
    
    # Run the query
    query_job = client.query(total_query)
    
    # Convert to pandas DataFrame
    total_df = query_job.to_dataframe()
     
    return total_df
```

================================================================================

## Geospatial FAQ
Path: tutorials/Geospatial with Fused/geo-faq.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geo-faq

Python is the go-to language for spatial data science. Although spatial SQL joins and transformations can be efficiently performed using PostGIS in an external database, you may eventually need to convert that data to Pandas and NumPy for further processing and analysis, especially for detailed operations on raster arrays. Additionally, you can run SQL directly on Fused using Python libraries like DuckDB, combining the strengths of both approaches.

It enables efficient reading of large datasets by strategically partitioning GeoParquet files. Fused's GeoParquet format includes metadata that allows for spatial filtering of any dataset, loading only the chunks relevant to a specific area of interest. This approach reduces memory usage and allows you to work with any size dataset with just Python.

Use cases like creating chips may call for running a UDF across a set of tiles that fall within a given geometry. This can be done by creating a list of tiles with the mercantile library then calling the UDF in parallel.

```python showLineNumbers

bounds = [32.4203, -14.0933, 34.6186, -12.42826]

tile_list = list(mercantile.tiles(*bounds,zooms=[15]))
```

You can use the App Builder create an app that loads the UDF's data then create a shareable link.

</details>

================================================================================

## Geospatial Data Ingestion
Path: tutorials/Geospatial with Fused/geospatial-data-ingestion.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geospatial-data-ingestion

# Geospatial Data Ingestion

Explore how to efficiently ingest and process various types of geospatial data using Fused.

Read here for generic data ingestion for ingesting data without a spatial component.

<DocCardList className="DocCardList--no-description"/>

================================================================================

## Geospatial File Formats
Path: tutorials/Geospatial with Fused/geospatial-data-ingestion/geospatial_file_formats.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geospatial-data-ingestion/geospatial-file-formats

# Geospatial File Formats

_This page specifies which File Formats for both raster & vector data we prefer working with at Fused, and why_

## What this page is about

Fused works with any file formats you might have, as all UDFs are running pure Python. This means you can use any file formats you want to process your data.
That being said the goal of Fused is to significantly speed up the workflow for data scientists, by leveraging modern cloud compute infrastructure and simplify it.

Some formats like Shapefile, CSV, JSON, while incredibly versatile, aren't the most appropriate for large datasets (even above a few Gb) and are slow to read / write (we consider anything above 10s of seconds to read to be extremely slow).

Take a look at our benchmark to see a comparison between loading a CSV, GeoParquet & Fused-partitioned GeoParquet to see a concrete example of this

To make the most out of Fused, we recommend ingesting your data into the following file formats:

### For rasters (images): Cloud Optimized GeoTIFF

For images (like satellite images) we recommend using **Cloud Optimized GeoTIFFs** (COGs). To paraphrase the Cloud Native Geo guide on them:

> Cloud-Optimized GeoTIFF (COG), a raster format, is a variant of the TIFF image format that specifies a particular layout of internal data in the GeoTIFF specification to allow for optimized (subsetted or aggregated) access over a network for display or data reading

    Fused does not (yet) have a build-in tool to ingest raster data. We suggest you create COGs yourself, for example by using `gdal`'s built-in options or `rio-cogeo`

Cloud Optimized GeoTIFFs have multiple different features making them particularly interesting for cloud native applications, namely:
- **Tiling**: Images are split into smaller tiles that can be individually accessed, making getting only parts of data a lot faster.
- **Overviews**: Pre-rendered images of lower zoom levels of images. This makes displaying images at different zoom levels a lot faster

[Image: A simple overview of Geoparquet benefits]

_A simple visual of COG tiling: If we only need the top left part of the image we can fetch only those tiles (green arrows). Image courtesy of Element 84's blog on COGs_

- Element84 wrote a simple explainer of what Cloud Optimized GeoTiffs are with great visuals
- Cloud Optimized Geotiff spec dedicated website
- Cloud Optimized Geotiff page on Cloud Native Geo guide

### For vectors (tables): GeoParquet

To handle vector data such as `pandas` `DataFrames` or `geopandas` `GeoDataFrames` we recommend using **GeoParquet** files. To (once again) paraphrase the Cloud Native Geo guide:

> GeoParquet is an encoding for how to store geospatial vector data (point, lines, polygons) in Apache Parquet, a popular columnar storage format for tabular data.

[Image: A simple overview of Geoparquet benefits]

_Image credit from the Cloud Native Geo slideshow_

    Refer to the next section to see all the details of how to ingest your data with Fused's built-in `fused.ingest()` to make the most out of geoparquet

- `geoparquet` Github repo
- `geoparquet` 1 page website with a list of companies & projects involved
- GeoParquet page on Cloud Native Geo guide

You can explore some of the uses of GeoParquet in some of our examples:
- Ingesting AIS point data from NOAA into GeoParquet format
- Explore the Overture Buildings dataset in this public UDF, which we repartitioned into GeoParquet. Read more about how we're leveraging this with the Overture Maps Foundation.

### Additional resources

- Read the Cloud-Optimized Geospatial Formats Guide written by the Cloud Native Geo Org about why we need Cloud Native formats
- Friend of Fused Kyle Barron did an interview about Cloud Native Geospatial Formats. Kyle provides simple introductions to some cloud native formats like `GeoParquet`

================================================================================

## Ingest your own data
Path: tutorials/Geospatial with Fused/geospatial-data-ingestion/Ingest-your-data.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geospatial-data-ingestion/ingest-your-data

_This guide explains how to use `fused.ingest` to geopartition and load vector tables into an S3 bucket so they can quickly be queried with Fused._

## Ingest geospatial table data

To run an ingestion job on vector data we need:
1. **Input data** - This is could be CSV files, a `.zip` containing shapely files or any other sort of non partitioned data
2. **A cloud directory** - This is where we will save our ingested data and later access it through UDFs

We've built our own ingestion pipeline at Fused that partitions data based on dataset size & location.
Our ingestion process:

1. Uploads the `input`
2. Creates geo-partitions of the input data

This is defined with `fused.ingest()`:

```python showLineNumbers
# Run this locally - not in Workbench
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract/",
)
```

Ingestion jobs often take more than a few seconds and require a lot of RAM (we open the whole dataset & re-partition it), which makes this a large run so we're going to use `run_batch()` so our ingestion job can take as long as needed.

To start an ingestion job, call `run_batch` on the job object returned by `fused.ingest`.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = job.run_batch()
```

    Refer to the dedicated documentation page for `fused.ingest()` for more details on all the parameters

Ingested tables can easily be read with the Fused utility function `table_to_tile`, which spatially filters the dataset and reads only the chunks within a specified polygon.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, 
    table="s3://fused-asset/infra/building_msft_us/"
):
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    return common.table_to_tile(bounds, table)

```

The following sections cover common ingestion implementations. It's recommended to run ingestion jobs from a Python Notebook or this web app.

### Ingest a table from a URL

Ingests a table from a URL and writes it to an S3 bucket specified with `fd://`.

```python showLineNumbers
# Run this locally - not in Workbench

job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract/",
).run_batch()
```

Get access to the logs:

```python showLineNumbers
# Run this locally - not in Workbench
job_id
```

If you encounter the message
`HTTPError: `, please contact Fused to increase the number of workers in your account.

### Ingest multiple files

```python showLineNumbers
# Run this locally - not in Workbench

job_id = fused.ingest(
    input=["s3://my-bucket/file1.parquet", "s3://my-bucket/file2.parquet"],
    output=f"fd://census/dc_tract",
).run_batch()
```

To ingest multiple local files, first upload them to S3 with fused.api.upload then specify an array of their S3 paths as the input to ingest.

### Row-based ingestion

Standard ingestion is row-based, where the user sets the maximum number of rows per chunk and file.

Each resulting table has one or more _files_ and each file has one or more _chunks_, which are spatially partitioned. By default, ingestion does a best effort to create the number of files specified by `target_num_files` (default `20`), and the number of rows per file and chunk can be adjusted to meet this number.

```python showLineNumbers   
# Run this locally - not in Workbench
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    explode_geometries=True,
    partitioning_method="rows",
    partitioning_maximum_per_file=100,
    partitioning_maximum_per_chunk=10,
).run_batch()
```

### Area-based ingestion

Fused also supports area-based ingestion, where the number of rows in each partition is
determined by the sum of their area.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_area",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
).run_batch()
```

### Geometry subdivision

Subdivide geometries during ingestion. This keeps operations efficient when geometries have many vertices or span large areas.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_geometry",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
    subdivide_start=0.001,
    subdivide_stop=0.0001,
    subdivide_method="area",
).run_batch()
```

### Ingest GeoDataFrame

Ingest a GeoDataFrame directly.

```python showLineNumbers
# Run this locally - not in Workbench
job_id = fused.ingest(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).run_batch()
```

### Ingest Shapefile

We recommend you `.zip` your shapefile and ingest it as a single file:

```python showLineNumbers
# Run this locally - not in Workbench
job_id = fused.ingest(
    input="s3://my_bucket/my_shapefile.zip",
    output="s3://sample-bucket/file.parquet",
).run_batch()
```

### Ingest with a predefined bounding box schema: `partitioning_schema_input`

Here is an example of an ingestion using an existing partition schema which comes from a previously ingested dataset. This assumes you've already ingested a previous dataset with `fused.ingest()`.
This may be useful if you are analyzing data across a time series and want to keep the bounding boxes consistent throughout your analysis.

```python showLineNumbers
# Run this locally - not in Workbench
@fused.udf
def read_ingested_parquet_udf(path: str = "s3://sample-bucket/ingested_data/first_set/"):

    # Built in fused method to reach the `_sample` file and return the bounding boxes of each parquet
    df = fused.get_chunks_metadata(path)

    # Since we want our `partitioning_schema_input` specified in `ingest()` to be a link to a parquet file containing bounds coordinates, we will save this metadata as a parquet file
    partition_schema_path = path + 'boxes.parquet'
    df.to_parquet(partition_schema_path)

    return partition_schema_path

partition_schema_path = fused.run(read_ingested_parquet_udf)

job_id = fused.ingest(
    input="s3://sample-bucket/file.parquet",
    output="s3://sample-bucket/ingested_data/second_set/",
    partitioning_schema_input=partition_schema_path
).run_batch()
```

## Ingest to your own S3 bucket

`fused.ingest` also supports writing output to any S3 bucket as long as you have the appropriate permissions:

1. Open the `S3 Policy` tab in the profile page of the workbench and enter your S3 bucket name. This should return a JSON object like this:

[Image: S3 Policy in the workbench]

2. Copy this IAM policy and paste it in your AWS S3 Policy tab.

3. Write the output to your S3 bucket using the `output` parameter:

```python showLineNumbers
# Assuming s3://my-bucket/ is your own managed S3 bucket
job_id = fused.ingest(
    input="s3://my-bucket/file.parquet",
    output="s3://my-bucket/ingested/", 
).run_batch()
```

## Troubleshooting

As with other Fused batch jobs, ingestion jobs require server allocation for the account that initiates them. If you encounter the following error message, please contact the Fused team to request an increase.

```text
Error: `Quota limit: Number of running instances`
```

================================================================================

## Why we need Ingestion
Path: tutorials/Geospatial with Fused/geospatial-data-ingestion/why-ingestion.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geospatial-data-ingestion/why-ingestion

# Why we need data Ingestion

_This page will give you all the tools to make your data fast to read to make your UDFs more responsive._

## What is this page about?

The whole purpose of Fused is to speed up data science pipelines.
To make this happen we need the data we're working with to be responsive, regardless of the dataset. The ideal solution is to have all of our data sitting in RAM right next to our compute, but in real-world applications:

- Datasets (especially geospatial data) can be in the Tb or Pb range which rarely fit in storage, let alone RAM
- Compute needs to be scaled up and down depending on workloads.

One solution to this is to build data around **Cloud Optimized formats**: Data lives in the cloud but also leverages file formats that are fast to access. Just putting a `.zip` file that needs to be uncompressed at every read on an S3 bucket is still very slow. Our ingested data should be:

- **On the cloud** so dataset size doesn't matter (AWS S3, Google Cloud Storage, etc.)
- **Partitioned** (broken down into smaller pieces that are fast to retrieve so we can load only sections of the dataset we need)

This makes it fast to read for any UDF (and any other cloud operation), so developing UDFs in Workbench UDF Builder & running UDFs is a lot faster & responsive!

## Benchmark & Demo

We're going to use a real-world example to show the impact of using different file formats & partitioning to make data a lot faster to access. For this demo, we'll be using AIS (Automatic Identification System) data as for our Dark Vessel Detection example.
These are points which represent the location of boats at any given time. We'll be using free & open data from NOAA Digital Coast.

[Image: Dark Vessel Detection AIS]

The NOAA Digital Coast platform gives us 1 zip file per day with the location of every boat with an AIS transponder as CSV (once unzipped).

We'll download 1 day and upload it as a CSV to Fused server with `fused.api.upload()`:

```python showLineNumbers
@fused.udf
def save_ais_csv_to_fused_udf():
    """Downloading a single day of data to Fused server"""

    response = requests.get("https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/AIS_2024_01_01.zip")

    with open("data.zip", "wb") as f:
        f.write(response.content)

    with zipfile.ZipFile("data.zip", "r") as zip_ref:
        zip_ref.extractall("./data")

    csv_df = fused.api.upload("./data/AIS_2024_01_01.csv", "fd://demo_reading_ais/AIS_2024_01_01.csv")
    print(f"Saved data to fd://demo_reading_ais/")

    return pd.DataFrame()
```

And simply running this UDF:

```python
fused.run(save_ais_csv_to_fused_udf)
```

We can check that our CSV was properly ingested with File Explorer by navigating to `s3://fused-users/fused/demo_reading_ais/`

[Image: Our AIS CSV properly uploaded on File Explorer]

That's one big CSV.

But opening it on its own doesn't do all that much for us. We're going to create 3 UDFs to showcase a more real-world application: Opening the dataset and returning a subset inside a bounding box. We'll do this 3 different ways to compare their execution time:
1. From the default CSV
2. From the same data but saved a `.parquet`
3. Ingesting this data with `fused.ingest()` and reading it from our ingested data

Since our AIS data covers the waters around the US, we'll use a simple bounding box covering a small portion of water:

```python showLineNumbers

bounds = gpd.GeoDataFrame(geometry=[shapely.box(-81.47717632893792,30.46235012285108,-81.33723531132267,30.58447317149745)])
```

This `bounds` is purposefully small (`print(smaller_bounds.iloc[0].geometry.area)` returns `0.02`) to highlight loading a very large dataset and recover only a small portion of data.

### 1. Reading directly from CSV

Here's a simple UDF to read our CSV in memory and return only points in within our bounding box:

```python showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)
    df = pd.read_csv(path)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 2. Reading from Parquet

First we need to save our AIS data a `.parquet`:

```python showLineNumbers
@fused.udf
def ais_to_parquet():

    # S3 bucket & dir for demo purpose here. Replace with your own
    csv_df = pd.read_csv("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv")
    csv_df.to_parquet("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet")
    print(f"Saved data (as parquet) to fd://demo_reading_ais/")

    return pd.DataFrame()
```

Here's our updated UDF to read a `.parquet` file:

```python  showLineNumbers
@fused.udf
def from_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet"
):

    df = pd.read_parquet(path)
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 3. Reading from Fused Ingested GeoParquet

Now, we're going to ingest the parquet file we have to create geo-partitioned files, using `fused.ingest()`. We'll go in more details on how to ingest your own data in the following section.

```python showLineNumbers
job = fused.ingest(
    "fd://demo_reading_ais/AIS_2024_01_01.parquet",
    "fd://demo_reading_ais/ingested/",
    target_num_chunks=500, # 500 is a rough default to use in most cases. We're not optimizing this value for now
    lonlat_cols=('LON','LAT')
)
```

Ingestion jobs are quite memory hungry so we'll run this on a remote machine as a large run:

```python
# Run this locally - not in Workbench
job.run_batch(
    instance_type='r5.8xlarge', # 256GiB RAM machine
)
```

Again using File Explorer to inspect our data we can see `fused.ingest()` didn't create 1 file but rather multiple:

[Image: File Explorer view of ingested files]

Our ingestion process broke down our dataset into smaller files (making each file easier to access) and a `_sample.parquet` file containing the bounding box of each individual file. This allows us to first intersect our `bounds` with `_sample` and then only open the smaller `.parquet` files we need:

```python showLineNumbers
@fused.udf
def read_ingested_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/file_format_demo/ingested/"
):

    # convert bounds to tile
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    zoom = common.estimate_zoom(bounds)
    tile = common.get_tiles(bounds, zoom=zoom)

    # Built in fused method to reach the `_sample` file and return only bounding box of each parquet holding our points
    df = fused.get_chunks_metadata(path)

    # Only keeping the tiles where our bounds is -> Only need to load actual data inside / touching our bounds
    df = df[df.intersects(tile.geometry.iloc[0])]

    # This is based on Fused's ingestion process
    chunk_values = df[["file_id", "chunk_id"]].values
    rows_df = pd.concat([
        fused.get_chunk_from_table(path, fc[0], fc[1], columns=['geometry'])
        for fc in chunk_values
    ])
    df = rows_df[rows_df.intersects(tile.geometry[0])]
    df.crs = tile.crs
    return df
```

    We've implemented a utils function in the publicly available common UDF that allows you to more simply read Fused ingested data: table_to_tile

    So instead of re-implementing the above in 2 lines of code you can read your ingested data:
    ```python showLineNumbers
    @fused.udf
    def udf(
        bounds: fused.types.Bounds, path: str='s3://your-bucket/your-dir/demo_reading_ais/ingested/'):
        common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
        df = common.table_to_tile(bounds, table=path)
        return df
    ```

### Comparing all 3 runs

We'll run each of these 3 methods in a Jupyter notebook using the `%%time` magic command to compare their run time:

[Image: Comparing all 3 methods of reading AIS]

There are a few conclusions to draw here:
- simply saving a CSV to `.parquet` makes files smaller & significantly faster to read just by itself (4x speed gain in this example)
- But proper geo-partitioning takes those gains ever higher (additional 8x speed gain in this specific example)

In short, by ingesting our data with `fused.ingest()` we're trading some up-front processing time for much faster read time. We only need to ingest our data once and every subsequent read will be fast and responsive.

## When is ingestion needed?

You don't _always_ need to ingest your file into a cloud, geo-partitioned format. There are a few situation when it might be simpler & faster to just load your data.
Small files (< 100Mb ) that are fast to open (already in `.parquet` for example) that you only read once (note that it might be read 1x in your UDF but your UDF might be run many times)

Example of data you should ingest: 1Gb `.zip` of shapefile
- `.zip` means you need to unzip your file each time you open it and then read it. This slows down working with the data _every minute_. This results in each individual files (a CSV when unzipped) containing millions of points.
- shapefile contains multiple files, it isn't the fastest to read

Example of data you don't need to ingest: 50Mb `.parquet`
- Even if the data isn't geo-partitioned, loading this data should be fast enough to make any UDF fast

### Using cache as a single-use "ingester"

We could actually significantly speed up the above example where we loaded the AIS data as a CSV without running `fused.ingest()`, by using cache:

```python  showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.ViewportGDF,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):

    @fused.cache
    def load_csv(path):

        return pd.read_csv(path)
    df = load_csv(path)

    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]

    return bounds_ais
```

The first run of this would still be very slow, but running this a second time would give us a much faster result:

[Image: Comparing first CSV read to cached CSV read]

We're using `@fused.cache` to cache the result of `load_csv()` which is the same regardless of our `bounds`, so this allows us to save an 'intermediate' result on disk.
There are some limitations to this approach though:
- This cache is emptied after 24h.
- This cache is overwritten any time you change the cached function or its inputs

This approach is only helpful if you want to 1 time explore a new dataset in UDF Builder and don't want to wait around for the ingestion run to be done. Beyond that, this will end up being slower (and more frustrating)

================================================================================

## H3 Tiling
Path: tutorials/Geospatial with Fused/h3-tiling.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/h3-tiling

# H3 Tiling

> H3 is a discrete global grid system for indexing geographies into a hexagonal grid

Taken from the H3 website

<DocCardList className="DocCardList--no-description"/>

================================================================================

## Dynamic Tile to H3
Path: tutorials/Geospatial with Fused/h3-tiling/dynamic-tile-to-h3.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/h3-tiling/dynamic-tile-to-h3

# Dynamic Tile to H3

Dynamically tile data into H3 hexagons in a given viewport. Hex resolution is calculated based on the bounds of the viewport.

The following examples are available in a dedicated Fused Canvas

The examples in this page all use the Tile UDF Mode to dynamically compute H3 based on the viewport bounds.

Read more about the differences between Tile UDF Mode and Single UDF Mode.

## Polygon to Hex

The following example uses a simplified Census Block Group dataset of the state of California, showing a heatmap of the population density per hex 9 cell:

Link to UDF in Fused Catalog

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-125.0, 24.0, -66.9, 49.0],
    path: str = "s3://fused-asset/demos/catchment_analysis/simplified_acs_bg_ca_2022.parquet",
    min_hex_cell_res: int= 11, # Increase this if working with high res local data
    max_hex_cell_res: int= 4,
):

    common = fused.load("https://github.com/fusedio/udfs/tree/f430c25/public/common/")
    
    # Dynamic H3 resolution
    def dynamic_h3_res(b):
        z = common.estimate_zoom(b)
        return max(min(int(2 + z / 1.5),min_hex_cell_res), max_hex_cell_res)
    
    parent_res = max(dynamic_h3_res(bounds) - 1, 0)
    
    # Load and clip data
    gdf = gpd.read_parquet(path)
    tile = common.get_tiles(bounds, clip=True)
    gdf = gdf.to_crs(4326).clip(tile)
    
    # Early exit if empty
    if len(gdf) == 0:
        return pd.DataFrame(columns=["hex", "POP", "pct"])
    
    # Hexify
    con = common.duckdb_connect()
    df_hex = common.gdf_to_hex(gdf, res=parent_res, add_latlng_cols=None)
    con.register("df_hex", df_hex)
    
    # Aggregate to parent hexagons and calculate percentages
    # In this case we're aggregating by sum
    query = f"""
    WITH agg AS (
        SELECT 
            h3_cell_to_parent(hex, ) AS hex, 
            SUM(POP) AS POP
        FROM df_hex
        GROUP BY hex
    )
    SELECT
        hex,
        POP,
        POP * 100.0 / SUM(POP) OVER () AS pct
    FROM agg
    ORDER BY POP DESC
    """
    
    return con.sql(query).df()
```

Link to UDF in Fused Catalog

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = [-90.691,-21.719,-21.300,75.897], stats_type="mean", type='hex', color_scale:float=1):

    common = fused.load("https://github.com/fusedio/udfs/tree/b7637ee/public/common/")
    # convert bounds to tile
    tile = common.get_tiles(bounds, clip=True)

    # 1. Initial parameters
    x, y, z = tile.iloc[0][["x", "y", "z"]]
    url = f"https://s3.amazonaws.com/elevation-tiles-prod/geotiff///.tif"
    if type=='png':
        return url_to_plasma(url, min_max=(-1000,2000/color_scale**0.5), colormap='plasma')
    else:
        
        res_offset = 1  # lower makes the hex finer
        h3_size = max(min(int(3 + z / 2), 12) - res_offset, 2)
        print(h3_size)
    
        # 2. Read tiff
        da_tiff = rioxarray.open_rasterio(url).squeeze(drop=True).rio.reproject("EPSG:4326")
        df_tiff = da_tiff.to_dataframe("data").reset_index()[["y", "x", "data"]]
    
        # 3. Hexagonify & aggregate
        df = aggregate_df_hex(
            df_tiff, h3_size, latlng_cols=["y", "x"], stats_type=stats_type
        )
        df["elev_scale"] = int((15 - z) * 1)
        df["metric"]=df["metric"]*color_scale
        df = df[df["metric"] > 0]
        return df

def url_to_plasma(url, min_max=None, colormap='plasma'):
    common = fused.load("https://github.com/fusedio/udfs/tree/b7637ee/public/common/")

    return common.arr_to_plasma(common.url_to_arr(url).squeeze(), min_max=min_max, colormap=colormap, reverse=False)

@fused.cache
def df_to_hex(df, res, latlng_cols=("lat", "lng")):  
    common = fused.load("https://github.com/fusedio/udfs/tree/b7637ee/public/common/")
    qr = f"""
            SELECT h3_latlng_to_cell(, , ) AS hex, ARRAY_AGG(data) as agg_data
            FROM df
            group by 1
          --  order by 1
        """
    con = common.duckdb_connect()
    return con.query(qr).df()

@fused.cache
def aggregate_df_hex(df, res, latlng_cols=("lat", "lng"), stats_type="mean"):

    df = df_to_hex(df, res=res, latlng_cols=latlng_cols)
    if stats_type == "sum":
        fn = lambda x: pd.Series(x).sum()
    elif stats_type == "mean":
        fn = lambda x: pd.Series(x).mean()
    else:
        fn = lambda x: pd.Series(x).mean()
    df["metric"] = df.agg_data.map(fn)
    return df

```

```python showLineNumbers
@fused.udf
def udf():
    """
    Visualize the California census block-group hex tiles on a map using the
    `hex_tile_map_template_with_tooltip_v2` template.

    The template is called via its shared token (or name) and is provided with:
      ‚Ä¢ tile_url_template ‚Äì points to the vector-tile UDF `census_hex_all_california_bg`
      ‚Ä¢ config_json ‚Äì a purple colour scale for the POP attribute (0-10‚ÄØ000)
      ‚Ä¢ tooltip_columns ‚Äì columns to show when hovering (POP, GEOID, NAME)
    """
    # URL that serves the vector tiles for the census hex data.
    tile_url_template = (
        "https://www.fused.io/server/v1/realtime-shared/"
        "fsh_3jeg2MDjozUl8kXqSAxFGE/run/tiles///?dtype_out_vector=json"
    )

    # Configuration for the H3HexagonLayer ‚Äì purple colour ramp on POP (0-10‚ÄØ000)
    config_json = r""",
      "hexLayer": 
      }
    }"""

    # Columns we want to show in the tooltip when hovering over a hex.
    tooltip_columns = ["POP", "GEOID", "NAME"]

    return fused.run(
        "UDF_Hex_Tile_Map_Template",
        tile_url_template=tile_url_template,
        config_json=config_json,
        center_lng=-119.4179,
        center_lat=36.7783,
        zoom=5,
        tooltip_columns=tooltip_columns,
    )
```
</details>

### In Workbench Map Viewer

- Vector `H3HexagonLayer` with Tiles

================================================================================

## File to H3
Path: tutorials/Geospatial with Fused/h3-tiling/file-to-h3.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/h3-tiling/file-to-h3

# File to H3

Turning a single small dataset into a grid of H3 hexagons.

## Point Count to Hex 

The following example uses a simple CSV of 311 calls in the New York City area, showing a heatmap of calls per hex 9 cell

```python showLineNumbers
@fused.udf
def udf(
    noise_311_link: str = "https://gist.githubusercontent.com/kashuk/670a350ea1f9fc543c3f6916ab392f62/raw/4c5ced45cc94d5b00e3699dd211ad7125ee6c4d3/NYC311_noise.csv",
    res: int = 9
):
    # Load common utilities (includes duckdb helper)
    common = fused.load("https://github.com/fusedio/udfs/tree/b7637ee/public/common/")
    con = common.duckdb_connect()

    # Keep latitude and longitude (averaged per hex) alongside the hex count
    qr = f"""
    SELECT
      h3_latlng_to_cell(lat, lng, ) AS hex,
      COUNT(*) AS cnt,
      AVG(lat) AS lat,
      AVG(lng) AS lng
    FROM read_csv_auto('')
    WHERE lat IS NOT NULL AND lng IS NOT NULL
    GROUP BY 1
    """

    df = con.sql(qr).df()

    # Debugging: print the resulting DataFrame schema
    print(df.T)

    return df
```
</details>

### Requirements

- Small single (< 100MB) file (GeoJSON, CSV, Parquet, etc.). In example:
```csv
lat,lng
40.7128,-74.0060
40.7128,-74.0060
```

- Hexagon resolution (10, 11, 12, etc.). In this example:
```python showLineNumbers
res: int = 9
```

- Field to hexagonify  & Aggregation function (max 'population', avg 'income', mean 'elevation', etc.). In this example simply counting the number of calls per hex:
```python showLineNumbers
# Keep latitude and longitude (averaged per hex) alongside the hex count
qr = f"""
SELECT
    h3_latlng_to_cell(lat, lng, ) AS hex,
    COUNT(*) AS cnt,
    AVG(lat) AS lat,
    AVG(lng) AS lng
FROM read_csv_auto('')
WHERE lat IS NOT NULL AND lng IS NOT NULL
GROUP BY 1
"""
```

## Visualize H3 

### In Workbench Map Viewer

- Vector `H3HexagonLayer`

================================================================================

## Ingesting Dataset to H3
Path: tutorials/Geospatial with Fused/h3-tiling/ingesting-dataset-to-h3.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/h3-tiling/ingesting-dataset-to-h3

# Ingesting Dataset to H3

## Vector to Hex

Example of ingesting a release of the Overture Building dataset to H3 hexagons:

The following is similar to a vector dataset ingestion, running on batch jobs. 

Therefore we recommend running this in a notebook or a Python shell, not in Workbench.

```python showLineNumbers
# Run this in a notebook or a Python shell, not in Workbench
username = fused.api.whoami() # Getting username to save data in your own S3 bucket
release = '2025-01-22-0'
```

```python showLineNumbers
# Run this in a notebook or a Python shell, not in Workbench
args = [/theme=buildings/type=building/',
         'output_path': f's3://fused-users/fused//overture_overview//',
         'hex_res': 11}]
```

In this example we'll be simply counting the number of buildings per hexagon.

```python showLineNumbers
# Run this in a notebook or a Python shell, not in Workbench
Overture_Hexify = fused.load("https://github.com/fusedlabs/fusedudfs/tree/main/Overture_Hexify/")
j = Overture_Hexify(arg_list=args)

j.run_remote(instance_type='r5.16xlarge', disk_size_gb=999)
```

Reading the ingested data becomes a lot simpler & faster:

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/blob/main/public/common/")
@fused.udf
def udf(bounds: fused.types.Bounds = [-122.71963771127753,36.53196328805067,-120.70395948802646,38.082911654639275]):
    res = bounds_to_res(bounds)
    print(res)
    releases = ['2024-02-15-alpha-0', '2024-03-12-alpha-0', '2024-08-20-0', '2024-09-18-0', '2024-10-23-0', '2024-11-13-0', '2024-12-18-0', '2025-01-22-0', '2025-03-19-1', '2025-04-23-0', '2025-05-21-0']
    release1 = releases[-1]
    df1 = common.read_hexfile(bounds, f"s3://fused-users/fused/sina/overture_overview//hex.parquet", clip=True)

    return df1

@fused.cache
def bounds_to_res(bounds, res_offset=0, max_res=14, min_res=3):
    z = common.estimate_zoom(bounds)
    return max(min(int(3 + max(0, z - 3) / 1.7 + res_offset), max_res), min_res)
```
</details>

================================================================================

## When to use H3
Path: tutorials/Geospatial with Fused/h3-tiling/when-to-use-h3.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/h3-tiling/when-to-use-h3

# When to use H3

Consider using H3 tiling when:
- Creating heatmaps of events (populations, counts, etc.)
- Wanting to compare datasets across resolutions & scale
- Working with sparse datasets

Example of buildings heatmap based on the Overture Buildings dataset:

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/blob/main/public/common/")
@fused.udf
def udf(bounds: fused.types.Bounds = [-122.71963771127753,36.53196328805067,-120.70395948802646,38.082911654639275]):
    res = bounds_to_res(bounds)
    print(res)
    releases = ['2024-02-15-alpha-0', '2024-03-12-alpha-0', '2024-08-20-0', '2024-09-18-0', '2024-10-23-0', '2024-11-13-0', '2024-12-18-0', '2025-01-22-0', '2025-03-19-1', '2025-04-23-0', '2025-05-21-0']
    release1 = releases[-1]
    df1 = common.read_hexfile(bounds, f"s3://fused-users/fused/sina/overture_overview//hex.parquet", clip=True)

    return df1

@fused.cache
def bounds_to_res(bounds, res_offset=0, max_res=14, min_res=3):
    z = common.estimate_zoom(bounds)
    return max(min(int(3 + max(0, z - 3) / 1.7 + res_offset), max_res), min_res)
```
</details>

This example is based on ingested buildings datasets in H3

### Converting Datasets to H3 tiles

We've provided a few options for converting datasets to H3 tiles:

| Type | Data Size | Hexagonification Type | H3 Output | Consideration |
|------|-----------|----------------------|-----------|---------------|
| File to H3 | Small (< 100MB) | On the fly | Static Resolution | Easy to adjust. Can easily change field to hexagonify |
| Dynamic Tile to H3 | Small (< 100MB) or Large (> 100MB) | On the fly | Dynamic Resolution | Easy to adjust. Can easily change field to hexagonify |
| Ingesting Dataset to H3 | Large (> 100MB) | Pre-computed | Dynamic Resolution | Requires deciding on which variable to hexagonify ahead of time |

### Example UDFs

- Ookla Download Speed Heatmap: Visualize download speeds across the world.
- US Crop Viewer: Explore crops around the US
- Overture Release Difference Heatmap: Visually identify differences between 2 Overture Building dataset releases.

================================================================================

## index
Path: tutorials/Geospatial with Fused/index.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused

# Geospatial Analysis in Fused

Welcome to geospatial analysis with Fused! This section covers everything you need to know to work with geospatial data in Fused.

<DocCardList className="DocCardList--no-description"/>

---
<ReactPlayer
  url="https://www.youtube.com/watch?v=btIhyq2KbN0" 
  controls=
  muted=
  playing=
  width="100%"
  height="400px"
/>

================================================================================

## other-integrations
Path: tutorials/Geospatial with Fused/other-integrations.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/other-integrations

# Other Integrations

This section shows how to integrate Fused UDFs with popular mapping and visualization tools.

## DeckGL

DeckGL is a highly performant framework to create interactive map visualizations that handle large datasets.

This guide shows how to load data from Fused into DeckGL maps created from a single standalone HTML page.
### Setup

1. First create a UDF and generate an HTTPS endpoint.

2. Create an `.html` file following this template. This code creates a DeckGL map then introduces a layer that renders data from the specified Fused endpoint.

```html

```

### H3HexagonLayer

Create an `H3HexagonLayer`.

```js
new H3HexagonLayer(),
```

### Vector Tile Layer

Vector Tile layers are created by placing a `GeoJsonLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a vector layer.

The layer in the sample map comes from Overture Buildings UDF.

```js
new TileLayer(//?format=geojson",
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new GeoJsonLayer(props, );
  },
});
```

### Raster Tile Layer

Raster Tile layers are created by placing a `BitmapLayer` sublayer within a `TileLayer`. Use the following snippet to introduce a raster layer. The sample layer below was created from the NAIP Tile UDF.

```js
new TileLayer(//?format=png`,
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) =>  = props.tile;

    return new BitmapLayer(props, );
  },
  pickable: true,
});
```

Learn more about DeckGL

## Felt

Felt is a collaborative mapping platform for creating interactive maps. Load Fused data directly via URLs.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `format=png`
   - Replace path with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=png
```

3. In Felt, click "Upload from URL" and paste the modified URL

### Vector Data

1. Create a UDF that returns vector data
2. Generate a shared URL and modify it:
   - Set `format=csv` or `format=parquet`
   - Add UDF parameters as needed

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?format=csv&param1=value1
```

3. In Felt, click "Upload from URL" and paste the URL

Learn more about Felt

## Kepler

Kepler is an open source tool for visualizing large geospatial datasets. The Fused UDF Builder provides direct integration with Kepler.

### Usage

1. Create a UDF that returns vector data
2. In the UDF Builder, click "Open in Kepler.gl" on the top-right menu
3. Wait for data transfer and click "Open in Kepler.gl" in the bottom-right

This opens your data directly in Kepler for advanced visualization and analysis.

Learn more about Kepler

//?format=png",
    tile_size=512,
    zoom_offset=-1,
)
m.add_layer(tile_layer)
m
```

### Vector Tiles

```python

m = ipyleaflet.Map(center=(37.7749, -122.4194), zoom=17)

# Add vector tile layer
vector_layer = ipyleaflet.VectorTileLayer(
    url="https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=mvt"
)
m.add_layer(vector_layer)
m
```

Learn more about Leaflet */}

## Mapbox

Mapbox GL JS creates interactive web maps. Load Fused data using tile sources.

### Basic Setup

- Generate a Mapbox token

```html

```

### Vector Tiles

```html

```

### Raster Tiles

```html

```

Learn more about Mapbox GL JS

## QGIS

QGIS is an open source desktop GIS platform. Load Fused data as raster tiles, vector tiles, or vector files.

### Raster Tiles

1. Create a UDF that returns raster tiles
2. Generate a shared URL and modify it:
   - Set `format=png`
   - Replace with `///` template

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=png
```

3. In QGIS: Right-click "XYZ Tiles" ‚Üí "New Connection"
4. Paste the URL and configure the layer

### Vector Tiles

1. Create a UDF that returns vector tiles
2. Generate a shared URL with `format=mvt`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/tiles///?format=mvt
```

3. In QGIS: Right-click "Vector Tiles" ‚Üí "New Connection"
4. Paste the URL and configure the layer

### Vector Files

1. Create a UDF that returns vector data
2. Generate a shared URL with `format=geojson`

```bash
https://www.fused.io/server/v1/realtime-shared/YOUR_UDF/run/file?format=geojson
```

3. In QGIS: Layer ‚Üí Add Layer ‚Üí Add Vector Layer
4. Paste the URL as the data source

Learn more about QGIS

## Related 

- Generate HTTPS endpoints for your UDFs
- Check the Fused catalog for ready-to-use UDFs

================================================================================

## processing-statistics
Path: tutorials/Geospatial with Fused/processing-statistics.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/processing-statistics

# Processing & Statistics

### Buffer analysis

```python
@fused.udf
def udf(n_points: int=1000, buffer: float=0.0025):

    from shapely.geometry import Point, Polygon, LineString

    # Create LineString to represent a road
    linestring_columbus = LineString([[-122.4194,37.8065],[-122.4032,37.7954]])
    gdf_road = gpd.GeoDataFrame()

    # Create Points to represent GPS pings
    minx, miny, maxx, maxy = gdf_road.total_bounds
    points = [Point(random.uniform(minx, maxx), random.uniform(miny, maxy)) for _ in range(n_points)]
    gdf_points = gpd.GeoDataFrame()

    # Create a buffer around the road
    buffered_polygon = gdf_road.buffer(buffer)

    # Color the points that fall within the buffered polygon
    points_within = gdf_points[gdf_points.geometry.within(buffered_polygon.unary_union)]
    gdf_points = gdf_points.loc[points_within.index]

    return gdf_points
```

'),
                        'mean_elevation_m': round(mean_elevation, 2) if not np.isnan(mean_elevation) else None
                    })
                except Exception:
                    results.append('),
                        'mean_elevation_m': None
                    })
    
    # Create results GeoDataFrame
    results_gdf = gpd.GeoDataFrame(results, geometry=gdf_stations.geometry, crs=4326)
    
    return results_gdf
``` */}

================================================================================

## read-data
Path: tutorials/Geospatial with Fused/read-data.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/read-data

# Read Data

Common examples for reading geospatial data in Fused.

## Python Packages

### `geopandas`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/subway_stations.geojson"):

    return gpd.read_file(path)
```

### `shapely`

```python
@fused.udf
def udf():

    from shapely.geometry import Point, Polygon
    
    # Create geometries with shapely
    points = [Point(-122.4, 37.8), Point(-122.3, 37.7)]
    polygon = Polygon([(-122.5, 37.7), (-122.3, 37.7), (-122.3, 37.9), (-122.5, 37.9)])
    
    gdf = gpd.GeoDataFrame(
        ,
        geometry=points + [polygon],
        crs=4326
    )
    
    return gdf
```

### `duckdb`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing_2024.parquet"):

    conn = duckdb.connect()
    result = conn.execute(f"""
        SELECT * 
        FROM ''
        WHERE latitude IS NOT NULL
        LIMIT 1000
    """).df()
    
    return result
```

### `rioxarray`

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/elevation.tif"):

    # Read raster data with rioxarray
    raster = rxr.open_rasterio(path)
    
    # Convert to DataFrame for display
    df = raster.to_dataframe().reset_index()
    
    return df.head(1000)
```

### `xarray`

```python
@fused.udf
def udf():

    # Download NetCDF data to mount disk for proper reading
    path = fused.download('s3://fused-sample/demo_data/2025_01_01_ERA5_surface.nc','2025_01_01_ERA5_surface.nc')
    ds = xr.open_dataset(path)
    
    # Convert to DataFrame
    df = ds.to_dataframe().reset_index()
    
    return df.head(1000)
```

## Table Formats (Vector)

### GeoJSON (.geojson, .json)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.geojson"):

    return gpd.read_file(path)
```

### Shapefile (.shp + .shx, .dbf, .prj)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_shapefile.shp"):

    return gpd.read_file(path)
```

### GeoPackage (.gpkg)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states_geopackage.gpkg"):

    return gpd.read_file(path)
```

### KML/KMZ (.kml, .kmz)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/US_states.kml"):

    return gpd.read_file(path)
```

")
    
    # Read specific layer
    return gpd.read_file(path, layer=0)
``` */}

### Parquet (.parquet)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/buildings.parquet"):

    return gpd.read_parquet(path)
```

### CSV with coordinates (.csv)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.csv"):

    from shapely.geometry import Point
    
    # Read CSV
    df = pd.read_csv(path)
    
    # Convert to GeoDataFrame
    gdf = gpd.GeoDataFrame(
        df, 
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs=4326
    )
    
    return gdf
```

### Excel (.xlsx)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/table/subway_stations.xlsx"):

    from shapely.geometry import Point
    
    # Read Excel file
    df = pd.read_excel(path)
    
    # Convert to GeoDataFrame if coordinates exist
    if 'longitude' in df.columns and 'latitude' in df.columns:
        gdf = gpd.GeoDataFrame(
            df,
            geometry=gpd.points_from_xy(df.longitude, df.latitude),
            crs=4326
        )
        return gdf
    
    return df
```

## Array Formats (Raster)

### GeoTIFF (.tif, .tiff)

```python
@fused.udf
def udf(
    path: str = 's3://fused-sample/demo_data/satellite_imagery/wildfires.tiff'
):

    with rasterio.open(path) as src:
        data = src.read()
        bounds = src.bounds

    return data, bounds
```

### NetCDF (.nc)

```python
@fused.udf
def udf():

    # Download to mount disk for proper NetCDF reading
    path = fused.download('s3://fused-sample/demo_data/climate_data.nc', 'climate_data.nc')
    
    # Open NetCDF dataset
    ds = xr.open_dataset(path)
    
    return ds.to_dataframe().reset_index().head(1000)
```

)
``` */}

### STAC Catalog

#### Earth on AWS

```python
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-77.083, 38.804, -76.969, 38.983],
):

    odc.stac.configure_s3_access(aws_unsigned=True)
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

    # Loading Elevation model
    items = catalog.search(
        collections=["cop-dem-glo-30"], 
        bbox=bounds
    ).item_collection()

    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)

    return xarray_dataset["data"], bounds
```

#### Microsoft Planetary Computer

```python
@fused.udf
def udf(
    bounds: fused.types.Bounds = [-122.463,37.755,-122.376,37.803],
):

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        modifier=planetary_computer.sign_inplace,
    )

    # Loading Elevation model
    items = catalog.search(collections=["cop-dem-glo-30"],bbox=bounds).item_collection()
    
    xarray_dataset = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=150,
        bbox=bounds,
    ).astype(int)
    
    return xarray_dataset["data"], bounds
```

================================================================================

## Use Cases
Path: tutorials/Geospatial with Fused/use-cases.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/geospatial-use-cases

Explore practical use Cases of using Fused for various geospatial and data processing tasks.

<DocCardList className="DocCardList--no-description"/>

================================================================================

## Climate Dashboard
Path: tutorials/Geospatial with Fused/use-cases/climate-dashboard.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/use-cases/Climate Dashboard

# Building a Climate dashboard

We're going to build an interactive dashboard of global temperature data, after processing 1TB of data in a few minutes!

### Install `fused`

```python
pip install "fused[all]"
```

Read more about installing Fused here.

In a notebook:

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the link to authenticate.

Read more about authenticating in Fused.

Each file being about 140MB a quick back of the envelope calculation gives us:
```python
recent_days = [day for day in available_days if day.split('datestr=')[1][:7] in recent_months]
len(recent_days) * 140 / 1000 # size in GB of files we'll process
```

```bash
1005.62
```
</details>

Fused allows us to run a UDF in parallel. So we'll process 1 month of data across hundreds of jobs:

```python
results = fused.submit(
  udf, 
  recent_months, 
  max_workers=250, 
  collect=False
)
```

See a progress bar of jobs running:

```python
results.wait()
```

See how long all the jobs took:

```python
results.total_time()
```

```bash
>>> datetime.timedelta(seconds=40, ...)
```

**We just processed 20 years of worldwide global data, over 1TB in 40s!!**

We can now write a UDF that gets all the monthly data and aggregates it by month:

```python
@fused.udf(cache_max_age='0s')
def udf():

    # Listing all files on our mount directory
    monthlys = fused.api.list(fused.file_path(f"monthly_climate/"))
    file_list = "', '".join(monthlys)
    
    result = duckdb.sql(f"""
       SELECT 
           LEFT(datestr, 7) as month,
           ROUND(AVG(daily_mean_temp), 2) as monthly_mean_temp
       FROM read_parquet([''])
       GROUP BY month
       ORDER BY month
    """).df()

    return result
```

Instead of running this locally, we'll open it in Workbench, Fused's web-based IDE:

```python
# Save to Fused
udf.to_fused("monthly_mean_temp")

# Load again to get the Workbench URL
loaded_udf = fused.load("monthly_mean_temp")
```

Return `loaded_udf` in a notebook and you'll get a URL that takes you to Workbench:

```python
loaded_udf
```

Click on the link to open the UDF in Workbench. Click "+ Add to UDF Builder" 

[Image: Monthly temperature aggregation in Workbench]

### Interactive graph (with AI)

You can use the AI Assistant to help you vibe code an interactive timeseries of your data

Simply ask the AI:

```
Make an interactive graph of the monthly temperature data
```

You can then share your graph:
- Save (`Cmd + S` on MacOS or click the "Save" button)
- Click "URL" button to see deployed graph!

Any time you make an update, your graph will automatically update!

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/sharing_grpah.mp4" 
  width="100%" 
/>

================================================================================

## Dark Vessel Detection
Path: tutorials/Geospatial with Fused/use-cases/dark-vessel-detection.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/use-cases/dark-vessel-detection

_A complete example show casing how to use Fused to ingest data into a geo-partitioned, cloud friendly format, process images & vectors and use UDFs to produce an analysis_

### Requirements
- Access to Fused
- Access to a Jupyter Notebook
- Installing `fused` with `[all]` dependencies (mainly to have `pandas` & `geopandas`):

```python showLineNumbers
pip install "fused[all]"
```

## 1. The problem: Detecting illegal boats

Monitoring what happens at sea isn't the easiest task. Shores are outfitted with radars and each ship has a transponder to publicly broadcast their location (using Automatic Identification System, AIS), but ships sometimes want to hide their location when taking part in illegal activities.

Global Fishing Watch has reported on "dark vessels" comparing Sentinel 1 radar images to public AIS data and matching the two to compare where boats report being versus where they _actually_ are.

In this example, we're going to showcase a basic implementation of a similar analysis to identify _potential_ dark vessels, all in Fused.

[Image: Dark Vessel Detection workflow]

Here's the result of our analysis, running in real time in Fused:

     For the nerds out there, we're using the Ground Range Detected product, not the Radiometrically Terrain Corrected because we're looking at boats in the middle of the ocean, so terrain shouldn't be any issue.
    - This dataset is available as Cloud Optimized GeoTiff through a STAC Catalog, meaning we can directly use this data as is.

</Tabs>

### 3.2 - Writing a UDF to open each AIS dataset

The rest of the logic is write a UDF to open each file, read it as a CSV and write it to parquet.

```python showLineNumbers
@fused.udf()
def read_ais_from_noaa_udf(datestr:str='2023_03_29', overwrite:bool=False):

    # This is the specific URL where daily AIS data is available
    url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler//AIS_.zip'

    # This is our local mount file path
    path=fused.file_path(f'/AIS//')
    daily_ais_parquet = f'/.parquet'

    # Skipping any existing files
    if os.path.exists(daily_ais_parquet) and not overwrite:
        print(f' exist')
        return pd.DataFrame()

    # Download ZIP file to mounted disk
    r=requests.get(url)
    if r.status_code == 200:
        with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
            with z.open(f'AIS_.csv') as f:
                df = pd.read_csv(f)
                # MMSI is the unique identifier of each boat. This is a simple clean up for demonstration
                df['MMSI'] = df['MMSI'].astype(str)
                df.to_parquet(daily_ais_parquet)
                print(f"Saved ")
        return pd.DataFrame()
    else:
        return pd.DataFrame(']})
```

We can run this UDF a single time to make sure it works:

```python showLineNumbers
# Run this locally - not in Workbench
single_ais_month = fused.run(read_ais_from_noaa_udf, datestr="2024_09_01")

>>> Saved /mnt/cache/AIS/2024_09/01.parquet
```

To recap what we've done so far:
- Build a UDF that takes a date, fetches a `.zip` file from NOAA's AISDataHandler portal and saves it to our UDFs' mount (so other UDFs can access it)
- Run this UDF 1 time for a specific date

### 3.3 - Run this UDF over a month of AIS data

Next step: Run this for a whole period!

Since each UDF takes a few seconds to run per date, we're going to use `fused.submit()` to call a large numbers of UDFs all at once that will each run over a single date.

With a bit of Python gymnastics we can create a `DataFrame` of all the dates we'd like to process. Preparing to get all of September 2024 would look like this:

```python showLineNumbers
# Run this locally - not in Workbench

date_ranges = pd.DataFrame()
print(f"")
print(date_ranges.head())

>>> date_ranges.shape=(30, 1)
      datestr
0  2024_09_01
1  2024_09_02
2  2024_09_03
3  2024_09_04
4  2024_09_05
```

`fused.submit()` requires inputs as a list or `DataFrame`, in which case columns should have the argument names that our UDF expects, in this case `datestr`. We also recommend you always do a first run with `debug_mode=True` to test your submit job:

```python  showLineNumbers
# Run this locally - not in Workbench
fused.submit(
    read_ais_from_noaa_udf, 
    date_ranges, 
    debug_mode=True
)

>>> Saved /mnt/cache/tmp/AIS/2024_09/01.parquet
  |status  | file_path
-------------------------------------------------
0 |Done    | /mnt/cache/tmp/AIS/2024_09/01.parquet

```

`debug_mode=True` runs the 1st value inside `date_ranges` with `fused.run()`. This allows you to make sure your UDF + arg_list combo is working properly. 

Now that we've seen our UDF is working we can run all 30 jobs in parallel by removing `debug_mode=True`:

```python showLineNumbers
# Run this locally - not in Workbench
fused.submit(read_ais_from_noaa_udf, date_ranges)
```

We've now unzipped, opened & saved 30 days of data!

`fused.submit()` has more parameters you can control allowing you to change the number of `max_workers`, `engine` or the retry policies. Also take a look at the technical docs for more details.

One handy way to make sure our data is in the right place is to check it in the Workbench File Explorer. In the search bar type: `file:///mount/AIS/2024_09/`:

[Image: Pool Runner live results]

You'll see all our daily files! Notice how each file is a few 100 Mb. These files are still big individual files, i.e. would take a little while to read.

### 3.4 - Ingest 1 month of AIS data into a geo-partitioned format

These individual parquet files are now store on our mount disk. We could save them directly onto cloud storage but before that we can geo-partition them to make them even faster to read. This will make us reduce the time it takes to access our data from minutes to seconds.
Fused provides a simple way to do this with the ingestion process.

[Image: A simple overview of Geoparquet benefits]

_Image credit from the Cloud Native Geo slideshow_

To do this we need a few things:
- **Our input dataset**: in this case our month of AIS data.
   
- **A target cloud bucket**: We're going to create a bucket to store our month of geo-partitioned AIS data in parquet files
- A target number of chunks to partition our data in. For now we're going to keep it at 500
- Latitude / Longitude columns to determine the location of each point

```python showLineNumbers
# Run this locally - not in Workbench
ais_daily_parquets = [f'file:///mnt/cache/AIS//.parquet' for day in range_of_ais_dates]

job = fused.ingest(
    ais_daily_parquets,
    's3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09',
    target_num_chunks=500,
    lonlat_cols=('LON','LAT')
)
```

We'll send this job to a large instance using `job.run_batch()` as we latency doesn't matter much (we can wait a few extra seconds) and we'd rather have a larger machine & a larger storage:

```python showLineNumbers
# Run this locally - not in Workbench
job.run_batch(
    instance_type='r5.8xlarge', # We want why big beefy machine to do the partitioning in parallel, so large amounts of CPUs
    disk_size_gb=999 # Set a large amount of disk because this job will open each output parquet file to calculate centroid
)
```

Running this in a notebook gives us a link to logs so we can follow the progress of the job on the offline machine:

[Image: Workbench run remote logs]

Following the link shows us the live logs of what our job is doing:

[Image: Workbench run remote logs]

We can once again check that our geo-partitioned images are available using File Explorer. This time because our files are a lot faster to read we can even see the preview in the map view:

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="60%" width="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/geopartioned_AIS_file_explorer.mp4"/>

Our AIS data is ready to be use for the entire month of September 2024. To narrow down our search, we now need to get a Sentinel 1 image. Since these images are taken every 6 to 12 days depending on the region, we'll find a Sentinel 1 image and then narrow our AIS data to just a few minutes before and after the acquisition time of the Sentinel 1 image.

## 4. Retrieving Sentinel 1 images

Sentinel 1 images are free & open, so thankfully for us others have already done the work of turning the archive into cloud native formats (and continue to maintain the ingestion as new data comes in).

We're going to use the Microsoft Planetary Computer Sentinel-1 Ground Range Detected dataset, because it offers:
- Access through a STAC Catalog helping us only get the data we need and nothing else
- Images are in Cloud Optimized Geotiff giving us tiled images that load even faster
- Examples of how to access the data so most of our work will be copy pasta

    Most of the following section was written in Workbench's UDF Builder rather than in Jupyter Notebooks.

    We'll have the code in code blocks, you can run these anywhere but as we're looking at images, it's helpful to have UDF Builders' live map updated as you write your code.

Let's start with a basic UDF just returning our area of interest:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    # Convert bounds to GeoDataFrame using Fused common function
    bounds = common.bounds_to_gdf(bounds)
    return bounds
```

The code above demonstrates a basic User-Defined Function (UDF) that utilizes Fused's common functions. These utilities provide a some general functions for geospatial operations and transformations. 
The utility function `bounds_to_gdf` is mentioned above is defined as:

```python
def bounds_to_gdf(bounds_list, crs=4326):

    box = shapely.box(*bounds_list)
    return gpd.GeoDataFrame(geometry=[box], crs=crs)
```

This function converts a bounding box into a GeoDataFrame .

This UDF simply returns our Map viewport as a `gpd.GeoDataFrame`, this is a good starting point for our UDF returning Sentinel 1 images

While you can do this anywhere around the continental US (our AIS dataset covered shores around the US, so we want to limit ourselves there), if you want to follow along this is the area of interest we'll be using. You can overwrite this in the UDF directly:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    # Define our specific bounds
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)
    return bounds
```

Following the Microsoft Planetary Computer examples for getting Sentinel-1 we can add a few of the imports we need and call the STAC catalog:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    # Define our specific bounds and convert to GeoDataFrame
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    return bounds
```

We already have a bounding box, but let's narrow down our search to the first week of September:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    time_of_interest="2024-09-03/2024-09-04"
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"")
    return bounds
```

This print statement should return something like:

```bash
Returned 15 Items
```

This will be the number of unique Sentinel 1 images covering our `bounds` and `time_of_interest`.

We can now use the `odc` package to load the first image & we'll use the VV polarisation from Sentinel 1 (VH could also work, and it would be good to iterate on this to visually assess which one would work best. We're keeping it simple for now, but feel free to test out both!).

We'll get an `xarray.Dataset` object back that we can simply open & return as a `uint8` array:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):

    time_of_interest="2024-09-03/2024-09-04"
    bands = ["vv"]

    # Convert bounds to GeoDataFrame for STAC operations
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"")

    ds = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=bands,
        resolution=10, # We want to use the native Sentinel 1 resolution which is 10m
        bbox=bounds.total_bounds,
    ).astype(float)

    da = ds[bands[0]].isel(time=0)
    image = da.values * 1.0
    return image.astype('uint8')
```

Which gives us a Sentinel 1 image over our area of interest:

[Image: Notebook run remote print]

We've simplified the process quite a bit here, you could also:
- Instead of loading `image.astype('uint8')`, do a more controlled calibration and conversation to dB
- Select a more specific image rather than the 1st one in our stack
- Use a different band or band combination
- Use Radiometrically Terrain Corrected images

### 4.1 Cleaning our Sentinel 1 UDF

Before adding any new functionality, we're going to clean our UDF up a bit more:
- Move some of the functionality into separate functions
- Adding common error catching (so our UDF doesn't fail if no Sentinel 1 images are found within our AOI + date range if it's too narrow)
- Add a cache decorator to code functions that retrieve data to speed up the UDF & reduce costs.

This will allow us to keep our UDF more readable (by abstracting code away) and more responsive. Cached functions store their result to disk, which makes a common query a lot more responsive and less expensive by using less compute

Here's our cleaned up UDF:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    da = get_data(bounds, time_of_interest, resolution, bands)

    image = da.values * 1.0
    return image.astype('uint8')

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    # Convert bounds to GeoDataFrame
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )

    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bbox,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da = ds[bands[0]].isel(time=0)
        return da
```

## 5. Simple boat detection in Sentinel 1 radar images

Now that we have a Sentinel 1 image over a Area of Interest and time range, we can write a simple algorithm to return bounding boxes of the boats in the image. We're going to keep this very basic as we're optimizing for:
- **Speed of execution**: We want our boat detection algorithm to run in a few seconds at most while we're iterating. Especially at first when we're developing our pipeline, we want a fast feedback loop
- **Simplicity**: We're focused on demo-ing how to build an end-to-end pipeline with Fused in this example, not making the most thorough analysis possible. This should be a baseline for you to build upon!

Radar images over calm water tend to look black (as all the radar signal is reflect _away_ from the sensor), while (mostly metallic) boats reflect back to the sensor appearing like bright spots in our image. A simple "boat detecting" algorithm is thus to do a 2D convolution and threshold the output to a certain pixel value:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    return convoled_image.astype('uint8')

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    # Convert bounds to GeoDataFrame
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )

    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da = ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]
```

This returns a filtered image highlighting the brightest spots and reducing the natural speckle of the radar image

[Image: Workbench run remote logs]

This is now relatively simple to vectorise (turn into a vector object, from image to polygons):

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds_gdf = common.bounds_to_gdf(bounds)
    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    gdf_predictions = vectorise_raster(
        raster=convoled_image.astype("uint8"),
        bounds=bounds_gdf,
        threshold=200 # Taking a high threshold in 0-255 range to keep only very bright spots
    )

    # Merging close polygons together into a single polygon so 1 polygon <-> 1 boat
    buffer_distance = 0.0001  # eyeballed a few meters in EPSG:4326 (degrees are annoying to work with ¬Ø\_(„ÉÑ)_/¬Ø)
    merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
        -buffer_distance/2
    )
    merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

    return merged_gdf

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)
    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )
    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da =  ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]: # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

@fused.cache
def vectorise_raster(raster, bounds, threshold: float = 0.5):
    from rasterio import features

    transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

    shapes = features.shapes(
        source=raster.astype(np.uint8),
        mask = (raster > threshold).astype('uint8'),
        transform=transform
    )

    gdf = gpd.GeoDataFrame(
        geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
        crs=bounds.crs
    )
    return gdf
```

And that's how we have turned our Sentinel 1 image into a vector `gpd.GeoDataFrame` of bright objects:

[Image: Workbench run remote logs]

## 6. Retrieving AIS data for our time of Interest

To get our AIS data, we now need to retrieve the exact moment our Sentinel 1 images were acquired. We can use this information to only keep AIS points within just a few minutes around that time.

```python showLineNumbers@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds_gdf = common.bounds_to_gdf(bounds)

    da = get_data(bounds, time_of_interest, resolution, bands)
    image = da.values * 1.0

    convoled_image = quick_convolution(np.array(image), kernel_size=2)

    gdf_predictions = vectorise_raster(
        raster=convoled_image.astype("uint8"),
        bounds=bounds_gdf,
        threshold=200 # Using higher threshold to make sure only bright spots are taken
    )

    # Merging close polygons together
    buffer_distance = 0.0001  # eyeballed value in EPSG:4326 so need to use degrees. I don't like degrees
    merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
        -buffer_distance/2
    )
    merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

    # Keeping metadata close by for merging with AIS data
    merged_gdf['S1_acquisition_time'] = da['time'].values

    return merged_gdf

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """

    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    bounds = common.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )
    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned  Items")

        def odc_load(bounds,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da =  ds[bands[0]].isel(time=0)

        return da

def quick_convolution(input_array, kernel_size):

    shifted_images = []

    # Shifting the image in all combinations of directions (x, y) with padding
    for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
        for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
            shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
            shifted_images.append(shifted)

    stacked_image = np.stack(shifted_images)
    return stacked_image.std(axis=0)

def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):

    """Pad and shift an image by x_shift and y_shift with specified pad_value."""

    padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
    shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

    return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

@fused.cache
def vectorise_raster(raster, bounds, threshold: float = 0.5):
    from rasterio import features

    transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

    shapes = features.shapes(
        source=raster.astype(np.uint8),
        mask = (raster > threshold).astype('uint8'),
        transform=transform
    )

    gdf = gpd.GeoDataFrame(
        geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
        crs=bounds.crs
    )
    return gdf
```

`merged_gdf` now returns a column called `S1_acquisition_time` with the time the Sentinel 1 image was taken.

If we save and call this UDF with a token we can call it from anywhere, from a Jupyter Notebook or from another UDF. Let's create a new UDF in Workbench:

```python showLineNumbers
# This is a new UDF
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: int="2024-09-03/2024-09-10",
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    return s1_detections
```

This prints out:

```bash
Found 16 Unique Sentinel 1 detections
Sentinel 1 image was acquired at : 2024-09-04 00:19:09
```

We can now create another UDF that will take this `s1_acquisition_month_day_hour_min` date + a bounding box in input and returns all the AIS points in that time + area.

We're going to leverage code from the community for this part, namely reading the AIS data from a geo-partitioned GeoParquet. Fused allows us to easily re-use any code we want and freeze it to a specific commit so it doesn't break our pipelines (read more about this here)

We can use this bit of code called `table_to_tile` which will either load the AIS data or the bounding box depending on our zoom level to keep our UDF fast & responsive.

    You could write a GeoParquet reader from scratch or call a UDF that you have that already does this, you don't have to use this option. But we want to show you how you can re-use bits of code from others here.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    s1_acquisition_month_day_hour_min:str = '2024-09-04T00:19:09.175874',
    time_delta_in_hours: float = 0.1, # by default 6min (60min * 0.1)
    min_zoom_to_load_data: int = 14,
    ais_table_path: str = "s3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09", # This is the location where we had ingested our geo-partitioned AIS data
    ):
    """Reading AIS data from Fused partitioned AIS data from NOAA (data only available in US)"""

    from datetime import datetime, timedelta

    # Load the utils module
    common = fused.load("https://github.com/fusedio/udfs/tree/b672adc/public/common/")
    zoom = common.estimate_zoom(bounds)

    sentinel1_time = pd.to_datetime(s1_acquisition_month_day_hour_min)
    time_delta_in_hours = timedelta(hours=time_delta_in_hours)

    month_date = sentinel1_time.strftime('%Y_%m')
    monthly_ais_table = f"prod_/"
    print(f"")

    @fused.cache
    def getting_ais_from_s3(bounds, monthly_table):
        return common.table_to_tile(
            bounds,
            table=monthly_ais_table,
            use_columns=None,
            min_zoom=min_zoom_to_load_data
        )

    ais_df = getting_ais_from_s3(bounds, monthly_ais_table)

    if ais_df.shape[0] == 0:
        print("No AIS data within this bounds & timeframe. Change bounds or timeframe")
        return ais_df

    if zoom > min_zoom_to_load_data:
        print(f"Zoom is  | Only showing bounds")
        return ais_df

    print(f"")
    ais_df['datetime'] = pd.to_datetime(ais_df['BaseDateTime'])
    mask = (ais_df['datetime'] >= sentinel1_time - time_delta_in_hours) & (ais_df['datetime'] <= sentinel1_time + time_delta_in_hours)
    filtered_ais_df = ais_df[mask]
    print(f'')
    return filtered_ais_df
```

In workbench UDF builder we can now see the output of both of our UDF:

[Image: Notebook run remote print]

We can now see that 1 of these boats doesn't have an associated AIS point (in red).

    You can change the styling of your layers in the Visualize tab to make them look like the screenshot above

Now all we need to do is merge these 2 datasets together and keep all the boats that don't match an AIS point.

## 7. Merging the 2 datasets together

We can expand the UDF we had started in section 6. to call our AIS UDF by passing a bounding box + `s1_acquisition_month_day_hour_min`.

We'll get the AIS data and join it with the Sentinel 1 detected boats by using geopandas `sjoin_nearest` to get the nearest distance of each boat to an AIS point.

Any point with the closest AIS point >100m from the Sentinel 1 boat will be considered a potentiel "dark vessel".

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: str="2024-09-03/2024-09-10",
    ais_search_distance_in_meters: int=10,
):

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found  Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : ")

    @fused.cache()
    def get_ais_from_s1_date(s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds):
        return fused.run("fsh_FI1FTq2CVK9sEiX0Uqakv", s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds)

    ais_gdf = get_ais_from_s1_date()

    # Making sure both have the same CRS
    s1_detections.set_crs(ais_gdf.crs, inplace=True)

    # Buffering AIS points to leverage spatial join
    ais_gdf['geometry'] = ais_gdf.geometry.buffer(0.005)

    joined = s1_detections.to_crs(s1_detections.estimate_utm_crs()).sjoin_nearest(
        ais_gdf.to_crs(s1_detections.estimate_utm_crs()),
        how="inner", # Using left, i.e. s1 as keys
        distance_col='distance_in_meters',
    )

    # Dark vessels will be unique S1 points that don't have an AIS point within 10m
    potential_dark_vessels = joined[joined['distance_in_meters'] > ais_search_distance_in_meters]
    print(f"Found  potential dark vessels")

    # back to EPSG:4326
    potential_dark_vessels.to_crs(s1_detections.crs, inplace=True)
    return potential_dark_vessels
```

And now we have a UDF that takes a `time_of_interest` and bounding box and returns potential dark vessel:

[Image: potential dark vessel detection]

## Limitations & Next steps

This is a simple analysis, that makes a lot of relatively naive assumptions (for ex: all bright spots in SAR are boats for example, which only works in open water and not near the shore or around solid structures like ocean wind mills or oil rigs). There's a lot of ways in which it could be improved but provides a good starting point.

This could be improved in a few ways:
- Masking out any shore or known areas with static infrastructure (to limit potential false positives around coastal wind mill farms)

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/false_positive_wind_mills.mp4" width="80%" />
Example of the Block Island Wind Farm in Rhode Island showing up as false positive "potential dark vessel": Wind mills appear as bright spots but don't have any AIS data associated to them
(`bounds=[-71.08534386325134,41.06338103121641,-70.89011235861962,41.15718153299125]` & `s1_acquisition_month_day_hour_min = "2024-09-03T22:43:33"`)

- Using a more sophisticated algorithm to detect boats. The current algorithm is naive, doesn't return bounding boxes but rather general shapes.
- Return more information derived from AIS data: Sometimes boats go dark for a certain period of time only, making it possible to tie a boat that was dark for a certain time to a known ship when it does have it's AIS on.
- Running this over all the coast of the continental US and/or over a whole year. This would be closer to the Global Fishing Watch dark vessel detection project.

If you want to take this a bit further check out:
- Running UDFs at scale with `run_batch()`. You could use this to run this pipeline over a larger area or over a much longer time series (or both!) to find out more potentiel dark vessels
- More about Fused core concepts like chosing between running UDFs based on Tile or File

================================================================================

## Exploring Maxar Open Data
Path: tutorials/Geospatial with Fused/use-cases/exploring_maxar_data.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/use-cases/exploring_maxar_data

_A guide showing how to use Fused to get all of Maxar's Open Data from all the available STAC catalogs and explore the imagery_

### Requirements
- Access to Fused

## Summary 

Working with Fused UDFs also give you the option to easily use functions defined in other UDFs. In practice this means we've created a `common` public UDF that contains some functions we've found useful when working with any type of data.

You can explore it yourself by directly reading the code in Github. If you see any functions you'd like to use, we strongly recommend you use `fused.load()` and pass the latest commit hash at the time you want to use it:

```python
commit_hash = "fbf5682" # Latest commit hash of https://github.com/fusedio/udfs/blob/main/public/common at time of writing
common = fused.load(f"https://github.com/fusedio/udfs/tree//public/common/")
```

Each Maxar Event itself contains multiple collections. We created a simple function that loops over all the available `UNIQUE_ID/collection.json`, reads them an appends them into a single GeoDataFrame:

Looking at the `WildFires-LosAngeles-Jan-2025/collections.json` file:
```json
,

    ],
    "extent": ,
        "temporal": 
    },
    "title": "Los Angeles Wildfires 2025",
    "description": "Driven by strong Santa Ana winds, multiple wildfires are burning in the Los Angeles, California, area. More than 40,000 acres and more than 12,300 structures have burned; at least 19 people have died.",
    "license": "CC-BY-NC-4.0"
}
```

So we create `stac_to_gdf_maxar` to:
- Loop over all the `UNIQUE_ID/collection.json` files
- Read each `collection.json` file
- Extract the metadata & extent of each collection 
- Convert the `extent` into a GeoDataFrame
- Concat all into a single GeoDataFrame

Once again, you can directly read the code in Github to see exactly how we do this

</details>

You can easily rename your UDFs in Workbench. Rename this UDF to `Maxar_Open_Data_STAC_single_catalog` so we can call it later directly by name.

Make sure to save your UDF with `Cmd + S` (or `Ctrl + S` on Windows / Linux) or in the Workbench UI for these changes to take effect.

[Image: Renaming UDF]

And here we get all the images for the Los Angeles Wildfires 2025 event:

[Image: Single collection Maxar STAC]

## Aggregating all available data

### Getting all `events`

To be able to explore all of Maxar's Open Data Program we now need to run this specific UDF over all the available events. 

We'll do this in 2 steps:
- Fetch all the event names 
- Use `fused.submit()` to fetch all the STAC Collections for each event name

```python showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    print(f"")

    return collections
```

Let's break this UDF down:
- We're using `@fused.cache` to cache the Catalog request so our UDF doesn't need to do this request each time we execute it. It prevents being rate limited and doing the same request over and over against the same endpoint
- We're returning a list (`collections`) so if you run this in Workbench you'll notice nothing shows up on the map! That's also why we print the first 5 rows. 

Read through the Best Practices for more handy tips on how to write efficient and easy to debug UDFs

This UDF returns a list of all the available event names currently accessible through Maxar's Open STAC Catalog:

```python 
>>> print(f"")
['BayofBengal-Cyclone-Mocha-May-23', 'Belize-Wildfires-June24', 'Brazil-Flooding-May24', 'Cyclone-Chido-Dec15', 'Earthquake-Myanmar-March-2025']
```

[Image: Maxar STAC Events]

### Preparing `fused.submit()` to run in parallel

We're going to use `fused.submit()` to run our first UDF in parallel. To do this we need a few things:
- Prepare our inputs (in this case the name of all the `events`). We recommend doing this as a dataframe as it's simple to read & work with
- Pass our first UDF to `fused.submit()`

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        debug_mode=True # Using debug to run just the 1st event at first
    )
    print(f"")

    return dfs_out
```

Let's unpack this:
- We're calling the UDF called `"Maxar_Open_Data_STAC_single_catalog"` that we renamed earlier over `collections_df`. At the time of writing this example this represents 46 events¬ß
- We use `fused.submit(..., debug_mode = True)` to run only the 1st value from `collections_df`. This allows us to test that our `fused.submit()` job is written correctly. 

`fused.submit()` allows you to run a single over a list / dataframe of inputs in parallel. Under the hood Fused spins up many realtime instances (see technical docs for details) that will each run the given UDF (in this case `"Maxar_Open_Data_STAC_single_catalog"`) all at the same time.

This is a powerful way to scale a process with just a single function call.

Read the dedicated Docs section on `fused.submit()` for more

[Image: Maxar submit debug mode]

### Getting all Maxar open data

Once we're confident that our `fused.submit()` job setup is correct, we can remove `debug_mode=True` (it's set to `False` by default) and run our UDF across all events.

We can also increase the number of `max_workers`, as we have 46 events and the default `max_workers` is set to 32. We can ask Fused server to thus spin up more instances for us so this parallel job is even faster:

```python  showLineNumbers
@fused.udf
def udf():

    from pystac import Catalog

    @fused.cache
    def getting_stac_collection(stac_url = "https://maxar-opendata.s3.amazonaws.com/events/catalog.json"):
        root_catalog = Catalog.from_file(stac_url)
        collections = root_catalog.get_collections()
        return [collection.id for collection in collections]

    collections = getting_stac_collection()
    collections_df = pd.DataFrame()

    dfs_out = fused.submit(
        "Maxar_Open_Data_STAC_single_catalog",
        collections_df,
        max_workers=50, # Increasing the number of max_workers as we have more than events than the default value
    )
    print(f"")

    return dfs_out
```

After a few seconds, we get back a `GeoDataFrame` containing all the Maxar open data STAC catalogs:

[Image: Maxar submit all STACs]

This allows us to do a few different things:
- Explore _all_ of the available Maxar Open Data on a map directly. This helps us see what data Maxar has available that might be of interest, to compare image quality across areas for example. 
- Offer a wide variety of high resolution imagery to query against. For example retrieving as much the cloud free imagery as possible

## Choosing 1 Event to display

With access to all the images from Maxar, we can navigate the map and choose any image we'd like to display. Let's select one and display it in Workbench.

First we can use the Runtime Tab to find the URL of an image we'd like to display:

<ReactPlayer 
    playsinline= 
    className="video__player" 
    playing= 
    muted= 
    controls height="100%" 
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/maxar_stac/maxar_choosing_image.mp4"
    width="100%" 
/>

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = True
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")  
        tiles = common.get_tiles(bounds)
    
        arr = common.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

Unpacking this UDF:
- This UDF takes :
    - a `bounds` object. This allows us to pass the current Workbench Map Viewport to our UDF
    - `path` represents the path on S3 to one of the images we want to display
    - `chip_len`: The size of the chip size we'd like our image to display in

These images can be loaded using `bounds` and Tile mode because Maxar has provided these images as Cloud Optimized Geotiffs. This allows us to leverage their tiles & overviews and only load the data we need as we pan around the map

We can check this by reading the metadata in CLI with `gdalinfo` and see that each band has `Block`, meaning is tiled:

```bash  
gdalinfo /vsis3/maxar-opendata/events/Cyclone-Chido-Dec15/ard/38/300200022120/2024-06-11/104001009713BA00-visual.tif

>>>
Driver: GTiff/GeoTIFF

...

Band 1 Block=512x512 Type=Byte, ColorInterp=Red
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 2 Block=512x512 Type=Byte, ColorInterp=Green
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
Band 3 Block=512x512 Type=Byte, ColorInterp=Blue
  Overviews: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
  Mask Flags: PER_DATASET
  Overviews of mask band: 8704x8704, 4352x4352, 2176x2176, 1088x1088, 544x544, 272x272
```

Running the above UDF we can for now return the extent of the image:

[Image: maxar return img extent]

This allows us to introduce 2 concepts in Workbench:
- 1. Zoom to layer
- 2. Tile / File modes

### 1. Setting a default view in Workbench

After getting the extent of our image, we're going to Zoom to layer and set this view as the default view:

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/maxar_stac/zoom_to_layer_default_view.mp4" width="100%" />

This allows us to to any change we want to this UDF or pan anywhere on the map and always be able to zoom back to this default view!

### 2. Displaying the image

Now we can edit our UDF to return the image by changing `display_extent` to `False`:

```python  showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds, 
    path: str = "https://maxar-opendata.s3.amazonaws.com/events/Emilia-Romagna-Italy-flooding-may23/ard/33/031111210233/2023-05-23/1050010033C95B00-visual.tif", 
    chip_len=256,
    display_extent: bool = False
):

    from shapely.geometry import box
    from rasterio.session import AWSSession

    # Getting just bounds of image so we can zoom to layer
    if display_extent:
        print("Returning extent")
        with rasterio.Env(session=AWSSession()):
            with rasterio.open(path) as src:
                bbox_gdf = gpd.GeoDataFrame(geometry=[box(*src.bounds)],crs=src.crs)
        bbox_gdf.to_crs(4326, inplace=True)
    
        return bbox_gdf

    # Otherwise reading the GeoTiff
    else:
        print("Returning image")
        common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")      
        tiles = common.get_tiles(bounds)
    
        arr = common.read_tiff(tiles, path, output_shape=(chip_len, chip_len))
        print(f"")
        return arr
```

This change returns the image instead of the extent, but it returns the image all at once, because Workbench is set to "File" mode by default

[Image: File mode array return]

If you reproduce this yourself and pan around the map you'll notice:
- We see the whole image but with a relatively low resolution.
- Nothing changes as we pan around the map (resolution doesn't change)

We can change this by setting Workbench to "Tile" mode:

<ReactPlayer 
    playsinline= 
    className="video__player" 
    playing= 
    muted= 
    controls height="100%" 
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/maxar_stac/workbench_image_switching_to_tile_video.mp4"
    width="100%" 
/>

Under the hood, switching to "Tile" mode tells Workbench to run this UDF not only 1 times, but by breaking it up into Mercantile tiles. This is why you see the image being broken up into a grid of tiles.

These different modes don't change _what_ code is being executed, as our `udf` didn't change. It's only changing what geospatial parameters are being passed:
- "File" mode passes the `bounds` of the current viewport (run 1 time¬ß)
- "Tile" mode passes the `bounds` of the current viewport broken up into a grid of tiles (run 1 per each tile)

## Next steps

We've shown you how to:
- Use `fused.submit()` to run a UDF in parallel
- Use `@fused.cache` to cache requests to reduce costs and improve performance
- Use Workbench to display images in different modes

If you want to go a bit further you could:
- Explore the Best Practices to make the most of UDFs or learn handy tips to use Workbench as its fullest
- Go more in depth with "File" & "Tile" modes in Workbench
- Dive into the different ways Fused allows you to use caching to improve performance

================================================================================

## Vibe Coded Timeseries
Path: tutorials/Geospatial with Fused/use-cases/vibe-coded-dashboard.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/use-cases/vibe-coded-timeseries

# Vibe Coded Timeseries

_This is a shorter version of the Climate Dashboard use case, but vibe coding our way to a timeseries dashboard!._

We already ingested 20y of ERA5 weather data, available on Fused: `s3://fused-sample/demo_data/ERA5/climate_data/`

### Read a single file

Taking the first file, we can simply ask the AI Assistant to read it:

```
Read this file with DuckDB and return the first 10 rows: s3://fused-sample/demo_data/ERA5/climate_data/2005-01.pq
```

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/opening_a_file_with_ai.mp4" 
  width="100%" 
/>

### Reading all available files

Next up:

```
Read all available files with DuckDB in the director and give me the monthly average
```

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/scaling_to_all_files_in_bucket.mp4" 
  width="100%" 
/>

### Making a time series graph

```
Make an interactive time series graph of the monthly average temperature
```

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/graphing_data_compressed.mp4" 
  width="100%" 
/>

### Sharing your graph

- Save (`Cmd + S` on MacOS or click the "Save" button)
- Click "URL" button to see deployed dashboard!

Any time you make an update, your graph will automatically update!

<LazyReactPlayer 
  playsinline= 
  className="video__player" 
  playing= 
  muted= 
  controls 
  height="100%" 
  url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/climate_dashboard/sharing_grpah.mp4" 
  width="100%" 
/>

================================================================================

## Zonal stats with Fused: 10 minute guide
Path: tutorials/Geospatial with Fused/use-cases/zonal-stats.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/use-cases/zonal-stats

_A step-by-step guide for data scientists._

### Requirements

- access to Fused
- access to a Jupyter Notebook
- the following Python packages installed locally:
  - `fused`
  - `pandas`
  - `geopandas`

## 1. Using Fused for a Zonal Statistics Example

In this guide, we'll estimate how much alfalfa grows in zones defined by polygon geometries.
This will show you how to:
- Bring in your data
- Write a UDF to process the data
- Run the UDF remotely & in parallel
- Create an app that shows your results and can be shared with anyone

```mermaid
---
title: Overview of Zonal Stats
---
graph LR
    A("Continuous Field<br/>(raster image)") --> D
    B("Areas of Interest<br/>(vector polygons)") --> D
    D --> E(Areas of Interest<br/>w/ Attributes)
```

## 2. Bring in your data

You'll first upload your own vector table with `fused.ingest`. This spatially partitions the table and writes it in your specified S3 bucket as a GeoParquet file. You'll then calculate zonal stats over a raster array of alfalfa crops in the USDA Cropland Data Layer (CDL) dataset.

This example shows how to geo partition polygons of Census Block Groups for Utah, which is a Parquet table with a `geometry` column. You can follow along with this file or any vector table you'd like. Read about other supported formats in Ingest your own data.

First, set up a local Python environment, install the latest Fused SDK with `pip install fused`, and authenticate.

Now, write the following script to geo partition your data. Pass the URL of the table to `fused.ingest`. When you kick off an ingest job with `run_batch`, Fused spins up a server to geo partition your table and writes the output to the path specified by the `output` parameter. In the codeblock below, `fd://tl_2023_49_bg/` is the base path to your account's S3 bucket.

```python showLineNumbers
# Run this locally - not in Workbench

  job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER2023/BG/tl_2023_49_bg.zip",
    output="fd://tl_2023_49_bg/"
  )
  job_id = job.run_batch()
  ```

After running the preceeding code, open fused.io/jobs to view the job status and logs.

Once the job is complete, you can preview the output dataset in the File Explorer.

You can also ingest data without installing anything by using this Fused App.

For the next step you can use the path to the data you just ingested or, if you prefer, this public sample table: `s3://fused-asset/data/tl_2023_49_bg/`.

[Image: Zonal Stats Parquet Files in Workbench File Explorer]

## 3. Write a UDF to process the data

To see the data as we process it, we will write a UDF in the Fused Workbench. As you write code in the UDF Builder you'll see how visualization results, logs, and errors show up immediately.

To write a UDF simply wrap a Python function with the decorator `@fused.udf`.

The first parameter of this UDF, `bounds`. It is reserved for Fused to pass a `GeoDataFrame` which the UDF may use to spatially filter datasets, and usually corresponds to a web map tile. This enables Fused to run the UDF for each tile in the viewport to distribute processing across multiple workers.

The `year` parameter is used to structure the S3 path of the CDL GeoTiff which the utility function `read_tiff` reads for the area defined by `bounds`. The `crop_id` parameter 36 corresponds to alfalfa the CDL colormap, which the UDF uses to mask the raster array.

Fused lets you import utility Modules from other UDFs with `fused.load`. Their code lives in the public UDFs repo.

- `read_tiff` loads an array of the CDL dataset for the specified `bounds` extent and `year`
- `table_to_tile` loads the table you geo partitioned for the specified `bounds` extent
- `geom_stats` calculates zonal statistics by aggregating the `arr` variable over the geometries specified by the `gdf`

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    year: int = 2020,
    crop_id: int = 36
):

    # Convert bounds to tile
    common = fused.load("https://github.com/fusedio/udfs/tree/fbf5682/public/common/")
    zoom = common.estimate_zoom(bounds)
    tile = common.get_tiles(bounds, zoom=zoom)

    # Load CDLS data
    arr = common.read_tiff(
      tile,
      input_tiff_path=f"s3://fused-asset/data/cdls/_30m_cdls.tif"
    )

    # Mask for crop
    arr = np.where(np.isin(arr, [crop_id]), 1, 0)

    # Load polygons
    gdf = common.table_to_tile(
      bounds,
      table='s3://fused-asset/data/tl_2023_49_bg/',
      min_zoom=5,
      use_columns=['NAMELSAD', 'GEOID', 'MTFCC', 'FUNCSTAT', 'geometry']
    )
    gdf.crs = 4326

    # Calculate zonal stats
    return common.geom_stats(gdf, arr)
```

Try running the UDF in the UDF Builder and visually inspect the output on the map. See what happens when you change `year`. Try introducing print statements such as `print(arr)` and `print(gdf)` to show logs in the console.

[Image: Zonal Stats Map]

You might receive a timeout error in the `Results` tab. Try zooming into Utah on the map where the zonal areas are highlighted, to reduce the size of the tile being passed into the UDF from the map.

```json showLineNumbers
,
  "rasterLayer": ,
  "vectorLayer": ,
    "getLineColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```

</div>

\
Click "Copy shareable link" to share the app with others!

## 6. Conclusion and next steps

We've shown how you can use Fused to develop a distributed Python workflow to power an app. Through a simple sequence of steps we loaded data, wrote analytics code, and created an app to interact with the data. With a single click you went from experimental development code to a live application.

We hope this overview gives you a glimpse of what you can build with Fused. You can continue to learn how to read data, process data, and integrate with other applications.

Find inspiration for your next project, ask questions, or share your work with the Fused community.

- GitHub
- Discord
- LinkedIn
- Twitter

================================================================================

## visualization
Path: tutorials/Geospatial with Fused/visualization.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/visualization

# Visualization

## Within Workbench

- Map View
- Layer Styling

## Outside of Workbench

### Make a Map Tile Server in seconds

This example works best in Workbench

```python
@fused.udf
def udf(
   bounds: fused.types.Bounds,
   path: str="s3://fused-asset/data/tiger/state/tl_rd22_us_state 1pct.parquet"
):

   df = gpd.read_parquet(path)
   df = df.cx[bounds[0]:bounds[2], bounds[1]:bounds[3]]
   df['area'] = df['geometry'].area.round(2)
   return df
```

- Switch to using a Tile UDF
- Save your UDF
- Copy the HTTPS endpoint

Edit the tile coordinates to use `//`:
```
https://.../run/tiles///?format=parquet
```

Connect this anywhere to deliver a map tile server!

================================================================================

## write-data
Path: tutorials/Geospatial with Fused/write-data.mdx
URL: https://docs.fused.io/tutorials/Geospatial%20with%20Fused/write-data

# Write Data

When working with Fused we recommend you save your files in 2 formats: Parquet & Cloud Optimized GeoTIFF (COG).

Read more about why we recommend those formats.

### Table: to parquet

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/housing/housing_2024.csv"):

    housing = pd.read_csv(path)
    housing['price_per_area'] = round(housing['price'] / housing['area'], 2)
    
    processed_data = housing[['price', 'price_per_area']]

    # Saving to user specific location
    username = fused.api.whoami()['handle']
    output_path = f"s3://fused-users/fused//housing_2024_processed.parquet"
    processed_data.to_parquet(output_path)

    return f"File saved to "
```

### Array: to Cloud Optimized GeoTIFF (COG)

```python
@fused.udf
def udf(path: str = "s3://fused-sample/demo_data/satellite_imagery/wildfires.tiff"):

    # Read the raster data
    with rasterio.open(path) as src:
        data = src.read()
        profile = src.profile
    
    # Process the data
    processed_data = np.where(data > np.percentile(data, 80), 255, 0).astype(np.uint8)
    
    # Update profile for writing
    profile.update()
    
    # Write to Fused's shared disk (accessible to all UDFs in org)
    username = fused.api.whoami()['handle']
    output_path = f"/mnt/cache/wildfires_processed_.tif"
    
    with rasterio.open(output_path, 'w', **profile) as dst:
        dst.write(processed_data)
    
    return f"File saved to shared disk at "
```

### Geo-partitioning large datasets: `fused.ingest()`

Large geospatial data might not be optimally formatted or partitioned. Fused offers a simple way to ingest your data at scale.

```python
# Get your user handle 
user = fused.api.whoami()['handle']

# Ingest Washington DC Census data
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd:///data/census/partitioned/", # Saving to your Fused bucket
)

job.run_batch()
```

You can tail logs to see how the job is progressing:

```python
fused.api.job_tail_logs("your-job-id")
```

Learn more about Fused data ingestion

================================================================================

# PYTHON SDK

## fused.api
Path: python-sdk/api-reference/api.mdx
URL: https://docs.fused.io/python-sdk/api-reference/api

## fused.api.whoami

```python
whoami()
```

Returns information on the currently logged in user

---

## fused.api.delete

```python
delete(path: str, max_deletion_depth: int | Literal['unlimited'] = 3) -> bool
```

Delete the files at the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì Directory or file to delete, like `fd://my-old-table/`
- **max_deletion_depth** (<code>int | Literal['unlimited']</code>) ‚Äì If set (defaults to 3), the maximum depth the operation will recurse to.
  This option is to help avoid accidentally deleting more data that intended.
  Pass `"unlimited"` for unlimited.

**Examples:**

```python
fused.api.delete("fd://bucket-name/deprecated_table/")
```

---

## fused.api.list

```python
list(path: str, *, details: bool = False) -> list[str] | list[ListDetails]
```

List the files at the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì Parent directory URL, like `fd://bucket-name/`
- **details** (<code>bool</code>) ‚Äì If True, return additional metadata about each record.

**Returns:**

- <code>list\[str\] | list\[ListDetails\]</code> ‚Äì A list of paths as URLs, or as metadata objects.

**Examples:**

```python
fused.api.list("fd://bucket-name/")
```

---

## fused.api.get

```python
get(path: str) -> bytes
```

Download the contents at the path to memory.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>bytes</code> ‚Äì bytes of the file

**Examples:**

```python
fused.api.get("fd://bucket-name/file.parquet")
```

---

## fused.api.download

```python
download(path: str, local_path: str | Path) -> None
```

Download the contents at the path to disk.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`
- **local_path** (<code>str | Path</code>) ‚Äì Path to a local file.

---

## fused.api.upload

```python
upload(
    local_path: str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame,
    remote_path: str,
    timeout: float | None = None,
) -> None
```

Upload local file to S3.

**Parameters:**

- **local_path** (<code>str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame</code>) ‚Äì Either a path to a local file (`str`, `Path`), a (Geo)DataFrame
  (which will get uploaded as Parquet file), or the contents to upload.
  Any string will be treated as a Path, if you wish to upload the contents of
  the string, first encode it: `s.encode("utf-8")`
- **remote_path** (<code>str</code>) ‚Äì URL to upload to, like `fd://new-file.txt`
- **timeout** (<code>float | None</code>) ‚Äì Optional timeout in seconds for the upload (will default to `OPTIONS.request_timeout` if not specified).

**Examples:**

To upload a local json file to your Fused-managed S3 bucket:

```py
fused.api.upload("my_file.json", "fd://my_bucket/my_file.json")
```

---

## fused.api.sign_url

```python
sign_url(path: str) -> str
```

Create a signed URL to access the path.

This function may not check that the file represented by the path exists.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>str</code> ‚Äì HTTPS URL to access the file using signed access.

**Examples:**

```python
fused.api.sign_url("fd://bucket-name/table_directory/file.parquet")
```

---

## fused.api.sign_url_prefix

```python
sign_url_prefix(path: str) -> dict[str, str]
```

Create signed URLs to access all blobs under the path.

**Parameters:**

- **path** (<code>str</code>) ‚Äì URL to a prefix, like `fd://bucket-name/some_directory/`

**Returns:**

- <code>dict\[str, str\]</code> ‚Äì Dictionary mapping from blob store key to signed HTTPS URL.

**Examples:**

```python
fused.api.sign_url_prefix("fd://bucket-name/table_directory/")
```

---

## fused.api.get_udfs

```python
get_udfs(
    n: int | None = None,
    *,
    skip: int = 0,
    by: Literal["name", "id", "slug"] = "name",
    whose: Literal["self", "public", "community", "team"] = "self"
) -> dict
```

Fetches a list of UDFs.

**Parameters:**

- **n** (<code>int | None</code>) ‚Äì The total number of UDFs to fetch. Defaults to All.
- **skip** (<code>int</code>) ‚Äì The number of UDFs to skip before starting to collect the result set. Defaults to 0.
- **by** (<code>Literal['name', 'id', 'slug']</code>) ‚Äì The attribute by which to sort the UDFs. Can be "name", "id", or "slug". Defaults to "name".
- **whose** (<code>Literal['self', 'public', 'community', 'team']</code>) ‚Äì Specifies whose UDFs to fetch. Can be "self" for the user's own UDFs or "public" for
  UDFs available publicly or "community" for all community UDFs. Defaults to "self".

**Returns:**

- <code>dict</code> ‚Äì A list of UDFs.

**Examples:**

Fetch UDFs under the user account:

```py
fused.api.get_udfs()
```

---

## fused.api.job_get_logs

```python
job_get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> ‚Äì Log messages for the given job.

---

## fused.api.job_print_logs

```python
job_print_logs(
    job: CoerceableToJobId, since_ms: int | None = None, file: IO | None = None
) -> None
```

Fetch and print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- **file** (<code>IO | None</code>) ‚Äì Where to print logs to. Defaults to sys.stdout.

**Returns:**

- <code>None</code> ‚Äì None

---

## fused.api.job_tail_logs

```python
job_tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = True,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) ‚Äì how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) ‚Äì if true, print out only a sample of logs. Defaults to True.
- **timeout** (<code>float | None</code>) ‚Äì if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

## fused.api.job_get_status

```python
job_get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

## fused.api.job_cancel

```python
job_cancel(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì A new job object.

---

## fused.api.job_get_exec_time

```python
job_get_exec_time(job: CoerceableToJobId) -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns:**

- <code>timedelta</code> ‚Äì Time the job took. If the job is in progress, time from first to last log message is returned.

---

## fused.api.job_wait_for_job

```python
job_wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until this job has finished

**Parameters:**

- **poll_interval_seconds** (<code>float</code>) ‚Äì How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) ‚Äì The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> ‚Äì if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

## fused.api.FusedAPI

```python
FusedAPI(
    *,
    base_url: str | None = None,
    set_global_api: bool = True,
    credentials_needed: bool = True
)
```

API for running jobs in the Fused service.

Create a FusedAPI instance.

**Other Parameters:**

- **base_url** (<code>str | None</code>) ‚Äì The Fused instance to send requests to. Defaults to `https://www.fused.io/server/v1`.
- **set_global_api** (<code>bool</code>) ‚Äì Set this as the global API object. Defaults to True.
- **credentials_needed** (<code>bool</code>) ‚Äì If True, automatically attempt to log in. Defaults to True.

---

### create_udf_access_token

```python
create_udf_access_token(
    udf_email_or_name_or_id: str | None = None,
    /,
    udf_name: str | None = None,
    *,
    udf_email: str | None = None,
    udf_id: str | None = None,
    client_id: str | Ellipsis | None = ...,
    public_read: bool | None = None,
    access_scope: str | None = None,
    cache: bool = True,
    metadata_json: dict[str, Any] | None = None,
    enabled: bool = True,
) -> UdfAccessToken
```

Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.

**Parameters:**

- **udf_email_or_name_or_id** (<code>str | None</code>) ‚Äì A UDF ID, email address (for use with udf_name), or UDF name.
- **udf_name** (<code>str | None</code>) ‚Äì The name of the UDF to create the token for.

**Other Parameters:**

- **udf_email** (<code>str | None</code>) ‚Äì The email of the user owning the UDF, or, if udf_name is None, the name of the UDF.
- **udf_id** (<code>str | None</code>) ‚Äì The backend ID of the UDF to create the token for.
- **client_id** (<code>str | Ellipsis | None</code>) ‚Äì If specified, overrides which realtime environment to run the UDF under.
- **cache** (<code>bool</code>) ‚Äì If True, UDF tiles will be cached.
- **metadata_json** (<code>dict\[str, Any\] | None</code>) ‚Äì Additional metadata to serve as part of the tiles metadata.json.
- **enable** ‚Äì If True, the token can be used.

---

### upload

```python
upload(
    path: str,
    data: bytes | BinaryIO,
    client_id: str | None = None,
    timeout: float | None = None,
) -> None
```

Upload a binary blob to a cloud location

---

### start_job

```python
start_job(
    config: JobConfig | JobStepConfig,
    *,
    instance_type: WHITELISTED_INSTANCE_TYPES | None = None,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: Sequence[str] | None = ("FUSED_CREDENTIAL_PROVIDER=ec2",),
    image_name: str | None = None,
    send_status_email: bool | None = None,
    cache_max_age: int | None = None
) -> RunResponse
```

Execute an operation

**Parameters:**

- **config** (<code>JobConfig | JobStepConfig</code>) ‚Äì the configuration object to run in the job.

**Other Parameters:**

- **instance_type** (<code>WHITELISTED_INSTANCE_TYPES | None</code>) ‚Äì The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- **region** (<code>str | None</code>) ‚Äì The AWS region in which to run. Defaults to None.
- **disk_size_gb** (<code>int | None</code>) ‚Äì The disk size to specify for the job. Defaults to None.
- **additional_env** (<code>Sequence\[str\] | None</code>) ‚Äì Any additional environment variables to be passed into the job, each in the form KEY=value. Defaults to None.
- **image_name** (<code>str | None</code>) ‚Äì Custom image name to run. Defaults to None for default image.
- **send_status_email** (<code>bool | None</code>) ‚Äì Whether to send a status email to the user when the job is complete.

---

### get_jobs

```python
get_jobs(
    n: int = 5,
    *,
    skip: int = 0,
    per_request: int = 25,
    max_requests: int | None = 1
) -> Jobs
```

Get the job history.

**Parameters:**

- **n** (<code>int</code>) ‚Äì The number of jobs to fetch. Defaults to 5.

**Other Parameters:**

- **skip** (<code>int</code>) ‚Äì Where in the job history to begin. Defaults to 0, which retrieves the most recent job.
- **per_request** (<code>int</code>) ‚Äì Number of jobs per request to fetch. Defaults to 25.
- **max_requests** (<code>int | None</code>) ‚Äì Maximum number of requests to make. May be None to fetch all jobs. Defaults to 1.

**Returns:**

- <code>Jobs</code> ‚Äì The job history.

---

### get_status

```python
get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

### get_logs

```python
get_logs(job: CoerceableToJobId, since_ms: int | None = None) -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) ‚Äì Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> ‚Äì Log messages for the given job.

---

### tail_logs

```python
tail_logs(
    job: CoerceableToJobId,
    refresh_seconds: float = 1,
    sample_logs: bool = False,
    timeout: float | None = None,
)
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) ‚Äì how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) ‚Äì if true, print out only a sample of logs. Defaults to False.
- **timeout** (<code>float | None</code>) ‚Äì if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

### wait_for_job

```python
wait_for_job(
    job: CoerceableToJobId,
    poll_interval_seconds: float = 5,
    timeout: float | None = None,
) -> RunResponse
```

Block the Python kernel until the given job has finished

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.
- **poll_interval_seconds** (<code>float</code>) ‚Äì How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) ‚Äì The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> ‚Äì if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> ‚Äì The status of the given job.

---

### cancel_job

```python
cancel_job(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) ‚Äì the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> ‚Äì A new job object.

---

### auth_token

```python
auth_token() -> str
```

Returns the current user's Fused environment (team) auth token

---

================================================================================

## fused.core
Path: python-sdk/api-reference/core.mdx
URL: https://docs.fused.io/python-sdk/api-reference/core

## `run_tile`

```python showLineNumbers
def run_tile(email: str,
             id: Optional[str] = None,
             *,
             x: int,
             y: int,
             z: int,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_shared_tile`

```python showLineNumbers
def run_shared_tile(token: str,
                    *,
                    x: int,
                    y: int,
                    z: int,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    _client_id: Optional[str] = None,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_file`

```python showLineNumbers
def run_file(email: str,
             id: Optional[str] = None,
             *,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF.

---
## `run_shared_file`

```python showLineNumbers
def run_shared_file(token: str,
                    *,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  The response from the server after executing the operation on the file.

**Raises**:

- `Exception` - Describes various exceptions that could occur during the function execution, including but not limited to invalid parameters, network errors, unauthorized access errors, or server-side errors.

This function is designed to access shared operations that require a token for authorization. It requires network access to communicate with the server hosting these operations and may incur data transmission costs or delays depending on the network's performance.

---
## `run_tile_async`

```python showLineNumbers
async def run_tile_async(
        email: str,
        id: Optional[str] = None,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to asynchronously run a UDF on a specific tile defined by its x, y, and z coordinates. It supports customization of the output data types for vector and raster data, and accommodates additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - User's email address. Used to identify the user's saved UDFs. If the ID is not provided, the email is also used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF on the specified tile and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_tile_async`

```python showLineNumbers
async def run_shared_tile_async(
        token: str,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared tile-based UDF using a specific access token.

This function constructs a URL for running an operation on a tile, defined by its x, y, and z coordinates, accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation on the specified tile.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the specified tile and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---
## `run_file_async`

```python showLineNumbers
async def run_file_async(
        email: str,
        id: Optional[str] = None,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a file-based UDF associated with the specific email and ID.

This function constructs a URL to run a UDF on a server, allowing for output data type customization for vector and raster outputs and supporting additional parameters for the UDF execution. If no ID is provided, the user's email is used as the identifier.

**Arguments**:

- `email` _str_ - The user's email address, used to identify the user's saved UDFs. If the ID is not provided, this email will also be used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the function fetches the user's email as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_file_async`

```python showLineNumbers
async def run_shared_file_async(
        token: str,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared file-based UDF using the specific access token.

Constructs a URL to run an operation on a file accessible via a shared token, enabling customization of the output data types for vector and raster data. It accommodates additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the file and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

================================================================================

## fused.ingest
Path: python-sdk/api-reference/job.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/api-reference/job

## `fused.ingest`

```python showLineNumbers
def ingest(
    input: Union[str, Sequence[str], Path, gpd.GeoDataFrame],
    output: Optional[str] = None,
    *,
    output_metadata: Optional[str] = None,
    schema: Optional[Schema] = None,
    file_suffix: Optional[str] = None,
    load_columns: Optional[Sequence[str]] = None,
    remove_cols: Optional[Sequence[str]] = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: Optional[bool] = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: Union[int, float, None] = None,
    partitioning_maximum_per_chunk: Union[int, float, None] = None,
    partitioning_max_width_ratio: Union[int, float] = 2,
    partitioning_max_height_ratio: Union[int, float] = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: Optional[float] = None,
    subdivide_stop: Optional[float] = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: Optional[Tuple[str, str]] = None,
    gdal_config: Union[GDALOpenConfig, Dict[str, Any], None] = None
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Arguments**:

- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

        ```python showLineNumbers
        fused.ingest(
            ...,
            lonlat_cols=("x", "y")
        )
        ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        ```python showLineNumbers
        fused.ingest(
            ...,
            gdal_config=
            }
        )
        ```

**Returns**:

Configuration object describing the ingestion process. Call `.run_batch` on this object to start a job.

**Examples**:

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).run_batch()
```

---
#### `job.run_batch`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_batch` on this object to start the ingestion job.

```python
def run_batch(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Begin job execution.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_batch()` returns a `RunResponse` object which has the following methods:

#### `get_status`

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

---

#### `print_logs`

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

**Returns**:

  None

---
#### `get_exec_time`

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

---
#### `tail_logs`

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1) -> None
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.

================================================================================

## JobPool
Path: python-sdk/api-reference/jobpool.mdx
URL: https://docs.fused.io/python-sdk/api-reference/jobpool

## JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### cancel

```python
cancel(wait: bool = False)
```

Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

---

### retry

```python
retry()
```

Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

---

### total_time

```python
total_time(since_retry: bool = False) -> timedelta
```

Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

---

### times

```python
times() -> list[timedelta | None]
```

Time taken for each task.

Incomplete tasks will be reported as None.

---

### done

```python
done() -> bool
```

True if all tasks have finished, regardless of success or failure.

---

### all_succeeded

```python
all_succeeded() -> bool
```

True if all tasks finished with success

---

### any_failed

```python
any_failed() -> bool
```

True if any task finished with an error

---

### any_succeeded

```python
any_succeeded() -> bool
```

True if any task finished with success

---

### arg_df

```python
arg_df()
```

The arguments passed to runs as a DataFrame

---

### status

```python
status()
```

Return a Series indexed by status of task counts

---

### wait

```python
wait()
```

Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### tail

```python
tail(stop_on_exception = False)
```

Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### results

```python
results(return_exceptions = False) -> list[Any]
```

Retrieve all results of the job.

Results are ordered by the order of the args list.

---

### results_now

```python
results_now(return_exceptions = False) -> dict[int, Any]
```

Retrieve the results that are currently done.

Results are indexed by position in the args list.

---

### df

```python
df(
    status_column: str | None = "status",
    result_column: str | None = "result",
    time_column: str | None = "time",
    logs_column: str | None = "logs",
    exception_column: str | None = None,
    include_exceptions: bool = True,
)
```

Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

---

### get_status_df

```python
get_status_df()
```

---

### get_results_df

```python
get_results_df(ignore_exceptions = False)
```

---

### errors

```python
errors() -> dict[int, Exception]
```

Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

---

### first_error

```python
first_error() -> Exception | None
```

Retrieve the first (by order of arguments) error result, or None.

---

### logs

```python
logs() -> list[str | None]
```

Logs for each task.

Incomplete tasks will be reported as None.

---

### first_log

```python
first_log() -> str | None
```

Retrieve the first (by order of arguments) logs, or None.

---

### success

```python
success() -> dict[int, Any]
```

Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

---

### pending

```python
pending() -> dict[int, Any]
```

Retrieve the arguments that are currently pending and not yet submitted.

---

### running

```python
running() -> dict[int, Any]
```

Retrieve the results that are currently running.

---

### cancelled

```python
cancelled() -> dict[int, Any]
```

Retrieve the arguments that were cancelled and not run.

---

### collect

```python
collect(ignore_exceptions = False, flatten = True)
```

Collect all results into a DataFrame

---

================================================================================

## fused.options
Path: python-sdk/api-reference/options.mdx
URL: https://docs.fused.io/python-sdk/api-reference/options

## fused.options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

**Examples:**

Change the `request_timeout` option from its default value to 60 seconds:

```py
fused.options.request_timeout = 60
```

## Options

### __dir__

```python
__dir__() -> List[str]
```

### base_url

```python
base_url: str = PROD_DEFAULT_BASE_URL
```

Fused API endpoint

### auth

```python
auth: AuthOptions = Field(default_factory=AuthOptions)
```

Options for authentication.

### show

```python
show: ShowOptions = Field(default_factory=ShowOptions)
```

Options for object reprs and how data are shown for debugging.

### max_workers

```python
max_workers: int = 16
```

Maximum number of threads, when multithreading requests

### run_timeout

```python
run_timeout: float = 130
```

Request timeout for UDF run requests to the Fused service

### request_timeout

```python
request_timeout: Union[Tuple[float, float], float, None] = 5
```

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### request_max_retries

```python
request_max_retries: int = 5
```

Maximum number of retries for API requests

### request_retry_base_delay

```python
request_retry_base_delay: float = 1.0
```

Base delay before retrying a API request in seconds

### realtime_client_id

```python
realtime_client_id: Optional[StrictStr] = None
```

Client ID for realtime service.

### max_recursion_factor

```python
max_recursion_factor: int = 5
```

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

```python
save_user_settings: StrictBool = True
```

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

```python
default_udf_run_engine: Optional[StrictStr] = None
```

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

```python
default_validate_imports: StrictBool = False
```

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

```python
prompt_to_login: StrictBool = False
```

Automatically prompt the user to login when importing Fused.

### no_login

```python
no_login: StrictBool = False
```

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

```python
pyodide_async_requests: StrictBool = False
```

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

```python
cache_directory: Path | None = None
```

The base directory for storing cached results.

### data_directory

```python
data_directory: Path = None
```

The base directory for storing data results. Note: if storage type is 'object', then this path is relative to
fd_prefix.

### temp_directory

```python
temp_directory: Path = Path(tempfile.gettempdir())
```

The base directory for storing temporary files.

### never_import

```python
never_import: StrictBool = False
```

Never import UDF code when loading UDFs.

### gcs_secret

```python
gcs_secret: str = 'gcs_fused'
```

Secret name for GCS credentials.

### gcs_filename

```python
gcs_filename: str = '/tmp/.gcs.fused'
```

Filename for saving temporary GCS credentials to locally or in rt2 instance

### gcp_project_name

```python
gcp_project_name: Optional[StrictStr] = None
```

Project name for GCS to use for GCS operations.

### logging

```python
logging: StrictBool = Field(default=False, validate_default=True)
```

Control logging for Fused

### verbose_udf_runs

```python
verbose_udf_runs: StrictBool = True
```

Whether to print logs from UDF runs by default

### default_run_headers

```python
default_run_headers: Optional[Dict[str, str]] = 

```

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

```python
default_dtype_out_vector: StrictStr = 'parquet'
```

Default transfer type for vector (tabular) data

### default_dtype_out_raster

```python
default_dtype_out_raster: StrictStr = 'npy,tiff'
```

Default transfer type for raster data

### fd_prefix

```python
fd_prefix: Optional[str] = None
```

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

```python
verbose_cached_functions: StrictBool = True
```

Whether to print logs from cache decorated functions by default

### local_engine_cache

```python
local_engine_cache: StrictBool = True
```

Enable UDF cache with local engine

### default_send_status_email

```python
default_send_status_email: StrictBool = True
```

Whether to send a status email to the user when a job is complete.

### cache_storage

```python
cache_storage: StorageStr = 'auto'
```

Specify the default cache storage type

### base_web_url

```python
base_web_url
```

### save

```python
save()
```

Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

================================================================================

## Udf
Path: python-sdk/api-reference/udf.mdx
URL: https://docs.fused.io/python-sdk/api-reference/udf

## Udf

The `Udf` class is the object you get when defining a UDF with the
`@fused.udf` decorator, or when loading
a saved UDF with `fused.load()`.

### to_fused

```python
to_fused(overwrite: bool | None = None, **kwargs: dict[str, Any])
```

Save this UDF on the Fused service.

**Parameters:**

- **overwrite** (<code>bool | None</code>) ‚Äì If True, overwrite existing remote UDF with the UDF object.

---

### to_directory

```python
to_directory(where: str | Path | None = None, *, overwrite: bool = False)
```

Write the UDF to disk as a directory (folder).

**Parameters:**

- **where** (<code>str | Path | None</code>) ‚Äì A path to a directory. If not provided, uses the UDF function name.

**Other Parameters:**

- **overwrite** (<code>bool</code>) ‚Äì If true, overwriting is allowed.

---

### to_file

```python
to_file(where: str | Path | BinaryIO, *, overwrite: bool = False)
```

Write the UDF to disk or the specified file-like object.

The UDF will be written as a Zip file.

**Parameters:**

- **where** (<code>str | Path | BinaryIO</code>) ‚Äì A path to a file or a file-like object.

**Other Parameters:**

- **overwrite** (<code>bool</code>) ‚Äì If true, overwriting is allowed.

---

### create_access_token

```python
create_access_token(
    *,
    client_id: str | Ellipsis | None = ...,
    public_read: bool | None = None,
    access_scope: str | None = None,
    cache: bool = True,
    metadata_json: dict[str, Any] | None = None,
    enabled: bool = True
) -> UdfAccessToken
```

---

### get_access_tokens

```python
get_access_tokens() -> UdfAccessTokenList
```

---

### delete_saved

```python
delete_saved(inplace: bool = True)
```

---

### delete_cache

```python
delete_cache()
```

---

### catalog_url

```python
catalog_url: str | None
```

Returns the link to open this UDF in the Workbench Catalog, or None if the UDF is not saved.

---

================================================================================

## Authentication
Path: python-sdk/authentication.mdx
URL: https://docs.fused.io/python-sdk/authentication

## Authenticate

Authenticate the Fused Python SDK in a Python Notebook.

Make sure to have the `fused` package installed.

```python showLineNumbers
pip install "fused[all]"
```

To use Fused you need to authenticate. The following will store a credentials file in `~/.fused/credentials`:

</Tabs>

## Log out

Log out the current user. This deletes the credentials saved to disk and resets the global Fused API.

```python showLineNumbers

fused.api.logout()
```

## Get Bearer (Access) token

Get the account's Bearer (sometimes referred to as Access) token.

```python showLineNumbers

fused.api.access_token()
```

This can be helpful when calling UDFs via HTTPS requests outside of the Fused Python SDK and Workbench to authenticate with the Fused API.

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

:::

================================================================================

## Batch jobs
Path: python-sdk/batch.mdx
Status: UNLISTED
URL: https://docs.fused.io/python-sdk/batch

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

```python showLineNumbers

@fused.udf
def udf(
    source_s3_path = '',
    destination_s3_path = ''
):

    file_path = fused.download(source_s3_path, source_s3_path.split('/')[-1])

    # Create a temporary directory to extract the files
    s3 = s3fs.S3FileSystem()
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/unzipped_files')

    # Upload each file to the S3 bucket
    for root, dirs, files in os.walk('/tmp/unzipped_files'):
        for file in files:
            file_path = os.path.join(root, file)
            print('file_path', file_path, destination_s3_path)
            s3.put(file_path, destination_s3_path)

    return pd.DataFrame() # UDFs must return a table or raster

```

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_batch()` to make kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

```python showLineNumbers
# Run this locally - not in Workbench
job = udf(
    source_s3_path = 'https://datadownload-production.s3.amazonaws.com/WCMC_carbon_tonnes_per_ha.zip', 
    destination_s3_path = 's3://fused-users/fused/<YOUR_DIR>/dswid/WCMC_carbon_tonnes_per_ha_10gb/'
)
job_id = job.run_batch()
```

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

```python showLineNumbers
fused.api.job_tail_logs("df335890-4406-4832-bf93-6a3b092e496d")
```

================================================================================

## Changelog
Path: python-sdk/changelog.mdx
URL: https://docs.fused.io/python-sdk/changelog

# Changelog

## v1.24.5 (2025-10-13)

**New Features:**
- You can now directly paste context items into AI chat with text like `[@my_other_udf]`.
- It is now possible to retrieve header values from within UDFs.

**Improvements:**
- Improved feedback that a UDF is running in Workbench.
- The default AI profiles in Workbench have been renamed to "Fast", "Explain", and "Smart".
- Enabled nesting of UDFs under certain circumstances.
- File explorer is now shown above the currently open page.
- Upgraded DuckDB to v1.4.1.
- Workbench will show cached UDF catalog data until it is refreshed.
- `Cmd+Shift+F` is now bound to toggle code folding.

**Bug Fixes:**
- Smoothed out the experience working with Canvas. Many bugs, keyboard shortcuts, UI elements, and interactions were adjusted.
- Collections were renamed to Canvas in a few places.
- Fixed a bug when opening Workbench with an AI prompt provided in the URL.
- Fixed a bug with opening pull requests on the public UDF repo from within Workbench.
- Adjusted the AI prompts in Workbench.
- UDF details page now shows code first.
- Adjusted versions page styling.
- Fixed the changes modal empty state styling.
- Uploaded UDFs will no longer inherit version control metadata.
- It is no longer possible to hide the Canvas in the Canvas page.
- Upload modal will now open data with file opener UDFs.
- `Udf.to_fused` is now more robust for flaky network situations.
- Fixed bugs where saving UDFs would not break the Canvas cache.

## v1.24.4 (2025-10-06)

**New Features:**

- Drag & Drop upload of files anywhere in Canvas view! 

**Improvements:**

- `fused.submit` now supports `instance_type` parameter.
- Workbench will remember which page you were on before switching to some tabs, so you can go back.
- Returning numpy arrays with 4 or more dimensions will be truncated to 3 dimensions for PNG visualization.
- Supervisor profile is available without enabling an experimental feature.

**Bug Fixes:**

- Various bug fixes for Canvas UI.
- Workbench will clean up old AI chats from your browser.
- Fixed `fused.api.job_get_exec_time`.
- Fixed fetching a single result for large (batch) UDF runs.
- `fused.run` will pass `cache_max_age` for large jobs.

## v1.24.3 (2025-10-03)

**New Features:**

- Sticky notes are supported in canvas view.
- Canvas view now has an auto-arrange button.
- It is possible to start AI changes directly from the canvas view.

**Improvements:**

- Improved how UDF context is passed to AI chat, and removed Rename from default tools.
- File Explorer will open files in the canvas view instead of the UDF builder view.
- Fused-py will prefer `npy` format for numpy arrays.
- UDFs can be deleted from the settings modal.

**Bug Fixes:**

- Fixed UI layout bugs in canvas view and AI chat.
- Fixed bugs with starting ingest jobs.
- Fixed bugs with AI auto-retry.

## v1.24.2 (2025-10-01)

**New Features:**

- Added Claude Sonnet 4.5 model.
- Canvas mode has a UDF selection list in its home view.
- AI chats can continue in the background.

**Improvements:**

- Improved highlighting of syntax errors in Workbench.
- Improved highlighting of UDF execution errors in Workbench.
- Adjusted the design of omnisearch at the top panel of Workbench.
- Batch jobs in Workbench run based on cache_max_age.
- Various canvas UI improvements.
- Upgraded package `arraylake`, and added `icechunk`.

**Bug Fixes:**

- Fixed pending changes still showing after switching UDFs.
- Fixed an issue with saving UDFs and shared tokens slowing down.
- Fixed stale data being cached locally when importing a canvas (collection).
- Fixed an issue where UDF renames would incorrect choose to show the "duplicate UDF name" modal.

## v1.24.0 (2025-09-29)

**New Features:**

- Canvas view now default in Workbench!
- UDF are now only shared to "Team" by default rather than "Public"

**Improvements:**

Canvas View:
- Ability to share Canvas in a single click
- Added Team / Public sharing options for Canvas (default to Team)
- Easily see if your Canvas was successfully run or failed
- Simplified 2 panel layout: Jump between code & AI on the left and Canvas on the right 
- Hide / Show UDFs 

AI Chat:
- Much better support for multiple AI chats running at the same time in different UDFs
- Improvements to Experimental Supervisor mode
- Adding inline diff view to code changes made by AI
- Improved charting ability in AI chat

Workbench:
- Added S3 policy tab to profile modal

Dependencies:
- Update `pyogrio` to 0.11
- Update `duckdb` to 1.4.0

**Bug Fixes:**

- Fixed bug in Version tab 

## v1.23.0 (2025-09-18)

**New Features:**

- New experimental "Supervisor" mode in AI chat, which features more Workbench capabilities and enhanced search capabilities. Different tools in Supervisor mode can be turned on and off in the AI chat settings.
- Workbench can now run UDFs in batch mode when `instance_type` is set on `@fused.udf()`.
- AI changes now show an inline diff in the code editor.

**Improvements:**

- Many UDF actions can now be done directly inside Canvas.
- Improvements to prompts and automatic suggestions in AI chat.
- You can now stop dictation and send by clicking the send button in AI chat.
- HTML view in Workbench will again show UDF output as it will be embedded.
- AI chat settings page has been visually updated to be easier to follow.
- Added "Fold all code" and "Unfold all code" actions in the search bar, and Workbench will now remember what code blocks were folded in a UDF.
- *Inline AI* and *Auto share on save* are now general preferenecs, rather than experimental.

**Bug Fixes:**

- Provided more informative warning messages in the Python SDK.
- Allowed non-authenticated users to retrieve public UDFs.
- Fixed JSON serialization of GeoDataFrames without geometry columns.
- Fixed `/public` UDF page.
- AI chat will scroll to bottom more consistently.
- AI chat will show tool calls in thinking blocks.
- Backend-informed autocomplete is re-enabled.

**API Changes:**

- Removed deprecated `job` function.
- Made batch job logs more concise.
- Upgraded DuckDB to v1.4.0.

## v1.22.8 (2025-09-11)

**Bug Fixes:**
- Fixed passing a parameter named `url` to `fused.run`.
- Fixed data table filter interactions with overflow and fullscreen, and fixed min/max labels.
- Fixes for the view changes modal and fixed a crash when duplicating UDFs.
- Fixed an issue where large error messages could cause no useful information to appear.
- Fixed an issue where GitHub PR information would not synchronize well.
- Fixed Workbench not showing a UDF was running and improved it to show the elapsed time.

## v1.22.7 (2025-09-09)

**New Features:**

- Experimental: Dataset discovery in AI chat.
- New Kimi K2 model added to AI chat.

**Improvements:**

AI & Chat:
- Improved AI chat window with better error handling and scrollback/redo functionality.
- AI can now fetch datasets via tools and S3 URLs in chat are now clickable.
- Better context management and the context size is now shown.
- Model selector synchronizes with profile model selector.

Workbench:
- Enhanced version control with improved diff viewer and more prominent titles.
- Auto-save UDFs after commit creation and when resolving stale UDFs.
- Enhanced data table with improved copy functionality, date filtering/sorting, and better header responsiveness.
- File UDFs now duplicate instead of showing conflict modals for better UX.
- Better UDF management with streamlined rename handling and duplicate detection.
- Theme fixes and UI improvements across the interface.

Data & Datasets:
- UTF-8 encoding by default for HTML data returns.
- Improved dataset sorting and search functionality.

**Bug Fixes:**

- Fixed serialization errors for UDFs returning interval categories with better error messages.
- Resolved UDF conflict modal issues and improved saving from action buttons.
- Improved UI performance when renaming UDFs, and in the AI chat.
- Corrected UDF icon display on hover in version pages.
- Fixed slider component thumb positioning.
- Fixed various canvas and UI stability issues.

**API Changes:**

- Polling-based UDF execution now enabled by default for `fused.run()`.
- Single serialization output format parameter in UDF run URLs, via `format`. Backward compatibility maintained for `dtype_out_vector/raster` parameters.

## v1.22.6 (2025-09-02)

**New Features:**

Re-Launching udf.ai! Let anyone talk to your data!

Learn how to connect your own data or analysis here

- Experimental: "MCP Creator" AI profile to simplify creation of MCP servers from UDFs. Read more.
- Experimental auto-retry on errors: AI will try fixing the error until it can either fix it or hits its limit. 
- Experimental: Dataset discovery scheduling in unstable environments.
- Experimental: Collection share tokens.

**Improvements:**
- AI Chat: More models, faster inference, better UX and error handling.
- AI now better understands data & has significantly improved ability to make charts especially for larger datasets.

- Workbench: Enhanced version page, improved UDF management flow, better data table filtering.
- Git integration: improved experience when pushing UDFs to GitHub.

**Bug Fixes:**
- AI & UDFs: Prevented AI writing to read-only UDFs, fixed serialization errors, JSON args, and File UDF save states.
- Version Control: Fixed stale UDF updates, staging detection, and backendId preservation.
- UI: Fixed Windows Ctrl+Click, modal flickering, autosave callbacks, and selection issues.
- [Experimental] Canvas: Fixed node persistence.

## v1.22.5 (2025-08-26)

**New Features:**
- Experimental Canvas mode.

**Improvements:**
- Added Deepseek v3.1 model to AI chat.

- AI chat has better layout and indicators for its settings. AI chat also shows user input when asking for more instructions.
- Versions button has moved to the bottom of the Workbench sidebar.

- Versions page shows the difference between upstream and a saved copy of a UDF.
- Versions page shows team repositories first, and will prompt when changing repository to a public one.
- Data table widget has `bigint` support, copy to clipboard, improved headers, and better filter support.

- Added html5lib package to the runtime.

**Bug Fixes:**
- AI chat will not suggest charting very small dataframes, along with other improvements to its suggestions and auto-fixes.
- Fixed bugs with the visualization menu in the map view. It works again and will not crash when no UDF is selected.
- Workbench won't show delete and versions button when they cannot do anything.
- Bug fixes for the conflict modal in Workbench not working or not updating the existing UDF.
- Fixed Workbench profile text for enterprise users.

**API changes:**
- Large jobs can now be run using `fused.run` and the `instance_type` parameter.

**Deprecations:**
- Parameters under the UDF in the advanced UDF editor is deprecated. It can be turned back on from the Workbench Preferences.

## v1.22.1 (2025-08-19)

**Improvements:**
- Inline AI editing is enabled by default.
- Improved data table component filtering.
- `fused.submit` results are cached by default.
- AI chat will now suggest new users to login after first prompt 
- Multiple open tabs message will appear at the top of the page instead of as a modal.

**Bug Fixes:**
- Fixed a bug when loading some HTML UDF outputs that caused them to not load scripts.
- Fixed bugs with AI prompts.
- Fixed bugs with navigating to particular UDFs on the version page.
- You can opt to keep a changed version of a UDF instead of reloading with changes.
- Fixed file explorer for basic tier users.
- Fixed the Add Billing modal appearing when it was not supposed to.
- The UDF selector has a minimum width and is no longer editable.

## v1.22.0 (2025-08-18)

**New Features:**
- Free tier is now open for all. Signing in to Fused now gives you access to a shared, free compute environment with a daily quota.
- New simplified UDF editor layout. The previous UDF editor with map view is still available as the *Advanced UDF editor*.

**Improvements:**
- Added inline AI editing features as an experimental feature.
- Added `@`-sign to select context in AI chat.
- Added profile selection in AI chat.
- Added new models to AI chat.
- AI chat context will understand the results history.
- Many adjustments to AI prompts.
- Table widget has more filter capabilities.
- Many widgets now load via Parquet for faster and better loading.
- UDF list has had many buttons consolidated into the three dot menu in the advanced UDF builder.

**Bug Fixes:**
- Significant improvements to self-service in Workbench.
- Fixed duplicate UDF and conflicting UDF modal bugs.
- Versions page will remember the last selected repository.
- Workbench will prompt to load new versions from GitHub.
- File explorer will show `/mount` instead of `file:///` URL.
- Fixed issues where HTML results would not be rendered with UTF-8 charset.
- Fixed issues where share token may not get cleared in Workbench when duplicating UDFs.
- Fixed bugs with the canvas map widget loading, filtering, and coloring. 
- Improved error messages and `run_*_async` when using `fused` from within Pyodide, for Pyodide-specific considerations.
- `to_fused(overwrite=True)` will delete the original UDF as expected.
- stderr will always be displayed for UDF runs.

**API changes:**
- Renamed `run_remote` to `run_batch`.
- Deprecate `fused.utils`, which has been replaced with `fused.load`.

## v1.21.6 (2025-07-31)

**New Features:**
- Added Qwen 3 to AI chat.

- Added a Table frame type.

**Improvements:**

AI:
- AI chat can be docker on the right or the left.
- You can now specify specific UDFs in AI chat.

Versioning: 
- Version control page will ask to confirm pushing to a different repository.

Rendering:
- HTML (previously *Embed*) view is always available.

**Bug Fixes:**
- Fixed bugs with caching POST requests for some UDFs.
- Fixed bugs with AI chat context.
- Fixed bugs with self service onboarding and billing portal.
- Fixed bugs where Table mode would not open by default in Workbench.
- Fixed bugs with cache_storage.

## v1.21.5 (2025-07-30)

**New Features:**
- Added self service signup for new users!
- Added voice input to udf.ai and to rich text frames.

**Improvements:**

AI Assistant:
- AI chat can now see results printed from a UDF.
- AI chat can now see sample rows returned from a UDF.
- AI chat can now show the code diff with generated code.

General:
- Profile modal now shows usage quota, when applicable.
- `@fused.cache` functions can now be profiled, and now supports object storage.
- Canvas now shows controls on hover.
- Canvas now has an Embed, Table, and Histogram frame types.
- Sped up UDF executions when `cache_max_age=0` was specified.
- Default UDF in Workbench is much simpler.

Canvas:
- Canvas now shows controls on hover.

**Bug Fixes:**
- Fixed saving UDFs where the UDF had previously been deleted on the server.
- Adjusted the default run timeout to match the backend run timeout.
- Fixed `fused.run` with `sync=False`.

## v1.21.4 (2025-07-24)

Minor bug fixes.

## v1.21.3 (2025-07-23)

Launching udf.ai! Ask your data questions, get AI to help you build answers.

**New Features:**
- Added support for speak-to-AI in the AI chat.
- Improved AI chat with one-click prompt suggestions and better tooltips.

**Improvements:**
- Improved color code handling in the editor.
- Moved experimental inline AI features behind feature flag.
- File-opener UDFs moved to the community catalog.

**Bug Fixes:**
- Fixed issues with undo-ing AI chat changes.
- Fixed issues with AI chat disconnections.
- Fixed caching for HTML return values from UDFs.
- Fixed handling of request parameters for UDF runs.

## v1.21.2 (2025-07-21)

**New Features:**
- Added ability to record audio into AI input.
- Added auto-share UDF on save functionality. (a shared token is created on first save for all new UDFs)
- Added compact mode for Canvas view.
- Added connection and save notifications for Canvas.
- Added ability to copy common load commands to clipboard.
- Added cache hit rate display on user profile.

**Improvements:**
- Enhanced AI Assistant with better UI/UX for changes and settings menu.
- Workbench will automatically add a default UDF, and updated the default UDF template.
- String return types will be assumed to be HTML by default.
- Improved version control page performance and styling. Clicking on the changes pending asterisk (`*`) will now go directly to the version control page.
- Improved performance of the GitHub integration.
- Improved saving all UDFs to do so in parallel.
- Enhanced map widget integration in Canvas.
- Shift+Enter will turn on the currently selected UDF.
- Changed default for `fused.ingest` to `target_num_chunks=500`.

**Bug Fixes:**
- Fixed issues with `fused.run` behaving differently for saved UDFs.
- Fixed issues with new apps not showing changes indicator
- Fixed selected preview tab in UDF builder flickering.
- Fixed preview image uploads during GitHub pushes.
- Fixed user assignment issues on GitHub PR creation.
- Fixed an issue with linking to GitHub accounts.
- Fixed an issue with tailing batch job logs.
- Fixed `fused.__version__` not being populated correctly.

## v1.21.1 (2025-07-14)

**`fused-py`**

- Updated LanceDB to 0.24.1.
- Fixed issues starting batch jobs.

**Workbench**

- AI Assistant now sees all UDFs in Workbench & has access to updated Fused documentation
- App builder now integrates with the new version control page.
- Fixed scrolling on the version control page.
- Fixed saving and layout issues on the Canvas view.

## v1.21.0 (2025-07-11)

**`fused-py`**

New Features:
- Added `fused.api.resolve`.
- Added `fused.api.team_info`.
- IPython magics now load automatically.
- When running a large (batch) job, it is now possible to specify the job's name.
- Upgraded `xarray` to 2025.4.0, DuckDB to 1.3.2, and H3 to 4.3.0.
- The `fd://` filesystem scheme will automatically be registered with `fsspec`.
- UDFs that return HTML will be loaded as `str` objects from `fused.run`.
- UDFs saved on Fused server now have a `catalog_url` property to get the Workbench UDF URL.
- `npy` output format is now supported for numpy (raster) return values.
- Arrow-compatible return values are now accepted.
- `fused.cache` will detect changes to referenced UDFs.
- `fused.run` accepts `verbose` keyword.

Bug Fixes:
- Fixed bugs with `udf.render` on some IPython versions.
- Fixed bugs with `repr`s for access tokens and UDFs.
- Fixed calling `fused.run` with UDF objects and `sync=False`.
- Renamed the UDF class to `Udf`.
- Removed some unused and deprecated code.
- Fixed bugs with `fused.cache` showing as not found, having stale files, or not passing arguments through.
- Removed the `n` keyword argument from `get_udfs` and `get_apps`.
- Fixed bugs with HEAD requests to UDF endpoint.
- `fused.submit` will warn about conflicting arguments.
- `/tmp/` size has been increased in realtime instances.
- `fused.api.list` and related functions supports `/mount` paths.
- Improved the performance of GitHub sync.
- Changed defaults for `JobPool.cancel` and fixed a bug where it would continue to retry.
- Fixed bugs with `JobPool.df` and UDF runs that result in exceptions.
- Fixed encoding URL paths in `fused.api.download` and related functions.

**Workbench**

New Features:
- Added AI editing and AI chat capabilities in the UDF builder.
- Workbench now has a code profiler: each line of code will show its execution runtime
- Added new Canvas dashboard builder mode (experimental).
- Added new Table data view mode.
- GitHub integration has a new page and is no longer beta.
- GitHub integration remembers relevant open PRs better.
- Fused apps uses a newer version of Streamlit and Stlite.
- Added a menu item to take a screenshot in higher-than-screen resolution.
- Added type-to-filter in File Explorer.
- File Explorer can be browsed without logging in.
- File Explorer now shows a summary of the current directory.
- File preview UDFs can now be specified with regular expressions.
- Cmd+Click on shared tokens will now take you to the UDF catalog.

Bug Fixes:
- Results panel shows when memory usage is unknown.
- Share code is more consistent with the selected output format.
- Fixed bugs with read-only app UI.
- The large data warning will now show in File Explorer as well when applicable.
- Fixed bugs with app/UDF catalog layout and sorting.
- Adjusted the UI for visualization settings and parameters in the UDF list.
- Reordered menu items in File Explorer.
- Cursor position will be remembered when switching between UDFs.
- Share tokens that are already shown on the page are no longer redacted.
- Map tooltip can now be scrolled.

## v1.20.1 (2025-06-09)

**`fused-py`**

New Features:
- Added `fused.api.resolve` and `fused.api.team_info`.
- IPython magics will automatically be loaded when importing `fused`.
- `run_batch` (batch jobs) can now accept a job name.

Bug Fixes:
- Fixed the `render` method of UDF objects.
- Fixed access tokens for apps being rendered in IPython.
- Fixed calling `fused.run(udf, sync=False)` with UDF objects.
- Removed some deprecated fields and arguments.

**Workbench**

- Workbench will now show the amount of time taken in UDF functions as a heatmap.
- Memory bars in the Results panel will show when usage is unknown.
- Update how shared token URLs are generated for different output formats.
- Updated stlite to 0.82.0.

## v1.20.0 (2025-06-03)

**`fused-py`**

New Features:
- Running a UDF with engine `local` will cache similarly to how it would when running on `remote` (supporting `cache_max_age` to control the caching).
- Large (batch) jobs will be in a Pending state if they cannot start immediately.
- UDFs can now accept `**kwargs` parameters, which will always be passed in as strings.
- `@fused.cache` has a new `cache_verbose` option. If set to `True` (default), it prints a message when a cached result is returned.
- `@fused.cache` renamed the `reset` parameter to `cache_reset`. The existing `reset` parameter is deprecated.
- Some file listing APIs like `fused.api.list` will work for public buckets when on free accounts.
- `fused.load` accepts `import_globals` (default `True`) for controlling importing UDF globals. Also, when globals cannot be imported, a warning is emitted instead of an exception.

Bug Fixes:
- Clarified login-needed message in Fused Apps.
- Fixed bugs with `@fused.cache` results not being ready.
- Fixed bugs with `@fused.cache` not detecting changes in the cached function.
- Loading a UDF from a file will autodetect the UDF function name.
- Fixed bugs with returning GeoDataFrames that do not contain geometry.
- Fixed calling `to_fused` on an app.

**Workbench**

- A message will appear above the UDF body when parameters are set in the UDF list.
- A lock icon will be shown next to read-only UDFs and Apps in Workbench.
- When pushing UDFs to GitHub, the preview image will be pushed to a public URL so that the README in GitHub is rendered correctly.
- When pushing UDFs to GitHub, Workbench will assign the PR to you if possible.
- It is now possible to delete UDFs and Apps directly from the catalog.
- Added a "Reload Collection" button. This pulls all latest version of UDF currently in your Collection.
- Workbench will minimize more changes from the PRs it creates on GitHub.
- "Open in Kepler.gl" supports H3 (string) data.
- The visibility button for a Fused App will now reset the app.
- A new Reset 3D View button is added to the UDF Builder map, and the keyboard shortcut has been updated to `Cmd+Shift+UpArrow` on MacOS (`Ctrl+Shift_UpArrow` on Windows / Linux).
- Workbench will show the current environment name above the map by default.
- Workbench will remember which UDF was selected when reopening the page.
- Adjusted which UDF mode label is shown when automatically detecting the UDF mode.
- Fixed some bugs with dynamic output mode.
- UI updates for the Pull Changes (history) and Push Changes views, including showing the README file in both views.
- Drag&Drop UDF into Workbench now works on the entire tab
- Added a button to download usage table in the Profile view.
- Fixed some visual bugs with light mode.
- File Explorer will no longer show file opener UDFs saved on your personal catalog, and will clarify when file opener UDFs are from your team catalog.

## v1.19.0 (2025-05-19)

**`fused-py`**

Breaking changes:
- Large (batch) jobs have been updated to pass parameters into the UDF the same way as other UDF runs. For compatibility, if the parameters passed into the job do not correspond with the parameters, a dictionary parameter is passed into the UDF instead. This will be deprecated and removed in a future release.
- The `context` and `bbox` parameters to a UDF are no longer treated as special.
- Python 3.9 support, which was previously deprecated, is now removed. The minimum Python version for the `fused` package is now 3.10.

New Features:
- PyArrow upgraded to version 16.0.
- Ingestion now supports input files without extensions, and filters out files with `.` or `_` prefix.
- `cache=False` is now a shortcut for disabling cache, e.g. `cache_max_age=0`.
- UDFs now support parameters annotated as `shapely.Geometry` or `shapely.Polygon`.
- `fused.load` now support loading UDFs from Github Pull Request URLs.
- Added a timeout parameter for `fused.api.upload`.
- Fused API functions now support `/mount` without `file://` prefix.
- `fused.api.download` now supports downloading files from `/mount`.
- `fused.api.list` now supports listing an individual file under `/mount`.
- `max_deletion_depth` in `fused.api.delete` default changed from 2 to 3.

Bug Fixes
- Fixed pickling UDF objects.
- Fixed UDF equality checks not conforming to Python specifications.
- Many fixes to ensure compatibility between the `fused` module available on PyPI and the `fused` module within the Fused backend.
- Fixed returning `pd.Timestamp` objects.
- Fixed bugs with handling of stdout if the UDF is async.
- Fixed bugs with UDF object `repr` in Jupyter.
- Fixed `@fused.cache` in Fused Apps.
- Fixed "multiple auth mechanisms" error when retrieving job results.
- Fixed deserialization of GeoDataFrame without geometry column.
- Fixed cases where UDFs would be indented when run.
- Various stability and performance updates, including new self-healing capabilities on the Fused backend.

**Workbench**

- Upgraded Streamlit in apps to 1.44.
- Fused Apps is no longer "beta".
- Fused Apps will now highlight syntax errors.
- Fused Apps will now autocomplete the `fused` module correctly.
- "Changes pending" in the map has been renamed to "Running" and now shows the time elapsed without needing to hover over it.
- UDF builder tooltips have been refreshed, it is now possible to click on data on the map to pin the tooltip on screen. Pinned tooltips show how many data records were under the mouse and allow paging through them.
- Workbench can now highlight H3 hexagons on click.
- Workbench can now detect decimal H3 indexes and Cmd+Click works on them.
- `udf://` URLs in Workbench now entirely overwrite parameters of the selected UDF.
- Fixed UDF builder showing partially updated map states for Tile UDFs.
- Collections catalog can now be sorted.
- Fixed a bug where Workbench would not detect newly added utils on UDFs.
- Parameters and Visualization sections are now styled slightly differently to make it easier to pick out your UDFs.
- Renamed Visualization "Surprise me" button to "Preset".
- Fixed a visual bug with the job status.
- Updated the share UDF and share app pages.
- Fixed bugs with UDF or app catalog showing the wrong content.
- Renamed "History" to "Pull Changes" and updated styling of that page.
- Fixed Pull Changes not showing diffs if utils had been added or deleted.
- Fixed Workbench showing the original Github link for forked UDFs.
- Workbench code editors will now remember scroll position.
- Clicking the viewport location label will now copy it.

## v1.18.0 (2025-04-28)

**`fused-py`**

Breaking changes:
- `fused.submit` now raises an error by default, if there is any run erroring

New Features:
- It is now possible set the cache storage location with `@fused.cache(storage=...)`.
- `@fused.cache` can now exclude arguments from the cache key.
- `@fused.cache` uses Pandas' own way of hashing DataFrames.
- Added storage argument to `fused.file_path(storage=...)`.
- Large jobs now pick up AWS credentials more consistently.
- Auth redirect can dynamically select the port when logging in locally.
- `udf.to_fused` will show the diff when UDF name conflicts with an existing UDF.
- `fused.load` can load by the unique UDF ID again.
- UDFs can be run by username and UDF name in addition to email and UDF name. (ex: `fused.run(user@team.com/my_cool_udf)`). 
- Preview images can now be specified in-directory in Github.
- Adjusted UDF caching behavior for performance.

Bug Fixes
- Fixed behavior when loading and running UDFs with code outside of the UDF function.
- Fixed `fused.api.list` being incompatible with some async stacks.
- Fixed a bug where strings inside UDFs would get extra spaces added to them.
- Shared tokens can be created by a team account.
- Fixed a bug that could occur where Fused would try to duplicate index column names of the returned DataFrame.
- Fixed various bugs when a UDF closes `stdout`.
- Fixed a bug where `fused.run` would not return printed messages from a UDF.
- Fixed a bug where `fused.load` would crash on very large strings.
- Fixed various bugs with exporting UDFs from within fused-py.
- Fixed a bug where `partitioning_schema_input` would not be found when ingesting.
- Fixed a bug where UDFs might import incorrectly when several pushes happen in quick succession in a linked Github repo.

**Workbench**

- `File (Viewport)` renamed to `Single (Viewport)`.
- Added `Single (Parameter)` UDF type that behaves like `Single (Viewport)` but does not pass the viewport bounds.
- File Explorer will show a per-user home favourite.
- Your UDF list will now sync across different browser tabs.
- Preference toggles are added to the Command Palette.
- The share page has been redesigned.
- Collections catalog page now shows the UDF in a given Collection by hovering over them.
- When adding a duplicate UDF to Workbench, you will be prompted to duplicate or replace it.
- Memory usage for UDFs can be found in the results panel (once displaying memory usage was been turned on in Preferences)
- Workbench will indicate that UDF runs were cached in all circumstances.
- The public map page is now compatible with any public-readable shared token.
- Added `udf://name?param=value` URL support to Workbench.
- Reduced the size of metadata diffs generated by Workbench when pushing to Github.
- Various performance improvements.
- Fixes for various UI layout bugs.

## v1.17.0 (2025-04-10)

**`fused-py`**

- Team UDFs can be loaded or run by specifying the name "team", as in: `fused.load("team/udf_name")`
- `Udf.to_fused` supports overwriting the UDF when saving.
- Added `fused.api.enable_gcs()` to configure using the Google Cloud Platform secret specified in Fused secret manager.
- `@fused.cache` locking mechanism has changed and will not allow multiple concurrent runs.
- Upgraded DuckDB to v1.2.2.
- Running a saved UDF by token or name will now also show the logs, including print statements and error tracebacks.
- All functions interacting with the Fused server will now retry automatically, by default 3 times.
- Python 3.9 support is deprecated. The next release of `fused` will require Python 3.10+.
- Deprecated `fused_batch` module is removed.

**Workbench**

- Cached UDF runs will show the original logs.
- "Change output parameters" in the Share UDF screen shows all detected parameters.
- Added a copy viewport bounds button in the Results panel.
- Improved the performance of the catalog screen.
- Fixed the job page showing times in inconsistent time zones.

App Builder:
- Deprecated `fused_app` module is removed.

## v1.16.3 (2025-04-03)

**`fused-py`**

- It is now possible to return general `list`s and `tuple`s from UDFs. (Note: a tuple of a raster and bounds will be treated as a raster return type.)

**Workbench**

- Workbench will now prompt you when loading large UDF results that could slow down or overwhelm your browser. The threshold for this prompt is configurable in your Workbench preferences.
- Fixed bugs with loading large UDF results.
- UDF list will show an error if a UDF has an empty name.
- Fixed running some public UDFs in Workbench.

## v1.16.2 (2025-04-01)

**`fused-py`**

- It is now possible to return dictionaries of objects from a UDF, for example a dictionary of a raster numpy array, a DataFrame, and a string.
- Whitespace in a UDF will be considered as changes when determining whether to return cached data. (a UDF with different whitespace will be rerun rather than cached)
- Fixed calling `fused.run` in large jobs.

**Workbench**

- Added experimental AI agent builder.
- Workbench will now prompt you to replace an existing UDF when adding the same UDF (by name) from the catalog.
- Added ability to download & upload an entire collection.
- Fixed saving collections with empty names.

Visualization:
- Added an H3-only visualization preset.
- Fixed a bug where changing TileLayer visualization type could result in a crash.

App Builder:
- Updated the runtime.

## v1.16.0 (2025-03-27)

**`fused-py`**

- The result object of running in `batch` now has `logs_url` property.
- Fixed `fused.submit` raising an error if some run failed.

**Workbench**

- Added a Download UDFs button for downloading an entire collection.
- Results will show a message at the top if UDF execution was cached.
- Non-visible UDFs will have a different highlight color on them in the UDF list.
- Collections will show as modified if the order of UDFs has been changed.
- Fixes for Collections saving the ordering and visibility of UDFs.
- Fixed the Team Jobs page in Workbench crashing in some cases.

**Shared tokens**

- Shared token URLs can be called with an arbitrary (ignored) file extension in the URL.

## v1.15.0 (2025-03-20)

**`fused-py`**

- Loading UDFs now behaves like importing a Python module, and attributes defined on the UDF can be accessed.
- The `fused.submit()` keyword `wait_on_result` has been renamed to `collect`, with a default of `collect=True` returning the collected results (pass `collect=False` to get the JobPool object to inspect individual results).
- New UDFs default to using `fused.types.Bounds`.
- Upgraded `duckdb` to v1.2.1.
- UDFs can now return simple types like `str`, `int`, `float`, `bool`, and so on.
- Files in `/mount/` can be listed through the API.
- UDFs from publicly accessible GitHub repositories can be loaded through `fused.load`.
- `fused.load` now supports loading a UDF from a local .py file or directory
- The `x`, `y` and `z` aren't protected arguments when running a UDF anymore (previously protected to pass X/Y/Z mercantile tiles).

**Workbench**

New:
- Added a new account page and redesigned preferences page.
- You can now customize the code formatter settings (available under Preferences > Editor preferences).
- UDFs can optionally be shared with their code when creating a share token.

General:
- Moved shared token page to bottom left bar, and adjusted the icons.
- The ordering of UDFs in collections is now saved.

App Builder:
- Updated app list UI.
- Fixed bugs with shared apps showing the wrong URL in the browser.

## v1.14.0 (2025-02-25)

v1.14.0 introduces a lot of new changes across `fused-py` and Workbench

**`fused-py`**

- Introducing `fused.submit()` method for multiple job run
- Improvement to UDF caching
    - All UDFs are now cached for 90 days by default
    - Ability to customize the age of cached data & UDFs with the new `cache_max_age` argument when defining UDFs, running UDFs or when caching regular Python functions
- `pandas` & `geopandas` are now optional for running non-spatial UDF locally
- Removed hardcoded `nodata=0` value for serializing raster data

**Workbench**

New:
- Introducing Collections to organize & aggregate UDFs together
- Redesigned "Share" button & page: All the info you need to share your UDFs to your team or the world

General:
- Improvements to Navigation in Command Pallette. Try it out in Workbench by doing `Cmd + K` (`Ctrl + K` on Windows / Linux)
- Autocomplete now works with `Tab` in Code Editor with `Tab`
- Added a Delete Button in the Shared Tokens page (under Account page)
- Ability to upload images for UDF Preview in Settings Page
- Adding ‚ÄúFullscreen‚Äù toggle in Map View
- Improved `colorContinuous` in Visualize Tab
- Allowing users to configure public/team access scopes for share tokens 
- No longer able to edit UDF & App name in read-only mode
- Fixing job loading logs

File Explorer:
- Download directories as `zip`
- Adding favorites to file path input search results 
- Ability to open `.parquet` files with Kepler.gl

## v1.13.0 (2025-01-22)

- Fixed shared UDFs not respecting the Cache Enabled setting.
- Added a cache TTL (time-to-live) setting when running a UDF via a shared token endpoint.
- Tags you or your team have already used will be suggested when editing a UDF's tags.
- Team UDFs will be shown as read-only in Workbench, similar to Public UDFs.
- File Explorer shows deletion in progress.
- File Explorer can accept more S3 URLs, and uses `/mount/` instead of `/mnt/cache`.
- UDF Builder will no longer select a UDF when clicking to hide it.
- Fixed how Push to Github chooses the directory within a repository to push to.
- Fixed the browser location bar in Workbench updating on a delay.
- Fixed writing Shapefile or GPKG files to S3.
- (Beta) New fusedio/apps repository for public Fused Apps.
- Navigating to Team UDFs or Saved UDFs in the UDF Catalog will now prompt for login.
- Fixed the "Select..." environment button in Workbench settings.
- UDF Builder will no longer replace all unaccepted characters with `_` (underscore).
- Fixed loading team UDFs when running a UDF with a shared token.
- Batch jobs that use `print` will now have that output appear in the job logs.
- Apps in the shared token list show an app icon.
- Removed some deprecated batch job options.
- Installed `vega-datasets` package.

## v1.12.0 (2025-01-10)

- (Beta) Added an App catalog in Workbench, and a new type of URL for sharing apps.
- Added `/mount` as an alias for `/mnt/cache`.
- More consistently coerce the type of inputs to UDFs.
- Added more visualization presets to UDF builder in Workbench.
- Fixed an issue where the tab icon in Workbench could unintentionally change.
- Fixed bugs in Workbench File Explorer for `/mnt/cache` when browsing directories with many files.
- Fixed bugs in `fused` Python API not being able to list all files that should be accessible.
- Fixed bugs in the Github integration, command palette, and file explorer in Workbench.
- Fixed bugs in caching some UDF outputs.
- The shareable URL for public and community UDFs will now show in the settings tab for those UDFs.
- UDFs can customize their data return with `Response` objects.

## v1.11.9 (2024-12-19)

- Accounts now have a *handle* assigned to them, which can be used when loading UDFs and pushing to community UDFs
- Account handle can be changed once by the user (for more changes please contact the Fused team.)
- Added a command palette to the Workbench, which can be opened with Cmd-k or Ctrl-k.
- When creating a PR for a community UDF or to update a public UDF, it will be under your account if you log in to Fused with Github.
- Bug fixes for pushing to Github, e.g. when pushing a saved UDF, and for listing the Fused bot account as an author.
- Batch (`run_batch`) jobs can call back to the Fused API.
- Team UDFs can be pinned to the end of the featured list.
- Speed improvements in ingestion.
- Ingestion will detect `.pq` files as Parquet.
- Format code shortcut in Workbench is shown in the keyboard shortcut list and command palette.
- Workbench will hide the map tooltip when dragging the map by default.
- Workbench will now look for a `hexLayer` visualization preset for tabular results that do not contain `geometry`.
- Workbench file explorer can now handle larger lists of files.
- Fix for browsing disk cache (`/mnt/cache`) in Workbench file explorer.
- Teams with multiple realtime instances can now set one as their default.
- Fix for saving UDFs with certain names. Workbench will show more descriptive error messages in more cases for issues saving UDFs.

## v1.11.8 (2024-12-04)

- New File Explorer interface, with support for managing Google Cloud Storage (GCS) and `/mnt/cache` files.
- Workbench will show an error when trying to save a UDF with a duplicate name.
- Fixed a few bugs with Github integration, including the wrong repository being selected by default when creating a PR.
- Updated `fsspec` and `pyogrio` packages.

## v1.11.7 (2024-11-27)

- Decluttered the interface on mobile browsers by default.
- Fixed redo (Cmd-Shift-z or Ctrl-Shift-z) sometimes being bound to the wrong key.
- Tweaked the logic for showing the selected object in Workbench.

## v1.11.6 (2024-11-26)

- Added Format with Black (Alt+Shift+f) to Workbench.
- Fix the CRS of DataFrame's returned by get_chunk_from_table.
- Added a human readable ID to batch jobs.
- Fused will send an email when a batch job finishes.
- Fix for opening larger files in Kepler.gl.
- Fix for accessing UDFs in a team.
- Improved messages for UDF recursion, UDF geometry arguments, and returning geometry columns.
- Adjusted the UDF list styling and behavior in Workbench.
- Fix for secrets in shared tokens.

## v1.11.5 (2024-11-20)

- Show message for keyword arguments in UDFs that are reserved.
- Added reset kernel button.
- Workbench layers apply visualization changes immediately when the map is paused.
- Show the user that started a job for the team jobs list.
- Fix for running nested UDFs with utils modules.
- Fix for returning xarray results from UDFs.
- Fix for listing files from within UDFs.
- Upgraded to GeoPandas v1.

## v1.8.0 (2024-06-25) :package:

- Added Workbench tour for first-time users.
- Undo history is now saved across UDFs and persists through reloads.
- Added autocomplete when writing UDFs in Workbench.
- Added `colorBins`, `colorCategories`, and `colorContinuous` functions to Workbench's Visualize tab.
- Migrated SDK to Pydantic v2 for improved data validation and serialization.
- Fixed a bug causing NumPy dependency conflicts.

## v1.7.0 (2024-06-04) :bird:

- Execution infrastructure updates.
- Update DuckDB package to v1.0.0.
- Improve responsivity of Workbench allotments.
- Crispen Workbench UI.

## v1.6.1 (2024-05-06) :guardsman:

_GitHub integration_

- Updates to team GitHub integration.
- Users are now able to create shared UDF token from a team UDF both in Workbench and Python SDK.

## v1.6.0 (2024-04-30) :checkered_flag:

- The Workbench file explorer now shows UDFs contributed by community members.
- Team admins can now set up a GitHub repository with UDFs that their team members can access from Workbench.

## v1.5.4 (2024-04-15) :telescope:

- Button to open slice of data in Kepler.gl.
- Minor UI design and button placement updates.

## v1.5.3 (2024-04-08) :duck:

- Improved compatibility with DuckDB requesting data from shared UDFs.
- Geocoder in Workbench now supports coordinates and H3 cell IDs.
- GeoDataFrame arguments to UDFs can be passed as bounding boxes.
- The package ibis was upgraded to 8.0.0.
- Utils modules no longer need to import fused.

## v1.5.2 (2024-04-01) :tanabata_tree:

- File browser can now preview images like TIFFs, JPEGs, PNGs, and more.
- Users can now open Parquet files with DuckDB directly from the file browser.

## v1.5.0 (2024-03-25) :open_file_folder:

- The upload view in Workbench now shows a file browser.
- Users can now preview files in the file browser using a default UDF.

## v1.4.1 (2024-03-19) :speech_balloon:

- UDFs now support typed function annotations.
- Introduced special types  `fused.types.TileXYZ`, `fused.types.TileGDF`, `fused.types.Bbox`.
- Workbench now autodetects Tile or File outputs based on typing.
- Added button to Workbench to autodetect UDF parameters based on typing.

## v1.1.1 (2024-01-17) :dizzy:

- Renamed `fused.utils.run_realtime` and `fused.utils.run_realtime_xyz` to `fused.utils.run_file` amd `fused.utils.run_tile`.
- Removed `fused.utils.run_once`.

## v1.1.0 (2024-01-08) :rocket:

- Added functions to run the UDFs realtime.

## v1.1.0-rc2 (2023-12-11) :bug:

- Added `fused.utils.get_chunk_from_table`.
- Fixed bugs in loading and saving UDFs with custom metadata and headers.

## v1.1.0-rc0 (2023-11-29) :cloud:

- Added cloud load and save UDFs.
- `target_num_files` is replaced by `target_num_chunks` in the ingest API.
- Standardize how a decorator's headers are preprocesses to set `source_code` key.
- Fixed a bug loading UDFs from a job.

## v1.0.3 (2023-11-7) :sweat_drops:

_Getting chunks_

- Added `fused.utils.get_chunks_metadata` to get the metadata GeoDataFrame for a table.
- `run_local` now passes a copy of the input data into the UDF, to avoid accidentally persisting state between runs.
- `instance_type` is now shown in more places for running jobs.
- Fixed a bug where `render()`ing UDFs could get cut off.
- Fixed a bug with defining a UDF that contained an internal `@contextmanager`.

## v1.0.2 (2023-10-26) :up:

_Uploading files_

- Added `fused.upload` for uploading files to Fused storage.
- Added a warning for UDF parameter names that can cause issues.
- Fixed some dependency validation checks incorrectly failing on built-in modules.

## v1.0.1 (2023-10-19) :ant:

- Added `ignore_chunk_error` flag to jobs.
- Added warning when sidecar table names are specified but no matching table URL is provided.
- Fixed reading chunks when sidecars are requested but no sidecar file is present.
- Upgraded a dependency that was blocking installation on Colab.

## v1.0.0 (2023-10-13) :ship:

_Shipping dependencies_

- Added `image_name` to `run_batch` for customizing the set of dependencies used.
- Added `fused.delete` for deleting files or tables.
- Renamed `output_main` and `output_fused` to `output` and `output_metadata` respectively in ingestion jobs.
- Adjusted the default instance type for `run_batch`.
- Fixed `get_dataframe` sometimes failing.
- Improved tab completion for `fused.options` and added a repr.
- Fixed a bug where more version migration messages were printed.
- Fixed a bug when saving `fused.options`.

================================================================================

## index
Path: python-sdk/index.mdx
URL: https://docs.fused.io/python-sdk

# Python SDK

> The latest version of `fused-py` is <FusedVersionLive />.

## Documentation overview

    Installing `fused` is required if you're running `fused` on your end (locally or in a development environment). If you're working in Workbench UDF Builder or App Builder `fused` is already installed for you.

1. Set up a Python environment:

We're using `venv` but you could use `conda` or any other environment manager in Python. 

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install the `fused` package:

You can only install the base package, though we recommend still adding the optional dependencies:
```bash
pip install fused
```

Or install with optional dependencies:
```bash
# For raster data processing
pip install "fused[raster]"

# For vector data processing
pip install "fused[vector]"

# Install all optional dependencies
pip install "fused[all]"
```

</details>

### Authenticate

The first time you use Fused you'll need to authenticate.

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the URL in your browser to authenticate.

## Basic API usage

Some basic examples of how to use `fused` to run UDFs:

#### Hello World UDF

```python

@fused.udf
def udf(x: int = 1):
    return f"Hello world "

fused.run(udf)
```

```bash
>> Hello world 2
```

#### Simple data UDF

```python

@fused.udf
def udf(x: int = 3):

    return pd.DataFrame()

fused.run(udf)
```

```bash
>>   | x |
|---|---|
| 0 | 0 |
| 1 | 1 |
| 2 | 2 |
```

###

================================================================================

## Top-Level Functions
Path: python-sdk/top-level-functions.mdx
URL: https://docs.fused.io/python-sdk/top-level-functions

## @fused.udf

```python
udf(
    fn: Optional[Callable] = None,
    *,
    name: Optional[str] = None,
    cache_max_age: Optional[str] = None,
    instance_type: Optional[str] = None,
    default_parameters: Optional[Dict[str, Any]] = None,
    headers: Optional[Sequence[Union[str, Header]]] = None,
    **kwargs: dict[str, Any]
) -> Callable[..., Udf]
```

A decorator that transforms a function into a Fused UDF.

**Parameters:**

- **name** (<code>Optional[str]</code>) ‚Äì The name of the UDF object. Defaults to the name of the function.

- **cache_max_age** (<code>Optional[str]</code>) ‚Äì The maximum age when returning a result from the cache.

- **instance_type** (<code>Optional[str]</code>) ‚Äì The default type of instance to use for remote execution
  ('realtime' or 'batch').

- **default_parameters** (<code>Optional\[Dict[str, Any]\]</code>) ‚Äì Parameters to embed in the UDF object, separately from the arguments
  list of the function. Defaults to None for empty parameters.

- **headers** (<code>Optional\[Sequence\[Union[str, Header]\]\]</code>) ‚Äì A list of files to include as modules when running the UDF. For example,
  when specifying `headers=['my_header.py']`, inside the UDF function it may be
  referenced as:

  ```py

  my_header.my_function()
  ```

  Defaults to None for no headers.

**Returns:**

- <code>Callable\..., [Udf\]</code> ‚Äì A callable that represents the transformed UDF. This callable can be used
- <code>Callable\..., [Udf\]</code> ‚Äì within GeoPandas workflows to apply the defined operation on geospatial data.

**Examples:**

To create a simple UDF that calls a utility function to calculate the area of geometries in a GeoDataFrame:

```py
@fused.udf
def udf(bbox, table_path="s3://fused-asset/infra/building_msft_us"):
    ...
    gdf = table_to_tile(bbox, table=table_path)
    return gdf
```

---

## @fused.cache

```python
cache(
    func: Callable[..., Any] | None = None,
    cache_max_age: str | int = DEFAULT_CACHE_MAX_AGE,
    cache_folder_path: str = "tmp",
    concurrent_lock_timeout: str | int = 120,
    cache_reset: bool | None = None,
    cache_storage: StorageStr | None = None,
    cache_key_exclude: Iterable[str] = None,
    cache_verbose: bool | None = None,
    **kwargs: Any
) -> Callable[..., Any]
```

Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function
to cache its return values. The cache behavior can be customized through
keyword arguments.

**Parameters:**

- **func** (<code>Callable</code>) ‚Äì The function to be decorated. If None, this
  returns a partial decorator with the passed keyword arguments.
- **cache_max_age** (<code>str | int</code>) ‚Äì A string with a numbered component and units. Supported units are seconds (s), minutes (m), hours (h), and
  days (d) (e.g. "48h", "10s", etc.).
- **cache_folder_path** (<code>str</code>) ‚Äì Folder to append to the configured cache directory.
- **concurrent_lock_timeout** (<code>str | int</code>) ‚Äì Max amount of time in seconds for subsequent concurrent calls to wait for a previous
  concurrent call to finish execution and to write the cache file.
- **cache_reset** (<code>bool | None</code>) ‚Äì Ignore `cache_max_age` and overwrite cached result.
- **cache_storage** (<code>StorageStr | None</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.
- **cache_key_exclude** (<code>Iterable[str]</code>) ‚Äì An iterable of parameter names to exclude from the cache key calculation. Useful for
  arguments that do not affect the result of the function and could cause unintended cache expiry (e.g.
  database connection objects)
- **cache_verbose** (<code>bool | None</code>) ‚Äì Print a message when a cached result is returned

Returns:
Callable: A decorator that, when applied to a function, caches its
return values according to the specified keyword arguments.

**Examples:**

Use the `@cache` decorator to cache the return value of a function in a custom path.

```py
@cache(path="/tmp/custom_path/")
def expensive_function():
    # Function implementation goes here
    return result
```

If the output of a cached function changes, for example if remote data is modified,
it can be reset by running the function with the `cache_reset` keyword argument. Afterward,
the argument can be cleared.

```py
@cache(path="/tmp/custom_path/", cache_reset=True)
def expensive_function():
    # Function implementation goes here
    return result
```

---

## fused.load

```python
load(
    url_or_udf: Union[str, Path],
    /,
    *,
    cache_key: Any = None,
    import_globals: bool = True,
) -> AnyBaseUdf
```

Loads a UDF from various sources including GitHub URLs,
and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused
platform-specific identifier composed of an email and UDF name. It intelligently
determines the source type based on the format of the input and retrieves the UDF
accordingly.

**Parameters:**

- **url_or_udf** (<code>Union[str, Path]</code>) ‚Äì A string representing the location of the UDF, or the raw code of the UDF.
  The location can be a GitHub URL starting with "https://github.com",
  a Fused platform-specific identifier in the format "email/udf_name",
  or a local file path pointing to a Python file.
- **cache_key** (<code>Any</code>) ‚Äì An optional key used for caching the loaded UDF. If provided, the function
  will attempt to load the UDF from cache using this key before attempting to
  load it from the specified source. Defaults to None, indicating no caching.
- **import_globals** (<code>bool</code>) ‚Äì Expose the globals defined in the UDF's context as attributes on the UDF object (default True).
  This requires executing the code of the UDF. To globally configure this behavior, use `fused.options.never_import`.

**Returns:**

- **AnyBaseUdf** (<code>AnyBaseUdf</code>) ‚Äì An instance of the loaded UDF.

**Raises:**

- <code>ValueError</code> ‚Äì If the URL or Fused platform-specific identifier format is incorrect or
  cannot be parsed.
- <code>Exception</code> ‚Äì For errors related to network issues, file access permissions, or other
  unforeseen errors during the loading process.

**Examples:**

Load a UDF from a GitHub URL:

```py
udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
```

Load a UDF using a Fused platform-specific identifier:

```py
udf = fused.load("username@fused.io/REM_with_HyRiver")
```

---

## fused.run

```python
run(
    udf: Union[str, None, UdfJobStepConfig, Udf, UdfAccessToken] = None,
    *,
    x: Optional[int] = None,
    y: Optional[int] = None,
    z: Optional[int] = None,
    sync: bool = True,
    engine: Optional[Literal["remote", "local"]] = None,
    instance_type: Optional[InstanceType] = None,
    type: Optional[Literal["tile", "file"]] = None,
    max_retry: int = 0,
    cache_max_age: Optional[str] = None,
    cache: bool = True,
    parameters: Optional[Dict[str, Any]] = None,
    _return_response: Optional[bool] = False,
    _ignore_unknown_arguments: bool = False,
    _cancel_callback: Callable[[], bool] | None = None,
    **kw_parameters: Callable[[], bool] | None
) -> Union[
    ResultType,
    Coroutine[ResultType, None, None],
    UdfEvaluationResult,
    Coroutine[UdfEvaluationResult, None, None],
]
```

Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote),
with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and
allows for both synchronous and asynchronous execution. It dynamically determines the execution
path based on the provided parameters.

**Parameters:**

- **udf** (<code>str, Udf or UdfJobStepConfig</code>) ‚Äì the UDF to execute.
  The UDF can be specified in several ways:
  - A string representing a UDF name or UDF shared token.
  - A UDF object.
  - A UdfJobStepConfig object for detailed execution configuration.
- **x, y, z** (<code>int</code>) ‚Äì Tile coordinates for tile-based UDF execution.
- **sync** (<code>bool</code>) ‚Äì If True, execute the UDF synchronously. If False, execute asynchronously.
- **engine** (<code>Optional\[Literal['remote', 'local']\]</code>) ‚Äì The execution engine to use ('remote' or 'local').
- **instance_type** (<code>Optional[InstanceType]</code>) ‚Äì The type of instance to use for remote execution ('realtime',
  or 'small', 'medium', 'large' or one of the whitelisted instance types).
  If not specified, gets the default from the UDF (if specified in the
  `@fused.udf()` decorator, and the UDF is not run as a shared token),
  or otherwise defaults to 'realtime'.
- **type** (<code>Optional\[Literal['tile', 'file']\]</code>) ‚Äì The type of UDF execution ('tile' or 'file').
- **max_retry** (<code>int</code>) ‚Äì The maximum number of retries to attempt if the UDF fails.
  By default does not retry.
- **cache_max_age** (<code>Optional[str]</code>) ‚Äì The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. ‚Äú48h‚Äù, ‚Äú10s‚Äù, etc.).
  Default is `None` so a UDF run with `fused.run()` will follow `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) ‚Äì Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **verbose** ‚Äì Set to False to suppress any print statements from the UDF.
- **parameters** (<code>Optional\[Dict[str, Any]\]</code>) ‚Äì Additional parameters to pass to the UDF.
- \*\***kw_parameters** ‚Äì Additional parameters to pass to the UDF.

**Raises:**

- <code>ValueError</code> ‚Äì If the UDF is not specified or is specified in more than one way.
- <code>TypeError</code> ‚Äì If the first parameter is not of an expected type.
- <code>Warning</code> ‚Äì Various warnings are issued for ignored parameters based on the execution path chosen.

**Returns:**

- <code>Union\[ResultType, Coroutine\[ResultType, None, None\], UdfEvaluationResult, Coroutine\[UdfEvaluationResult, None, None\]\]</code> ‚Äì The result of the UDF execution, which varies based on the UDF and execution path.

**Examples:**

Run a UDF saved in the Fused system:

```py
fused.run("username@fused.io/my_udf_name")
```

Run a UDF saved in GitHub:

```py
loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

Run a UDF saved in a local directory:

```py
loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

This function dynamically determines the execution path and parameters based on the inputs.
It is designed to be flexible and support various UDF execution scenarios.
</details>

---

## fused.submit

```python
submit(
    udf: AnyBaseUdf | FunctionType | str,
    arg_list: AnyBaseUdf | FunctionType | str,
    /,
    *,
    engine: Literal["remote", "local"] | None = "remote",
    instance_type: InstanceType | None = None,
    max_workers: int | None = None,
    n_processes_per_worker: int | None = None,
    max_retry: int = 2,
    debug_mode: bool = False,
    collect: bool = True,
    execution_type: ExecutionType = "thread_pool",
    cache_max_age: str | None = None,
    cache: bool = True,
    ignore_exceptions: bool = False,
    flatten: bool = True,
    _before_run: float | None = None,
    _before_submit: float | None = 0.01,
    **kwargs: float | None,
) -> Union[BaseJobPool, ResultType, pd.DataFrame]
```

Executes a user-defined function (UDF) multiple times for a list of input
parameters, and return immediately a "lazy" JobPool object allowing
to inspect the jobs and wait on the results.

Each individual UDF run will be cached following the standard caching logic as with `fused.run()`
and the specified `cache_max_age`. Additionally, when `collect=True` (the default), the collected results
are cached locally for the duration of `cache_max_age` or 12h by default.

See `fused.run` for more details on the UDF execution.

**Parameters:**

- **udf** (<code>AnyBaseUdf | FunctionType | str</code>) ‚Äì the UDF to execute.
  See `fused.run` for more details on how to specify the UDF.
- **arg_list** ‚Äì a list of input parameters for the UDF. Can be specified as:
  - a list of values for parametrizing over a single parameter, i.e.
    the first parameter of the UDF
  - a list of dictionaries for parametrizing over multiple parameters
  - A DataFrame for parametrizing over multiple parameters where each
    row is a set of parameters
- **engine** (<code>Literal['remote', 'local'] | None</code>) ‚Äì The execution engine to use. Defaults to 'remote'.
- **instance_type** (<code>InstanceType | None</code>) ‚Äì The type of instance to use for remote execution ('realtime',
  or 'small', 'medium', 'large' or one of the whitelisted instance types).
  If not specified, gets the default from the UDF (if specified in the
  `@fused.udf()` decorator, and the UDF is not run as a shared token),
  or otherwise defaults to 'realtime'.
- **max_workers** (<code>int | None</code>) ‚Äì The maximum number of workers to use. Defaults to 32 for
  the realtime engine (with a maximum of 1024), and 1 for other batch
  instances (with a maximum of 5).
- **n_processes_per_worker** (<code>int | None</code>) ‚Äì The number of processes to use per worker.
  Always 1 for the realtime engine, but defaults to the number of cores
  for batch engines. Specify this keyword to reduce the number of processes.
- **max_retry** (<code>int</code>) ‚Äì The maximum number of retries for failed jobs. Defaults to 2.
- **debug_mode** (<code>bool</code>) ‚Äì If True, executes only the first item in arg_list directly using
  `fused.run()`, useful for debugging UDF execution. Default is False.
- **collect** (<code>bool</code>) ‚Äì If True, waits for all jobs to complete and returns the collected DataFrame
  containing the results. If False, returns a JobPool object, which is non-blocking
  and allows you to inspect the individual results and logs.
  Default is True.
- **execution_type** (<code>ExecutionType</code>) ‚Äì The type of batching to use. Either "thread_pool" (default) for
  ThreadPoolExecutor-based concurrency or "async_loop" for asyncio-based concurrency.
- **cache_max_age** (<code>str | None</code>) ‚Äì The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d)
  (e.g. "48h", "10s", etc.).
  Default is `None` so a UDF run with `fused.run()` will follow
  `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) ‚Äì Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **ignore_exceptions** (<code>bool</code>) ‚Äì Set to True to ignore exceptions when collecting results.
  Runs that result in exceptions will be silently ignored. Defaults to False.
- **flatten** (<code>bool</code>) ‚Äì Set to True to receive a DataFrame of results, without nesting of a
  `results` column, when collecting results. When False, results will be nested
  in a `results` column. If the UDF does not return a DataFrame (e.g. a string
  instead,) results will be nested in a `results` column regardless of this setting.
  Defaults to True.
- \*\***kwargs** ‚Äì Additional (constant) keyword arguments to pass to the UDF.

**Returns:**

- <code>Union\[BaseJobPool, ResultType, DataFrame\]</code> ‚Äì JobPool, AsyncJobPool, or DataFrame depending on execution_type and collect parameters

**Examples:**

Run a UDF multiple times for the values 0 to 9 passed to as the first
positional argument of the UDF:

```py
df = fused.submit("username@fused.io/my_udf_name", range(10))
```

Using async batch type:

```py
df = fused.submit(udf, range(10), execution_type="async_loop")
```

Being explicit about the parameter name:

```py
df = fused.submit(udf, [dict(n=i) for i in range(10)])
```

Get the pool of ongoing tasks:

```py
pool = fused.submit(udf, [dict(n=i) for i in range(10)], collect=False)
```

---

## fused.download

```python
download(url: str, file_path: str, storage: StorageStr = 'auto') -> str
```

Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

üí° Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

üí° Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.

**Parameters:**

- **url** (<code>str</code>) ‚Äì The URL to download.
- **file_path** (<code>str</code>) ‚Äì The local path where to save the file.
- **storage** (<code>StorageStr</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> ‚Äì The function downloads the file only on the first execution, and returns the file path.

**Examples:**

```python
@fused.udf
def geodataframe_from_geojson():

    url = "s3://sample_bucket/my_geojson.zip"
    path = fused.core.download(url, "tmp/my_geojson.zip")
    gdf = gpd.read_file(path)
    return gdf
```

---

## fused.ingest

```python
ingest(
    input: str | Path | Sequence[str | Path] | gpd.GeoDataFrame,
    output: str | None = None,
    *,
    output_metadata: str | None = None,
    schema: Schema | None = None,
    file_suffix: str | None = None,
    load_columns: Sequence[str] | None = None,
    remove_cols: Sequence[str] | None = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: bool | None = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: int | float | None = None,
    partitioning_maximum_per_chunk: int | float | None = None,
    partitioning_max_width_ratio: int | float = 2,
    partitioning_max_height_ratio: int | float = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: float | None = None,
    subdivide_stop: float | None = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 500,
    lonlat_cols: tuple[str, str] | None = None,
    partitioning_schema_input: str | pd.DataFrame | None = None,
    gdal_config: GDALOpenConfig | dict[str, Any] | None = None,
    overwrite: bool = False,
    as_udf: bool = False
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Parameters:**

- **input** (<code>str | Path | Sequence[str | Path] | gpd.GeoDataFrame</code>) ‚Äì A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.

- **output** (<code>str | None</code>) ‚Äì Location on S3 to write the `main` table to.

- **output_metadata** (<code>str | None</code>) ‚Äì Location on S3 to write the `fused` table to.

- **schema** (<code>Schema | None</code>) ‚Äì Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.

- **file_suffix** (<code>str | None</code>) ‚Äì filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.

- **load_columns** (<code>Sequence[str] | None</code>) ‚Äì Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.

- **remove_cols** (<code>Sequence[str] | None</code>) ‚Äì The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.

- **explode_geometries** (<code>bool</code>) ‚Äì Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.

- **drop_out_of_bounds** (<code>bool | None</code>) ‚Äì Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.

- **partitioning_method** (<code>Literal['area', 'length', 'coords', 'rows']</code>) ‚Äì The method to use for grouping rows into partitions. Defaults to `"rows"`.

  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.

- **partitioning_maximum_per_file** (<code>int | float | None</code>) ‚Äì Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/10th the total area of all geometries. Defaults to `None`.

- **partitioning_maximum_per_chunk** (<code>int | float | None</code>) ‚Äì Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/100th the total area of all geometries. Defaults to `None`.

- **partitioning_max_width_ratio** (<code>int | float</code>) ‚Äì The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.

- **partitioning_max_height_ratio** (<code>int | float</code>) ‚Äì The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.

- **partitioning_force_utm** (<code>Literal['file', 'chunk', None]</code>) ‚Äì Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".

- **partitioning_split_method** (<code>Literal['mean', 'median']</code>) ‚Äì How to split one partition into children. Defaults to `"mean"` (this may change in the future).

  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.

- **subdivide_method** (<code>Literal['area', None]</code>) ‚Äì The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).

- **subdivide_start** (<code>float | None</code>) ‚Äì The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.

- **subdivide_stop** (<code>float | None</code>) ‚Äì The value below which geometries will not be subdivided into smaller parts, according to `subdivide_method`. Recommended to be equal to subdivide_start. If `None`, geometries will be subdivided up to a recursion depth of 100 or until the subdivided geometry is rectangular.

- **split_identical_centroids** (<code>bool</code>) ‚Äì If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".

- **target_num_chunks** (<code>int</code>) ‚Äì The target for the number of files if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
  Defaults to 500, rough default to use in most cases.

- **lonlat_cols** (<code>tuple[str, str] | None</code>) ‚Äì Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

  ```py
  fused.ingest(
      ...,
      lonlat_cols=("x", "y")
  )
  ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- **gdal_config** (<code>GDALOpenConfig | dict[str, Any] | None</code>) ‚Äì Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

  ```py
  fused.ingest(
      ...,
      gdal_config=
      }
  )
  ```

- **overwrite** (<code>bool</code>) ‚Äì If True, overwrite the output directory if it already exists
  (by first removing all existing content of the directory, i.e. it
  does not only overwrite conflicting files).
  Defaults to False.

- **as_udf** (<code>bool</code>) ‚Äì Return the ingestion workflow as a UDF that can be executed using
  `fused.run()`. Local files or python objects passed to `input` or
  `partitioning_schema_input` are still uploaded to S3 first such that
  those are available when executing the UDF remotely. Defaults to False.

**Returns:**

- <code>GeospatialPartitionJobStepConfig</code> ‚Äì Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

**Examples:**

For example, to ingest the California Census dataset for the year 2022:

```py
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).execute()
```

---

#### `job.run_batch`

```python showLineNumbers
def run_batch(output_table: Optional[str] = ...,
    instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
    *,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: List[str] | None = None,
    image_name: Optional[str] = None,
    ignore_no_udf: bool = False,
    ignore_no_output: bool = False,
    validate_imports: Optional[bool] = None,
    validate_inputs: bool = True,
    overwrite: Optional[bool] = None) -> RunResponse
```

Begin execution of the ingestion job by calling `run_batch` on the job object.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

#### Monitor and manage job

Calling `run_batch` returns a `RunResponse` object with helper methods.

```python showLineNumbers
# Declare ingest job
job = fused.ingest(
  input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
  output="s3://fused-sample/census/ca_bg_2022/main/"
)

# Start ingest job
job_id = job.run_batch()
```

Fetch the job status.

```python showLineNumbers
job_id.get_status()
```

Fetch and print the job's logs.

```python showLineNumbers
job_id.print_logs()
```

Determine the job's execution time.

```python showLineNumbers
job_id.get_exec_time()
```

Continuously print the job's logs.

```python showLineNumbers
job_id.tail_logs()
```

Cancel the job.

```python showLineNumbers
job_id.cancel()
```

---

#### `job.run_remote`

Alias of `job.run_batch` for backwards compatibility. See `job.run_batch` above
for details.

---

## fused.ingest_nongeospatial

```python
ingest_nongeospatial(
    input: str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame,
    output: str | None = None,
    *,
    output_metadata: str | None = None,
    partition_col: str | None = None,
    partitioning_maximum_per_file: int = 2500000,
    partitioning_maximum_per_chunk: int = 65000
) -> NonGeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Parameters:**

- **input** (<code>str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame</code>) ‚Äì A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- **output** (<code>str | None</code>) ‚Äì Location on S3 to write the `main` table to.
- **output_metadata** (<code>str | None</code>) ‚Äì Location on S3 to write the `fused` table to.
- **partition_col** (<code>str | None</code>) ‚Äì Partition along this column for nongeospatial datasets.
- **partitioning_maximum_per_file** (<code>int</code>) ‚Äì Maximum number of items to store in a single file. Defaults to 2,500,000.
- **partitioning_maximum_per_chunk** (<code>int</code>) ‚Äì Maximum number of items to store in a single file. Defaults to 65,000.

**Returns:**

- <code>NonGeospatialPartitionJobStepConfig</code> ‚Äì Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

**Examples:**

```py
job = fused.ingest_nongeospatial(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).execute()
```

---

## fused.file_path

```python
file_path(
    file_path: str, mkdir: bool = True, storage: StorageStr = "auto"
) -> str
```

Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF.
It takes a relative file_path, creates the corresponding directory structure,
and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files,
such as when writing intermediary files to disk when processing large datasets.
`file_path` ensures that necessary directories exist.
The directory is kept for 12h.

**Parameters:**

- **file_path** (<code>str</code>) ‚Äì The relative file path to locate.
- **mkdir** (<code>bool</code>) ‚Äì If True, create the directory if it doesn't already exist. Defaults to True.
- **storage** (<code>StorageStr</code>) ‚Äì Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> ‚Äì The located file path.

---

## fused.get_chunks_metadata

```python
get_chunks_metadata(url: str) -> gpd.GeoDataFrame
```

Returns a GeoDataFrame with each chunk in the table as a row.

**Parameters:**

- **url** (<code>str</code>) ‚Äì URL of the table.

---

## fused.get_chunk_from_table

```python
get_chunk_from_table(
    url: str,
    file_id: Union[str, int, None],
    chunk_id: Optional[int],
    *,
    columns: Optional[Iterable[str]] = None
) -> gpd.GeoDataFrame
```

Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

**Parameters:**

- **url** (<code>str</code>) ‚Äì URL of the table.
- **file_id** (<code>Union[str, int, None]</code>) ‚Äì File ID to read.
- **chunk_id** (<code>Optional[int]</code>) ‚Äì Chunk ID to read.
- **columns** (<code>Optional\[Iterable[str]\]</code>) ‚Äì Read only the specified columns.

---

================================================================================

# BLOG POSTS

## Analytics is Changing (Again)
Path: blog/2025-08-01-analytics_is_changing/index.mdx
URL: https://docs.fused.io/blog/2025-08-01-analytics_is_changing

# Analytics is Changing (Again)

AI has revolutionized how we write code; now it's reshaping how data teams can work.

    className="video__player"
    playing=
    muted=
    controls
    height="450px"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/from_udf_ai_to_dashboard_exploring_data_compressed.mp4"
    width="800px"
    style=}
/> */}

### Data + Compute + LLMs = Analytics at the Speed of Thought

The way we **interact with data changes constantly**; we only need to look at the last few years to see that we are seeing a shift as part of the "Great Decoupling‚Äù ‚Äì a broader movement toward zero-infrastructure. 

By adding Serverless Compute ‚Äì via **cloud functions** or the **browser** ‚Äì to formats like Apache Parquet and engines such as DuckDB, we unlock blazing fast analytics without clusters or VMs. Now AI is entering the mix, and we think this is the perfect timing to put all the pieces together.

We don‚Äôt think analysts, data engineers & scientists are going anywhere. We believe that ‚ÄúAI won‚Äôt replace you, a human using AI will‚Äù. AI amplifies the creativity and speed of the people already doing the work.

Here‚Äôs how we think the future of analytics looks like:

- **Data context limits hallucination**: Having LLMs know exactly about the structure and content of your data means AI is much less likely to hallucinate.
- **Code execution as validation**: AI code editors help write code, but can‚Äôt actually run it. At Fused we‚Äôre building on top of our real time execution engine, so errors in code are spotted and fixed much faster, all in the same workflow.
- **Human analysts to guide the work**: LLMs can write every possible query imaginable to parse a dataset, or make whichever chart you want. Answering the questions becomes easier, so asking the right questions becomes more valuable.

[Image: Three ways working with AI is changing analytics]

### So... What is Vibe Analytics?

A few months ago Andrej Karpathy coined the term Vibe Coding: letting LLMs loose and just give in to whatever code suggestion comes around, to quickly get a prototype of a new project, specifically for web development.

We‚Äôve since seen the term Vibe Analytics starting to pop up around, applying a similar approach but to analytics: leveraging LLMs to make it easier and faster to create analysis, make dashboards and share them to the rest of the team. 

### Come Build With Us!

Help us build the future of data analytics. Reach out to us directly (hello@fused.io) or see our open positions.

---
We did take inspiration from Andrej Karpathy's recent Software is Changing (Again) talk at AI Startup School, as you may have noticed from the title.

================================================================================

## Fused is SOC2 Type 1 Compliant!
Path: blog/2025-07-08-soc2_type1/index.mdx
URL: https://docs.fused.io/blog/2025-07-08-soc2_type1

# Fused is now SOC 2 Type 1 compliant!

We're excited to announce that Fused has achieved SOC 2 Type 1 compliance. This is a significant milestone for us, as it demonstrates our commitment to security and transparency.

At Fused, we want to allow all data teams to get work done quickly. We want to make sure to do this on top of a secure and compliant platform.

Through the comprehensive auditing process overseen by Insight Assurance, we've demonstrated our adherence
to the stringent requirements outlined by the SOC 2 Type 1 standard, reinforcing our dedication to safeguarding
sensitive data and maintaining operational resilience.

Request the full report by email: `info@fused.io`. 

We're actively pursuing SOC 2 Type 2 certification, building on our existing Type 1 compliance.

================================================================================

## Notes from EO Summit 2025
Path: blog/2025-06-19-eo_summit/index.mdx
URL: https://docs.fused.io/blog/2025-06-19-eo_summit

# Notes from EO Summit 2025

Last week, we were at EO Summit, Fused came out of beta, we officially launched our new site and are open for business! 

Our mission is to help data teams get stuff done quickly, which is relevant for the people we talked to at EO Summit.

[Image: Fused at EO Summit]

Here are some of our takeaways:

### 1. AI makes writing code simpler, but executing it at scale is still hard

A lot of us are going through a bit of an existential crisis, while realising that damn, yep, AI can help us write code to glue datasets together quickly. Knowing the intricacies of a Python library isn‚Äôt a competitive advantage for individuals and companies to build the best analytics anymore. 

That‚Äôs all well and good when working on a small, one-off project. But conferences like EO Summit keep showing that there‚Äôs more data than ever before. Archives of imagery, 3D point clouds and any dataset keeps growing in scale, resolution & time backlog. Problems aren‚Äôt always local, nor limited to a small time & place. 

AI is changing what it means to build software & products, there‚Äôs not doubt about that. But executing it at scale is still a dark art. 

### 2. So much data, yet we still struggle to get things done in a timely manner

At another conference, the Cloud Native Geopaptial last month Brianna Pag√°n shared her story of the Los Angeles fires taking down her home, and while having so much data available, so little was actually accessible in a helpful manner quickly. This topic came again at this conference, wildfires being the prime example of the complexity of making timely and updated use of data in times of need despite having so much.

A few months ago, our own Milind Soni made a quick interactive dashboard in a few hours as a proof of concept of what rapid tools could look like for wildfire updates using Fused.

These conversations are directly going into our internal product development discussion; as we write this, we're developing tools to build these dashboard for rapid iteration. Stay tuned for more on that soon! 

### 3. The gap between data & applications

Nadine Alameh, at the head of the Taylor Geospatial Institute & former CEO of the OGC, the consortium in charge of standards for all things geospatial, mentioned the lack of companies in the middle solving problems between data providers and companies building analytics products. This is something Aravind, head of Terrawatch -who organised this conference- has been saying for a while.  

That‚Äôs why we were excited about being at EO Summit, we think we can make a difference tackling these problems!

[Image: Talking to attendees]

We‚Äôre building tools that make getting data from all types of places & formats together, execute code (however it was written) at any scale with a few lines of code and sharing it all to whomever needs it, be it interactive dashboard, CSVs or tile servers. 

If you‚Äôd like to learn more about Fused, book some time with Max, our Developer Advocate right here to get a demo!

================================================================================

## Scaling Environmental Insights with Fused and H3
Path: blog/2025-05-27-environmental-insights/index.mdx
URL: https://docs.fused.io/blog/2025-05-27-environmental-insights

# Scaling Environmental Insights with Fused and H3

Farmers and analysts face a familiar challenge: weather and crop data is fragmented, slow to process, and hard to act on.

We worked with Emma Quirk (Senior Data Analyst) and Majid Alivand (Senior Data & Analytics Manager) to showcase how Fused can help bring all these datasets together. In this webinar they give an overview of the industry challenges interfacing backend data analytics with frontend data consumption. Emma walks through the notebook she used to model climate and irrigation patterns for vineyards.

<ReactPlayer
    playsinline=
    className="video__player"
    muted=
    playing=
    justifyContent="center"
    width="100%"
    controls
    style=}
    url="https://youtu.be/ESokph_0660"
/>

<br />

## Addressing the Challenge

Building useful & actionable weather models for environmental insights requires bringing datasets from:
- Different sources
- Different resolutions
- Different formats

To address these challenges, Emma & Majid used Fused + H3 to bring together datasets like GridMET climate, CDL crops, LANID irrigation, and gSSURGO soils by converting them from raster to H3 to build unified parquet files.

By building UDFs to manipulate each dataset, the team can iterate on their analysis in seconds while H3 allows them to more easily combine all the datasets together.

## Why H3?

- Harmonized format across datasets and regions
- Scalable queries at multiple resolutions
- Compact storage parquets with improved spatial performance
- No reprojection needed for global analyses
- Equal neighbors for clean spatial logic

## Try it out for yourself

Try out the notebook from the presentation for yourself:
- Colab Notebook

Sign up for a free Fused account and try it out for yourself!

================================================================================

## Launching Fused Apps
Path: blog/2025-05-20-launching-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-launching-fused-apps

# Launching Fused Apps

Today we're launching Fused Apps, allowing you to write Python in the browser, creating, editing & sharing interactive apps 

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/Introducing_fused_apps_intro.mov"
/>

Watch the announcement video: 

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="30vh" width="100%" url="https://youtu.be/yjiNxqAcPdw" alignItems='center'/>

## üêç Write Python in the browser

Fused Apps are one of the the fastest way to write Python, directly in the browser. 
- No environment setup, just start writing Python
- Create interactive apps using Streamlit 
- Save & rename your Apps

## üîó Share your apps with anyone

As soon as you save your Fused App, you can create a shareable link for anyone to open

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>

## üîí Standalone & Private, running in your browser 

Fused Apps are built on top of Pyodide, Streamlit & Stlite but offer many features:
- Github Integration for team collaboration
- Run light-weight LLMs locally in your browser with libraries like `transformers.js.py`

[Image: CDL Bunnies]

## üìö Access a Catalog of existing Fused Apps to get started

We've curated a catalog of existing Fused Apps to get you started, including:
- üåΩ Explore different crops around the US
- üõ∞Ô∏è Visualize the trend of objects sent to Space
- üå≤ Pan a map of Forest statistics per global municipality

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/App_catalog_browsing.mp4"
/>

You can try out Fused Apps for yourself directly in Fused Workbench and read our dedicated Docs page

## ‚öôÔ∏è How we built Fused Apps: Technical Details

We wrote a dedicated technical blog post going into details of how Fused Apps are built:
- How we originally built Fused Apps
- The challenges of Python in the browser
- Our approach to building a product

## Join our webinar on May 22nd!

On Thursday May 22nd we'll be hosting a webinar showcasing how our customers use Fused

- Join on LinkedIn

[Image: Webinar thumbnail]

================================================================================

## Inside Fused Apps: Python in The Browser
Path: blog/2025-05-20-inside-fused-apps/index.mdx
URL: https://docs.fused.io/blog/2025-05-20-inside-fused-apps

# Inside Fused Apps: Python in The Browser

This is a technical deep dive into how we built **Fused Apps**, a way to build a Python-based workflow in your browser, that you can save and share with someone. You can read the full product announcement here.

At Fused, we‚Äôre building tools to help data scientists work more efficiently: we want to give them the ability to work on any dataset, create an analysis and scale it to the whole world with just a few lines of code. But data scientists don‚Äôt work in a vacuum, and analysis aren‚Äôt (always) done because people are simply curious about a topic. 

We built Fused Apps in the spirit of allowing a single person to do all the work themselves; and in this first episode of *Inside Fused*, a series of blogposts about how we‚Äôre building Fused, we want to take you behind the curtain to show how Fused Apps is built. 

### Fused Apps at a glance

Fused provides both a Python package to run User Defined Functions (UDFs), and Workbench, a browser-based IDE to write, execute, visualize them as well as create Fused Apps to make interactive frontends. 

Here‚Äôs what Fused Apps look like:

[Image: Fused apps preview]

_Fused Apps. From left to right: the list of apps that have been loaded in Workbench, the app code editor, and the running app itself._

The App code editor & renderer allow users to write their own Python code using Streamlit to build a frontend entirely in Python, a language most data scientists already work with.

We want data scientists to be able to go from ‚Äúhey, that‚Äôs a cool idea‚Äù to ‚Äúhere‚Äôs what it looks like‚Äù without tech getting in their way. Especially in a world where LLMs make writing code simpler, the bottleneck becomes the speed at which data scientists can execute & ship code, not write it.

Fused Apps offers a way for data scientists to orchestrate their entire workflow using Python, without having to worry about backends, scaling, or clusters. Fused Apps complement our UDF builder, which offers a way to build data processing and backend functions, by offering an end-to-end workflow. At Fused, we use this for ingesting and managing datasets, managing resources we make available to our UDFs, and finalizing analyses.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/Vision_Pro_cdl_exploring_sped_up.mp4"
/>

_Fused Apps work on *any* device, as long as there's a browser!_

### How we built it

In short, Fused Apps tie together a frontend application for users to write Python code with Streamlit, a backend that saves & serves these apps and provides shareable links, and a product experience tying all of this together (error handling, autocomplete suggestions, async UI functionalities, etc.)

Fused Apps are built on top of Stlite & Streamlit. Streamlit being a library allowing you to write a frontend application, entirely in Python, and Stlite an in-browser version of Streamlit. This allows people familiar with Python but not so much frontend development (like data scientists) to have something rendered on screen in HTML, but only with writing Python. 

Stlite provides the ability to run apps in the browser, and while Stlite Sharing does support URL-encoded mechanisms for app sharing, the URL is the same as the code, preventing users from *updating* said code; there‚Äôs also no such thing as app catalogs, etc. So we ended up using Stlite as the engine and built a product experience around that. 

The frontend for building a Python-in-the-browser application already exists, with Streamlit & Stlite. However the backend had to be built from the ground up.

Originally, we didn‚Äôt even have a way for people to save their apps! Our internal workflow while developing & testing this was to have a Slack channel where we pasted our app links in to be able to find them later on

[Image: Early days Slack sharing links]

This is at the the core of our development philosophy: Build a prototype, use it a bunch, find the pain points, fix them, build more. In this case, saving & tracking apps was the next piece to build.

This is how we added functionality like Github integration, sharing options, and an app catalog. All these were only added after we got some traction internally and from early customers.

<LazyReactPlayer
    playsinline=
    className="video__player"
    muted=
    controls
    style=}
    width="100%"
    url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/blogs/2025-05-13-inside-fused-apps/sharing_fused_apps_links_higher_res.mp4"
/>
_Sharing Fused Apps is just a couple of clicks now_

### Python, in the browser

The next piece of this puzzle is to realise that running Python locally on your machine and in the browser is quite different. We want to make the experience for the user as seamless as possible: we‚Äôre building products for data scientists to build everything in Python.

We use Pyodide to run Python in WASM. This enables the same Python language to be used in a new environment ‚Äì the browser. The browser environment is key because it gives us a way to safely ship applications to users.

Safely shipping software to people wasn‚Äôt always the simplest. Previous efforts via Flash and Java applets enabled a generation of rich web content. The promise of Java applets was run anywhere ‚Äì that the same code could run on anyone‚Äôs computer. These technologies died out mainly because of security model problems. (Java lives on in Blu-ray disks.)

At the time of writing this, in mid 2025, browser applications are considered well sandboxed (with the possible exceptions of RowHammer/Spectre type issues), browser applications open when you want them, and go away completely when you dismiss them, all the while performing more and more complex tasks. Browser applications handle video encoding/decoding for video conferencing, graphics rendering for games, maps, and design, and more frequently, for programming. Companies like Figma are building complex, professional grade applications for the browser first.

From a development perspective, the browser becomes an operating system. It becomes less important to know what exact hardware & underlying operating system the users have, and we develop applications for the browsers‚Äô APIs. Those applications are inherently portable to new devices, because the heavy lifting is porting the browser to these new devices. 

### Building a Product

There are other projects out there that allow building hosted Stlite applications, however these miss some of the features that people expect from a well rounded product. Here are few things we‚Äôve added to Fused Apps:
- Package Handling (Every Fused Apps comes with `fused` and `pyarrow` pre-installed)
- Error handling 
- Naming & saving of Apps
- Github integration for teams
- Creating shareable links allowing users to send their apps to anyone with a browser

Beyond the technical challenges we want to build a product that helps data scientists build, iterate and ship faster. A data scientist can build an analysis that takes in different parameters and make an interactive graph for their project manager to test out directly all without needing support

A big part of this is the philosophy of how we build things at Fused: We‚Äôre a team of engineers & data scientists. A lot of the features we‚Äôre listing above here come directly from our own usage of Fused Apps while making real applications with some of our customers.

We care about how fast it takes for a new user to click on a Fused App and start running it, or what the experience of saving & sharing an app looks like. 

Every Fused App comes with some packages pre-installed, such as `fused` and `pyarrow`, which are helpful for data scientists. (Streamlit comes with a number of other common packages like Pandas and NumPy already.) But this comes at the expense of loading time, as each new app is a self-contained application which requires downloading all the required packages. This is leading us to spend time optimizing the initial loading time of Pyodide.

### What can we expect from Pyodide

Fused Apps, and any implementation of Python in the browser, isn‚Äôt as customizable as a fully local setup. Packages with native code that are not prebuilt for WASM will not work. At the time of writing, some popular packages like Pandas & Numpy are built and supported, but others aren‚Äôt. The backbone of the map processing pipeline, GDAL, for example, isn‚Äôt currently supported. This isn‚Äôt from lack of popularity, but reflects the complexity of building GDAL.

Some packages will need architecture updates. As another example people currently call into ffmpegfrom Python using the CLI. The WASM environment does not have a CLI concept, and this would need to be replaced with library calls. Other packages which are not well suited to this architecture (such as Torch) might have alternatives developed because the wasm ecosystem is so attractive a development target.

#### LLMs in the Browser

An example of this is transformers.js.py. This library (developed by Yuichiro Tachibana, also behind Stlite) needed more than one level of architectural adaptation to get running in the browser, but once adapted it brought the capability of running lightweight LLMs. This allows us to ship applications that run small LLMs models, running locally in the browser! 

We made a Fused App exploring the USDA‚Äôs Crop Data Layer (CDL) dataset, a dataset of different crop types in the US. Instead of showing all 130+ categories, we can just take a text input, run a similarity analysis on the fly against all the categories and find the closest CDL crop type! 

[Image: CDL Bunnies]

_You too can find out what Bunnies prefer by trying this Fused App here_

#### Compatibility & load times

This is a fast changing ecosystem though, for example we‚Äôve made a small contribution of a build of the H3 package specifically because we wanted to have H3 supported in Python in the browser. The list of supported packages built in Pyodide is growing quickly. 

Moving large amounts of data in and out of the browser is slow, so data-intensive libraries like Torch or Dask are not well-suited for this. We solve this by bringing in the compute performance & flexibility of Fused UDFs, which run native (non-WASM) Python in a cloud environment. Fused UDFs run code much closer to the data, improving performance, and are not limited by what packages are available for Pyodide, improving flexibility.

Install times for Pyodide are relatively long from a user point of view, as the application isn‚Äôt stored locally and installed each time it's opened. This will most likely keep improving over time as the technology is developed further. For example Pyodide wants to add memory snapshotting to help with this, but it isn‚Äôt stable yet.

Even when the application is stored locally, we found we needed to reinitialize Pyodide in some cases. There is no concept of switching virtual environments in Pyodide, since running a new script reuses the same Python VM. When switching between apps in Fused, we reinitialize Pyodide in order to prevent packages from one app interfering with another. If we didn‚Äôt do this, a user might accidentally rely on a package installed by App A in developing App B, which would then not work when sharing App B.

We also needed to be careful with when the app can run, since the app has access to the user‚Äôs browser context. We chose to give users the chance to inspect the app‚Äôs code before running it if it would have access to their Fused account.

Code written in Stlite and Pyodide looks almost the same as regular Python, but there are slight differences. Many of these come in the form of adapting synchronous and asynchronous code. Stlite for example allows for top-level `await` because the browser‚Äôs event loop is being used. This can be tricky to work with because regular advice for working around asynchronous code in Python does not work with Pyodide.

[Image: CDL loading async model & logging]
_Async-aware UI allows us to provide improved feedback as an app loads a light weight LLM model or data to display_

In order to create a good product experience, we added our own syntax checking and linting on top of Pyodide. 

[Image: Error handling]

_Lots of small features go a long way to delighting users with a smooth experience._

### Delivering at near-zero cost

Fused Apps are part of the free tier of Fused. You can use Fused Apps without logging in at all, and with a free login you can save and share your apps. We want to take a moment here to explain why this is naturally a free offering and will continue to be free.

As a browser-based application, all of the code execution and data transfer happens in the user's browser. This means that we do not need to sandbox code execution or pay for cloud compute resources to run anyone‚Äôs Fused Apps. 

Where we do incur costs are in the shared control plane layer of Fused. Technically, the control place layer doesn‚Äôt see much difference between an app and a UDF. As a result, the incremental cost of serving an app user is very low and it is easy for us to offer that for free. The core offering of Fused is the backend serverless execution of code, which is our paid product.

### Try it out for yourself 

Don‚Äôt take our word for it, give Fused Apps a try for yourself! As we mentioned, Fused Apps are free and don‚Äôt require login. You can check out our catalog of public Apps in Fused Workbench.

Recently we announced re-partitioning the Crop Data Layer dataset into H3 hexagons for anyone to use and hosted the resulting dataset on Source Cooperative.

Alongside this we created a public Fused App allowing you to explore any crop for the 2024 dataset

### We're hiring: Help us build the future of data science workflows!

We firmly believe data scientists need tools that give them the independence to do their work rather than asking for support to scale their analysis or share their results. 

We need smart people to help us build all of this. We are hiring for:

- Deep knowledge of Python & Pyodide
- Opinionated thinking in building the future of data science pipelines
- People wanting to join a fast moving startup and build things

If you‚Äôd like to join the team, **send us your info here**!

[Image: The team]

_Join the team!_

================================================================================

## How Sylvera uses Fused to prototype and power DeckGL applications
Path: blog/2025-05-16-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2025-05-16-danieljahn

**TL;DR Sylvera quickly builds and tests new app features by serving data to DeckGL applications using Fused HTTPS endpoints.**

At its core, Sylvera rates carbon projects. Our ratings are powered by several earth observation and geospatial analysis data products. From climate risk data, and deforestation indicators, to biomass-predicting ML models, a wealth of data goes into generating a single-letter rating.

```javascript showLineNumbers
const BOUNDS_AFRICA: [number, number, number, number] = [
  -25.35, -46.95, 51.35, 37.35,
];
const UDF_H_AOI_FILE_CALL = "FUSED_UDF";

function createTileLayer(id: string, data: string[]): TileLayer<ImageBitmap>  = props;

      return [
        new BitmapLayer(otherProps, ),
      ];
    },
  });
}

function createBitmapLayer(
  id: string,
  image: string,
  bounds: [number, number, number, number]
) );
}

const createFusedFileLayer = (
  layerId: string,
  fusedId: string,
  bounds: [number, number, number, number],
  year: number
) => -$`;
  const param = `year=$`;
  const imageUrl = `https://www.fused.io/server/v1/realtime-shared/$/run/file?dtype_out_raster=png&dtype_out_vector=csv&$`;

  const layer = createBitmapLayer(key, imageUrl, bounds);

  return layer;
};

const createBasemapLayer = () => //.png",
  ]);
};

  const bounds = BOUNDS_AFRICA;
  const year = 2000;

  const basemap = createBasemapLayer();
  const fused = createFusedFileLayer(
    "fused",
    UDF_H_AOI_FILE_CALL,
    bounds,
    year
  );

  return (

  );
}

  createRoot(container).render(<App />);
}
```
</details>

## Conclusion and future work

The application we built isn't yet fully featured to be put in front of users ‚Äì but that's the point. We were not aiming for a finished product yet. Instead, we achieved rapid iteration that enabled us to gather relevant stakeholder feedback.

The speed we could reach wouldn't have been possible without Fused's development platform. Fused unifies three traditionally separate stages ‚Äî prototyping, scaling, and visualization‚Äîinto a single seamless solution. Thanks to this, Fused was an indispensable tool for product iteration.

In the future, we would like to explore the coming integration with Zarr stores. Being able to not only visualize the results but also to immediately persist them into a Zarr store will be a game-changing capability for anyone who uses Zarr as the persistence layer.

================================================================================

## Repartitioning Crop Data Layer & US Census into H3 hexagons
Path: blog/2025-05-06-cdl-census-hex/index.mdx
URL: https://docs.fused.io/blog/2025-05-06-cdl-census-hex

# Repartitioning Crop Data Layer & US Census into H3 hexagons

Fused has repartitioned the USDA Crop Data Layer and US Census into H3 hexagons.

These 2 datasets are both available on Source Cooperative for anyone to use, free of charge!

[Image: Source coop CDL hex]

### What are these datasets?

We've taken 2 popular datasets and made them available in H3 Hexagons, this provides a number of benefits:
- Much faster ability to aggregate data at different scales
- A simple way to join datasets together (as they, and any other H3 tiled dataset are using the same grid)

You can read more about H3 Indexes on the dedicated page.

We're providing:

1. Crop Data Layer (from USDA CroplandCros)

- Available for `[2012, 2014, 2016, 2018, 2020, 2022, 2024]`
- Available in `hex7` and `hex8` resolutions

2. US Census (from data.census.gov)

- Available for `2020`
- Available in `hex7` and `hex8` resolutions

### Using these datasets

These datasets are completely free for anyone to use, with or without Fused. To showcase what's possible we have made a public Fused UDF anyone can use without an account to explore these.

[Image: CDL from source udf]

You can try it here without any account.

================================================================================

## Fused featured in Maxar TED Talk
Path: blog/2025-04-08-TED/index.mdx
URL: https://docs.fused.io/blog/2025-04-08-TED

# Fused featured in Maxar TED Talk

[Image: Fused Powering Maxar]

Fused is excited to be featured in a TED Talk today by Peter Wilczynski, CPO at Maxar, which explored a multi-source site monitoring scenario of Vancouver's greening initiatives over the past five years powered by the Fused platform.

[Image: Fused Maxar Workbench]

Site monitoring scenarios are at their most effective when data from disparate sources is combined‚Äîfrom satellite imagery and map data to municipal and demographic data. Analytics today requires bringing all these different sources of data together: from raw pixels to contextual intelligence ‚Äî in real-time, seamlessly, and at scale.

This is an iterative process, each dataset being in its own format & hosted on different cloud providers, updated independently and on different schedules.

Fused is built by data scientists, engineers & open source contributors who know the reality of building pipelines with breaking changes in libraries, new file formats that require rethinking how data is ingested and business requirements that constantly change.

We firmly believe we need tools that help us bring as many datasets together as possible, especially in a world where LLMs and AI can increasingly talk directly to data.

If you‚Äôd like to try out an instance of the Fused workbench with pre-loaded data from Maxar later this month, which Peter demoed on screen, sign up for the waitlist on fused.maxar.com to get access and see for yourself this is more than a pre-recorded tech demo!

================================================================================

## Announcing Fused AI Builder
Path: blog/2025-04-01-announcing-ai-builder/index.mdx
URL: https://docs.fused.io/blog/2025-04-01-announcing-ai-builder

# Announcing Fused AI Builder

**Fused AI Builder let's you create LLM Agents that can directly call & execute deployed Python code through Fused UDFs!**

### Give LLMs the ability the talk to your data & code

We're launching Fused AI Builder, a simple way for you to give an LLM access to any UDF you want

### Data, Python & LLMs all in one

You can now use LLM + Fused UDFs to do a whole host of tasks:

</Tabs>

================================================================================

## Announcing Fused 2.0
Path: blog/2025-02-25-fused/index.mdx
URL: https://docs.fused.io/blog/2025-02-25-fused

# Announcing Fused 2.0

**Fused 2.0 is our biggest update to date, with changes across Workbench & `fused-py`!**

### A New UDF Editor in Workbench

[Image: New Fused Workbench]

#### Introducing Collections: A simple way to organize your UDFs

Collections now allows you to organize your UDFs together as you work on different projects, save them together and keep your editor focused on the project at hand.

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/load_collections_new.mp4" width="100%" />

Collection is still in early access so you need to enable it under "Preferences -> Enable UDF Collections [Beta]" to access it

#### Redesigned UDF Editor

UDF Editor now comes with a host of new features & redesigned UI:
- Adding a new Full Screen Map View
- The Visualize Tab is now under the UDF expanded parameters, allowing you to hide your Code Editor and just focus on tweaking the visualization of your data
- Split screen "Editor" & "Module" on top of each other: Keeping your code clean in the main "Editor" Tab is now easier by moving functions under the "Module" tab.

#### A New Share Page

We've moved all the tools & information you need to easily share your UDFs into a dedicated page (and button). You can easily:
- Create a token to share your UDFs 
- Edit the Description, Tags & Image preview of your UDFs

<LazyReactPlayer className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/announcements/fused_2_0/share_udf.mp4" width="100%" />

### Changes to `fused-py`

Our Python library `fused-py` is getting some updates to make processing data in Python simpler, from a small one time task to processing huge datasets

#### Simplifying how Fused handles geometries

- Moving away from Fused having many different `fused.types` to only having 2 simple types: `fused.types.Bounds` and `fused.types.Tile`.
- Replacing `bbox` object with `bounds`: a more generic term to pass geometries to UDFs

#### `fused.submit()` for simple, multi job runs

We're introducing `fused.submit()` as a simple way to run many UDFs all at once in parallel

[Image: fused.submit() demo]

This can significantly speed up parallel tasks like:
- Fetching lots of individual data points from an API
- Scaling small processing steps to lots of data points

#### Improved caching

Under the hood we've significantly improved how Fused caches results of recurring UDFs & cached functions + we've introduced new tools for developers to control caching:
- A default 90 days cache time for all UDFs
- Editing the cache duration with the new `cache_max_age` argument

Read more about Caching

### Full Fused 2.0 Changelog

Read our Changelog to see every change happening with Fused 2.0

================================================================================

## Enhance your data with GERS IDs
Path: blog/2025-02-18-jennings/index.mdx
URL: https://docs.fused.io/blog/2025-02-18-jennings

**TL;DR Enriching your spatial data with GERS IDs can make it interoperable across the data ecosystem. You can use Fused UDFs to create custom HTTPS endpoints to enrich your data with GERS.**

The Global Entity Reference System (GERS) is a framework that structures, encodes, and matches map data to a shared universal reference within Overture. GERS helps organizations identify and reference their own datasets with standard identifiers to Overture data to help unify datasets.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gers_sheets.mp4" width="100%" />

\
In this blog post we show how to create simple endpoints with Fused UDFs to enrich a dataset with GERS IDs. We'll use the Overture Building footprints to first enrich a polygon with GERS IDs then another one to look-up metadata for a specified GERS ID.

## The benefit of GERS

When third-party dataset is spatially matched to an Overture feature it's "enriched" with that feature's GERS ID and becomes "GERS-enabled". This makes it easy to associate it by ID to any other GERS-enabled dataset.

```
                            gers_id             buliding_name
0  08b2a1072534cfff020018b8a6efde22  James A. Farley Building
1  08b2a100d2cb6fff02000821de8bdff1      Pennsylvania Station
```

For example, a municipal government with a dataset of building footprints for local offices, coffee shops, and museums could match those entities to a GERS ID. This would enable the government to easily join its data other "GERS-enabled" datasets to enrich them with additional information such as insurance data, historical property values, restaurant reviews, fire risk, or rooftop solar potential.

## Create a UDF to enrich a polygon with GERS IDs

We can create a UDF that takes in a polygon and returns a GERS ID. This UDF will spatially match the polygon to Overture Buildings and return the GERS ID of the building that intersects the polygon. This is useful for enriching a dataset with GERS IDs.

Users can design and preview the workflow interactively, allowing them to test assumptions and visualize their effect. Parameters can be adjusted, and the output can be previewed before running the UDF on the entire dataset.

```python showLineNumbers

aoi = json.dumps(,"geometry":}]})

@fused.udf
def udf(bbox: fused.types.TileGDF=aoi):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Convert bbox to GeoDataFrame
    if isinstance(bbox, str):
        bbox = gpd.GeoDataFrame.from_features(json.loads(bbox))

    # 2. Load Overture Buildings that intersect the given bbox centroid
    gdf = utils.get_overture(bbox=bbox.geometry.centroid, overture_type='building', min_zoom=10)

    # How many Overture buildings fall within the bbox centroid?
    print("Buildings in centroid: ", len(gdf))

    # 3. Rule to set only one GERS on the input polygon
    bbox['id'] = gdf.id.values[0]

    return bbox
```

### Create an HTTPS endpoint

With Fused, it's easy to turn your UDF into an HTTPS endpoint. This enables you to run the UDF it programmatically via HTTPS requests to integrate the functionality into various workflows and applications.

This endpoint runs a public UDF with the code above. You can call it with a geojson of a single polygon in the bbox query parameter and it will return a geojson with the polygon and an assigned GERS ID.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Enrich/run/file?dtype_out_vector=geojson&bbox=,"geometry":}]}
```

## Create a UDF to look-up metadata for a GERS IDs

We can also create a sample UDF to do a reverse operation: look-up a Building and its attributes by passing a GERS id. A user will be able to pass a GERS id and the UDF will look-up the building and return its geometry along with attributes about the building.

To do this, we create a UDF that takes in a `gers_id` parameter. Because the first 16 digits of GERS correspond to an H3 cell, we can use the ID to create a polygon to spatially filter the dataset. It'll bring up any buildings that intersect the H3 cell. Once we have the building, we can easily work with its geometry object and attributes using GeoPandas.

```python showLineNumbers
@fused.udf
def udf(gers_id: str='08b2a100d2cb6fff02000821de8bdff1'):

    from shapely.geometry import Polygon

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. H3 from GERS
    h3_index = gers_id[:16]
    print('h3_index', h3_index)

    # 2. Polygon from H3
    bounds = Polygon([coord[::-1] for coord in h3.cell_to_boundary(h3_index)])
    bbox = gpd.GeoDataFrame()

    # 3. Load Overture Buildings
    gdf = utils.get_overture(bbox=bbox, overture_type='building', min_zoom=10)

    # 4. Subselect building
    gdf = gdf[gdf['id'] == gers_id]

    # 5. De-struct the names column
    normalized_df = pd.json_normalize(gdf['names'])
    gdf = gdf.reset_index(drop=True).join(normalized_df)

    return gdf[['id', 'primary', 'subtype', 'class', 'geometry']]
```

### Create an HTTPS endpoint

Here's how you can create and use an HTTPS endpoint for your GERS building lookup UDF.

This endpoint returns a CSV table of the building's GERS ID, primary name, subtype, class, and geometry. You can use this endpoint to enrich your dataset with GERS IDs by calling it with a GERS ID query parameter.

```
https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Buildings_GERS_Lookup/run/file?08b2a100d2cb6fff02000821de8bdff1&dtype_out_vector=csv
```

For example, you could call this endpoint from a Google Sheet to enrich a dataset with GERS IDs. This sample Google Sheet returns the enriches the "primary" name column for any given Building GERS. Just drag the formula to apply it to any row below. It works by calling the "GERS lookup" endpoint with a GERS ID query parameter.

================================================================================

## We're partnering with Overture to make their Data easily accessible with Fused
Path: blog/2025-02-11-overture/index.mdx
URL: https://docs.fused.io/blog/2025-02-11-overture

**_TL;DR: We've made it easier to work with Overture data by leveraging Fused._**

Fused has been working with the team at The Overture Maps Foundation to enable direct access to their data through Fused UDFs. We are excited to share that the Overture docs now show examples on how to see how to integrate any Overture data into your workflows using Fused.

[Image: Alt]

Overture Maps aims to provide foundational building blocks of data across various themes designed to be broadly applicable across industries. Our goal at Fused is to make it easy to work with Overture data and adopt standards (such as GERS). To this end, we are creating easy abstractions to access data, tools to perform foundational operations such as conflation and enrichment, and example workflows to inspire and help you understand how to leverage this data.

One of the key usecases for Overture + Fused is enriching datasets with Overture Maps data. This tutorial showcases 2 simple Python workflows that do this by performing a spatial join with Overture Buildings. This example of a simple enrichment operation will help you understand how to work with Overture data in your own workflows.

To follow along, check out the:
- Overture Maps Docs "Getting Data" Page
- Overture Maps Docs "Examples" Page
- Overture Maps Example UDF
- Overture + NSI UDF

## Overview

The Overture Buildings dataset is dividen into themes. Two key themes are:
- Buildings is composed of building footprints represented as polygons
- Places is composed of business establishment locations and associated metadata, represented with point coordinates.

We'll first show how you can load Overture data by reusing an existing Fused UDF, then write a User Defined Function (UDF) with custom logic to perform enrichment with a spatial join. You'll be able to run the resulting UDF for any custom area of interest (AOI).

## Step 1: Load data with the Overture Maps UDF

Fused has a catalog of pre-made UDFs you can easily copy and repurpose for your own data analysis workflows. In the catalog, you'll find the Fused Overture UDF which enables you to quickly load Overture data from any of the themes for an area of interest (AOI). You can run the UDF with `fused.run` and specify an AOI to load data for using the bbox parameter. You may also pass optional parameters to select between Overture releases, themes, and columns - that way you can fetch only the data you need. In this example, we can specify the 'building' theme by setting the `overture_type` parameter.

```python showLineNumbers

bbox = gpd.GeoDataFrame(
    geometry=[shapely.box(-73.9847, 40.7666, -73.9810, 40.7694)],
    crs=4326
)

fused.run('UDF_Overture_Maps_Example', bbox=bbox, overture_type='building')
```

The output should look like this:

```python showLineNumbers

        id	                                geometry	                                        class   ...
24134	08b2a100d65a6fff0200b45ce7e2b99b	POLYGON ((-73.98552 40.76736, -73.98557 40.767...	apartments  ...
24135	08b2a1008b259fff02007917db1c32d3	POLYGON ((-73.98441 40.76703, -73.98431 40.767...	apartments  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	apartments  ...
24179	08b2a100d6516fff02006dc174022a7e	POLYGON ((-73.98346 40.76623, -73.98327 40.766...	commercial  ...
24180	08b2a1008b248fff0200315983940aa8	POLYGON ((-73.98407 40.76749, -73.98402 40.767...	None    ...

```

By browsing the UDF's catalog page, you can see its code and even copy it to run it interactively on the Fused Workbench. You'll notice that the UDF uses the `get_overture` helper function to read from spatially partitioned parquets of the overture data releases, hosted in a Source Cooperative S3 bucket. The source code of the helper function is fully open and hosted on GitHub here.

Here's a simplified version to show the core of what's going on in `get_overture`. It constructs the table path on S3 and then uses the table_to_tile helper function from Fused to load data that falls within the specified bounding box. This approach allows you to efficiently perform spatial queries on a large dataset and load only the records within the given area.

```python showLineNumbers
# Structure the table path with input parameters
table_path = f"s3://us-west-2.opendata.source.coop/fused/overture//theme=/type="

# Load the data within the bounding box
df = utils.table_to_tile(bbox, table=part_path)
```

## Step 2: Write a Custom UDF to join Places with Buildings
The example above shows how to run an existing UDF to load Overture data, but it's likely you want to write your own data transformations. You can borrow `get_overture` to load data into your own UDFs. As an example, here is a UDF to load Overture Buildings polygons and Overture Places points. The UDF perform a spatial join between them to determine which points fall within each building.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Buildings
    gdf_buildings = utils.get_overture(bbox=bbox, theme='buildings')

    # 2. Load Places
    gdf_places = utils.get_overture(bbox=bbox, theme='places')

    # 3. Create a column with the Buliding Name
    gdf_buildings['primary_name'] = gdf_buildings['names'].apply(lambda x: x.get('primary') if isinstance(x, dict) else None)

    # 4. Spatial join between Places and Buildings
    gdf_joined = gdf_places.sjoin(gdf_buildings[['geometry', 'primary_name']])[['id', 'names', 'primary_name', 'geometry']]

    return gdf_joined
```

To run this UDF, you simply call it with your AOI. Fused will execute the code with the given parameter then return the UDF's output.

```python showLineNumbers

bbox = gpd.GeoDataFrame(geometry=[shapely.box(-73.9847, 40.7666, -73.9810, 40.7694)], crs=4326)

fused.run(udf, bbox=bbox)
````

This will return a GeoDataFrame with the geometry of the Place, the `primary_name` of the building it falls within, and other attributes as defined in the UDF's return statement.

The output should look like this:

```python showLineNumbers
        id	                                names                                                   primary_name	    geometry
3883	08f2a100d65160860308e7269804dcb7	 className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/join_places.mp4" width="100%" />

## Step 3: Write a Custom UDF to join Buildings with the NSI dataset

As a second example, you can also enrich the Overture Buildings dataset with metadata data from the National Structure Inventory (NSI) API. The NSI API offers point data on buildings in the U.S. that is relevant to hazard analyses.

This UDF loads the Overture and NSI datasets, performs a spatial join to enrich the building polygons with hazard metadata, and returns the enriched GeoDataFrame. It can be used within a larger analysis workflow to enrich building polygons to calculate risk indices. You can read more about performing spatial operations to enrich Overture Buildings with NSI in our geospatial processing guide.

```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.TileGDF = None):

    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/Overture_Maps_Example/").utils

    # 1. Load Overture Buildings
    gdf_overture = utils.get_overture(bbox=bbox)

    # 2. Load NSI from API
    response = requests.post(
        url="https://nsi.sec.usace.army.mil/nsiapi/structures?fmt=fc",
        json=bbox.__geo_interface__,
    )

    # 3. Create NSI gdf
    gdf = gpd.GeoDataFrame.from_features(response.json()["features"])

    # 4. Join Overture and NSI
    cols = ["id","geometry","metric","ground_elv_m","height","num_floors","num_story"]
    join = gdf_overture.sjoin(gdf, how='left')
    join["metric"] = join.apply(lambda row: row.height if pd.notnull(row.height) else row.num_story*3, axis=1)
    return join[cols]

```

The output should look like this:

```python showLineNumbers

	    id	                                geometry	                                        val_struct      med_yr_blt  ...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	348190.820	1939	...
24178	08b2a100d6516fff0200ded2bf849c8a	POLYGON ((-73.98375 40.76693, -73.98381 40.766...	378633.733	1939	...
```

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/overture_nsi_2025.mp4" width="100%" />

## Conclusion

In this short tutorial, we outlined how you can integrate Overture data into your workflows using Fused. We saw how you can use Fused to load the data, write a custom Python workflow, and run it for a custom AOI. We hope the these foundational pieces help you see how you can unlock the full potential of Overture Maps and start creating your own workflows to enrich your own datasets.

================================================================================

## How Pilot Fiber creates internal tools to support telecom operations
Path: blog/2025-01-23-kyle/index.mdx
URL: https://docs.fused.io/blog/2025-01-23-kyle

**TL;DR Pilot Fiber creates apps with Fused to quickly identify and resolve service interruptions for its New York City customers.**

Pilot Fiber is a commercial Internet Service Provider primarily in New York City. Our primary value proposition in competing with national-scale ISPs is our commitment to customer experience‚Äìwe are fast and flexible in responding to customer needs in all aspects of the business:

- During the sales process, we aim to answer customer questions quickly and accurately, including technical details and routing.
- When designing our deployment into new buildings, we equip our engineers with as much detail as possible before they arrive on-site to maximize the efficiency of time spent with building engineers.
- When an incident interrupts a service we will immediately jump into action to address the cause and restore connectivity.

We use Fused to support all of these areas, and this post focuses on the last one: incident management.
- What happens when a service interruption occurs?
- How do we identify the likely location of an issue and get to a solution as quickly as possible?

### The Problem: Why Speed Matters

One of the most common ways a customer's service can be impacted is through damage to the physical fiber cables connecting them back to a data center and the internet. Almost all fiber optic cables in Manhattan run through a shared manhole-and-duct system beneath the streets. As such, road construction or work by other providers in a manhole has the potential to damage the equipment of multiple providers. When that damage occurs, it is first come, first served to get your network repaired and customers back online. Field teams from multiple providers can't work in the same manhole simultaneously, so being onsite first and ready to repair can mean a difference of hours in customer downtime.

Because of this, Pilot uses an active fiber monitoring system across our network. Sophisticated devices in our data centers are constantly shooting light down the fibers in our network looking for potential damage. Those devices return a reflectance signature from the fiber and compare it with a reference "snapshot" created when that fiber was initially installed in a building.

When an anomaly is registered, it immediately fires an alert giving a fiber route and distance to the potential problem (i.e. "There is unexpected light loss on the fiber serving 1234 5th Avenue at a distance of 2.351 kilometers from the data center."). When this happens, our engineering and support teams analyze the data within minutes to determine the issue's exact location and, if necessary, get crews headed to the site to begin repairs.

## The Process: Fused As The Glue-Layer

Historically, this analysis has required the attention of an Outside Plant engineer with access to specialized software and network knowledge, regardless of the time of day or day of the week. This bottleneck is not ideal when time is of the essence, even at 3 a.m. So today, we are creating a more sophisticated future using Fused to make this information accessible to more support team members and make our response times even faster.

Using Fused as a back-end glue layer, we built a web app allowing users to select a route and distance and calculate where the system has registered the fault. We also created a simple user interface that provides the user a view of nearby network infrastructure and automatically generates the reports field crews would need to complete repairs based on that nearby infrastructure.

The workflow requires a series of calls to UDFs that act as intermediaries to a Postgres/PostGIS database, which in turn is sourcing data from other internal sources. This structure allows us to easily keep the business logic organized at the UDF layer while limiting the scope of data access and security via Postgres and internal processes maintaining the sync.

The basic process is seen below:

_Workflow diagram._

Two separate flows are initiated when the user inputs a route and a distance to process. One retrieves the selected route to load onto the Mapbox-based map within the app, while the second kicks off a processing chain to analyze the fault information. This chain utilizes UDFs that assist in isolating the location of the fault and relevant nearby infrastructure and adding elements to the map display to assist the user in visualizing what may be occurring.

_UDF to find fault location._

If you consider the cables you see strung along utility poles, they are not perfectly straight: they can sag, bend, go up and down, and have coils of extra cable along the way. The same is true of our cables under the streets. All of those variations add distance to the run, which needs to be considered when determining where a fault is likely to be located.

Taking those variables into account, we use Fused to apply GeoPandas and PostGIS spatial functionality to assess where the fault is most likely to be located. After calculating that location, the tool loads splice cases along that route that point to where problems are most likely to have occurred, and slack loops built into the route to make the user aware of nearby capacity that could enable faster repair of more significant damage. We next determine which splice cases are closest to the likely damage point and if any of those are within 150m of the automated distance calculation. These manholes would be the first locations our field teams would be sent to investigate.

Once we have determined the relevant nearby splice cases, we use another UDF to build a CSV that reproduces what a splice report ## The Impact of Fused

The impact of Fused across this process is many-fold:
- The ability to easily work with data across several systems.
- Centralizing business logic to the UDFs involved eliminates the tendency for this logic to be spread across client-side processes, server-side processes, and possibly the database itself.
- Modularization of operations into UDFs. For example, the UDF that generates the splice reports for this process can easily be reused in any other method that requires the same functionality.
- The effortless ability for the same UDF to simultaneously serve as a modular processing unit where needed in one workflow and a map service for display in another.
- Using Python and standard libraries enables developers who may not be spatial data experts to read, understand, and modify UDFs as necessary.

## Conclusion

Pilot's success in the market is largely based on our flexibility and responsiveness to customer needs, which is never more important than when physical damage to the network is impacting their service. Within this scenario, Fused is providing a critical layer enabling us to offer more non-technical users access to data in multiple systems through a simple UI that will result in repair teams moving to restore service to our customers as quickly as possible.

Fused is an ideal product for Pilot Fiber in that we can increasingly make highly technical information available in a usable format to additional teams throughout the company in support of our drive to be fast, flexible, and accurate in delivering service to our customers in all aspects of the business.

================================================================================

## How Fused Powers BlackPrint's Acquisition Intelligence Platform
Path: blog/2025-01-22-blackprint/index.mdx
URL: https://docs.fused.io/blog/2025-01-22-blackprint

**TL;DR BlackPrint streamlines and transforms fragmented real estate data into actionable insights across Latin America.**

BlackPrint Technologies began its journey as a satellite mapping service designed to help municipalities modernize their property registries. Recognizing a greater opportunity, we transitioned into the private sector to address a significant gap in the commercial real estate market: the need for accessible, actionable data. Today, our platform empowers professionals with precise acquisition intelligence, transforming how decisions are made across Mexico and, soon, all of Latin America.

In this blog post, we will explore how BlackPrint built the backend of its intelligence platform with Fused to provide a comprehensive view of the commercial real estate market in Mexico.

## The Problem: Challenges in Real Estate Data
Real estate professionals face fragmented data sources and manual processes. This complexity limits access to critical metrics such as zoning regulations, demographic patterns, and traffic trends, making site selection decisions both high-stakes and prone to errors. BlackPrint recognized this challenge and set out to create an all-in-one platform, transforming disjointed data into actionable intelligence to empower professionals across the commercial real estate landscape in Latin America.
Our Solution: The BlackPrint Approach

BlackPrint's platform offers a comprehensive suite of tools and insights, including property and zoning data, demographic and socioeconomic analytics, and detailed foot and vehicle traffic analysis. Underpinning this solution is a staggering volume of geospatial data sourced from diverse datasets such as cadastral records, demographic studies, and traffic sensors. This vast amount of information is meticulously analyzed to deliver actionable insights, all while being presented through an intuitive and user-friendly interface. By simplifying complex data into accessible formats, BlackPrint empowers users‚Äîregardless of their technical expertise‚Äîto navigate and leverage insights with ease. Building such a robust platform required not only expertise but also the right tools. That's where Fused came in.

## Leveraging Fused: A Partnership for Efficiency

Fused became an indispensable partner in turning our vision for BlackPrint into reality. Its end-to-end cloud platform allowed us to move from concept to MVP significantly faster by simplifying our data processing and data delivery workflows. Before using Fused, we faced daunting challenges, such as processing and visually inspecting terabytes of traffic data and massive datasets from the Overture Maps Foundation for points of interest.

These tasks, which previously required extensive time and resources, became streamlined and efficient with Fused. Its ability to process geospatial datasets via https-accessible Python functions made the development process seamless, enabling us to focus on creating an intuitive, high-performance platform. With Fused, we could deliver real-time insights and user-friendly visualizations at a scale that was unimaginable only a few years ago.

## Empowering Professionals with Unprecedented Insights

BlackPrint's platform is transforming real estate decision-making by delivering unprecedented insights to professionals. For example, a retailer in Mexico City utilized our tools to identify a high-traffic site with optimal customer reach, saving weeks of manual analysis and significantly improving accuracy.

Our platform's effectiveness is measured through key metrics like time saved, improved precision, and enhanced ROI for our clients. Users have reported up to a 30% increase in efficiency when planning site expansions or evaluating investments.

Looking ahead, BlackPrint's vision extends beyond Mexico, aiming to revolutionize acquisition intelligence across Latin America. With partners like Fused, we continue to innovate, making geospatial analytics scalable, intuitive, and accessible to professionals at every level.

## Conclusion

BlackPrint Technologies is on a mission to redefine real estate intelligence by delivering actionable insights that drive smarter, faster decisions. This journey has been significantly accelerated thanks to Fused, whose serverless data delivery empowered us to process and serve complex geospatial data at an unprecedented scale. Together, we are shaping a future where real estate professionals can access intuitive, data-driven tools that simplify their workflows and enhance their outcomes. Join us in transforming the industry‚Äîvisit blackprint.ai to see how we are revolutionizing real estate decision-making.

================================================================================

## Hot-spot analysis for invasive species using Overture Maps
Path: blog/2025-01-21-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2025-01-21-elizabeth

**TL;DR Elizabeth Rosenbloom creates hotspot maps to identify key areas where Arundo donax is likely to spread, streamlining analysis to improve invasive species mitigation.**

In 2020 while working in Silicon Valley for the county of Santa Clara Valley, I became obsessed with improving monitoring and prevention efforts surrounding Arundo donax. The search and mitigation process this invasive plant species, Arundo donax, was a Sisophisian struggle that had been subject to the same procedures year in and year out, with no progress on beating the spread. To improve the efficacy and efficiency in battling against this notorious weed, I decided to build a tool that would identify the key areas for mitigation - both for the frequency of propagation (occurrence) and for spread potential.

In this blog post I show how I used Fused to create a map of key hotspots where Arundo donax is likely to spread based on built-environment factors derived from Overture Maps data.

To follow along, you check out the UDF associated with the blog post:
- Invasive_Species_Hotspot UDF

## Introduction

In 2020, the problem with managing Arundo was that many agency employees considered this to be a hopeless pursuit, given the exorbitant amount of time cleaning, layering, and calculating the weighted analysis would take. Despite our vast ArcGIS library of tools, engineers, hydrologists, and GIS managers all warned me that I was going to drive myself crazy trying to get the enterprise software to successfully run my analysis. My only regret in building the tool back then was that I didn't have a tool like Fused to expedite the data pulling, processing, and calculating - as it would have saved me from the very lunacy I was warned about.

The obsession with the grass species, Arundo donax, began with an insight to the positive feedback loops created by increased flooding and the spread of invasive species due to climate change. Arundo donax is one of the most invasive plant species worldwide. In addition to destroying biodiversity and disrupting habitats for native species, this large grass also contributes to significant flooding patterns. As weather events become more severe and biodiversity declines, these changes create compounded consequences in our changing climate.

## Challenges with Hot-spot Analysis

Hot-spot analysis tools using weighted sums can significantly increase accuracy in targeting key areas for prevention. To build a hot-spot analysis for Arundo donax, the following variables need to be scored according to their degree of influence: distance to nutrient loading sources (such as a golf course), distance to a riparian buffer (creek or river), distance to a water-flow disruptor (such as a bridge), and size of the stream.

The key pain points of running a weighted sum on traditional GIS software include:
- Slow calculations: "State-of-the-art" software like ArcGIS can take several hours to calculate weighted sums. Furthermore, running weighted sums on large datasets/geographic areas can be nearly impossible given exhaustive RAM and GPU demands, so analyses over 1000 sq miles often require a user to split analyses into different geographic regions.
- Program crashes: beyond the significant wait time required for typical weighted sum calculations, users of prominent GIS software often experience output delays as a result of runtime errors and other issues spurring a program crash.
- Data transformations, cleaning, and standardization: most users of traditional GIS software will find they need to start from scratch when compiling data for a hot-spot analysis or weighted sum. Sometimes, standard base layers like slope and aspect will be searchable on the local software basis. Still, often, lengthy transformations are required to make the layers compatible with the final overlay calculation.

## How Fused Changed My Workflow

Using cloud-based systems like Fused can significantly increase calculation speeds, program resilience, and access to public Cloud Native datasets.

After encountering the Fused and learning about how I could improve the speed and geographic spread of site suitability analyses, I wanted to put it to the test by expanding on a previous analysis I did in 2021 using ArcGIS. The original 2021 analysis took several months of data collection, interviews with other local agencies, and extensive data cleaning, standardization, and transformations. I experienced all the aforementioned pain points of hot-spot analysis/weighted sum calculations and more.

Flash forward to today, where I am compiling global data sets, layering them, and deriving statistical insights within 1% of the time that it took me using ArcGIS. Running buffer analyses, weighting variables, and procuring data has taken a fraction of the time for GLOBAL data - and if you remember from before, the previous analysis from 2021 took months for a county-wide calculation and final product.

The most satisfying aspect of my new hot-spot analysis application wasn't just the expansive end product; the process was more seamless and engaging than I had imagined.

## Workflow Design

I created a UDF with a simple model to identify hotspots susceptible to arundo. The model uses a weighted sum of several base Overture data classes:

- Golf Courses
- Bridges
- Water bodies (rivers, streams, etc.)

The UDF performs the following steps:
1. Create GeoDataFrames from the Overture maps dataset using get_overture
2. Generate an H3 score based on buffers around each feature
3. Aggregate the H3 scores to create a weighted sum

## Key Takeaways
Given the complexity of procuring, layering, and interweaving data along with significant wait times and resource consumption, many governmental agencies, non-profits, and even private corporations struggle to run spatial analyses such as site-suitability and hot-spot tools. Insights and tools can be created by improving the speed and efficacy of operations such as weighted sums and fuzzy analysis across spatial layers with UDFs.

Site-suitability and hot-spot analysis go beyond species detection. By simplifying the approach to these types of tools, we can more quickly and accurately detect climate-vulnerable zones, prioritize habitat restoration, and create models to build resilient communities. Industries such as real estate development, retail, and logistics can more quickly understand the variables that affect their businesses by using cloud-based systems like Fused, which can easily manage large datasets.

================================================================================

## Calculating Fire Ratings with Overture Buildings and Places
Path: blog/2025-01-20-amico/index.mdx
URL: https://docs.fused.io/blog/2025-01-20-amico

**TL;DR Chris Amico shows how to combine Overture Maps data with fire perimeters to analyze wildfire impact on buildings and businesses.**

As communities continue to rebuild and recover from the devastation caused by natural disasters such as wildfires, the question remains: How can we quantify what was lost, especially in the built environment? With the ability to analyze detailed building footprints and overlay fire boundaries, we can begin to answer this by providing a rough estimate of the damage and identifying which structures were impacted by the flames.

In this blog post, Chris Amico shows how by leveraging data such as Overture Building footprints and fire progression maps, we can gain insight into the extent of fire risk. This enables news agencies to derive insights such as count of shops or homes exposed or even assess the capacity of highway routes for residents to evacuate before a prospect fire reaches them.

To follow along, you check out the UDFs and app associated with the blog post:

- Fire Proximity GERS_Lookup UDF
- Fire Proximity Building_Score UDF
- Fire Proximity Buffer UDF
- Google Sheet that enriches the "Fire Risk" column for any given Building GERS
- App: H3 rollups within water buffer

### Introduction

Fused simplifies the process of spatially joining datasets with Overture Maps data, such as integrating with fire-related datasets.

For this example, we use the Inter Agency Fire Perimeter Historical dataset (published by the National Interagency Fire Center (NIFC)) which includes historical fire perimeters up to 2024. Joining fire perimeters with Overture Buildings and Places data enables us to highlight service gaps or identify regions that may require immediate response.

This demo will first select buildings within a buffer zone to determine which fire perimeter they fall within. Then, it will perform an H3 stats roll-up of business categories from Overture Places, counting the number and types of businesses that fall within each distance range. This will involve rolling up Overture Places business categories into H3 hexagons.

## The Workflow

These UDFs return Overture Buildings and Places within a buffer distance from a fire. They offers a simple way to determine the scope of possible fire damage and quickly assess the number of businesses, homes, and other significant structures within the affected area. By adjusting the buffer or selecting fire extent based on dates, users can fine-tune their analysis to gain deeper insights into how far the fire's reach extends and what establishments were most at risk.

### b. Fire Proximity Building Score

Next, we load the Overture Buildings dataset and spatially join it with the fire buffer zones. This workflow categorizes buildings based on their proximity to the fire, helping us assess which structures are most at risk.

1. Load the NIFC fire perimeter data
2. Create buffer zones around the fire perimeters
3. Load Overture Buildings
4. Spatially join buildings within the buffer zones to categorize them by proximity to the fire

### c. Overture Places Rollup by H3

Finally, we perform a spatial aggregation by calculating the H3 index for the centroids of Overture Places within the fire buffer. This allows us to roll up business categories into H3 hexagons, enabling a holistic overview of business distribution in relation to the fire perimeter.

1. Load the NIFC fire perimeter data
2. Load Overture Places
3. Determine the H3 for the centroid of each building
4. Normalize the 'categories' column into individual columns
5. Roll-up categories by H3, create categories primary set

## Conclusion and Next Steps

This kind of analysis helps understand not only the immediate impact but also in planning for future fire preparedness and recovery efforts. In this post, we saw how Overture and Fire-extent data can help us estimate the extent of the damage, from individual buildings to entire neighborhoods.

Organizations looking to integrate these types of perspectives into their workflows could create apps or API services that deliver derivative products, such as GERS lookups to categorize "fire risk" based on buffer proximity.

They could also use Fused HTTPS endpoints from the UDF to return a CSV with the GERS and "Fire Risk" score for buildings within a defined bounding box, as specified by a query parameter. Additionally, they could also use the HTTPS endpoints to automatically enrich the "Fire Risk" column of an arbitrary dataset for any given Building GERS. Users could apply it to any row of their table, with the functionality powered by a "GERS lookup" endpoint using a GERS ID query parameter.

================================================================================

## Characterize cities with embeddings of Overture Place categories
Path: blog/2025-01-16-maribel/index.mdx
URL: https://docs.fused.io/blog/2025-01-16-maribel

**TL;DR Maribel Hernandez shows how to create clusters of business categories using Overture Places data.**

Maribel Hernandez is a computer scientist and researcher at CINVESTAV, a multidisciplinary academic institution in Mexico. She specializes in graph theory in the field of computational genomics and complex networks. In this blog post, Maribel shows how she characterizes cities based on the distribution of businesses by rolling-up business categories by H3.

As someone who works with urban networks, Maribel's focus is on exploring how cities function and how their design impacts inclusivity. The core question driving this analysis is: Does the city have an even distribution of business services? Or are there shortages such as food deserts or unequal access to health facilities?

To follow along, you can clone and run the associated:
- Colab Notebook
- Overture Places Embedding Clusters UDF

## Introduction

Consider these scenarios:
- How connected are areas dominated by upper-class establishments to the broader city fabric?
- Are health services distributed with equal access from low-income neighborhoods?
- Do certain metropolis have better accessibility and service availability compared to others?

Take, for example, a comparison between 3 key cities in Mexico: Mexico City, Le√≥n, and M√©rida. Are neighborhoods in these cities equally served by essential services such as healthcare, food, and transportation? Could some areas be considered food deserts, while others enjoy easy access to all services?

_Preview of UDF on Workbench._

## Descriptive analysis

When analyzing the urban fabric of cities like Mexico City, Leon, and Merida, we can obsere distinct patterns of service distribution that manifest as rings around the city center.

_Mexico City._

### a. The heart of the city
Especially in Leon and Merida, the central area tends to form a cohesive, dense cluster of services. The core zone houses a variety of businesses including health services, retail, and food outlets, which are generally well-connected and easily accessible. Intuitively, the central cluster can be thought of as the "heart" of the city, serving as the primary hub for commerce, social interaction, and access to essential services.

_Leon._

### b. Islands of Services
Beyond the center, we notice smaller clusters of services scattered across the city, often in the form of "islands." These islands represent pockets of neighborhoods that, while not part of the dense city center, offer an array of unique services. These can serve to highlight the phenomenon of emergent neighborhoods within a greater whole.

_Merida._

### c. Peripheral ring
The periphery of these cities forms another distinct pattern. These outer regions also tend to cluster together in similar service categories, forming a ring that surrounds the central core. It's possible services in these peripheral areas are often more limited in scope and may reflect a focus on residential and less commercial needs, or reflect lower-income neighborhoods with scarcer access to key services.

## Conclusion
This ring-like pattern of service distribution suggests a common trend where the core of the city is highly accessible, while the periphery often lacks the same diversity and depth of business services. The islands of services in between can be seen as attempts to bridge this gap, but they are not always as effective in meeting the needs of the population on the periphery.

These spatial patterns offer valuable insights into the inclusivity of urban networks, highlighting areas that may need attention to ensure strategic business placement and guarantee equitable access to essential services.

## Future work
- Create Network: Create a bipartite network between place categories and H3 indices, using weights as counts for each category.
- Tie in Demographic Data: Integrate INEGI sociodemographic data at the census block group level to understand how services align with population needs.
- Access by Neighborhood: Use origin-destination (OD) movement networks to evaluate service accessibility by neighborhood of residence.
- Segregation Analysis: Identify zones with low visit rates or difficult accessibility. These zones may represent areas of segregation or neglect in the urban network.

================================================================================

## Streamlining the design of parcel delivery routes with H3
Path: blog/2025-01-09-antonius/index.mdx
URL: https://docs.fused.io/blog/2025-01-09-antonius

**TL;DR GLS uses Fused to create internal tooling to optimize routing for parcel delivery operations.**

In the parcel delivery business, geospatial analyses are crucial to answer questions about daily operations. Do delivery drivers visit the same regions each day, letting them know their areas intimately? Or is there a high volatility of the regions? And of course, how do we optimize the routes of multiple drivers servicing the same region?

Those are the questions Antonius is working on at GLS Studio, an innovation lab by GLS (General Logistics Systems) which is an international parcel delivery service provider.

In this blog post, Antonius highlights how he uses Fused to create stable delivery areas for single-day and multi-day aggregates.

## The Challenge Designing Delivery Areas

While the planned areas of driver tours to delivery packages can be rather well-defined, an evaluation of the areas actually served by drivers is equally important and might not be as easy. It can be used for guiding delivery drivers to become more efficient while also enabling managers to identify planned areas that are suboptimal due to built environment features like rivers or big highways.

Creating delivery areas out of single geospatial data points is challenging. A na√Øve approach would be to use convex polygons, but this causes multiple issues:
- A single data point can have a high influence on the shape and area of the polygon
- A tour consisting of multiple not connected sub-areas is hard to detect and correctly display
- This limits the area to what is covered by historic data which leaves a gap in new target regions
- Calculations using polygons are computationally expensive, hindering ad-hoc changes to the selected time span or the calculation parameters

Building new and sometimes experimental features means that the database is often not optimized for the use case. Data needs to be joined between multiple tables and even between multiple data sources. Therefore, fetching all data can be slow, highly limiting the usefulness of having an experimental feature to play around with.

## Solving the Challenge with H3 and Fused UDFs

### Dynamic Delivery Areas with H3

The solution to the first problem came by using H3 cells instead of polygons. By assigning cells to the drivers based on their historic deliveries to the cell, driver areas result automatically. Using H3 cells across different resolutions also allows us to represent the differences between urban and rural areas which see different parcel volumes. While there exists one "base resolution" to ensure non-overlapping and complete areas, the logical hierarchy among H3 cells can be used to calculate on lower resolutions for rural areas that see fewer deliveries, speeding up the computation and ensuring a broader coverage of those areas beyond the historical data points.

On the other hand, disputed H3 cells can be broken down to a higher resolution and assigned to different drivers or, when the "base resolution" has been reached, assigned to the driver delivering most parcels to the cell. As H3 cells have clearly defined neighborhoods, areas can easily be extended beyond their historical limits when desired, covering the empty space around to include a new parcel that falls outside of historically served area boundaries.

_Fused app to show dynamic delivery areas at different H3 resolutions._

### Streamlining workflows with Fused UDFs and Caching

Fused UDFs helped us solve problems around the latency of querying and calculations. When a user looks at an area for a day, they are probably interested in the same area on some of the previous and following days as well, right? So why not pre-calculate that already?

Using Fused, it is simply a matter of fire-and-forget to trigger the UDF with parameters for some previous and following days which are then already running to cache. So when the user views an adjacent day, the data will already be there. And more broadly, when it is possible to limit the number of parameter combinations in an experimental feature to a manageable amount, this fire-and-have-it-cached approach is not limited to caching data from previous and following days, but can also be used for a range of other cases.

_Sample workflow with Fused UDFs._

## Conclusion

When developing new features that are not yet supported by the current data infrastructure, Fused UDFs enabled us to easily test things without having to change the underlying infrastructure in advance. The UDFs are easily shareable and adjustable, allowing testing by multiple people without having to run code locally while automatically making sure everyone is using the same code that is hosted in the UDF. And because we can easily call UDFs with HTTPS endpoints, when we have verified the feasibility of a feature, it's easy to integrate into our product.

================================================================================

## The Strength in Weak Data Part 3: Prepping the Model Dataset
Path: blog/2024-12-12-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-12-12-kristin

**TL;DR Kristin shares a UDF to create training data for a corn yield prediction model using Zonal Statistics.**

Now, suppose we want to do this where corn is grown in the midwest US. Here is what the states that grew corn in 2018:

Within these states, we have **1,333 counties**. Assuming each is similar in size to my home county of Lyon County, we can calculate:

1,333 counties √ó 20,000 data points = **26 million data points**

That's **20,000 times** the statistical power! üéâ

Let me say that louder for the people in the back: **26,000,000 vs. 1,333 data points**

And that's just for one time period. If we run a model on 2‚Äì4 time periods, we're looking at nearly **100 million data points**. Now, building a model on 100 million data points isn't trivial, but at Fused, this process becomes a breeze.

## Building the Training Dataset

I'm aiming to predict corn yield based on my SIF readings from early May, late May, early June, and late June. So, we need to build out a table with this structure:

| County  | Year | Yield (bushels per acre) | Area of Corn (acres) | Area of County (acres) | SIF Value-201605a (early May) | SIF Value-201605a (late May) |
| --- | --- | --- | --- | --- | --- | --- |
| 17015 | 2016 | 205 | 124,145 | 3,032,0383 | .15 | .65 |
|  |  |  |  |  |  |  |

To quickly validate this table against a map, I'll build out my workflow in Fused using Python and query the table with SQL. In the past, working with these two languages would have required complex tooling, storing data in a warehouse, and roughly **five hours** to run. With Fused, I can simply reference the a GeoDataFrame object and query in SQL with DuckDB all within the same UDF‚Äîtaking just **five seconds**!

Here's what it looks like:

## Splitting the Data

But we're not stopping there! To ensure our model is both robust and unbiased, we need to carefully split our data. Enter Walk-Forward Cross-Validation ‚Äî a game-changer for time series data. Think of it like planting seeds each season and harvesting them before the next planting. By always training on past data and testing on future data, we respect the natural flow of time. This method is perfect for corn yields because, just like how last year's harvest influences this year's, our model benefits from understanding those temporal dependencies. Plus, it prevents any sneaky data leakage, ensuring our predictions are based solely on what's known up to that point.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/krv3.mp4" width="100%" />

## Conclusion and Next Steps

Keep going or end here?

By progressively expanding our training set, each fold builds on the last, capturing more nuanced patterns and trends. To bring this to life, I'm leveraging TimeSeriesSplit from **sklearn**, seamlessly integrating it into our workflow. This tool simplifies the process, allowing us to focus on what truly matters‚Äîunlocking the full potential of our 100 million data points for actionable predictions.

And there you have it! We've taken weak, infrequent data and transformed it into a powerhouse dataset ready to drive smarter decisions in the $21 billion corn commodities market. Stay tuned for the next part of our journey, where we'll dive into building and fine-tuning our predictive models. Until then, keep cultivating those insights! üåΩ

================================================================================

## Streamlining Infrastructure Risk Analysis with Fused
Path: blog/2024-12-11-jacob/index.mdx
URL: https://docs.fused.io/blog/2024-12-11-jacob

**TL;DR Jacob at VIDA uses Fused to streamline processing and rendering of CMIP6 climate risk models, improving data sharing and sanity checks.**

================================================================================

## Map Overture Buildings and Foursquare Places with Leafmap
Path: blog/2024-12-10-qiusheng/index.mdx
URL: https://docs.fused.io/blog/2024-12-10-qiusheng

**TL;DR Dr. Qiusheng walks through how you can call Fused UDFs to load data into leafmap maps using Jupyter Notebooks.**

- Google Colab Notebook Walkthrough
- Leafmap Docs

## Calling Fused UDFs to load data

You first use leafmap to create a bounding box over an area of interest (AOI) `user_aoi` and create a GeoDataFrame `gdf_aoi` with it. Then, you can run the Overture Maps UDF, passing the AOI as a parameter to define the area to fetch data for.

================================================================================

## From query to map: Exploring GeoParquet Overture Maps with Ibis, DuckDB, and Fused
Path: blog/2024-12-06-naty/index.mdx
URL: https://docs.fused.io/blog/2024-12-06-naty

**TL;DR Naty shares a UDF to use Ibis with DuckDB's spatial extension to query and explore Overture Maps data.**

Naty is a Senior Software Engineer and a contributor to Ibis, the portable Python dataframe library. One of her main contributions was enabling the DuckDB spatial extension for Ibis in 2023.

In this blog post, she shows us how to leverage the spatial extension in DuckDB with Ibis to query Overture data. Ibis works by compiling Python expressions into SQL, you write Python dataframe-like code, and Ibis takes care of the SQL. Thanks to Ibis integration with Pandas and GeoPandas, you only need to do `to_pandas()` to get your expression as a GeoDataFrame.

}
  title="Overture H3 Skyline"
/> */}

We first establish a connection to a DuckDB database, in this particular case we have an in-memory connection. Then, we do `read_parquet` and we receive a table expression. Since our result, `t`, is a table expression, we can now run queries against the file using Ibis expressions. In this example, to start, we filter by some infrastructure subtypes (pedestrian, and water), select only a few columns, and limit our "search" to a bounding box `bbox`. Notice that this `bbox` is the Fused bounding box, not the overture maps one.

We then rename the column `class` to avoid conflicts with the deferred operator, and finally filter the expression by specific infrastructure classes like toilets, ATMs, drinking water, and other useful information. Up to this point, we only have a table expression, Ibis has a deferred execution model. It builds up expressions based on what you ask it to do and then executes those expressions on request.

To show an example of an aggregate, we executed and printed the `value_counts()` as a Pandas DataFrame. Ibis can execute the table against the DuckDB backend, and return it as a Pandas DataFrame or a GeoPandas GeoDataFrame (if `geometry` column is present), by only doing `to_pandas()`.

### Conclusion

The synergy between Ibis, DuckDB, and Fused has redefined the ease of querying and visualizing geospatial data. These frameworks provide an intuitive and powerful toolkit, enabling users to express geospatial queries, perform efficient transformations, and access high-performance analytics with minimal setup.

By leveraging this stack, interacting with vast geospatial datasets like Overture Maps becomes straightforward, efficient, and accessible.

### Resources

If you want to learn more about Ibis geospatial capabilities, check some of the geospatial blog posts here.

You might also find these resources useful as you dive into Ibis, DuckDB, and Overture:

- Overture Maps Data Repo
- Ibis Docs
- DuckDB spatial extension
- DuckDB spatial functions docs
- Ibis Zulip Chat

================================================================================

## Creating an app to model road mobility networks in Lima, Peru
Path: blog/2024-12-05-claudio/index.mdx
URL: https://docs.fused.io/blog/2024-12-05-claudio

**TL;DR Claudio used Fused to create an app to model road mobility networks in Lima, Peru, using GeoPandas, and OSMnx.**

On December 2023, I visited the Institute for Metropolitan Planning (IMP) in Lima. The director had invited me to share some of my geospatial analysis projects from my master's studies and explore potential collaborations. Around that time, Lima's mayor had announced a bold infrastructure initiative: building 60 flyover bridges to ease traffic congestion in one of the most gridlocked cities in Latin America.

1. Extracting the Road Network: Using OSMnx, I extracted road networks within a defined Area of Interest (AOI).
2. Enriching Data: Each road segment was assigned speed and travel time values.
3. Defining Population Data: A 1km¬≤ grid with population density and zoning data was loaded into a GeoPandas GeoDataFrame.
4. Setting Simulation Parameters:
    - Population size: Derived from density data.
    - Trips per person: Assumed at 2 trips/day (commute to and from work).
    - Origins and Destinations: Residential zones were assigned as homes and commercial zones as workplaces.
    - Trip Schedules: Normal probability distributions were used for departure (6-8 AM) and return times (5-7 PM).

With these parameters, the simulation sampled "home" and "work" nodes, calculated start times, and determined the shortest paths between origins and destinations. Async UDF calls made the process parallelized and efficient. The final output was a GeoDataFrame with:

- Start Time (Unix timestamp)
- Trip Type ("home" or "work")
- Path (list of coordinates)
- Timestamps (for each coordinate)

## Future Plans

This project is far from over. Here are the features I aim to add to make it a valuable tool for urban planners, especially in resource-constrained settings like Lima:

1. Larger AOI Support: Handle bigger datasets and simulate more trips.
2. Multimodal Routing: Incorporate walking, biking, driving, and public transit options, akin to OSRM profiles.
3. Custom Infrastructure: Allow users to model new transit infrastructure within the OSM road network.
4. Mobility Metrics: Provide detailed metrics (e.g., travel times, congestion levels) for each simulation.

With these enhancements, this tool could empower city stakeholders to make data-driven decisions on critical urban interventions‚Äîwhether it's building flyovers or optimizing public transit routes. The ultimate goal? Improving mobility for over 11 million residents in Lima and beyond.

You can try out the public UDF here
}>
<Iframe
  id="claudio"
  code=
  height="600px"
  useResizer=
/>
</div> */}

================================================================================

## Beyond RGB: Interactive Exploration of NEON's Hyperspectral Data
Path: blog/2024-12-03-guillermo/index.mdx
URL: https://docs.fused.io/blog/2024-12-03-guillermo

**TL;DR Guillermo used Fused to build an interactive tool for exploring NEON hyperspectral data, making large-scale geospatial analysis more accessible and actionable for researchers.**

_Source: NEON Imaging Spectrometer._

The real challenge, however, isn't collecting this rich data - it's making it accessible and actionable for researchers. This is where Fused enters the picture. Diving into their documentation and gallery of click-and-run examples, I found myself inspired by the platform's potential. By combining elements from various examples, I began building my first User Defined Function (UDF), eventually discovering the App Builder - a feature that would prove crucial in creating an interactive interface for hyperspectral data exploration. Having worked extensively with Google Earth Engine (GEE) and NEON data, the recent announcement of NEON AOP's availability through GEE presented the perfect opportunity to test Fused's capabilities. My goal was simple yet powerful: create a user-friendly application that could tap into this wealth of hyperspectral data and make it instantly accessible to researchers.

## Looking Ahead

Looking ahead, I'm already planning the next phase of this app. I just want to add functionality that tracks sampling locations and allows users to The experience of building this demo app has reinforced my belief in the importance of platforms like Fused in the geospatial community. They serve as crucial bridges between massive datasets and practical applications, eliminating infrastructure headaches and letting researchers focus on what matters most - the science.

For those interested in exploring NEON's hyperspectral data or learning more about this application, feel free to connect with me or try the app for yourself here.

The future of remote sensing analysis lies in making powerful data more accessible, and I'm excited to be working in this field.

================================================================================

## How DigitalTwinSim Models Wireless Networks with DuckDB, Ibis, and Fused
Path: blog/2024-11-26-sameer/index.mdx
URL: https://docs.fused.io/blog/2024-11-26-sameer

**TL;DR DigitalTwinSim uses Fused with Ibis and DuckDB to model high-resolution wireless networks.**

Sameer, co-founder of DigitalTwinSim, leads the development of advanced geospatial analysis tools to support the telecom industry in strategic network planning. DigitalTwinSim specializes in using high-resolution data to optimize the placement of network towers ensuring reliable, high-speed connectivity.

In this blog post, Sameer shares how he leverages Ibis with a DuckDB backend, and Fused to model wireless networks at high resolution. This approach enables him to quickly generate network coverage models for his clients. He explains and shares a Fused UDF that processes data in an H3 grid to evaluate optimal locations for network towers.

# Fused for Interactive Processing With Instant Visualization

Here, tools like Fused have become essential. Fused allows us to filter and visualize raw output data in a more interactive way, which we can also share with clients to illustrate network design and coverage areas.

To set up the UDF in Fused, we uploaded our data as a Hive-partitioned Parquet folder and created a UDF in Ibis to generate visualizations on demand based on zoom level and area of interest. At higher zoom levels, we compute the parent H3 index and aggregate data to show broader coverage areas; at lower zoom levels, we display individual H3 indices. The H3 polygons are generated and colored dynamically based on the data in the Parquet folder, allowing us to interactively filter data and share visualizations with clients.

Click here to launch the UDF in Fused Workbench.

# Conclusion

As network demands grow and requirements for high-speed internet access become more stringent, accurate, high-resolution modeling is essential for effective planning and deployment.

DigitalTwinSim's integration of tools like DuckDB and Fused, alongside Ibis and H3 grids, enables us to tackle the challenges of processing, analyzing, and visualizing massive datasets. By leveraging DuckDB's powerful data aggregation capabilities, we can manage and analyze high-resolution data efficiently, irrespective of memory constraints. Meanwhile, Fused empowers us to deliver interactive, client-ready visualizations, allowing stakeholders to better understand network coverage and performance.

================================================================================

## The Fastest Way to Download Foursquare's new POI Dataset
Path: blog/2024-11-21-foursquare-poi/index.mdx
URL: https://docs.fused.io/blog/2024-11-21-foursquare-poi

**TL;DR The Fused Team made Foursquare's open dataset of 100M global places accessible via GeoParquet files which you can access via a UDF.**

Foursquare just released an open dataset of over 100M global places of interest.

We at Fused have partitioned these points into easily accessible GeoParquet files, and hosted them on Source Cooperative

On top of that, we've build a publicly available User Defined Function (UDF) that anyone can use to easily look at & download to GeoJSON, all from the browser

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="30vh" url="https://youtu.be/ubRnHvPuY40" width="100%" />

\
**Try it out for yourself!**

You don't need to login or create an account to easily access the Foursquare POI points

- Try out Fused for yourself for free!
- Get familiar with Fused by checkout out our Quickstart docs
- Follow us on LinkedIn to keep up with updates
- Read this more in-depth look at the whole dataset from our community member Mark Litwintschik

================================================================================

## How I Got Started Making Maps with Python and SQL
Path: blog/2024-11-04-kent/index.mdx
URL: https://docs.fused.io/blog/2024-11-04-kent

**TL;DR Stephen Kent shares his journey making maps with Fused using Python and SQL.**

I am a self taught developer and data enthusiast. I first came across the spatial data community when I saw a Matt Forrest video on LinkedIn where he demonstrated how to visualize buildings from the Vida Combined Building Footprints dataset with DuckDB. Immediately I thought, what if you could see all the buildings in a country, say, Egypt? I set out to do just that and made this map with DuckDB and Datashader.

_Buildings in Egypt._

Find Stephen's UDF code here:
- Five Minutes Away in Bushwick Brooklyn UDF

## Starting with Fused

The day after I posted that image on LinkedIn, in April, 2024, I had a call with Plinio Guzman of Fused. I told him what I had been up to, and he was enthusiastic and confident that Fused would fit my needs. One key feature he mentioned was the live development. While I was developing that Egypt map, I had to start the ETL to the final product over and over until I got it looking the way I wanted.

So I got started right away. I found Fused User Defined Functions (UDFs) like Overture Maps and the S2 Explorer and traveled all over the world looking for stunning images. It was thrilling to fly from New York to Tokyo and see the results render instantly.

_Exploring the world with Sentinel 2._

I then began to change the components of these UDFs to see different Overture types, but at this point I was hesitant to build my own UDF from scratch.

## Instant UDFs

That was until Fused launched its File Explorer. With one click, it was now possible for me to create a UDF from providers like Source Cooperative and visualize with numerous presets like DuckDB or GeoPandas. With this new feature, I recreated my Egypt map with the same Vida dataset, this time using DuckDB with the H3 extension. It was liberating, I came to realize the components were simpler than I thought.

## Local Tests

I used DuckDB with the H3 extension without Fused to query Overture Maps for countries and continents all locally in a Jupyter notebook. The benefit with the H3 extension is that if you set up the query right you can aggregate larger than in memory datasets at ease from your notebook.

_Road Density in Africa._

And made this Egypt building map with H3, how does it compare with the Datashader version up top?

_Egypt Building Density with H3._

## Fused and Overture Maps

In August, Fused announced a tighter partnership with Overture Maps Foundation and that came with even more Overture features. Like with Source Cooperative I could now instantly generate UDF of buildings, places, land use, or roads, etc by joining parquet files (and more). I proceeded to use the framework of that UDF to join all kinds of data.

_Proximity analysis between Road Networks and Hospitals in Paris._

## Joining H3 with GeoJSON

One day I was looking at the DuckDB_H3_Example, and I was struck ‚Äî what if I joined those cells with Overture Buildings? I learned how to use the DuckDB H3 extension from all of the example UDFs on Fused. So I called that UDF in an Overture UDF and used GeoPandas to join the two. The result is the map below. The color of the buildings comes from the count of corresponding Yellow Cab pickups. There are millions of points in this TLC parquet file, and H3 helped me to aggregate to thousands for an easier spatial join.

_Overture Buildings joined with H3 Yellow Cab pickups in New York City._

I made this particular map with Kepler.gl, with two clicks from the Workbench. I could have also exported the data to tools like Felt and Mapbox. You can find the code I used to recreate this map here.

## App Builder

I just started working on the Fused App Builder, and made a dashboard to view and interact with NYC‚Äôs 311 call data as a 3D H3 heatmap. Anyone using it can set the date range and resolution to change the display. Very fast and easy to use.

## Community

There's so much exciting data science happening on Fused. Check out Kevin Lacaille's post on ML-less global vegetation segmentation at scale. And Christopher Kyed's Analyzing traffic speeds from 100 billion drive records, that is the kind of project I would love to work on.

I continuously find inspiration as I browse community UDFs. Here's a join of H3 heatmaps with Overture types. This is a heatmap of connectors (intersections) joined with segments (roads) in London. The darker colors have more intersections. I am looking to incorporate traffic counts.

_Road density in London._

## Conclusion

I have been using Fused for several months but it feels like I am just getting started. It seems like the only real limit here is what I can dream up.

_This is a cross-post of Stephen Kent's Medium Article published October 14, 2024._

================================================================================

## Discovering NYC Chronotypes with Fused
Path: blog/2024-10-30-elizabeth/index.mdx
URL: https://docs.fused.io/blog/2024-10-30-elizabeth

**TL;DR Elizabeth Cultrone analyzed NYC Taxi pickup data to identify neighborhood boundaries based on activity patterns. She created a UDF to implement H3 binning and similarity metrics.**

Neighborhoods within a city have consistent characteristics but often have ill-defined boundaries. Some neighborhoods are more similar than others even though they're not nearby. Understanding these local boundaries and the demographics, dynamics and behaviors of different areas affects a wide range of business applications, including advertising, site selection, business analytics, and many more.

_Highlighting natural catchment area boundaries around Koreatown._

## Statistical Analysis

In the App Builder, we created graphs to summarize the similarity values shown in the map. Histograms of the pickups across the most and least similar hexes to each location confirm that the distributions are different for each. We can also explore the cumulative count of hexes to help determine an optimal threshold for similarity values, depending on the application.

_Comparison between most and least similar hexes of two AOIs._

## Conclusion

The Fused UDF Builder makes developing and iterating on these analyses swift and convenient, with no need to jump between different environments for developing vs viewing the results. And although the taxi dataset is small, the Fused Tiling functionality offer the possibility of developing similar analyses with larger datasets. With more data and richer features this proof-of-concept could be expanded to discover more robust, fully-defined neighborhoods, allowing us to develop data-driven approaches to local geography.

================================================================================

## Earth-scale AI pipelines for Earth Observation (Part 1: Data Curation)
Path: blog/2024-10-29-durkin/index.mdx
URL: https://docs.fused.io/blog/2024-10-29-durkin

**TL;DR Fused simplifies how Earth Observation data is processed to curate training data for AI models. Gabriel Durkin shows a Streamlit app he created to train and run land use and crop detection models.**

Later, as part of the workflow, I will demonstrate spatial querying that will allow us to choose custom regions on which to train a model - simply by outlining polygons on a map.

## Article structure
This is the first of a 3 part series that will show how Fused can be used in querying, processing/ETL, visualization, and inference tasks across stages of the model development lifecycle:
1. **Data curation:** Prepare and generate training images and masks.
2. **Model training:** Feed curated data into a state-of-the-art multiclass segmentation model.
3. **Model Inference:** Establish generalization limits by evaluating model performance on user-selected holdout regions from different locations and timeframes.

## The role of Fused

Fused is a platform that simplifies the engineering challenges involved in building data workflows for Earth Observation (EO) analysis. Its key features for curating training data include:
- **Python User Defined Functions (UDFs):** UDFs define transformations on data and can easily be called with parameters and across different areas of interest.
- **Remote Accessibility:** UDFs can be called from anywhere via HTTPS requests, delivering data exactly where it's needed without the need to store or transfer large datasets.
- **Parallel Execution:** UDFs can run in parallel, processing thousands of data chips per minute for efficient scaling.
- **UDF Workbench:** The UDF Builder provides instant feedback during algorithm development, allowing users to visually inspect the resulting data chips on a dynamic map. Any changes are immediately deployed upon saving.

The synergy of these Fused components enables researchers to dedicate more time to experimentation and analysis rather than data engineering.

> "Ultimately we want Data Scientists to be able to deliver autonomously ‚Äî without operational reliance on a dedicated engineering team, especially given the unwieldy scale and volume of earth observation data."

## Problem overview

An example workflow addresses the challenge of developing a model to categorize and quantify agricultural land use across the continental U.S. with multispectral satellite imagery. This task involves complex multi-class segmentation with several key challenges:

- **Image variability:** Satellite images can vary in resolution, quality, brightness, and cloud cover. Crop reflectivity fluctuates across regions and growing seasons, impacting data selection and algorithm design.
- **Engineering limitations:** Traditional approaches restrict the number of iterations researchers can perform to design and tune a model.
- **Increasing data complexity:** The growing number of spectral bands and satellite sources available requires a systematic approach to selecting an index combination.

By automating engineering processes such as image chipping and source harmonization (time, space, and projection) to prepare training data, researchers can have more iteration cycles as they define spectral indices and band combinations that generalize images.

## Input datasets

Our example model will use 3 bands of Sentinel 2 satellite imagery as input to our ML model to predict a CDL crop mask value - the target layer. We will leverage a Fused "Cube Factory" app we built to generate the chips or "datacubes" that contains both inputs and target. Thinking bigger, there are multiple fused assets which can take the role of input or target. A strength of the fused Cube Factory app is the implicit matching of data in both spatial and temporal dimensions. A further goal of this blog is to demonstrate how easily custom apps like this one can be created and shared on the Fused App Builder.

- Sentinel 2 offers 13-bands of 10 meter resolution imagery with broad temporal and spatial coverage. We chose the Glacier index as the Pseudo Red Channel and Channels 8 (NIR) and 11 as Green and Blue, respectively (image above).
- Land Use Land Cover (LULC) dataset provides a global land cover classification
- Digital Elevation Model (DEM) dataset provides terrain elevation
- Sentinel 1 offers HV and VV polarization bands of 10 meter resolution - active imaging data derived from SAR with broad temporal and spatial coverage and a 6 day cadence.
- The USDA Cropland Data Layer (CDL) dataset maps individual pixels of 30m resolution to hundreds of crops and land types like 'soybean' and 'corn', or even 'pumpkins' and 'cherries'. Below you can see a hyperspectral image of central california, and the dominant crop and land classes in the associated CDL image below.

An additional flexibility offered by the app is the variety of Fused assets that can be called as either inputs or targets. For the viewport visualization we set the left side of the split map as the input layers and the right side as the targets.

Returning to our chosen target, the CDL dataset, it contains up to 256 named crop classes (see: CDL_names_codes_colors), we grouped these together into 10 superclasses to simplify our task.

1. Background/Missing BG
2. Wheat WH
3. Corn CO
4. Soybeans SB
5. Grass/shrub/open lands OL
6. Forest and wetlands FW
7. Open water OW
8. Developed DV
9. Other agricultural (catch-all) OA
10. Other/barren OT

```json

```

## Data curation workflows

The Mask2Former model we will employ for this exercise is optimized for 3 channel RGB images, so we'll use Fused UDFs to create a 4 layer datacube, three inputs from Sentinel 2 and one CDL layer as target. Then, we'll call them across a set of tiles to create the training dataset.

For this example, I selected 3 bands to predict with: glacier index (red-green normalized difference index) is on top, and bands 8 and 11 from Sentinel palette in the middle. The CDL mask at the base of the datacube has pixel values that correspond to major crop classes.

## Create datacube

The first 3 channels of the datacube encode Sentinel 2 bands as RGB false color channels, and the fourth channel encodes the CDL mask are associated with the following 3 UDFs:

1. Choose Right and Left map layers from the Fused asset collection.

2. Choose a Survey Period and location using the left side pop-out menu.

3. If using Sentinel 2 - select 3 of the available 13 bands or one of 6 normalized difference indices.

4. If using digital elevation model as input or target (left or right map side), choose whether terrain, or gradient modes, choose scale parameters, and choose a colormap for visualization.
5. Draw polygons on the map.

## Model training

To train a model on our datacube we downloaded the Mask2Former model from Hugging Face and fine-tuned it on the 10,000 datacubes of 240x240 pixels (resulted in ~1GB of training data). This can be done locally on a GPU machine, or in the cloud on colab or paperspace gradient (now digital ocean). In our next post we will demonstrate how to train ML models natively on the Fused platform.

### Model Hosting

The final model is available on a dedicated Inference endpoint hosted by Hugging Face, and accessed via a API call through a custom Fused UDF. It is available in the Cube Factory app for inspection as the Mask2Former ML layer. This allows a qualitative side by side comparison with the CDL layer. The model was trained on July imaged crops in the midwest among other areas, here is how it performs vs ground truth near Jacksonville Illinois. The left side shows the target layer and the right side shows the model prediction.

<LazyReactPlayer playsinline= className="video__player" playing= muted= controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/blog-assets/gabe_slider_HD.mp4" width="100%" />

## Conclusion

This post showed how Fused makes it easy to generate training datasets. It gave a practical example of how with Fused UDFs the data loading, data preprocessing, and data loading happen behind an HTTPS endpoint, enabling easy data retrieval from anywhere. There's no need to pre-compute, store preprocessed data, or manually generate tiles, as Fused dynamically generates data for the specified tiles using simple HTTPS calls.

This approach offers several advantages to build customized training datasets. Since data generation is on demand and gets cached, a data scientist can quickly iterate, adjusting spatial or temporal parameters without worrying about managing storage or running jobs to generate datasets. The flexibility to load data that is needed, when it's needed, accelerates experimentation and refinement of models.

To replicate this work for your own AOIs, you can try out the Cube Factory app yourself or run the underlying UDFs on your own Fused Workbench environment. Please feel free to contact me on LinkedIn.

## References
- [1] L. L. Zhang, S. Dhaka, et al., Building a Crop Segmentation Machine Learning Model with Planet Data and Amazon SageMaker Geospatial Capabilities (2023)
- [2] S. Hamdani, Supervised Wheat Classification Using PyTorch's TorchGeo ‚Äî Combining Satellite Imagery and Python (2023)
- [3] Sentinel-2 Mission Overview (2023)
- [4] Digital Elevation Model (DEM) Data (2023)
- [5] ESA WorldCover 10m 2020 V100 (2023)

## Additional Resources
- USDA Cropland Data Layer Methodology
- Sentinel 2 Spectral Band Indices
- Fused Documentation

================================================================================

## DuckDB, Fused, and your data warehouse
Path: blog/2024-10-24-stefano/index.mdx
URL: https://docs.fused.io/blog/2024-10-24-stefano

**TL;DR GLS Studio uses Fused to optimize Snowflake queries. This enables route planning in their ParcelPlanner app with H3-partitioned geospatial data served to a Honeycomb Maps frontend.**

We then created two key UDFs within Fused. These work in tandem to handle authentication, caching, and efficient data retrieval for our DuckDB-powered map:

- **The "Public" UDF (Hammer):** This UDF isn't cached and serves as the entry point. It handles authentication and collects the full date range requested by the customer.
- **The "Private" UDF (Nail):** This cached UDF takes a single date and returns the necessary data for that specific day.

The "Hammer" UDF spawns up to 1,000 asynchronous Fused workers, each running an instance of the "Nail" UDF to fetch data for an individual date, as specified with a parameter. Once the data is retrieved, it is stitched together into a single GeoPandas DataFrame, ready for use.

With this approach, historical data only needs to be read once from Snowflake. For the present date, which is subject to updates, we handle caching differently and apply a one-hour cache to optimise performance.

## Conclusion

In the end, Fused allowed us to integrate our Honeycomb maps directly with Snowflake, handling caching and security concerns. This approach saved us significant backend development and data engineering work‚Äîall with just a few dozen lines of Python.

================================================================================

## The Strength in Weak Data Part 2: Zonal Statistics
Path: blog/2024-10-22-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-10-22-kristin

**TL;DR Kristin created a UDF to mask cropland areas using USDA data and run a Zonal Statistics workflow for corn yield predictions.**

Farming isn't static‚Äîcorn fields rotate with soybeans or cover crops yearly, adding noise to our data. Here's a 25 km¬≤ area:

This block includes not only farmland but also trees, towns, and water bodies. Our challenge is to isolate the specific areas where corn is grown to enhance the precision of our analysis. Enter Fused, which has a Public CDLs UDF that reads the USDA Cropland Data Layer, letting me specify the year and crop type to pinpoint corn accurately.

## Masking crop areas with a UDF

To tackle this, I created a Fused UDF that loads the USDA Cropland Data Layer for a specified year and crop type to identify corn-growing regions. I then used corn-growing regions to mask a Solar Induced Fluorescence raster. Finally, I calculate its mean values for each county.

Now for the fun part:

1. **SIF Data:** Display SIF for a specific month from a NetCDF file.
2. **Corn Areas:** Map corn cultivation that year from a GeoTIFF file of the Cropland Data Layer (CDL) data product.
3. **Precision Clipping:** Clip layers to show SIF values only where corn grows.
4. **Zonal Statistics:** Aggregate the SIF that incides on corn crops for each county.

You can see the UDF code here and even clone it to your Fused workspace.

**Voila!** From one county's weak data to creating summary statistics for the county. This provides the ingredients to boost the prediction strength and reduce noise in the prediction model I want to build.

## Scaling Up

Applying this to **400 Midwest counties** transforms our dataset from 400 points to **60 million**. The results?

- **Enhanced statistical power:** More data = stronger, more reliable predictions.
- **Improved accuracy:** Predictions are more closely aligned with actual outcomes.

Here is how the data compares on a map.

## Why It's simple with Fused

With Fused, working with rasters and vectors is straightforward. This blog post showed how I'm turning weak, unreliable data into a powerhouse of insights effortlessly.

## Ready to transform?

Curious to see the magic? Interact with the UDF in the Fused UDF Builder and elevate your data from weak to strong. Harness your data's full potential and make impactful decisions!

Feel free to reach out if you have any questions.

================================================================================

## Blazing Fast Geospatial SQL in DuckDB
Path: blog/2024-10-17-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-10-17-isaac

In this video from the FOSS4G 2024 conference, Isaac Brodsky, CTO and co-founder of Fused, shows the power of combining H3 with DuckDB to enhance geospatial data analysis.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://www.youtube.com/watch?v=hckjt7gfP20" width="100%" />

<br />

As an example, Isaac shows a Fused User Defined Function (UDF) that joins the Overture Places dataset and the Natura 2000 biodiversity areas dataset, achieving significant reductions in file size and query execution times. He showcases how the integration allows for efficient data exploration, filtering, and real-time queries, emphasizing the power of DuckDB's H3 extension and spatial extension.

Isaac explains how H3 simplifies geospatial analytics by offering common spatial index to join datasets and enables efficient storage & processing by converting spatial features into 64-bit integers. Additionally, DuckDB enables developers to conveniently transition between Python and SQL. He also highlights how DuckDB can simplify data processing architectures by querying data in real-time from object storage systems such as S3.

================================================================================

## Analyzing traffic speeds from 100 billion drive records
Path: blog/2024-09-25-pacific/index.mdx
URL: https://docs.fused.io/blog/2024-09-25-pacific

**TL;DR Pacific Spatial Solutions uses Fused to streamline data workflows and feature engineering to predict national traffic risk in Japan.**

_Drive recorder data points from a single day in a specific part of downtown Tokyo._

## Moving to Fused
Specifically, what we are trying to achieve is to map the speed values from the drive recorder data points to their nearest road. Using a traditional "nearest neighbor" approach would not be feasible, as we would need to measure the distance between billions of points and thousands of roads.
With our current cloud service provider we therefore had to rely on "clustering", so that data points that are close location wise would be close in memory too. This definitely increases performance, but adds some randomness to the processing time and cost because depending on where the area of interest lies in memory, you might have to search through all of your data to find it. As a result, to keep cost and processing time reasonable, we had to limit the nearest neighbor search area using a very small buffer. This was the only way to make our analysis with a dataset of this magnitude feasible.

_Nationwide drive recorder data points and their Fused spatial partitions._

### UDF Design
1. Use `bbox` to load GPS points and roads in the viewport.
2. Structure `DataFrame`s with the GPS points, road krings and road geometry.
3. For each point identify the road with the closest kring cell within a certain k distance, and map the speed to it.
4. Aggregate all of the speed values.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF=None,
    base_path: str = '...'
):
    from utils import df_to_gdf, list_s3, run_pool, get_GPS_road_data

    # Load ingested GPS and road data
    L = list_s3(f'/GPS_hex/')
    df_GPS, df_road_hex, df_road_geom = get_GPS_road_data(bbox, L)

    # Nearest neighbor calculation
    df_final = df_GPS.merge(df_road_hex, on='hexk')
    df_final['distance'] = (df_final['k']+0.5)*k
    df_final['cnt'] = 1
    dfg = df_final.groupby('segment_id')[['cnt', 'speed', 'distance']].sum().reset_index()
    dfg['speed'] = dfg['speed']/dfg['cnt']
    dfg['distance'] = dfg['distance']/dfg['cnt']

    # Introduce geometry to roads
    df = df_road_geom.merge(dfg)
    df['width_metric'] = df['cnt']**0.5/5
    return df.sjoin(bbox[['geometry']])
```

We now have our result which is a DataFrame representing the road network within our bbox. All the roads have their respective aggregated speed, distance and metric values as well as how many points were used for the aggregation. This result can easily be enriched by bringing in more columns from the base data such as the timestamp. This would make it possible to create hourly speed pattern analysis or maybe even a time series visualization.

For demonstration purposes, the video above shows this UDF running on a fraction of the ingested dataset.

_UDF result of Osaka Japan. Line width shows point density. Brighter yellow colors indicate high speed and darker purple colors low speed._

## Conclusion

By leveraging the spatial partitioning that Fused does during ingestion and the flexibility of the h3 library, we have created a method to reliably map our drive recorder points to their nearest segment.

The natural next step will be to scale our analysis using multiple machines and run on all of our data. To achieve this, we would iterate over each of the chunks that fused produced when ingesting our road data, instead of the bbox. This will ensure that our calculations are only run once for each of our roads. The modification can be achieved fairly easily in Fused and we are very excited to see how well Fused will be able to perform in this case.

================================================================================

## Creating cloud-free composite HLS imagery with Fused
Path: blog/2024-09-24-marie/index.mdx
URL: https://docs.fused.io/blog/2024-09-24-marie

**TL;DR Pachama partnered with Fused to generate cloud-free HLS image composites, improving tropical forest monitoring and canopy height mapping for carbon conservation projects.**

_Example composites highlight how the HLS-L30 product alone can have gaps when attempting to make a seasonal composite, as fewer cloud-free observations._

This blog post explores how Pachama's engineering team partnered with Fused to generate cloud-free seasonal composites using Harmonized Landsat Sentinel-2 (HLS) data, enabling higher quality optical imagery and better canopy height map creating ML model performance.

## Obstacles to create a cloud-free HLS image composite

The HLS dataset is an exciting development put forward by NASA's Satellite Needs Working Group. It provides consistent surface reflectance data with global observations every 2-3 days at a 30-meter resolution. The dataset harmonizes data from Landsats 8 & 9 with the European Space Agency's Sentinel-2A & 2B satellites such that the results are high quality, standardized, and able to be combined [2].

The HLS dataset consists of scene-level harmonized data, and does not create any cloud-free composite images by default. A significant amount of compute power is needed to process and combine this data, which contains multiple petabytes of data. Iteration on the compositing algorithm is also essential to quickly experiment and refine the process.

_Example of HLS image for a region in Brazil with clouds._

One common solution to this problem is to use Google Earth Engine (GEE). However, only the Landsat portion of this dataset (HLS-L30) is available on GEE. Without the Sentinel-2 portion of this dataset (HLS-S30), we do not get a 2-3 day temporal resolution that is required for cloud-free imagery in frequently cloudy areas.

## With Fused

Pachama turned to Fused to create scalable workflows for quickly iterating on a compositing algorithm. Fused's UDF model allowed Pachama to design algorithms that parallelize image processing, generate cloud-free composites, and run these workflows at scale.

### Pachama's UDF workflow

Here's the workflow we created with a Fused User Defined Function (UDF) to generate cloud-free composite HLS imagery.

### 1. Write a UDF to load imagery

This sample UDF loads data for the Landsat and Sentinel2 data products. It queries for a specific date range and does a first pass at filtering out images with too many clouds.

```python showLineNumber
# To Get your username and password, Please visit https://urs.earthdata.nasa.gov
@fused.udf
def udf(
    bbox: fused.types.TileGDF,
    mask_url: str,
    band_url: str,
    username="<INSERT USERNAME>",
    password="<INSERT PASSWORD>",
    env="earthdata",
):

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils
    # Authenticate
    aws_session = utils.earth_session(cred=)
    cred = 
    overview_level = max(0, 12 - bbox.z[0])

    # Read band data
    band_arr = utils.read_tiff(
        bbox,
        band_url,
        overview_level=overview_level,
        cred=cred,
    )

    # Read and apply cloud mask
    mask_arr = utils.read_tiff(
        bbox,
        mask_url,
        overview_level=overview_level,
        cred=cred,
    )
    cloud_mask = (mask_arr & 0b00000010) >> 1
    band_arr = np.where(cloud_mask == 1, np.nan, band_arr)

    # Filter nan's and convert to RGB values
    band_arr = np.where(band_arr == -9999, np.nan, band_arr)
    band_arr = band_arr / 10
    band_arr += 1 # workaround for uint8 and nan values
    band_arr = band_arr.astype("uint8")

    return np.array(band_arr)
```

### 2. Call the UDF asynchronously

This UDF queries the LP DAAC STAC catalog for data that matches the time and location of interest. This UDF then calls the previous one in parallel asynchronously to fetch each cloud-free image in parallel. It then combines the outputs, taking the median of each band to create a cloud-free composite.

```python showLineNumber

@fused.udf
async def udf(
    bbox: fused.types.TileGDF,
    date_range="2023-05/2023-06"
):

    from collections import defaultdict

    RGB_BANDS = ["B04", "B03", "B02"]
    F_MASK_BAND = "Fmask"

    # Query STAC catalog
    band_urls = get_band_urls(bbox, date_range)

    # Call the image loading/masking UDF in parallel
    tasks = defaultdict(list)
    for band in RGB_BANDS:
        for mask_url, band_url in zip(band_urls[F_MASK_BAND], band_urls[band]):
            arr_task = fused.run(
                "<INSERT UDF TOKEN>",
                bbox=bbox,
                sync=False,
                parameters=)
            tasks[band].append(arr_task)

    # Combine each band
    rgb = []
    for band in RGB_BANDS:
        task_results = await asyncio.gather(*tasks[band])
        composite_values = []

        # Convert back to format with nan's
        for arr in task_results:
            arr = arr.image.values.astype("uint8")
            arr = np.where(arr == 0, np.nan, arr)
            arr += 1
            composite_values.append(arr)

        # Take median of the composite values
        band_composite = np.nanmedian(composite_values, axis=0)
        band_composite = band_composite.astype("uint8")
        rgb.append(band_composite)

    return np.array(rgb)
```

The UDF above generates a cloud-free composite image and gives Pachama control and transparency over the image inputs.

_Example of cloud-free HLS image composite for the same region in Brazil._

## Benefits of using Fused

The best part is that Pachama's Data Science team can design UDF while looking at a specific area, and to run it for a different region by simply changing the input bounding box (bbox). This flexibility allows Pachama to create individual image tiles for any location worldwide. They can easily experiment and generate composites for different date ranges by adjusting the input parameters.

- Easy parallelization with simple Python function calls, no need to manage clusters
- Iterate on both UDFs in the same code editor with the UDF Builder
- Instant feedback during algorithm development, no need to wait for pipelines to run
- Invoke UDF and load its data into a Jupyter Notebook with `fused.run` for downstream analysis

## Conclusion

Thanks to Fused, Pachama's scientists and engineers can quickly iterate and experiment with different algorithms to optimize their image composites. Scaling the algorithm to apply to a larger area also becomes trivial by using Fused. Pachama can more efficiently improve transparency into forest carbon projects through better data and better insights, faster.

## References

- [1] On the Advantages of Using Harmonized Landsat Sentinel-2 Data for Monitoring Environmental Change
- [2] An Update on NASA's Harmonized Landsat and Sentinel-2 Project
- [3] An initial evaluation of carbon proxies for dynamic reforestation baselines

================================================================================

## The Strength in Weak Data Part 1: Navigating the NetCDF
Path: blog/2024-09-23-kristin/index.mdx
URL: https://docs.fused.io/blog/2024-09-23-kristin

**TL;DR Fused streamlined Kristin's workflow to integrate CSV and NetCDF data directly from S3.**

The resolution differences are huge‚Äîgoing from 30 square meters up to 5 billion! Traditional tools would have you pulling your hair out, but Fused lets you turn this "weak" data into something powerful.

## Actual Variable: Handling the Data Mismatch

When dealing with data that doesn't quite match up‚Äîlike trying to combine different resolutions‚Äîyou need to align everything to the coarsest resolution. In this case, that's the county level.

Here's how I tackled it: I grabbed a CSV file of county ANSI codes along with my actual variable data. Using Fused's Fused's File Explorer, I plotted the data easily. Just a quick visit to the File Explorer S3 bucket, a double-click on the file, and the entire map rendered instantly.

Remember the days of wrestling with shapefile resolutions? No more. I edited the UDF to pull my actual data CSV straight from my S3 bucket in under 30 seconds. Boom.

## Predictor Variable: Navigating the NetCDF

Now, let's get into the predictor variable‚Äîa NetCDF file from 5 degrees off the equator, covering around 25 square kilometers. NetCDF files can be a bit tricky to work with due to their complex formats, but Fused's utility modules make it easier. I imported some key functions directly into my UDF to clip the array, convert it into an image, and add a colormap.

```python showLineNumber
@fused.udf
def udf(bbox: fused.types.TileGDF=None, path: str='s3://fused-asset/misc/kristin/sif_ann_201508b.nc'):
    xy_cols=['lon','lat']
    utils = fused.load("https://github.com/fusedio/udfs/tree/057a273/public/common/").utils
    # Get the data array using the constructed path
    da = utils.get_da(path, coarsen_factor=3, variable_index=0, xy_cols=xy_cols)
    # Clip the array based on the bounding box
    arr_aoi = utils.clip_arr(da.values,
                       bounds_aoi=bbox.total_bounds,
                       bounds_total=utils.get_da_bounds(da, xy_cols=xy_cols))
    # Convert the array to an image with the specified colormap
    img = (arr_aoi*255).astype('uint8')
    return utils.arr_to_plasma(arr_aoi, min_max=(0, 1), colormap="rainbow", include_opacity=False, reverse=True)
```
Once I saved the UDF and created an HTTPS endpoint, I visualized the data interactively in the App Builder.

## The Variable That is Going to Make this Weak Data Strong

Okay, I have prepped my actual and predictor variables. Now, I will focus on how to fuse the geometries together using the variable that is going to make this Weak Data Strong (30 square meters). For that, stay tuned for Part 2, where I'll dive into the techniques for aligning and merging these spatial layers into a cohesive analysis. See you in the next installment!

================================================================================

## Enrich your dataset with GERS and create a Tile server
Path: blog/2024-09-19-overture/index.mdx
URL: https://docs.fused.io/blog/2024-09-19-overture

**TL;DR Fused enables on-the-fly enrichment of Overture datasets using simple spatial joins.**

================================================================================

## The App That Finds Your City's Rainfall Twin Globally
Path: blog/2024-09-17-milindsoni/index.mdx
URL: https://docs.fused.io/blog/2024-09-17-milindsoni

**TL;DR Milind analyzes global precipitation patterns using H3 indexing, cosine similarity, and Earth Engine data to create an interactive rainfall comparison app.**

## How It Works

Our UDF utilizes the following key components:

1. Earth Engine API: To fetch global precipitation data
2. H3 Index: For efficient spatial indexing
3. DuckDB: For fast query execution on geospatial data
4. Cosine Similarity: To compare rainfall vectors

## The Workflow

1. **Data Aggregation with DuckDB**: The data retrieval process is streamlined using Fused and Xarray:
   - **Fused and Earth Engine**: Fused simplifies access to Google Earth Engine's vast catalog. It provides a more intuitive and faster interface with a much better file manager for working with spatial data compared to the Earth Engine platform itself.

   - **Xarray Integration**: We use Xarray to work with our multi-dimensional rainfall data. It allows for easy handling of labeled arrays and datasets, particularly useful for time-series climate data.

2. **Data Aggregation with DuckDB**: After retrieving the raw data, we use DuckDB to efficiently aggregate it. This involves:
   - Grouping the data by H3 hexagon and month
   - Calculating the average monthly rainfall for each hexagon
   - Creating 12-element vectors representing annual rainfall patterns for each location

3. **Cosine Similarity Calculation**: Finally, we use cosine similarity to compare these rainfall vectors. This allows us to quantify how similar the rainfall pattern of one location is to another, or a reference pattern.

4. **Converting UDF to an app with Fused App Builder**: To make the rainfall similarity comparison UDF accessible and interactive, I used the Fused App Builder to help quickly build an app from the UDF that I just created. Every data scientists favourite prototyping tool is Streamlit which helps to build frontends in Python quickly and that's what the app builder brings to you! Convenience of Streamlit with the Power of Fused.

## The App Builder

If you are familiar with Streamlit, it is super convenient to build UI from just Python code. Folium maps helped me build interactive maps where I can draw areas to compare with and I could also write a custom HTML-based iframe to integrate Mapbox GL within the app itself, the snippets of which again are available in the Fused documentation.

1. **Interactive Folium Map**

I implemented a Streamlit Folium based map that allows users to select a location of interest.

2. **Plotly Charts**

A bar chart displays monthly rainfall data for the selected location in the folium map after querying the UDF and passing the GeoJSON shape as a parameter in the UDF,

3. **Iframe Integration**

- The hex-similarity map shows global rainfall pattern similarities.

### Calling the UDF within the App

Just one line of code to call my UDFs within the app to

- Fetch the historical rainfall data from Google Earth Engine for the marked area.
- Aggregate rainfall vectors
- Calculate the similarities of the location with the vectors in the bounding box in the iframe

It was as easy as `fused_app.run("fsh_****")`

### Performance and Optimization

Fused and Streamlit already have excellent caching mechanisms which helped me cache large amounts of data and information prior to the usage so that the next time the app loads, the computations are much faster! I can compare the rainfall patterns of any two locations on the Earth in seconds with a few lines of code. How cool is that!

> Building scalable Geospatial Applications have never been so quick and easy!

================================================================================

## Six ways to use Fused
Path: blog/2024-09-12-danieljahn/index.mdx
URL: https://docs.fused.io/blog/2024-09-12-danieljahn

**TL;DR: Fused is a versatile platform that serves as a code catalog, a parallel data processing engine, an app creation tool, a serverless HTTPS endpoint generator, and an IDE.**

*Example from How Pachama creates maps on-the-fly with Fused*

## 5. Geospatial Streamlit

Streamlit is a Python library that helps you create and
deploy web apps
with a few lines of code.

Streamlit is also the best first-time-user experience I've had with a library.
Without prior experience, I could immediately go from a Python script straight to an interactive web app.

With Fused's App Builder, any UDF can be turned into an interactive Streamlit app.
Fused also automatically serves the app for you.
While the app itself runs in the browser using Pyodide, it can call any Fused UDF, processing the data using the Fused engine.

  height="800px"
  useResizer=
  requirements=
/> */}

## 6. Geospatial-first IDE

Of the six, this is the most aspirational use case.
It's also potentially the most impactful.

Fused provides the Workbench, a great web-based IDE.
Working with it started changing how I think of developing geospatial applications.

[Image: ]

Today, there are two worlds.

- On one side, the software engineer uses test-driven-development to develop well-designed code in quick iterations.

- On the other side, the data scientist develops code directly against real data using notebooks and visualizations.

Fused can bring these worlds together. Simply annotating your function as `@fused.udf` gives you the ability to immediately visualize the results with real data, over any geographic region.
Fused Workbench does this, but you could equally develop in VSCode and switch to QGIS to immediately inspect the results.

By developing your code as a web of stateless UDFs and utilizing `@fused.cache`, you gain the ability to develop automatically cached pipelines whose results can be inspected in tools like Felt or served with an HTTPS endpoint without any added work.

Often the greatest cost of data pipelines is developer time.
Fused has the potential to tighten the development feedback loop and catch errors early, reducing the time needed to develop robust data pipelines.

## Conclusion

This article gave six concrete examples of how you can use Fused today.

However, the possibilities of Fused are not limited to these examples. With its powerful execution engine, visual IDE, growing host of integrations, and just-copy-the-link app deployment, Fused is generic enough to enable use cases not even the team behind it has thought of.

I'm excited about the future of Fused. I wouldn't be surprised to see it become a ubiquitous tool in the geospatial world.

================================================================================

## AI for object detection on 50cm imagery
Path: blog/2024-09-05-dl4eo/index.mdx
URL: https://docs.fused.io/blog/2024-09-05-dl4eo

**TL;DR Jeff Faudi used Fused for real-time object detection on 50cm satellite imagery, displaying results as an interactive web map.**

To display this image on the web, you typically need to project it in Web Mercator projection with gdal and cut it into 256x256 pixels tiles that will be displayed nicely by web-mapping applications such as GoogleMaps, OpenLayers, Mapbox, MapLibre, Leaftlet or Deck.gl.

Until recently, I would have done this physically and generated thousands of tiles. Now, we will do this almost magically with Fused.

## Creating a UDF

Basically, I just have to write the piece of code that generate the content of a tile and Fused takes care of running the code and providing the urls to share the layer in any application. The Python function that I have to write is called a UDF and it has at least one parameter which contains the bounding box (bounds) on which I need to generate the tile.

```python showLineNumbers
@fused.udf
def udf(
    bbox: fused.types.TileGDF = None,
    chip_len: int = 256):

    from utils import read_geotiff_rgb_3857

    geotiff_file = 's3://fused-users/dl4eo/my_image.tif'
    return read_geotiff_rgb_3857(bbox, geotiff_file, output_shape=(chip_len, chip_len))
```

First, it is worth noting that we extract all content from a GeoTIFF image (ideally a COG i.e. Cloud Optimized GeoTIFF) which contains the bands and geometric information about the satellite image. This GeoTIFF is stored anywhere on the cloud. Here, it is stored in the AWS S3 bucket provided by Fused. Also, note that the function returns an array for raster tiles but could return a GeoJSON for vector tiles.

We use the bounding box of the tile provided as a parameter, convert it from lat/long to Web Mercator (EPSG:3857), get the corresponding bounding box in the original image, and project it in Web Mercator projection in the destination array with the correct desired tile size (typically 256x256 pixels).

The Fused UDF Builder enables one to view the result and logs while coding.

## Implementing aircraft detection

Now, if we want to display a real-time aircraft detection layer, we could replicate the previous step: send the resulting image extract to the API and display a vector layer. However, we must avoid applying deep learning algorithms to images that might have been zoomed. These algorithms are typically trained at a specific resolution, and the Web Mercator projection does not preserve size.

_https://en.wikipedia.org/wiki/Mercator_projection_

We read the content of the Pleiades image in its original projection (either the raw geometry or a transverse mercator projection in which the central meridian would pass through the center of the image). In this case, the resolution is guaranteed to be the correct native resolution of the image.

The UDF gets the Pleiades image in the correct projection, then calls the prediction API, and finally returns the predictions in a GeoDataFrame which will be dynamically rendered on the map. For performance, we have added the @fused.cache decorators which make the function automatically cache results for identical parameters. The predictions are returned in pixels in the source image and then converted into lat/long so they render on a map. Then, when we look at the result in the workbench, we get some issues at the border of the tiles.

The reason is that if an aircraft is on the tile border, it will be detected partially on the lower tile and potentially on the upper tile. The two bounding boxes might not align perfectly so we cannot merge them. The solution here is to extract a image larger than the tile: if the center of the predicted box is inside the tile we keep it, if it is outside we discard it. We usually use a margin that is the upper size of the objects we are trying to detect i.e. 100 meters for aircrafts. After these little improvements, the result is much nicer

## Building a web app

Now that everything is running fine in the workbench, it is time to extract the layers and include them in a webpage. Fused provides an easy way to integrate layers in external applications via HTTPS requests. You just need to go to Settings, click Share and copy the provided URL.

Then, you can integrate this URL as the tile source in any mapping application. I am not diving into that here, but you can read how to do this in the DeckGL Fused docs. You can check the code source of the demonstration below. Here is the extract of the JavaScript Deck.gl code where the URL is integrated.

And here it is: the final working demonstration!

## Conclusion

Huge thanks to the amazing team at Fused for their incredible support, and to my former colleagues at Airbus for providing the stunning Pleiades image. I think that this application turned out to be very sleek and powerful. If the underlying satellite image changes, the AI layer gets automatically recomputed on the fly.

I'd love to hear your thoughts!

_This article was originally published in LinkedIn on June 20th 2024._

================================================================================

## Summarizing building energy ratings
Path: blog/2024-09-03-isaac/index.mdx
URL: https://docs.fused.io/blog/2024-09-03-isaac

In this video tutorial, I show a complete data app workflow in Fused. Starting with exploring the data in Fused, the tutorial walks through developing a UDF to serve the data, and then a Fused App to share results.

With Fused, this whole workflow takes just minutes from beginning to end. Fused helps me visualize the data at every step, iterate on my analytical logic, and finally publish a dashboard.

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://youtu.be/crakOM4Pytg?start=180" width="100%" />

================================================================================

## ML-less global vegetation segmentation at scale
Path: blog/2024-08-29-kevin/index.mdx
URL: https://docs.fused.io/blog/2024-08-29-kevin

**TL;DR Kevin used Fused to create a global vegetation segmentation layer without machine learning, displaying results as an interactive web map.**

================================================================================

## How Pachama creates maps on-the-fly with Fused
Path: blog/2024-08-27-pachama/index.mdx
URL: https://docs.fused.io/blog/2024-08-27-pachama

**TL;DR Pachama uses Fused to create maps on-the-fly for their sustainability platform.**

Pachama recently built the Land Suitability Tool within their Reforestation Partner Portal to revolutionize how project developers assess the restoration potential of prospective project sites. In this portal, organizations and landowners looking to start a reforestation project define an Area Of Interest (AOI) by drawing or uploading a polygon, then estimate the land's eligibility based on data layers derived from environment models that take into account country-level data about land cover, vegetation history, and natural risks. For example, a project may look to derive credits from carbon sequestration through native reforestation and ally with local communities that earn an income as stewards of the land.

One of Pachama's challenges was making preprocessed data available for user-defined AOIs that aren't known ahead of time. This would require generating and storing data for entire countries, which is expensive given that a preprocessing step is billed for each square kilometer.

Furthermore, the process involved transferring data between backend and frontend teams, each with different requirements. This resulted in converting datasets between formats, workflows with complex infrastructure, long-running jobs, and slow turn-around times.

## The Solution: Serverless Tile Generation with Fused

To overcome these challenges, Pachama turned to Fused to generate maps on the fly with serverless API endpoints. Fused now provides them an elegant way to write custom workflows to crunch data with Python and serve it behind tile endpoints that natively integrate with map tile layers. This makes it possible to process and visualize any dataset with manageable operation costs.

> **"Fused has been critical in our product lifecycle. The speed at which we were able to iterate based on new requirements is unrivaled."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

The ability to trigger a UDF that generates a vector directly from a Zarr file was a game-changer for Pachama's ability to close the gap between their analytics and their end-users. This innovation has made the team more productive and enabled them to streamline complex tasks that were previously cumbersome and impractical.

> **"Fused takes DevOps out of our hands to focus on our core mission, building technology to restore nature."**
>
> **Marie Hoeger, Staff Software Engineer @ Pachama**

The Land Suitability tool covers the contiguous USA, Brazil, Mexico, Argentina, Guatemala, Panama, Paraguay, Colombian Amazon, and the Peruvian Amazon. Pachama plans to expand to more regions around the world. It processes a variety of datasets including Pachama's proprietary canopy height map. Pachama generates regional maps of average top-of-canopy height using a combination of lidar from GEDI and a suite of satellite observations at varying spatial scales, including optical and radar imagery, topography, and climate data. Fused's on-the-fly tiling simplifies the workflows to generate and load the data into the user-facing app.

By combining analytical and visualization capabilities, Fused enables powerful and productive workflows. Instead of pre-computing tiles for entire datasets, Pachama now generates tiles dynamically only for user-defined AOIs, reducing system complexity and cost.

Here's a minimalist example of how Pachama uses a Fused User Defined Function (UDF) to generate a vector from a raster file in COG format:

```python
@fused.udf
def udf(bbox: fused.types.TileGDF=None):

    from utils import raster_to_vector

    table_path = "s3://pachama-fused-data/dataset.tiff"
    gdf = raster_to_vector(table_path, bbox)
    return gdf
```

This UDF can be called via HTTPS request with the following URL structure:

```
https://www.fused.io/server/v1/realtime-shared/fsh_1gcTv/run/tiles///?dtype_out_vector=mvt
```

## Key Features

The Fused automatically provisions an endpoint for each of Pachama's UDFs. The prospecting application then loads the endpoint into a Mapbox application, which consumes the output in MVT format as defined by the `dtype_out_vector` parameter.

- HTTPS Endpoints work with slippy maps, which is standard across map tiling applications.
- Map clients call the endpoint for each tile in the viewport, passing values for z, x, and y. Fused then runs the UDF, passing a GeoDataframe with the Tile coordinates.
- The UDF code spatially filters the referenced dataset, processes the fraction of data, and returns it to the client app as the response of the HTTPS call in the format specified via a query parameter. This avoids the need to pre-compute data or manage files.

## The Result: Simplify Data Workflows By 50%
Fused and its UDF environment revolutionize how Pachama renders tile-based maps by leveraging analytical tools: cloud-optimized data formats, the flexibility of Python for spatial operations, and the scalability of serverless. Engineers at Pachama used to see a gap between the analytical data formats (e.g. COGs & GeoParquets) and visualization data formats (MVT, PMTiles, XYZ Tiles). Fused closed the gap and let them retire a major piece of the pipeline.

> **"Fused replaced 4 steps of the pipeline with a single Fused UDF."**
>
> **Andrew Campbell, Senior Software Engineer @ Pachama**

## Future innovation for Pachama
Looking ahead, Pachama aims to expand this powerful tool worldwide, catalyzing high-integrity reforestation projects in the regions that need it the most. With Fused's infrastructure underpinning its platform, Pachama can stay focused on making powerful science and analytics accessible to everyone through intuitive visual interfaces.

Read about Pachama's mission and learn how they use technology to evaluate forest carbon projects to assess carbon projects.

================================================================================

## Geospatial workflows of any size
Path: blog/2024-04-22-webinar/index.mdx
URL: https://docs.fused.io/blog/2024-04-22-webinar

<ReactPlayer playsinline= className="video__player" playing= muted= controls height="50vh" url="https://www.youtube.com/watch?v=SztyQoBtfh0" width="100%" />

Isaac Brodsky, the CTO of Fused, delved into the power of Fused during a LinkedIn live session with Matt Forrest. They discussed the contrast of Python vs. SQL for data analytics, the advantages of serverless geospatial processing, and showcased a live demo of the UDF Builder. During the demo, Isaac created a User Defined Function visualize Overture building footprints that are within a certain proximity of water.

You can re-watch the webinar on LinkedIn, YouTube, or below.

================================================================================

## DuckDB + Fused: Fly beyond the serverless horizon
Path: blog/2024-04-09-duckdb/index.mdx
URL: https://docs.fused.io/blog/2024-04-09-duckdb

**TL;DR Fused extends DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.**

The combination of Fused serverless operations and DuckDB offers blazing fast data processing. Fused embraced Python to create serverless User Defined Functions (UDFs). Now, with the help of DuckDB, Fused enables developers to leverage the ease and familiarity of SQL in these functions ‚Ää- ‚Ääwithout compromising performance and parallelism.

This blog explains how Fused User-Defined Functions (UDFs) can extend DuckDB to bring quick serverless operations on any scale dataset. The result is a lightweight, portable, and flexible system that is simultaneously scalable, cost-efficient, and simple to integrate across the stack.

The blog post illustrates three complimentary implementations:
1. Run DuckDB in a Fused UDF
2. Call Fused UDFs from DuckDB
3. Integrate DuckDB in applications using Fused

## The evolution of the data processing landscape
For companies with bottom lines that depend on time to insight, the data landscape is driven by the need to process increasing data volumes and make operations easier to express. This section discusses how Fused and DuckDB can address these needs within the context of the latest wave of the data processing ecosystem.

### Increasing data¬†volumes
When the size of data required for an operation is larger than memory, it becomes a bottleneck. In the early 2010's, the effort to process increasing volumes of data created MapReduce, Hadoop, and Spark to help companies scale out clusters. The complexity of managing clusters gave way to managed services like Databricks and Snowflake, but their high cost and inefficient data transfer with Python (by now a staple of data science) still left parts of the market unaddressed.

Many technologies emerged to attempt to address latent gaps, but it was DuckDB that surged around 2020 as a fast, easy to use, and cost effective solution to process large volumes of data with SQL while reducing the switching cost of having to learn new frameworks. At around the same time, serverless solutions to address the scale out problem started to gain traction.

Now, as AI training and inference require ever more data, the speed of processing and the speed of development become critical bottlenecks. DuckDB and serverless processing together enable new applications. DuckDB gives workflows an in-process performant SQL engine with:

- Fast processing of large datasets through larger than memory processing with a vectorized query engine.
- Zero-copy interoperability with Python, thanks to formats like Apache Arrow.
- Portability and unprecedented developer experience with easy set-up and without the need to maintain a database server.
- Extensibility thanks to an ecosystem of plugins and extensions (C++), scalar Python UDFs, and WebAssembly compatibility.

DuckDB's modularity in data interchange and query execution makes it an ideal choice for serverless architectures. The combination of DuckDB and serverless has unique advantages:

- Fast and cheap data access thanks to cloud optimized data formats that enable retrieving part of the file (e.g. Parquet for tabular data, Cloud Optimized GeoTIFF for imagery.)
- Scalability, distributed compute without managing infrastructure and without expense when code is not running.
- Easy to share results and create integrations by triggering jobs and loading data via simple HTTPS calls.

## Python + SQL¬†synergy
Python is the lingua franca of data science and AI. It's an imperative language‚Ää-‚Ääwhich means it's easy to write complex logic without sacrificing readability, and interface a broader range of data formats‚Ää-‚Ääenabling operations inaccessible to SQL like calling API clients, fine-grained analytic calculations, and processing arrays and rasters. The Python ecosystem recently adopted Rust to write high performance, memory safe modules. However, Python historically struggled with concurrency and managing the memory of distributed clusters, which hindered its ability to process large datasets.

Declarative languages like SQL offer simple syntax to define data manipulations for performant query engines, but they lack explicit control flow and are limited to select data structures.

Two approaches to intertwine SQL and Python emerged, each with particular tradeoffs in portability and efficiency:
- **SQL queries in Python.** These tend to sacrifice data transfer efficiency between runtimes or require specialized, complicated data warehousing.
- **Python UDFs within SQL.** These tend to incur performance costs and require maintaining a Python runtime within the DBMS.

These are offered, to different extents, by tools like Databricks, BigQuery, and Postgres.
- **Databricks** offers a notebook environment, familiar to the data scientist, that enables workflows to transition between Python and SQL‚Ää-‚Ääbut requires specialized data warehousing, complicated cluster management, and lacks debuggability.
- **BigQuery** UDFs bring an imperative language to SQL engine‚Ää-‚Ääbut it's constrained to Javascript which lacks Python's powerful data operations and libraries.
- **Postgres** and other databases can bring SQL to a Python runtime with connector libraries such as Psycopg2 and SQLAlchemy‚Ää-‚Ääbut this pattern has the infrastructure overhead of needing to run a separate database server.

However versatile, DuckDB is founded on SQL and still needs to rely on Python and plugins for expressibility. But its support for Python UDFs and plugins is yet to mature.
- DuckDB only supports scalar Python UDFs.
- Constrained to the capabilities of the local runtime process.
- There's no seamless way to share Python UDFs across databases or runtimes.
- Plugins are difficult to write and deploy.

## Fused + DuckDB¬†synergy
Fused is a framework to author and run serverless operations. Every Fused UDF is an HTTPS API that can be called to run and load data from any application that can make HTTPS requests. Integrating UDFs into workflows is as easy as passing the endpoint as a string. Spreadsheets, web maps, ETL pipelines, and DuckDB can all load data from HTTPS API endpoints, and dynamically parametrize calls with query parameters.

- Eliminates the need to provision, manage, and scale instances‚Ää-‚Ääwhich is what caused the initial break away from the Map Reduce, Hadoop, and Spark era. Its just-in-time backend scales from zero to cluster as quickly as needed.
- UDFs can call UDFs‚Ää-‚Ääwhich results in blazing fast execution by running thousands of parallel jobs -without worrying about orchestration.
- Pay only when code runs, and run from anywhere‚Ää-‚Ääwhich speaks to market segments unaddressed by managed platforms like Databricks and Snowflake.
- Natively runs on a standard Python interpreter‚Ää-‚Ääso it seamlessly runs DuckDB while keeping Python's expressibility and ecosystem of libraries.
- Dovetails with cloud-native data formats. Their atomic data loading and compressed formats make for reduced data transfer between local processes and third party cloud warehouses.

Fused and DuckDB together reduce architectural complexity and make it easy to have cutting-edge analytic processing in any application. Together, they eliminate the need for cumbersome distributed query engines which are slow to start-up and are overkill for smaller datasets.

Fused UDFs are easy to share and can run from anywhere. The examples in this post are available as community UDFs you can find on the open source Github repo and run them in any Python environment with the Fused SDK.

## Example patterns

This section shows and discusses three powerful patterns at the intersection of Fused and DuckDB.
### 1. Run DuckDB in a Fused¬†UDF

DuckDB parallelizes its own operations under the hood thanks to its columnar vectorized query engine that provides compelling performance for querying using SQL. However, there can still be bottlenecks in operations upstream or downstream of DuckDB. To resolve this, Fused UDFs easily run DuckDB and create a seamless experience between Python and SQL.

See the full example in our documentation.

### 2. Call Fused UDFs from¬†DuckDB

Any database that supports querying data via HTTPS can call and load data from Fused UDF endpoints using common formats like Parquet or CSV. This means that DuckDB can dispatch operations to Fused that otherwise would be too complex or impossible to express with SQL, or would be unsupported in the local runtime.

In this example, a Fused UDF returns a table where each record is a polygon generated from the contour of a raster provided by the Copernicus Digital Elevation Model as a Cloud Optimized GeoTIFF. DuckDB can easily trigger a UDF and load its output with this simple query, which specifies that the UDF endpoint returns a Parquet file.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sql.gif" alt="overture" width="600"/>

This pattern enables DuckDB to address use cases and data formats that it doesn't natively support or would otherwise see high data transfer cost, such as raster operations, API calls, and control flow logic.

See the full example in our documentation or open it in this [DuckDB shell%0ALIMIT-10~).

### 3. Integrate DuckDB in applications using¬†Fused

Fused is the glue layer between DuckDB and apps. This enables seamless integrations that trigger Fused UDFs and load their results with simple parameterized HTTPS calls.

DuckDB is an embedded database engine and doesn't have built-in capability to share results other than writing out files. As a corollary of the preceding example, it's possible to query and transform data with DuckDB and seamlessly integrate the results of queries into any workflow or app.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sheets.gif" alt="overture" width="600"/>

To try this example simply make a copy of this Google Sheets spreadsheet (File > Make a copy) and click, and modify the parameters in B2:4 to trigger the Fused UDF endpoint and load data.

See the full example in our documentation.

## Conclusion

While the pendulum of the data landscape swung from distributed compute to single-node, Fused's serverless operations swing the conversation back with a simple and cost-efficient scale-out.

This blog post discussed how gaps in the modern data stack can be addressed by integrating Fused and DuckDB, two emerging data processing tools. The intersection between DuckDB's portable SQL and Fused's scalable python operations creates a stack that is:
Flexible due to the seamless interaction of Python and SQL.
Scalable, simple, and cost efficient.

Easy for data scientists to create, and easy for non-coders to consume.

DuckDB is an early example of how Fused integrates with the modern data stack. We're eager to share the growing list of compelling integrations over the following months.

We would like to extend our thanks to Wes McKinney and Michael Driscoll for reviewing drafts of this post before it went out.

## Get started with¬†Fused

Want to get involved?

- Try out Fused for yourself for free!
Give back to the community by contributing a UDF.
- You can also join the conversation by becoming a member of the Fused Discord community. We are always happy to hear your thoughts.
- Does taking serverless operations to the next level sound exciting to you? Fused is hiring! Shoot us a note at `sina@fused.io`.

================================================================================

## Fused redefines geospatial with instant maps
Path: blog/2024-03-06-pressrelease/index.mdx
URL: https://docs.fused.io/blog/2024-03-06-pressrelease

Fused is a modern geospatial toolkit for companies to code, scale, and ship geospatial workflows of any size.

This week we are unveiling Fused, a toolkit to enable interoperability between all geospatial datasets and tools in the modern data stack. Fused is the glue layer that integrates data platforms with data tools via a managed serverless API.

## Overview

Co-founders Sina Kashuk and Isaac Brodsky met while working at Uber. They co-founded Unfolded to commercialize the popular open source geospatial visualization projects Kepler.gl, Deck.gl, and H3. Unfolded was acquired by Foursquare in June, 2021.

Fused has raised $1 million in pre-seed funding from Fontinalis Partners, Wes McKinney, Michael Driscoll, Jason Richman, and angels from Uber, Airbnb, DoorDash, and others.

Fused delivers serverless geospatial operations at any point of the stack ‚Äî with a simple HTTPS call. This is like when users pull-up information from map apps, but with custom and transparent logic. This shields developers from hours of burdensome engineering, enabling businesses to serve their customers with timely insights, faster.

Teams supercharge their favorite IDEs, tools, and frameworks with Fused. They build with the Python SDK, preview on the browser with Fused Workbench, and run in their stack via the Hosted API.

_Fused ecosystem and product line._

## The problem

We're now in a moment where large-scale geospatial datasets are migrating to open cloud-enabled formats. However, we have personally seen how it can be challenging to utilize this data at scale. At the same time, there has been a rise in Earth observation imagery, which will only accelerate as we monitor climate change and as satellites continue scaling, enabling. However, the sheer volume of data, complexity of operations, and fragmentation of tooling holds back how we process and present that data that is critical for making informed decisions about critical company operations.

Today, data scientists and analysts manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. The size of data limits the possible depth of insight of last-mile analytics and the speed at which they can be delivered ‚Äî leaving problems unaddressed. Moreover, data scientists handoff algorithms to data engineers who then translate code to work with orchestrators that run on distributed compute systems maintained by an entire devops team. An analyst needs to navigate a sea of buzzwords like CRS, GDAL, Spark clusters, geo-partitions, raster and vector joins, zonal stats, and census blocks ‚Äî just to prepare for the analysis they actually want to do.

## The solution

Today, a leading global media company animates atmospheric rivers to report weather news ‚Äî 36x speed improvement, from hours to minutes. An EV company uses Fused to optimize its EV charging station network planning capabilities ‚Äî blending data at an unprecedented coverage and detail. A carbon offset company creates custom deforestation basemaps with better operational efficiency ‚Äî closing the analysis loop for stakeholder reports.

Fused empowers teams in these companies to seamlessly layer weather, infrastructure, road, and deforestation data; while transforming it with custom Python code to create apps for real-time decision making. Fused simplifies workflows so small teams can deliver novel business-critical insights where it wasn't possible before.

Read more: Founder's Blog Post

## Vibrant community

As a founding tenet, Fused promotes open source, transparency, and collaboration. To this end, data scientists and app builders engage the Fused community on GitHub and Discord to find, reuse, and share verified code snippets that they can bring into their workflows.

Community UDF of hydrology model by Taher Chegini
In sum, Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when for any geospatial analysis you would have to send it to your GIS person to analyze it and get it back in 3 weeks, if you get lucky (and forget about integrating that with any other tools). Fused is built to be the interoperable glue between geospatial data systems, and we're excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join the journey to break away from old geospatial infrastructure. Let's revolutionize geospatial technology together! fused.io. üåéüöÄ

Join the journey

- Read the announcement on Tech Crunch
- Follow us on Twitter/X
- Follow us on LinkedIn
- Star our GitHub repo
- Join the conversation on Discord
- Read Fused's founding principles

================================================================================

## Founder's blog post: why Fused?
Path: blog/2024-03-01-welcome/index.mdx
URL: https://docs.fused.io/blog/2024-03-01-welcome

Fused enables interoperability between datasets and tools in the modern data stack. It's a glue layer to integrate data platforms with data tools via a managed serverless API.

## Current limitations with data processing

Today, there is a fragmented ecosystem around scalable geospatial data processing. Python geospatial libraries like GeoPandas, Shapely, and Rasterio make it easy to do small jobs but are single-threaded and operate entirely in-memory. For bigger jobs, there are Python parallel processing tools like Dask that require complex installations and are liable to memory pressure errors. Spark-based tools like Apache Sedona and RasterFrames have a steep learning curve and are hard to debug and orchestrate. Postgres and its geospatial extension PostGIS operate on larger-than-memory datasets but are hard to scale larger than the disk of one machine, aren‚Äôt designed for OLAP workloads, and can be hard to administer. Cloud data warehouses like Databricks and Snowflake are monolithic systems that tend to bring lock-in and pricing that is hard to anticipate.

Spatial SQL is a great way to run scalable operations on tables with vector data - but falls short on raster data and does not have native access to libraries for the finesse operations of data science. Geospatial data science teams largely use Python and would prefer to use it both in development and in production - but tooling fragmentation forces them to juggle languages and frameworks. The present paradigm accepts the inefficiencies of complexity as a necessary evil because there hasn‚Äôt been a better way to work with both raster and vector data at scale. Data teams have an unaddressed need for a friendly Python API that scales. To increase development velocity it‚Äôs convenient for most code to run in Python, moving only computationally heavy code into specialized frameworks - as efficiently as possible. Additionally, scaling Python from local development to massive cloud workloads calls for efficient parallelization.

## Seizing the moment

The last several years have seen a commoditization of modular building blocks of OLAP systems and increased adoption of geospatial cloud-native data formats. With the convergence and popularity of columnar memory formats like Apache Arrow and Apache Parquet, easy-to-use columnar OLAP databases like DuckDB, and broader adoption of geospatial cloud-native data formats like Cloud-Optimized GeoTIFF and GeoParquet, we believe there‚Äôs a window for a serverless geospatial OLAP engine. Moreover, serverless computing has emerged as a prominent trend, delegating infrastructure management and dynamically scaling resources in response to demand, leading to heightened flexibility and cost efficiency. Leveraging serverless cloud infrastructure like AWS Lambda, Azure Functions, Google Cloud Functions, or Cloudflare Workers enables event-driven processing closer to the data source.

Parquet files have become the standard file format for columnar data and have helped to commoditize the decoupling of storage and compute by enabling queries directly on object storage like AWS S3. GeoParquet ‚Äì a specification for storing point, line, and polygon geometries in Parquet ‚Äì has seen recent momentum as a fast storage format for geospatial vector data and has started to be integrated into industry-standard tools like GDAL. Moreover, with spatial partitioning, operations can be broken down into small independent parts that execute simultaneously in multiple processes. For geospatial array data like satellite imagery, Cloud-Optimized GeoTIFF ‚Äì an extension to GeoTIFF that enables chunked access via HTTPS range requests ‚Äì has taken hold as the standard way to store geospatial image data, with petabytes publicly available from AWS‚Äô open data program and buy-in from major vendors like USGS and Planet.

Apache Arrow has become the universal in-memory columnar data format for columnar, analytic data because its language-independent specification enables easier movement of data between languages and frameworks. Moreover, GeoArrow ‚Äì an incubating specification for storing geospatial data in Arrow ‚Äì gives us a way to move geospatial data from Python to compiled code for free, and will likely serve as the foundation for an ecosystem of large-data geospatial tools. Already in the frontend, deck.gl can use GeoArrow-style data buffers to visualize millions of coordinates with no serialization costs.

As a result of all these trends, smaller data can be transferred to and processed on serverless cloud services in ways that are not possible ever before. Public clouds enable event-driven compute services that automatically scale, which makes for simple infrastructure and dependency management. Managed offerings reduce the complexities of data pipelines enabling geospatial workloads of any size to run on demand ‚Äì to empower users with the ability to go from code to map, instantly.

## Why Fused?

Fused instantly converts user‚Äôs Python code to workflows and maps in Jupyter notebooks, low-code web apps, the Fused Workbench web-app, ETL pipelines, or any tool that consumes HTTPS API endpoints. Fused lets developers run real-time serverless operations at any scale and build responsive maps, dashboards, and reports. Developers develop in production and run on any scale data without infrastructure friction using serverless parallel computing powered by advanced caching of geo-partitioned data. This enables bringing interoperable workflows, apps, and maps to the user's preferred stack and avoiding vendor lock-in.

With Fused, users find, reuse, and share User Defined Functions (UDFs) in the Fused vibrant community. Fused UDFs are building blocks of serverless geospatial operations that integrate across the stack - with Planetary Computer, Google Earth Engine, Big Query, Snowflake, DuckDB, and more. They load datasets from the cloud ecosystem such as NASA, NOAA, US Census, and Overture. Fused serverless API turns these UDFs into live HTTPS endpoints that load their output into any tools.

Fused allows people for the first time to easily work with geospatial data and integrate it with modern data tools. This is a radical departure from times when you manually conduct multistep processes fragmented across tools and data standards with the help of an army of data engineers and infra (if they are lucky) just to render data on a map. Fused is built to be the interoperable glue between geospatial data systems, and we‚Äôre excited to bring best-in-class cloud infrastructure and distributed computing to this industry.

Join us in our journey to break from old geospatial infrastructure. Let's revolutionize geospatial technology together! üåéüöÄ

- The Fused Founding Team

================================================================================


---

Generated automatically from Fused documentation. Last updated: 2025-10-29
Total sections: 3
