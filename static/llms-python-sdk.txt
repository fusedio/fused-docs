# Fused Python SDK Documentation

> Complete reference for the Fused Python SDK - a Python library for creating and running User Defined Functions (UDFs) that can be executed via HTTPS requests.

## Python SDK Reference

### fused.api

**URL:** https://docs.fused.io/python-sdk/api-reference/api

## fused.api.whoami

```python
whoami()
```

Returns information on the currently logged in user

---

## fused.api.delete

```python
delete() -> bool
```

Delete the files at the path.

**Parameters:**

- **path** (<code>str</code>) – Directory or file to delete, like `fd://my-old-table/`
- **max_deletion_depth** (<code>int | Literal['unlimited']</code>) – If set (defaults to 3), the maximum depth the operation will recurse to.
  This option is to help avoid accidentally deleting more data that intended.
  Pass `"unlimited"` for unlimited.

**Examples:**

```python
fused.api.delete()
```

---

## fused.api.list

```python
list() -> list[str] | list[ListDetails]
```

List the files at the path.

**Parameters:**

- **path** (<code>str</code>) – Parent directory URL, like `fd://bucket-name/`
- **details** (<code>bool</code>) – If True, return additional metadata about each record.

**Returns:**

- <code>list\[str\] | list\[ListDetails\]</code> – A list of paths as URLs, or as metadata objects.

**Examples:**

```python
fused.api.list()
```

---

## fused.api.get

```python
get() -> bytes
```

Download the contents at the path to memory.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>bytes</code> – bytes of the file

**Examples:**

```python
fused.api.get()
```

---

## fused.api.download

```python
download() -> None
```

Download the contents at the path to disk.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`
- **local_path** (<code>str | Path</code>) – Path to a local file.

---

## fused.api.upload

```python
upload() -> None
```

Upload local file to S3.

**Parameters:**

- **local_path** (<code>str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame</code>) – Either a path to a local file (`str`, `Path`), a (Geo)DataFrame
  (which will get uploaded as Parquet file), or the contents to upload.
  Any string will be treated as a Path, if you wish to upload the contents of
  the string, first encode it: `s.encode("utf-8")`
- **remote_path** (<code>str</code>) – URL to upload to, like `fd://new-file.txt`
- **timeout** (<code>float | None</code>) – Optional timeout in seconds for the upload (will default to `OPTIONS.request_timeout` if not specified).

**Examples:**

To upload a local json file to your Fused-managed S3 bucket:

```py
fused.api.upload("my_file.json", "fd://my_bucket/my_file.json")
```

---

## fused.api.sign_url

```python
sign_url() -> str
```

Create a signed URL to access the path.

This function may not check that the file represented by the path exists.

**Parameters:**

- **path** (<code>str</code>) – URL to a file, like `fd://bucket-name/file.parquet`

**Returns:**

- <code>str</code> – HTTPS URL to access the file using signed access.

**Examples:**

```python
fused.api.sign_url()
```

---

## fused.api.sign_url_prefix

```python
sign_url_prefix() -> dict[str, str]
```

Create signed URLs to access all blobs under the path.

**Parameters:**

- **path** (<code>str</code>) – URL to a prefix, like `fd://bucket-name/some_directory/`

**Returns:**

- <code>dict\[str, str\]</code> – Dictionary mapping from blob store key to signed HTTPS URL.

**Examples:**

```python
fused.api.sign_url_prefix()
```

---

## fused.api.get_udfs

```python
get_udfs() -> dict
```

Fetches a list of UDFs.

**Parameters:**

- **n** (<code>int | None</code>) – The total number of UDFs to fetch. Defaults to All.
- **skip** (<code>int</code>) – The number of UDFs to skip before starting to collect the result set. Defaults to 0.
- **by** (<code>Literal['name', 'id', 'slug']</code>) – The attribute by which to sort the UDFs. Can be "name", "id", or "slug". Defaults to "name".
- **whose** (<code>Literal['self', 'public', 'community', 'team']</code>) – Specifies whose UDFs to fetch. Can be "self" for the user's own UDFs or "public" for
  UDFs available publicly or "community" for all community UDFs. Defaults to "self".

**Returns:**

- <code>dict</code> – A list of UDFs.

**Examples:**

Fetch UDFs under the user account:

```py
fused.api.get_udfs()
```

---

## fused.api.job_get_logs

```python
job_get_logs() -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> – Log messages for the given job.

---

## fused.api.job_print_logs

```python
job_print_logs() -> None
```

Fetch and print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- **file** (<code>IO | None</code>) – Where to print logs to. Defaults to sys.stdout.

**Returns:**

- <code>None</code> – None

---

## fused.api.job_tail_logs

```python
job_tail_logs()
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) – how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) – if true, print out only a sample of logs. Defaults to True.
- **timeout** (<code>float | None</code>) – if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

## fused.api.job_get_status

```python
job_get_status() -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

## fused.api.job_cancel

```python
job_cancel() -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – A new job object.

---

## fused.api.job_get_exec_time

```python
job_get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns:**

- <code>timedelta</code> – Time the job took. If the job is in progress, time from first to last log message is returned.

---

## fused.api.job_wait_for_job

```python
job_wait_for_job() -> RunResponse
```

Block the Python kernel until this job has finished

**Parameters:**

- **poll_interval_seconds** (<code>float</code>) – How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) – The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> – if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

## fused.api.FusedAPI

```python
FusedAPI()
```

API for running jobs in the Fused service.

Create a FusedAPI instance.

**Other Parameters:**

- **base_url** (<code>str | None</code>) – The Fused instance to send requests to. Defaults to `https://www.fused.io/server/v1`.
- **set_global_api** (<code>bool</code>) – Set this as the global API object. Defaults to True.
- **credentials_needed** (<code>bool</code>) – If True, automatically attempt to log in. Defaults to True.

---

### create_udf_access_token

```python
create_udf_access_token() -> UdfAccessToken
```

Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.

**Parameters:**

- **udf_email_or_name_or_id** (<code>str | None</code>) – A UDF ID, email address (for use with udf_name), or UDF name.
- **udf_name** (<code>str | None</code>) – The name of the UDF to create the token for.

**Other Parameters:**

- **udf_email** (<code>str | None</code>) – The email of the user owning the UDF, or, if udf_name is None, the name of the UDF.
- **udf_id** (<code>str | None</code>) – The backend ID of the UDF to create the token for.
- **client_id** (<code>str | Ellipsis | None</code>) – If specified, overrides which realtime environment to run the UDF under.
- **cache** (<code>bool</code>) – If True, UDF tiles will be cached.
- **metadata_json** (<code>dict\[str, Any\] | None</code>) – Additional metadata to serve as part of the tiles metadata.json.
- **enable** – If True, the token can be used.

---

### upload

```python
upload() -> None
```

Upload a binary blob to a cloud location

---

### start_job

```python
start_job() -> RunResponse
```

Execute an operation

**Parameters:**

- **config** (<code>JobConfig | JobStepConfig</code>) – the configuration object to run in the job.

**Other Parameters:**

- **instance_type** (<code>WHITELISTED_INSTANCE_TYPES | None</code>) – The AWS EC2 instance type to use for the job. Acceptable strings are "m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "r5.large", "r5.xlarge", "r5.2xlarge", "r5.4xlarge". Defaults to None.
- **region** (<code>str | None</code>) – The AWS region in which to run. Defaults to None.
- **disk_size_gb** (<code>int | None</code>) – The disk size to specify for the job. Defaults to None.
- **additional_env** (<code>Sequence\[str\] | None</code>) – Any additional environment variables to be passed into the job, each in the form KEY=value. Defaults to None.
- **image_name** (<code>str | None</code>) – Custom image name to run. Defaults to None for default image.
- **send_status_email** (<code>bool | None</code>) – Whether to send a status email to the user when the job is complete.

---

### get_jobs

```python
get_jobs() -> Jobs
```

Get the job history.

**Parameters:**

- **n** (<code>int</code>) – The number of jobs to fetch. Defaults to 5.

**Other Parameters:**

- **skip** (<code>int</code>) – Where in the job history to begin. Defaults to 0, which retrieves the most recent job.
- **per_request** (<code>int</code>) – Number of jobs per request to fetch. Defaults to 25.
- **max_requests** (<code>int | None</code>) – Maximum number of requests to make. May be None to fetch all jobs. Defaults to 1.

**Returns:**

- <code>Jobs</code> – The job history.

---

### get_status

```python
get_status() -> RunResponse
```

Fetch the status of a running job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

### get_logs

```python
get_logs() -> list[Any]
```

Fetch logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **since_ms** (<code>int | None</code>) – Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns:**

- <code>list\[Any\]</code> – Log messages for the given job.

---

### tail_logs

```python
tail_logs()
```

Continuously print logs for a job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **refresh_seconds** (<code>float</code>) – how frequently, in seconds, to check for new logs. Defaults to 1.
- **sample_logs** (<code>bool</code>) – if true, print out only a sample of logs. Defaults to False.
- **timeout** (<code>float | None</code>) – if not None, how long to continue tailing logs for. Defaults to None for indefinite.

---

### wait_for_job

```python
wait_for_job() -> RunResponse
```

Block the Python kernel until the given job has finished

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.
- **poll_interval_seconds** (<code>float</code>) – How often (in seconds) to poll for status updates. Defaults to 5.
- **timeout** (<code>float | None</code>) – The length of time in seconds to wait for the job. Defaults to None.

**Raises:**

- <code>TimeoutError</code> – if waiting for the job timed out.

**Returns:**

- <code>RunResponse</code> – The status of the given job.

---

### cancel_job

```python
cancel_job() -> RunResponse
```

Cancel an existing job

**Parameters:**

- **job** (<code>CoerceableToJobId</code>) – the identifier of a job or a `RunResponse` object.

**Returns:**

- <code>RunResponse</code> – A new job object.

---

### auth_token

```python
auth_token() -> str
```

Returns the current user's Fused environment (team) auth token

---

================================================================================

### fused.core

**URL:** https://docs.fused.io/python-sdk/api-reference/core

## `run_tile`

```python showLineNumbers
def run_tile(email: str,
             id: Optional[str] = None,
             *,
             x: int,
             y: int,
             z: int,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_shared_tile`

```python showLineNumbers
def run_shared_tile(token: str,
                    *,
                    x: int,
                    y: int,
                    z: int,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    _client_id: Optional[str] = None,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_file`

```python showLineNumbers
def run_file(email: str,
             id: Optional[str] = None,
             *,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  The response from the server after executing the UDF.

---
## `run_shared_file`

```python showLineNumbers
def run_shared_file(token: str,
                    *,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  The response from the server after executing the operation on the file.

**Raises**:

- `Exception` - Describes various exceptions that could occur during the function execution, including but not limited to invalid parameters, network errors, unauthorized access errors, or server-side errors.

This function is designed to access shared operations that require a token for authorization. It requires network access to communicate with the server hosting these operations and may incur data transmission costs or delays depending on the network's performance.

---
## `run_tile_async`

```python showLineNumbers
async def run_tile_async(
        email: str,
        id: Optional[str] = None,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to asynchronously run a UDF on a specific tile defined by its x, y, and z coordinates. It supports customization of the output data types for vector and raster data, and accommodates additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - User's email address. Used to identify the user's saved UDFs. If the ID is not provided, the email is also used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF on the specified tile and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_tile_async`

```python showLineNumbers
async def run_shared_tile_async(
        token: str,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared tile-based UDF using a specific access token.

This function constructs a URL for running an operation on a tile, defined by its x, y, and z coordinates, accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation on the specified tile.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the specified tile and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---
## `run_file_async`

```python showLineNumbers
async def run_file_async(
        email: str,
        id: Optional[str] = None,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a file-based UDF associated with the specific email and ID.

This function constructs a URL to run a UDF on a server, allowing for output data type customization for vector and raster outputs and supporting additional parameters for the UDF execution. If no ID is provided, the user's email is used as the identifier.

**Arguments**:

- `email` _str_ - The user's email address, used to identify the user's saved UDFs. If the ID is not provided, this email will also be used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the function fetches the user's email as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_file_async`

```python showLineNumbers
async def run_shared_file_async(
        token: str,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared file-based UDF using the specific access token.

Constructs a URL to run an operation on a file accessible via a shared token, enabling customization of the output data types for vector and raster data. It accommodates additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.

**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the file and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

================================================================================

### fused.ingest

**URL:** https://docs.fused.io/python-sdk/api-reference/job

## `fused.ingest`

```python showLineNumbers
def ingest(
    input: Union[str, Sequence[str], Path, gpd.GeoDataFrame],
    output: Optional[str] = None,
    *,
    output_metadata: Optional[str] = None,
    schema: Optional[Schema] = None,
    file_suffix: Optional[str] = None,
    load_columns: Optional[Sequence[str]] = None,
    remove_cols: Optional[Sequence[str]] = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: Optional[bool] = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: Union[int, float, None] = None,
    partitioning_maximum_per_chunk: Union[int, float, None] = None,
    partitioning_max_width_ratio: Union[int, float] = 2,
    partitioning_max_height_ratio: Union[int, float] = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: Optional[float] = None,
    subdivide_stop: Optional[float] = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: Optional[Tuple[str, str]] = None,
    gdal_config: Union[GDALOpenConfig, Dict[str, Any], None] = None
) -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Arguments**:

- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

        ```python showLineNumbers
        fused.ingest(
            ...,
            lonlat_cols=("x", "y")
        )
        ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        ```python showLineNumbers
        fused.ingest(
            ...,
            gdal_config=
            }
        )
        ```

**Returns**:

Configuration object describing the ingestion process. Call `.run_batch` on this object to start a job.

**Examples**:

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).run_batch()
```

---
#### `job.run_batch`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_batch` on this object to start the ingestion job.

```python
def run_batch(output_table: Optional[str] = ...,
               instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
               *,
               region: str | None = None,
               disk_size_gb: int | None = None,
               additional_env: List[str] | None = None,
               image_name: Optional[str] = None,
               ignore_no_udf: bool = False,
               ignore_no_output: bool = False,
               validate_imports: Optional[bool] = None,
               validate_inputs: bool = True,
               overwrite: Optional[bool] = None) -> RunResponse
```

Begin job execution.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_batch()` returns a `RunResponse` object which has the following methods:

#### `get_status`

```python
def get_status() -> RunResponse
```

Fetch the status of this job

**Returns**:

  The status of the given job.

---

#### `print_logs`

```python
def print_logs(since_ms: Optional[int] = None,
               file: Optional[IO] = None) -> None
```

Fetch and print logs

**Arguments**:

- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

**Returns**:

  None

---
#### `get_exec_time`

```python
def get_exec_time() -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

  Time the job took. If the job is in progress, time from first to last log message is returned.

---
#### `tail_logs`

```python
def tail_logs(refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1) -> None
```

Continuously print logs

**Arguments**:

- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

```python
def cancel() -> RunResponse
```

Cancel this job

**Returns**:

  A new job object.

================================================================================

### JobPool

**URL:** https://docs.fused.io/python-sdk/api-reference/jobpool

## JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### cancel

```python
cancel()
```

Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

---

### retry

```python
retry()
```

Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

---

### total_time

```python
total_time() -> timedelta
```

Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

---

### times

```python
times() -> list[Optional[timedelta]]
```

Time taken for each task.

Incomplete tasks will be reported as None.

---

### done

```python
done() -> bool
```

True if all tasks have finished, regardless of success or failure.

---

### all_succeeded

```python
all_succeeded() -> bool
```

True if all tasks finished with success

---

### any_failed

```python
any_failed() -> bool
```

True if any task finished with an error

---

### any_succeeded

```python
any_succeeded() -> bool
```

True if any task finished with success

---

### arg_df

```python
arg_df()
```

The arguments passed to runs as a DataFrame

---

### status

```python
status()
```

Return a Series indexed by status of task counts

---

### wait

```python
wait()
```

Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### tail

```python
tail()
```

Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

---

### results

```python
results() -> List[Any]
```

Retrieve all results of the job.

Results are ordered by the order of the args list.

---

### results_now

```python
results_now() -> Dict[int, Any]
```

Retrieve the results that are currently done.

Results are indexed by position in the args list.

---

### df

```python
df()
```

Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

---

### get_status_df

```python
get_status_df()
```

---

### get_results_df

```python
get_results_df()
```

---

### errors

```python
errors() -> Dict[int, Exception]
```

Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

---

### first_error

```python
first_error() -> Optional[Exception]
```

Retrieve the first (by order of arguments) error result, or None.

---

### logs

```python
logs() -> list[Optional[str]]
```

Logs for each task.

Incomplete tasks will be reported as None.

---

### first_log

```python
first_log() -> Optional[str]
```

Retrieve the first (by order of arguments) logs, or None.

---

### success

```python
success() -> Dict[int, Any]
```

Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

---

### pending

```python
pending() -> Dict[int, Any]
```

Retrieve the arguments that are currently pending and not yet submitted.

---

### running

```python
running() -> Dict[int, Any]
```

Retrieve the results that are currently running.

---

### cancelled

```python
cancelled() -> Dict[int, Any]
```

Retrieve the arguments that were cancelled and not run.

---

### collect

```python
collect()
```

Collect all results into a DataFrame

---

================================================================================

### fused.options

**URL:** https://docs.fused.io/python-sdk/api-reference/options

## fused.options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

**Examples:**

Change the `request_timeout` option from its default value to 60 seconds:

```py
fused.options.request_timeout = 60
```

## Options

### __dir__

```python
__dir__() -> List[str]
```

### base_url

```python
base_url: str = PROD_DEFAULT_BASE_URL
```

Fused API endpoint

### auth

```python
auth: AuthOptions = Field(default_factory=AuthOptions)
```

Options for authentication.

### show

```python
show: ShowOptions = Field(default_factory=ShowOptions)
```

Options for object reprs and how data are shown for debugging.

### max_workers

```python
max_workers: int = 16
```

Maximum number of threads, when multithreading requests

### run_timeout

```python
run_timeout: float = 130
```

Request timeout for UDF run requests to the Fused service

### request_timeout

```python
request_timeout: Union[Tuple[float, float], float, None] = 5
```

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### request_max_retries

```python
request_max_retries: int = 5
```

Maximum number of retries for API requests

### request_retry_base_delay

```python
request_retry_base_delay: float = 1.0
```

Base delay before retrying a API request in seconds

### realtime_client_id

```python
realtime_client_id: Optional[StrictStr] = None
```

Client ID for realtime service.

### max_recursion_factor

```python
max_recursion_factor: int = 5
```

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

```python
save_user_settings: StrictBool = True
```

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

```python
default_udf_run_engine: Optional[StrictStr] = None
```

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

```python
default_validate_imports: StrictBool = False
```

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

```python
prompt_to_login: StrictBool = False
```

Automatically prompt the user to login when importing Fused.

### no_login

```python
no_login: StrictBool = False
```

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

```python
pyodide_async_requests: StrictBool = False
```

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

```python
cache_directory: Path | None = None
```

The base directory for storing cached results.

### data_directory

```python
data_directory: Path = None
```

The base directory for storing data results. Note: if storage type is 'object', then this path is relative to
fd_prefix.

### temp_directory

```python
temp_directory: Path = Path(tempfile.gettempdir())
```

The base directory for storing temporary files.

### never_import

```python
never_import: StrictBool = False
```

Never import UDF code when loading UDFs.

### gcs_secret

```python
gcs_secret: str = 'gcs_fused'
```

Secret name for GCS credentials.

### gcs_filename

```python
gcs_filename: str = '/tmp/.gcs.fused'
```

Filename for saving temporary GCS credentials to locally or in rt2 instance

### gcp_project_name

```python
gcp_project_name: Optional[StrictStr] = None
```

Project name for GCS to use for GCS operations.

### logging

```python
logging: StrictBool = Field(default=False, validate_default=True)
```

Control logging for Fused

### verbose_udf_runs

```python
verbose_udf_runs: StrictBool = True
```

Whether to print logs from UDF runs by default

### default_run_headers

```python
default_run_headers: Optional[Dict[str, str]] = 

```

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

```python
default_dtype_out_vector: StrictStr = 'parquet'
```

Default transfer type for vector (tabular) data

### default_dtype_out_raster

```python
default_dtype_out_raster: StrictStr = 'tiff'
```

Default transfer type for raster data

### fd_prefix

```python
fd_prefix: Optional[str] = None
```

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

```python
verbose_cached_functions: StrictBool = True
```

Whether to print logs from cache decorated functions by default

### local_engine_cache

```python
local_engine_cache: StrictBool = True
```

Enable UDF cache with local engine

### default_send_status_email

```python
default_send_status_email: StrictBool = True
```

Whether to send a status email to the user when a job is complete.

### cache_storage

```python
cache_storage: StorageStr = 'auto'
```

Specify the default cache storage type

### base_web_url

```python
base_web_url
```

### save

```python
save()
```

Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

================================================================================

### Udf

**URL:** https://docs.fused.io/python-sdk/api-reference/udf

## Udf

The `Udf` class is the object you get when defining a UDF with the
`@fused.udf` decorator, or when loading
a saved UDF with `fused.load()`.

### to_fused

```python
to_fused()
```

Save this UDF on the Fused service.

**Parameters:**

- **overwrite** (<code>bool | None</code>) – If True, overwrite existing remote UDF with the UDF object.

---

### to_directory

```python
to_directory()
```

Write the UDF to disk as a directory (folder).

**Parameters:**

- **where** (<code>str | Path | None</code>) – A path to a directory. If not provided, uses the UDF function name.

**Other Parameters:**

- **overwrite** (<code>bool</code>) – If true, overwriting is allowed.

---

### to_file

```python
to_file()
```

Write the UDF to disk or the specified file-like object.

The UDF will be written as a Zip file.

**Parameters:**

- **where** (<code>str | Path | BinaryIO</code>) – A path to a file or a file-like object.

**Other Parameters:**

- **overwrite** (<code>bool</code>) – If true, overwriting is allowed.

---

### create_access_token

```python
create_access_token() -> UdfAccessToken
```

---

### get_access_tokens

```python
get_access_tokens() -> UdfAccessTokenList
```

---

### delete_saved

```python
delete_saved()
```

---

### delete_cache

```python
delete_cache()
```

---

### catalog_url

```python
catalog_url: str | None
```

Returns the link to open this UDF in the Workbench Catalog, or None if the UDF is not saved.

---

================================================================================

### Authentication

**URL:** https://docs.fused.io/python-sdk/authentication

## Authenticate

Authenticate the Fused Python SDK in a Python Notebook.

Make sure to have the `fused` package installed.

```python showLineNumbers
pip install "fused[all]"
```

To use Fused you need to authenticate. The following will store a credentials file in `~/.fused/credentials`:

</Tabs>

## Log out

Log out the current user. This deletes the credentials saved to disk and resets the global Fused API.

```python showLineNumbers

fused.api.logout()
```

## Get Bearer (Access) token

Get the account's Bearer (sometimes referred to as Access) token.

```python showLineNumbers

fused.api.access_token()
```

This can be helpful when calling UDFs via HTTPS requests outside of the Fused Python SDK and Workbench to authenticate with the Fused API.

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

:::

================================================================================

### Batch jobs

**URL:** https://docs.fused.io/python-sdk/batch

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

```python showLineNumbers

@fused.udf
def udf(
    source_s3_path = '',
    destination_s3_path = ''
):

    file_path = fused.download(source_s3_path, source_s3_path.split('/')[-1])

    # Create a temporary directory to extract the files
    s3 = s3fs.S3FileSystem()
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/unzipped_files')

    # Upload each file to the S3 bucket
    for root, dirs, files in os.walk('/tmp/unzipped_files'):
        for file in files:
            file_path = os.path.join(root, file)
            print('file_path', file_path, destination_s3_path)
            s3.put(file_path, destination_s3_path)

    return pd.DataFrame() # UDFs must return a table or raster

```

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_batch()` to make kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

```python showLineNumbers
# Run this locally - not in Workbench
job = udf(
    source_s3_path = 'https://datadownload-production.s3.amazonaws.com/WCMC_carbon_tonnes_per_ha.zip', 
    destination_s3_path = 's3://fused-users/fused/<YOUR_DIR>/dswid/WCMC_carbon_tonnes_per_ha_10gb/'
)
job_id = job.run_batch()
```

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

```python showLineNumbers
fused.api.job_tail_logs("df335890-4406-4832-bf93-6a3b092e496d")
```

================================================================================

### Changelog

**URL:** https://docs.fused.io/python-sdk/changelog

# Changelog

## v1.24.4 (2025-10-06)

**New Features:**

- Drag & Drop upload of files anywhere in Canvas view! 

**Improvements:**

- `fused.submit` now supports `instance_type` parameter.
- Workbench will remember which page you were on before switching to some tabs, so you can go back.
- Returning numpy arrays with 4 or more dimensions will be truncated to 3 dimensions for PNG visualization.
- Supervisor profile is available without enabling an experimental feature.

**Bug Fixes:**

- Various bug fixes for Canvas UI.
- Workbench will clean up old AI chats from your browser.
- Fixed `fused.api.job_get_exec_time`.
- Fixed fetching a single result for large (batch) UDF runs.
- `fused.run` will pass `cache_max_age` for large jobs.

## v1.24.3 (2025-10-03)

**New Features:**

- Sticky notes are supported in canvas view.
- Canvas view now has an auto-arrange button.
- It is possible to start AI changes directly from the canvas view.

**Improvements:**

- Improved how UDF context is passed to AI chat, and removed Rename from default tools.
- File Explorer will open files in the canvas view instead of the UDF builder view.
- Fused-py will prefer `npy` format for numpy arrays.
- UDFs can be deleted from the settings modal.

**Bug Fixes:**

- Fixed UI layout bugs in canvas view and AI chat.
- Fixed bugs with starting ingest jobs.
- Fixed bugs with AI auto-retry.

## v1.24.2 (2025-10-01)

**New Features:**

- Added Claude Sonnet 4.5 model.
- Canvas mode has a UDF selection list in its home view.
- AI chats can continue in the background.

**Improvements:**

- Improved highlighting of syntax errors in Workbench.
- Improved highlighting of UDF execution errors in Workbench.
- Adjusted the design of omnisearch at the top panel of Workbench.
- Batch jobs in Workbench run based on cache_max_age.
- Various canvas UI improvements.
- Upgraded package `arraylake`, and added `icechunk`.

**Bug Fixes:**

- Fixed pending changes still showing after switching UDFs.
- Fixed an issue with saving UDFs and shared tokens slowing down.
- Fixed stale data being cached locally when importing a canvas (collection).
- Fixed an issue where UDF renames would incorrect choose to show the "duplicate UDF name" modal.

## v1.24.0 (2025-09-29)

**New Features:**

- Canvas view now default in Workbench!
- UDF are now only shared to "Team" by default rather than "Public"

**Improvements:**

Canvas View:
- Ability to share Canvas in a single click
- Added Team / Public sharing options for Canvas (default to Team)
- Easily see if your Canvas was successfully run or failed
- Simplified 2 panel layout: Jump between code & AI on the left and Canvas on the right 
- Hide / Show UDFs 

AI Chat:
- Much better support for multiple AI chats running at the same time in different UDFs
- Improvements to Experimental Supervisor mode
- Adding inline diff view to code changes made by AI
- Improved charting ability in AI chat

Workbench:
- Added S3 policy tab to profile modal

Dependencies:
- Update `pyogrio` to 0.11
- Update `duckdb` to 1.4.0

**Bug Fixes:**

- Fixed bug in Version tab 

## v1.23.0 (2025-09-18)

**New Features:**

- New experimental "Supervisor" mode in AI chat, which features more Workbench capabilities and enhanced search capabilities. Different tools in Supervisor mode can be turned on and off in the AI chat settings.
- Workbench can now run UDFs in batch mode when `instance_type` is set on `@fused.udf()`.
- AI changes now show an inline diff in the code editor.

**Improvements:**

- Many UDF actions can now be done directly inside Canvas.
- Improvements to prompts and automatic suggestions in AI chat.
- You can now stop dictation and send by clicking the send button in AI chat.
- HTML view in Workbench will again show UDF output as it will be embedded.
- AI chat settings page has been visually updated to be easier to follow.
- Added "Fold all code" and "Unfold all code" actions in the search bar, and Workbench will now remember what code blocks were folded in a UDF.
- *Inline AI* and *Auto share on save* are now general preferenecs, rather than experimental.

**Bug Fixes:**

- Provided more informative warning messages in the Python SDK.
- Allowed non-authenticated users to retrieve public UDFs.
- Fixed JSON serialization of GeoDataFrames without geometry columns.
- Fixed `/public` UDF page.
- AI chat will scroll to bottom more consistently.
- AI chat will show tool calls in thinking blocks.
- Backend-informed autocomplete is re-enabled.

**API Changes:**

- Removed deprecated `job` function.
- Made batch job logs more concise.
- Upgraded DuckDB to v1.4.0.

## v1.22.8 (2025-09-11)

**Bug Fixes:**
- Fixed passing a parameter named `url` to `fused.run`.
- Fixed data table filter interactions with overflow and fullscreen, and fixed min/max labels.
- Fixes for the view changes modal and fixed a crash when duplicating UDFs.
- Fixed an issue where large error messages could cause no useful information to appear.
- Fixed an issue where GitHub PR information would not synchronize well.
- Fixed Workbench not showing a UDF was running and improved it to show the elapsed time.

## v1.22.7 (2025-09-09)

**New Features:**

- Experimental: Dataset discovery in AI chat.
- New Kimi K2 model added to AI chat.

**Improvements:**

AI & Chat:
- Improved AI chat window with better error handling and scrollback/redo functionality.
- AI can now fetch datasets via tools and S3 URLs in chat are now clickable.
- Better context management and the context size is now shown.
- Model selector synchronizes with profile model selector.

Workbench:
- Enhanced version control with improved diff viewer and more prominent titles.
- Auto-save UDFs after commit creation and when resolving stale UDFs.
- Enhanced data table with improved copy functionality, date filtering/sorting, and better header responsiveness.
- File UDFs now duplicate instead of showing conflict modals for better UX.
- Better UDF management with streamlined rename handling and duplicate detection.
- Theme fixes and UI improvements across the interface.

Data & Datasets:
- UTF-8 encoding by default for HTML data returns.
- Improved dataset sorting and search functionality.

**Bug Fixes:**

- Fixed serialization errors for UDFs returning interval categories with better error messages.
- Resolved UDF conflict modal issues and improved saving from action buttons.
- Improved UI performance when renaming UDFs, and in the AI chat.
- Corrected UDF icon display on hover in version pages.
- Fixed slider component thumb positioning.
- Fixed various canvas and UI stability issues.

**API Changes:**

- Polling-based UDF execution now enabled by default for `fused.run()`.
- Single serialization output format parameter in UDF run URLs, via `format`. Backward compatibility maintained for `dtype_out_vector/raster` parameters.

## v1.22.6 (2025-09-02)

**New Features:**

Re-Launching udf.ai! Let anyone talk to your data!

Learn how to connect your own data or analysis here

- Experimental: "MCP Creator" AI profile to simplify creation of MCP servers from UDFs. Read more.
- Experimental auto-retry on errors: AI will try fixing the error until it can either fix it or hits its limit. 
- Experimental: Dataset discovery scheduling in unstable environments.
- Experimental: Collection share tokens.

**Improvements:**
- AI Chat: More models, faster inference, better UX and error handling.
- AI now better understands data & has significantly improved ability to make charts especially for larger datasets.

- Workbench: Enhanced version page, improved UDF management flow, better data table filtering.
- Git integration: improved experience when pushing UDFs to GitHub.

**Bug Fixes:**
- AI & UDFs: Prevented AI writing to read-only UDFs, fixed serialization errors, JSON args, and File UDF save states.
- Version Control: Fixed stale UDF updates, staging detection, and backendId preservation.
- UI: Fixed Windows Ctrl+Click, modal flickering, autosave callbacks, and selection issues.
- [Experimental] Canvas: Fixed node persistence.

## v1.22.5 (2025-08-26)

**New Features:**
- Experimental Canvas mode.

**Improvements:**
- Added Deepseek v3.1 model to AI chat.

- AI chat has better layout and indicators for its settings. AI chat also shows user input when asking for more instructions.
- Versions button has moved to the bottom of the Workbench sidebar.

- Versions page shows the difference between upstream and a saved copy of a UDF.
- Versions page shows team repositories first, and will prompt when changing repository to a public one.
- Data table widget has `bigint` support, copy to clipboard, improved headers, and better filter support.

- Added html5lib package to the runtime.

**Bug Fixes:**
- AI chat will not suggest charting very small dataframes, along with other improvements to its suggestions and auto-fixes.
- Fixed bugs with the visualization menu in the map view. It works again and will not crash when no UDF is selected.
- Workbench won't show delete and versions button when they cannot do anything.
- Bug fixes for the conflict modal in Workbench not working or not updating the existing UDF.
- Fixed Workbench profile text for enterprise users.

**API changes:**
- Large jobs can now be run using `fused.run` and the `instance_type` parameter.

**Deprecations:**
- Parameters under the UDF in the advanced UDF editor is deprecated. It can be turned back on from the Workbench Preferences.

## v1.22.1 (2025-08-19)

**Improvements:**
- Inline AI editing is enabled by default.
- Improved data table component filtering.
- `fused.submit` results are cached by default.
- AI chat will now suggest new users to login after first prompt 
- Multiple open tabs message will appear at the top of the page instead of as a modal.

**Bug Fixes:**
- Fixed a bug when loading some HTML UDF outputs that caused them to not load scripts.
- Fixed bugs with AI prompts.
- Fixed bugs with navigating to particular UDFs on the version page.
- You can opt to keep a changed version of a UDF instead of reloading with changes.
- Fixed file explorer for basic tier users.
- Fixed the Add Billing modal appearing when it was not supposed to.
- The UDF selector has a minimum width and is no longer editable.

## v1.22.0 (2025-08-18)

**New Features:**
- Free tier is now open for all. Signing in to Fused now gives you access to a shared, free compute environment with a daily quota.
- New simplified UDF editor layout. The previous UDF editor with map view is still available as the *Advanced UDF editor*.

**Improvements:**
- Added inline AI editing features as an experimental feature.
- Added `@`-sign to select context in AI chat.
- Added profile selection in AI chat.
- Added new models to AI chat.
- AI chat context will understand the results history.
- Many adjustments to AI prompts.
- Table widget has more filter capabilities.
- Many widgets now load via Parquet for faster and better loading.
- UDF list has had many buttons consolidated into the three dot menu in the advanced UDF builder.

**Bug Fixes:**
- Significant improvements to self-service in Workbench.
- Fixed duplicate UDF and conflicting UDF modal bugs.
- Versions page will remember the last selected repository.
- Workbench will prompt to load new versions from GitHub.
- File explorer will show `/mount` instead of `file:///` URL.
- Fixed issues where HTML results would not be rendered with UTF-8 charset.
- Fixed issues where share token may not get cleared in Workbench when duplicating UDFs.
- Fixed bugs with the canvas map widget loading, filtering, and coloring. 
- Improved error messages and `run_*_async` when using `fused` from within Pyodide, for Pyodide-specific considerations.
- `to_fused(overwrite=True)` will delete the original UDF as expected.
- stderr will always be displayed for UDF runs.

**API changes:**
- Renamed `run_remote` to `run_batch`.
- Deprecate `fused.utils`, which has been replaced with `fused.load`.

## v1.21.6 (2025-07-31)

**New Features:**
- Added Qwen 3 to AI chat.

- Added a Table frame type.

**Improvements:**

AI:
- AI chat can be docker on the right or the left.
- You can now specify specific UDFs in AI chat.

Versioning: 
- Version control page will ask to confirm pushing to a different repository.

Rendering:
- HTML (previously *Embed*) view is always available.

**Bug Fixes:**
- Fixed bugs with caching POST requests for some UDFs.
- Fixed bugs with AI chat context.
- Fixed bugs with self service onboarding and billing portal.
- Fixed bugs where Table mode would not open by default in Workbench.
- Fixed bugs with cache_storage.

## v1.21.5 (2025-07-30)

**New Features:**
- Added self service signup for new users!
- Added voice input to udf.ai and to rich text frames.

**Improvements:**

AI Assistant:
- AI chat can now see results printed from a UDF.
- AI chat can now see sample rows returned from a UDF.
- AI chat can now show the code diff with generated code.

General:
- Profile modal now shows usage quota, when applicable.
- `@fused.cache` functions can now be profiled, and now supports object storage.
- Canvas now shows controls on hover.
- Canvas now has an Embed, Table, and Histogram frame types.
- Sped up UDF executions when `cache_max_age=0` was specified.
- Default UDF in Workbench is much simpler.

Canvas:
- Canvas now shows controls on hover.

**Bug Fixes:**
- Fixed saving UDFs where the UDF had previously been deleted on the server.
- Adjusted the default run timeout to match the backend run timeout.
- Fixed `fused.run` with `sync=False`.

## v1.21.4 (2025-07-24)

Minor bug fixes.

## v1.21.3 (2025-07-23)

Launching udf.ai! Ask your data questions, get AI to help you build answers.

**New Features:**
- Added support for speak-to-AI in the AI chat.
- Improved AI chat with one-click prompt suggestions and better tooltips.

**Improvements:**
- Improved color code handling in the editor.
- Moved experimental inline AI features behind feature flag.
- File-opener UDFs moved to the community catalog.

**Bug Fixes:**
- Fixed issues with undo-ing AI chat changes.
- Fixed issues with AI chat disconnections.
- Fixed caching for HTML return values from UDFs.
- Fixed handling of request parameters for UDF runs.

## v1.21.2 (2025-07-21)

**New Features:**
- Added ability to record audio into AI input.
- Added auto-share UDF on save functionality. (a shared token is created on first save for all new UDFs)
- Added compact mode for Canvas view.
- Added connection and save notifications for Canvas.
- Added ability to copy common load commands to clipboard.
- Added cache hit rate display on user profile.

**Improvements:**
- Enhanced AI Assistant with better UI/UX for changes and settings menu.
- Workbench will automatically add a default UDF, and updated the default UDF template.
- String return types will be assumed to be HTML by default.
- Improved version control page performance and styling. Clicking on the changes pending asterisk (`*`) will now go directly to the version control page.
- Improved performance of the GitHub integration.
- Improved saving all UDFs to do so in parallel.
- Enhanced map widget integration in Canvas.
- Shift+Enter will turn on the currently selected UDF.
- Changed default for `fused.ingest` to `target_num_chunks=500`.

**Bug Fixes:**
- Fixed issues with `fused.run` behaving differently for saved UDFs.
- Fixed issues with new apps not showing changes indicator
- Fixed selected preview tab in UDF builder flickering.
- Fixed preview image uploads during GitHub pushes.
- Fixed user assignment issues on GitHub PR creation.
- Fixed an issue with linking to GitHub accounts.
- Fixed an issue with tailing batch job logs.
- Fixed `fused.__version__` not being populated correctly.

## v1.21.1 (2025-07-14)

**`fused-py`**

- Updated LanceDB to 0.24.1.
- Fixed issues starting batch jobs.

**Workbench**

- AI Assistant now sees all UDFs in Workbench & has access to updated Fused documentation
- App builder now integrates with the new version control page.
- Fixed scrolling on the version control page.
- Fixed saving and layout issues on the Canvas view.

## v1.21.0 (2025-07-11)

**`fused-py`**

New Features:
- Added `fused.api.resolve`.
- Added `fused.api.team_info`.
- IPython magics now load automatically.
- When running a large (batch) job, it is now possible to specify the job's name.
- Upgraded `xarray` to 2025.4.0, DuckDB to 1.3.2, and H3 to 4.3.0.
- The `fd://` filesystem scheme will automatically be registered with `fsspec`.
- UDFs that return HTML will be loaded as `str` objects from `fused.run`.
- UDFs saved on Fused server now have a `catalog_url` property to get the Workbench UDF URL.
- `npy` output format is now supported for numpy (raster) return values.
- Arrow-compatible return values are now accepted.
- `fused.cache` will detect changes to referenced UDFs.
- `fused.run` accepts `verbose` keyword.

Bug Fixes:
- Fixed bugs with `udf.render` on some IPython versions.
- Fixed bugs with `repr`s for access tokens and UDFs.
- Fixed calling `fused.run` with UDF objects and `sync=False`.
- Renamed the UDF class to `Udf`.
- Removed some unused and deprecated code.
- Fixed bugs with `fused.cache` showing as not found, having stale files, or not passing arguments through.
- Removed the `n` keyword argument from `get_udfs` and `get_apps`.
- Fixed bugs with HEAD requests to UDF endpoint.
- `fused.submit` will warn about conflicting arguments.
- `/tmp/` size has been increased in realtime instances.
- `fused.api.list` and related functions supports `/mount` paths.
- Improved the performance of GitHub sync.
- Changed defaults for `JobPool.cancel` and fixed a bug where it would continue to retry.
- Fixed bugs with `JobPool.df` and UDF runs that result in exceptions.
- Fixed encoding URL paths in `fused.api.download` and related functions.

**Workbench**

New Features:
- Added AI editing and AI chat capabilities in the UDF builder.
- Workbench now has a code profiler: each line of code will show its execution runtime
- Added new Canvas dashboard builder mode (experimental).
- Added new Table data view mode.
- GitHub integration has a new page and is no longer beta.
- GitHub integration remembers relevant open PRs better.
- Fused apps uses a newer version of Streamlit and Stlite.
- Added a menu item to take a screenshot in higher-than-screen resolution.
- Added type-to-filter in File Explorer.
- File Explorer can be browsed without logging in.
- File Explorer now shows a summary of the current directory.
- File preview UDFs can now be specified with regular expressions.
- Cmd+Click on shared tokens will now take you to the UDF catalog.

Bug Fixes:
- Results panel shows when memory usage is unknown.
- Share code is more consistent with the selected output format.
- Fixed bugs with read-only app UI.
- The large data warning will now show in File Explorer as well when applicable.
- Fixed bugs with app/UDF catalog layout and sorting.
- Adjusted the UI for visualization settings and parameters in the UDF list.
- Reordered menu items in File Explorer.
- Cursor position will be remembered when switching between UDFs.
- Share tokens that are already shown on the page are no longer redacted.
- Map tooltip can now be scrolled.

## v1.20.1 (2025-06-09)

**`fused-py`**

New Features:
- Added `fused.api.resolve` and `fused.api.team_info`.
- IPython magics will automatically be loaded when importing `fused`.
- `run_batch` (batch jobs) can now accept a job name.

Bug Fixes:
- Fixed the `render` method of UDF objects.
- Fixed access tokens for apps being rendered in IPython.
- Fixed calling `fused.run(udf, sync=False)` with UDF objects.
- Removed some deprecated fields and arguments.

**Workbench**

- Workbench will now show the amount of time taken in UDF functions as a heatmap.
- Memory bars in the Results panel will show when usage is unknown.
- Update how shared token URLs are generated for different output formats.
- Updated stlite to 0.82.0.

## v1.20.0 (2025-06-03)

**`fused-py`**

New Features:
- Running a UDF with engine `local` will cache similarly to how it would when running on `remote` (supporting `cache_max_age` to control the caching).
- Large (batch) jobs will be in a Pending state if they cannot start immediately.
- UDFs can now accept `**kwargs` parameters, which will always be passed in as strings.
- `@fused.cache` has a new `cache_verbose` option. If set to `True` (default), it prints a message when a cached result is returned.
- `@fused.cache` renamed the `reset` parameter to `cache_reset`. The existing `reset` parameter is deprecated.
- Some file listing APIs like `fused.api.list` will work for public buckets when on free accounts.
- `fused.load` accepts `import_globals` (default `True`) for controlling importing UDF globals. Also, when globals cannot be imported, a warning is emitted instead of an exception.

Bug Fixes:
- Clarified login-needed message in Fused Apps.
- Fixed bugs with `@fused.cache` results not being ready.
- Fixed bugs with `@fused.cache` not detecting changes in the cached function.
- Loading a UDF from a file will autodetect the UDF function name.
- Fixed bugs with returning GeoDataFrames that do not contain geometry.
- Fixed calling `to_fused` on an app.

**Workbench**

- A message will appear above the UDF body when parameters are set in the UDF list.
- A lock icon will be shown next to read-only UDFs and Apps in Workbench.
- When pushing UDFs to GitHub, the preview image will be pushed to a public URL so that the README in GitHub is rendered correctly.
- When pushing UDFs to GitHub, Workbench will assign the PR to you if possible.
- It is now possible to delete UDFs and Apps directly from the catalog.
- Added a "Reload Collection" button. This pulls all latest version of UDF currently in your Collection.
- Workbench will minimize more changes from the PRs it creates on GitHub.
- "Open in Kepler.gl" supports H3 (string) data.
- The visibility button for a Fused App will now reset the app.
- A new Reset 3D View button is added to the UDF Builder map, and the keyboard shortcut has been updated to `Cmd+Shift+UpArrow` on MacOS (`Ctrl+Shift_UpArrow` on Windows / Linux).
- Workbench will show the current environment name above the map by default.
- Workbench will remember which UDF was selected when reopening the page.
- Adjusted which UDF mode label is shown when automatically detecting the UDF mode.
- Fixed some bugs with dynamic output mode.
- UI updates for the Pull Changes (history) and Push Changes views, including showing the README file in both views.
- Drag&Drop UDF into Workbench now works on the entire tab
- Added a button to download usage table in the Profile view.
- Fixed some visual bugs with light mode.
- File Explorer will no longer show file opener UDFs saved on your personal catalog, and will clarify when file opener UDFs are from your team catalog.

## v1.19.0 (2025-05-19)

**`fused-py`**

Breaking changes:
- Large (batch) jobs have been updated to pass parameters into the UDF the same way as other UDF runs. For compatibility, if the parameters passed into the job do not correspond with the parameters, a dictionary parameter is passed into the UDF instead. This will be deprecated and removed in a future release.
- The `context` and `bbox` parameters to a UDF are no longer treated as special.
- Python 3.9 support, which was previously deprecated, is now removed. The minimum Python version for the `fused` package is now 3.10.

New Features:
- PyArrow upgraded to version 16.0.
- Ingestion now supports input files without extensions, and filters out files with `.` or `_` prefix.
- `cache=False` is now a shortcut for disabling cache, e.g. `cache_max_age=0`.
- UDFs now support parameters annotated as `shapely.Geometry` or `shapely.Polygon`.
- `fused.load` now support loading UDFs from Github Pull Request URLs.
- Added a timeout parameter for `fused.api.upload`.
- Fused API functions now support `/mount` without `file://` prefix.
- `fused.api.download` now supports downloading files from `/mount`.
- `fused.api.list` now supports listing an individual file under `/mount`.
- `max_deletion_depth` in `fused.api.delete` default changed from 2 to 3.

Bug Fixes
- Fixed pickling UDF objects.
- Fixed UDF equality checks not conforming to Python specifications.
- Many fixes to ensure compatibility between the `fused` module available on PyPI and the `fused` module within the Fused backend.
- Fixed returning `pd.Timestamp` objects.
- Fixed bugs with handling of stdout if the UDF is async.
- Fixed bugs with UDF object `repr` in Jupyter.
- Fixed `@fused.cache` in Fused Apps.
- Fixed "multiple auth mechanisms" error when retrieving job results.
- Fixed deserialization of GeoDataFrame without geometry column.
- Fixed cases where UDFs would be indented when run.
- Various stability and performance updates, including new self-healing capabilities on the Fused backend.

**Workbench**

- Upgraded Streamlit in apps to 1.44.
- Fused Apps is no longer "beta".
- Fused Apps will now highlight syntax errors.
- Fused Apps will now autocomplete the `fused` module correctly.
- "Changes pending" in the map has been renamed to "Running" and now shows the time elapsed without needing to hover over it.
- UDF builder tooltips have been refreshed, it is now possible to click on data on the map to pin the tooltip on screen. Pinned tooltips show how many data records were under the mouse and allow paging through them.
- Workbench can now highlight H3 hexagons on click.
- Workbench can now detect decimal H3 indexes and Cmd+Click works on them.
- `udf://` URLs in Workbench now entirely overwrite parameters of the selected UDF.
- Fixed UDF builder showing partially updated map states for Tile UDFs.
- Collections catalog can now be sorted.
- Fixed a bug where Workbench would not detect newly added utils on UDFs.
- Parameters and Visualization sections are now styled slightly differently to make it easier to pick out your UDFs.
- Renamed Visualization "Surprise me" button to "Preset".
- Fixed a visual bug with the job status.
- Updated the share UDF and share app pages.
- Fixed bugs with UDF or app catalog showing the wrong content.
- Renamed "History" to "Pull Changes" and updated styling of that page.
- Fixed Pull Changes not showing diffs if utils had been added or deleted.
- Fixed Workbench showing the original Github link for forked UDFs.
- Workbench code editors will now remember scroll position.
- Clicking the viewport location label will now copy it.

## v1.18.0 (2025-04-28)

**`fused-py`**

Breaking changes:
- `fused.submit` now raises an error by default, if there is any run erroring

New Features:
- It is now possible set the cache storage location with `@fused.cache(storage=...)`.
- `@fused.cache` can now exclude arguments from the cache key.
- `@fused.cache` uses Pandas' own way of hashing DataFrames.
- Added storage argument to `fused.file_path(storage=...)`.
- Large jobs now pick up AWS credentials more consistently.
- Auth redirect can dynamically select the port when logging in locally.
- `udf.to_fused` will show the diff when UDF name conflicts with an existing UDF.
- `fused.load` can load by the unique UDF ID again.
- UDFs can be run by username and UDF name in addition to email and UDF name. (ex: `fused.run(user@team.com/my_cool_udf)`). 
- Preview images can now be specified in-directory in Github.
- Adjusted UDF caching behavior for performance.

Bug Fixes
- Fixed behavior when loading and running UDFs with code outside of the UDF function.
- Fixed `fused.api.list` being incompatible with some async stacks.
- Fixed a bug where strings inside UDFs would get extra spaces added to them.
- Shared tokens can be created by a team account.
- Fixed a bug that could occur where Fused would try to duplicate index column names of the returned DataFrame.
- Fixed various bugs when a UDF closes `stdout`.
- Fixed a bug where `fused.run` would not return printed messages from a UDF.
- Fixed a bug where `fused.load` would crash on very large strings.
- Fixed various bugs with exporting UDFs from within fused-py.
- Fixed a bug where `partitioning_schema_input` would not be found when ingesting.
- Fixed a bug where UDFs might import incorrectly when several pushes happen in quick succession in a linked Github repo.

**Workbench**

- `File (Viewport)` renamed to `Single (Viewport)`.
- Added `Single (Parameter)` UDF type that behaves like `Single (Viewport)` but does not pass the viewport bounds.
- File Explorer will show a per-user home favourite.
- Your UDF list will now sync across different browser tabs.
- Preference toggles are added to the Command Palette.
- The share page has been redesigned.
- Collections catalog page now shows the UDF in a given Collection by hovering over them.
- When adding a duplicate UDF to Workbench, you will be prompted to duplicate or replace it.
- Memory usage for UDFs can be found in the results panel (once displaying memory usage was been turned on in Preferences)
- Workbench will indicate that UDF runs were cached in all circumstances.
- The public map page is now compatible with any public-readable shared token.
- Added `udf://name?param=value` URL support to Workbench.
- Reduced the size of metadata diffs generated by Workbench when pushing to Github.
- Various performance improvements.
- Fixes for various UI layout bugs.

## v1.17.0 (2025-04-10)

**`fused-py`**

- Team UDFs can be loaded or run by specifying the name "team", as in: `fused.load("team/udf_name")`
- `Udf.to_fused` supports overwriting the UDF when saving.
- Added `fused.api.enable_gcs()` to configure using the Google Cloud Platform secret specified in Fused secret manager.
- `@fused.cache` locking mechanism has changed and will not allow multiple concurrent runs.
- Upgraded DuckDB to v1.2.2.
- Running a saved UDF by token or name will now also show the logs, including print statements and error tracebacks.
- All functions interacting with the Fused server will now retry automatically, by default 3 times.
- Python 3.9 support is deprecated. The next release of `fused` will require Python 3.10+.
- Deprecated `fused_batch` module is removed.

**Workbench**

- Cached UDF runs will show the original logs.
- "Change output parameters" in the Share UDF screen shows all detected parameters.
- Added a copy viewport bounds button in the Results panel.
- Improved the performance of the catalog screen.
- Fixed the job page showing times in inconsistent time zones.

App Builder:
- Deprecated `fused_app` module is removed.

## v1.16.3 (2025-04-03)

**`fused-py`**

- It is now possible to return general `list`s and `tuple`s from UDFs. (Note: a tuple of a raster and bounds will be treated as a raster return type.)

**Workbench**

- Workbench will now prompt you when loading large UDF results that could slow down or overwhelm your browser. The threshold for this prompt is configurable in your Workbench preferences.
- Fixed bugs with loading large UDF results.
- UDF list will show an error if a UDF has an empty name.
- Fixed running some public UDFs in Workbench.

## v1.16.2 (2025-04-01)

**`fused-py`**

- It is now possible to return dictionaries of objects from a UDF, for example a dictionary of a raster numpy array, a DataFrame, and a string.
- Whitespace in a UDF will be considered as changes when determining whether to return cached data. (a UDF with different whitespace will be rerun rather than cached)
- Fixed calling `fused.run` in large jobs.

**Workbench**

- Added experimental AI agent builder.
- Workbench will now prompt you to replace an existing UDF when adding the same UDF (by name) from the catalog.
- Added ability to download & upload an entire collection.
- Fixed saving collections with empty names.

Visualization:
- Added an H3-only visualization preset.
- Fixed a bug where changing TileLayer visualization type could result in a crash.

App Builder:
- Updated the runtime.

## v1.16.0 (2025-03-27)

**`fused-py`**

- The result object of running in `batch` now has `logs_url` property.
- Fixed `fused.submit` raising an error if some run failed.

**Workbench**

- Added a Download UDFs button for downloading an entire collection.
- Results will show a message at the top if UDF execution was cached.
- Non-visible UDFs will have a different highlight color on them in the UDF list.
- Collections will show as modified if the order of UDFs has been changed.
- Fixes for Collections saving the ordering and visibility of UDFs.
- Fixed the Team Jobs page in Workbench crashing in some cases.

**Shared tokens**

- Shared token URLs can be called with an arbitrary (ignored) file extension in the URL.

## v1.15.0 (2025-03-20)

**`fused-py`**

- Loading UDFs now behaves like importing a Python module, and attributes defined on the UDF can be accessed.
- The `fused.submit()` keyword `wait_on_result` has been renamed to `collect`, with a default of `collect=True` returning the collected results (pass `collect=False` to get the JobPool object to inspect individual results).
- New UDFs default to using `fused.types.Bounds`.
- Upgraded `duckdb` to v1.2.1.
- UDFs can now return simple types like `str`, `int`, `float`, `bool`, and so on.
- Files in `/mount/` can be listed through the API.
- UDFs from publicly accessible GitHub repositories can be loaded through `fused.load`.
- `fused.load` now supports loading a UDF from a local .py file or directory
- The `x`, `y` and `z` aren't protected arguments when running a UDF anymore (previously protected to pass X/Y/Z mercantile tiles).

**Workbench**

New:
- Added a new account page and redesigned preferences page.
- You can now customize the code formatter settings (available under Preferences > Editor preferences).
- UDFs can optionally be shared with their code when creating a share token.

General:
- Moved shared token page to bottom left bar, and adjusted the icons.
- The ordering of UDFs in collections is now saved.

App Builder:
- Updated app list UI.
- Fixed bugs with shared apps showing the wrong URL in the browser.

## v1.14.0 (2025-02-25)

v1.14.0 introduces a lot of new changes across `fused-py` and Workbench

**`fused-py`**

- Introducing `fused.submit()` method for multiple job run
- Improvement to UDF caching
    - All UDFs are now cached for 90 days by default
    - Ability to customize the age of cached data & UDFs with the new `cache_max_age` argument when defining UDFs, running UDFs or when caching regular Python functions
- `pandas` & `geopandas` are now optional for running non-spatial UDF locally
- Removed hardcoded `nodata=0` value for serializing raster data

**Workbench**

New:
- Introducing Collections to organize & aggregate UDFs together
- Redesigned "Share" button & page: All the info you need to share your UDFs to your team or the world

General:
- Improvements to Navigation in Command Pallette. Try it out in Workbench by doing `Cmd + K` (`Ctrl + K` on Windows / Linux)
- Autocomplete now works with `Tab` in Code Editor with `Tab`
- Added a Delete Button in the Shared Tokens page (under Account page)
- Ability to upload images for UDF Preview in Settings Page
- Adding “Fullscreen” toggle in Map View
- Improved `colorContinuous` in Visualize Tab
- Allowing users to configure public/team access scopes for share tokens 
- No longer able to edit UDF & App name in read-only mode
- Fixing job loading logs

File Explorer:
- Download directories as `zip`
- Adding favorites to file path input search results 
- Ability to open `.parquet` files with Kepler.gl

## v1.13.0 (2025-01-22)

- Fixed shared UDFs not respecting the Cache Enabled setting.
- Added a cache TTL (time-to-live) setting when running a UDF via a shared token endpoint.
- Tags you or your team have already used will be suggested when editing a UDF's tags.
- Team UDFs will be shown as read-only in Workbench, similar to Public UDFs.
- File Explorer shows deletion in progress.
- File Explorer can accept more S3 URLs, and uses `/mount/` instead of `/mnt/cache`.
- UDF Builder will no longer select a UDF when clicking to hide it.
- Fixed how Push to Github chooses the directory within a repository to push to.
- Fixed the browser location bar in Workbench updating on a delay.
- Fixed writing Shapefile or GPKG files to S3.
- (Beta) New fusedio/apps repository for public Fused Apps.
- Navigating to Team UDFs or Saved UDFs in the UDF Catalog will now prompt for login.
- Fixed the "Select..." environment button in Workbench settings.
- UDF Builder will no longer replace all unaccepted characters with `_` (underscore).
- Fixed loading team UDFs when running a UDF with a shared token.
- Batch jobs that use `print` will now have that output appear in the job logs.
- Apps in the shared token list show an app icon.
- Removed some deprecated batch job options.
- Installed `vega-datasets` package.

## v1.12.0 (2025-01-10)

- (Beta) Added an App catalog in Workbench, and a new type of URL for sharing apps.
- Added `/mount` as an alias for `/mnt/cache`.
- More consistently coerce the type of inputs to UDFs.
- Added more visualization presets to UDF builder in Workbench.
- Fixed an issue where the tab icon in Workbench could unintentionally change.
- Fixed bugs in Workbench File Explorer for `/mnt/cache` when browsing directories with many files.
- Fixed bugs in `fused` Python API not being able to list all files that should be accessible.
- Fixed bugs in the Github integration, command palette, and file explorer in Workbench.
- Fixed bugs in caching some UDF outputs.
- The shareable URL for public and community UDFs will now show in the settings tab for those UDFs.
- UDFs can customize their data return with `Response` objects.

## v1.11.9 (2024-12-19)

- Accounts now have a *handle* assigned to them, which can be used when loading UDFs and pushing to community UDFs
- Account handle can be changed once by the user (for more changes please contact the Fused team.)
- Added a command palette to the Workbench, which can be opened with Cmd-k or Ctrl-k.
- When creating a PR for a community UDF or to update a public UDF, it will be under your account if you log in to Fused with Github.
- Bug fixes for pushing to Github, e.g. when pushing a saved UDF, and for listing the Fused bot account as an author.
- Batch (`run_batch`) jobs can call back to the Fused API.
- Team UDFs can be pinned to the end of the featured list.
- Speed improvements in ingestion.
- Ingestion will detect `.pq` files as Parquet.
- Format code shortcut in Workbench is shown in the keyboard shortcut list and command palette.
- Workbench will hide the map tooltip when dragging the map by default.
- Workbench will now look for a `hexLayer` visualization preset for tabular results that do not contain `geometry`.
- Workbench file explorer can now handle larger lists of files.
- Fix for browsing disk cache (`/mnt/cache`) in Workbench file explorer.
- Teams with multiple realtime instances can now set one as their default.
- Fix for saving UDFs with certain names. Workbench will show more descriptive error messages in more cases for issues saving UDFs.

## v1.11.8 (2024-12-04)

- New File Explorer interface, with support for managing Google Cloud Storage (GCS) and `/mnt/cache` files.
- Workbench will show an error when trying to save a UDF with a duplicate name.
- Fixed a few bugs with Github integration, including the wrong repository being selected by default when creating a PR.
- Updated `fsspec` and `pyogrio` packages.

## v1.11.7 (2024-11-27)

- Decluttered the interface on mobile browsers by default.
- Fixed redo (Cmd-Shift-z or Ctrl-Shift-z) sometimes being bound to the wrong key.
- Tweaked the logic for showing the selected object in Workbench.

## v1.11.6 (2024-11-26)

- Added Format with Black (Alt+Shift+f) to Workbench.
- Fix the CRS of DataFrame's returned by get_chunk_from_table.
- Added a human readable ID to batch jobs.
- Fused will send an email when a batch job finishes.
- Fix for opening larger files in Kepler.gl.
- Fix for accessing UDFs in a team.
- Improved messages for UDF recursion, UDF geometry arguments, and returning geometry columns.
- Adjusted the UDF list styling and behavior in Workbench.
- Fix for secrets in shared tokens.

## v1.11.5 (2024-11-20)

- Show message for keyword arguments in UDFs that are reserved.
- Added reset kernel button.
- Workbench layers apply visualization changes immediately when the map is paused.
- Show the user that started a job for the team jobs list.
- Fix for running nested UDFs with utils modules.
- Fix for returning xarray results from UDFs.
- Fix for listing files from within UDFs.
- Upgraded to GeoPandas v1.

## v1.8.0 (2024-06-25) :package:

- Added Workbench tour for first-time users.
- Undo history is now saved across UDFs and persists through reloads.
- Added autocomplete when writing UDFs in Workbench.
- Added `colorBins`, `colorCategories`, and `colorContinuous` functions to Workbench's Visualize tab.
- Migrated SDK to Pydantic v2 for improved data validation and serialization.
- Fixed a bug causing NumPy dependency conflicts.

## v1.7.0 (2024-06-04) :bird:

- Execution infrastructure updates.
- Update DuckDB package to v1.0.0.
- Improve responsivity of Workbench allotments.
- Crispen Workbench UI.

## v1.6.1 (2024-05-06) :guardsman:

_GitHub integration_

- Updates to team GitHub integration.
- Users are now able to create shared UDF token from a team UDF both in Workbench and Python SDK.

## v1.6.0 (2024-04-30) :checkered_flag:

- The Workbench file explorer now shows UDFs contributed by community members.
- Team admins can now set up a GitHub repository with UDFs that their team members can access from Workbench.

## v1.5.4 (2024-04-15) :telescope:

- Button to open slice of data in Kepler.gl.
- Minor UI design and button placement updates.

## v1.5.3 (2024-04-08) :duck:

- Improved compatibility with DuckDB requesting data from shared UDFs.
- Geocoder in Workbench now supports coordinates and H3 cell IDs.
- GeoDataFrame arguments to UDFs can be passed as bounding boxes.
- The package ibis was upgraded to 8.0.0.
- Utils modules no longer need to import fused.

## v1.5.2 (2024-04-01) :tanabata_tree:

- File browser can now preview images like TIFFs, JPEGs, PNGs, and more.
- Users can now open Parquet files with DuckDB directly from the file browser.

## v1.5.0 (2024-03-25) :open_file_folder:

- The upload view in Workbench now shows a file browser.
- Users can now preview files in the file browser using a default UDF.

## v1.4.1 (2024-03-19) :speech_balloon:

- UDFs now support typed function annotations.
- Introduced special types  `fused.types.TileXYZ`, `fused.types.TileGDF`, `fused.types.Bbox`.
- Workbench now autodetects Tile or File outputs based on typing.
- Added button to Workbench to autodetect UDF parameters based on typing.

## v1.1.1 (2024-01-17) :dizzy:

- Renamed `fused.utils.run_realtime` and `fused.utils.run_realtime_xyz` to `fused.utils.run_file` amd `fused.utils.run_tile`.
- Removed `fused.utils.run_once`.

## v1.1.0 (2024-01-08) :rocket:

- Added functions to run the UDFs realtime.

## v1.1.0-rc2 (2023-12-11) :bug:

- Added `fused.utils.get_chunk_from_table`.
- Fixed bugs in loading and saving UDFs with custom metadata and headers.

## v1.1.0-rc0 (2023-11-29) :cloud:

- Added cloud load and save UDFs.
- `target_num_files` is replaced by `target_num_chunks` in the ingest API.
- Standardize how a decorator's headers are preprocesses to set `source_code` key.
- Fixed a bug loading UDFs from a job.

## v1.0.3 (2023-11-7) :sweat_drops:

_Getting chunks_

- Added `fused.utils.get_chunks_metadata` to get the metadata GeoDataFrame for a table.
- `run_local` now passes a copy of the input data into the UDF, to avoid accidentally persisting state between runs.
- `instance_type` is now shown in more places for running jobs.
- Fixed a bug where `render()`ing UDFs could get cut off.
- Fixed a bug with defining a UDF that contained an internal `@contextmanager`.

## v1.0.2 (2023-10-26) :up:

_Uploading files_

- Added `fused.upload` for uploading files to Fused storage.
- Added a warning for UDF parameter names that can cause issues.
- Fixed some dependency validation checks incorrectly failing on built-in modules.

## v1.0.1 (2023-10-19) :ant:

- Added `ignore_chunk_error` flag to jobs.
- Added warning when sidecar table names are specified but no matching table URL is provided.
- Fixed reading chunks when sidecars are requested but no sidecar file is present.
- Upgraded a dependency that was blocking installation on Colab.

## v1.0.0 (2023-10-13) :ship:

_Shipping dependencies_

- Added `image_name` to `run_batch` for customizing the set of dependencies used.
- Added `fused.delete` for deleting files or tables.
- Renamed `output_main` and `output_fused` to `output` and `output_metadata` respectively in ingestion jobs.
- Adjusted the default instance type for `run_batch`.
- Fixed `get_dataframe` sometimes failing.
- Improved tab completion for `fused.options` and added a repr.
- Fixed a bug where more version migration messages were printed.
- Fixed a bug when saving `fused.options`.

================================================================================

### index

**URL:** https://docs.fused.io/python-sdk

# Python SDK

> The latest version of `fused-py` is <FusedVersionLive />.

## Documentation overview

    Installing `fused` is required if you're running `fused` on your end (locally or in a development environment). If you're working in Workbench UDF Builder or App Builder `fused` is already installed for you.

1. Set up a Python environment:

We're using `venv` but you could use `conda` or any other environment manager in Python. 

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install the `fused` package:

You can only install the base package, though we recommend still adding the optional dependencies:
```bash
pip install fused
```

Or install with optional dependencies:
```bash
# For raster data processing
pip install "fused[raster]"

# For vector data processing
pip install "fused[vector]"

# Install all optional dependencies
pip install "fused[all]"
```

</details>

### Authenticate

The first time you use Fused you'll need to authenticate.

```python
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
print(credentials.url)
```

Follow the URL in your browser to authenticate.

## Basic API usage

Some basic examples of how to use `fused` to run UDFs:

#### Hello World UDF

```python

@fused.udf
def udf(x: int = 1):
    return f"Hello world "

fused.run(udf)
```

```bash
>> Hello world 2
```

#### Simple data UDF

```python

@fused.udf
def udf(x: int = 3):

    return pd.DataFrame()

fused.run(udf)
```

```bash
>>   | x |
|---|---|
| 0 | 0 |
| 1 | 1 |
| 2 | 2 |
```

###

================================================================================

### Top-Level Functions

**URL:** https://docs.fused.io/python-sdk/top-level-functions

## @fused.udf

```python
udf() -> Callable[..., Udf]
```

A decorator that transforms a function into a Fused UDF.

**Parameters:**

- **name** (<code>Optional[str]</code>) – The name of the UDF object. Defaults to the name of the function.

- **cache_max_age** (<code>Optional[str]</code>) – The maximum age when returning a result from the cache.

- **instance_type** (<code>Optional[str]</code>) – The default type of instance to use for remote execution
  ('realtime' or 'batch').

- **default_parameters** (<code>Optional\[Dict[str, Any]\]</code>) – Parameters to embed in the UDF object, separately from the arguments
  list of the function. Defaults to None for empty parameters.

- **headers** (<code>Optional\[Sequence\[Union[str, Header]\]\]</code>) – A list of files to include as modules when running the UDF. For example,
  when specifying `headers=['my_header.py']`, inside the UDF function it may be
  referenced as:

  ```py

  my_header.my_function()
  ```

  Defaults to None for no headers.

**Returns:**

- <code>Callable\..., [Udf\]</code> – A callable that represents the transformed UDF. This callable can be used
- <code>Callable\..., [Udf\]</code> – within GeoPandas workflows to apply the defined operation on geospatial data.

**Examples:**

To create a simple UDF that calls a utility function to calculate the area of geometries in a GeoDataFrame:

```py
@fused.udf
def udf(bbox, table_path="s3://fused-asset/infra/building_msft_us"):
    ...
    gdf = table_to_tile(bbox, table=table_path)
    return gdf
```

---

## @fused.cache

```python
cache() -> Callable[..., Any]
```

Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function
to cache its return values. The cache behavior can be customized through
keyword arguments.

**Parameters:**

- **func** (<code>Callable</code>) – The function to be decorated. If None, this
  returns a partial decorator with the passed keyword arguments.
- **cache_max_age** (<code>str | int</code>) – A string with a numbered component and units. Supported units are seconds (s), minutes (m), hours (h), and
  days (d) (e.g. "48h", "10s", etc.).
- **cache_folder_path** (<code>str</code>) – Folder to append to the configured cache directory.
- **concurrent_lock_timeout** (<code>str | int</code>) – Max amount of time in seconds for subsequent concurrent calls to wait for a previous
  concurrent call to finish execution and to write the cache file.
- **cache_reset** (<code>bool | None</code>) – Ignore `cache_max_age` and overwrite cached result.
- **cache_storage** (<code>StorageStr | None</code>) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.
- **cache_key_exclude** (<code>Iterable[str]</code>) – An iterable of parameter names to exclude from the cache key calculation. Useful for
  arguments that do not affect the result of the function and could cause unintended cache expiry (e.g.
  database connection objects)
- **cache_verbose** (<code>bool | None</code>) – Print a message when a cached result is returned

Returns:
Callable: A decorator that, when applied to a function, caches its
return values according to the specified keyword arguments.

**Examples:**

Use the `@cache` decorator to cache the return value of a function in a custom path.

```py
@cache(path="/tmp/custom_path/")
def expensive_function():
    # Function implementation goes here
    return result
```

If the output of a cached function changes, for example if remote data is modified,
it can be reset by running the function with the `cache_reset` keyword argument. Afterward,
the argument can be cleared.

```py
@cache(path="/tmp/custom_path/", cache_reset=True)
def expensive_function():
    # Function implementation goes here
    return result
```

---

## fused.load

```python
load() -> AnyBaseUdf
```

Loads a UDF from various sources including GitHub URLs,
and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused
platform-specific identifier composed of an email and UDF name. It intelligently
determines the source type based on the format of the input and retrieves the UDF
accordingly.

**Parameters:**

- **url_or_udf** (<code>Union[str, Path]</code>) – A string representing the location of the UDF, or the raw code of the UDF.
  The location can be a GitHub URL starting with "https://github.com",
  a Fused platform-specific identifier in the format "email/udf_name",
  or a local file path pointing to a Python file.
- **cache_key** (<code>Any</code>) – An optional key used for caching the loaded UDF. If provided, the function
  will attempt to load the UDF from cache using this key before attempting to
  load it from the specified source. Defaults to None, indicating no caching.
- **import_globals** (<code>bool</code>) – Expose the globals defined in the UDF's context as attributes on the UDF object (default True).
  This requires executing the code of the UDF. To globally configure this behavior, use `fused.options.never_import`.

**Returns:**

- **AnyBaseUdf** (<code>AnyBaseUdf</code>) – An instance of the loaded UDF.

**Raises:**

- <code>ValueError</code> – If the URL or Fused platform-specific identifier format is incorrect or
  cannot be parsed.
- <code>Exception</code> – For errors related to network issues, file access permissions, or other
  unforeseen errors during the loading process.

**Examples:**

Load a UDF from a GitHub URL:

```py
udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
```

Load a UDF using a Fused platform-specific identifier:

```py
udf = fused.load("username@fused.io/REM_with_HyRiver")
```

---

## fused.run

```python
run(
    udf: Union[str, None, UdfJobStepConfig, Udf, UdfAccessToken] = None,
    *,
    x: Optional[int] = None,
    y: Optional[int] = None,
    z: Optional[int] = None,
    sync: bool = True,
    engine: Optional[Literal["remote", "local"]] = None,
    instance_type: Optional[InstanceType] = None,
    type: Optional[Literal["tile", "file"]] = None,
    max_retry: int = 0,
    cache_max_age: Optional[str] = None,
    cache: bool = True,
    parameters: Optional[Dict[str, Any]] = None,
    _return_response: Optional[bool] = False,
    _ignore_unknown_arguments: bool = False,
    _cancel_callback: Callable[[], bool] | None = None,
    **kw_parameters: Callable[[], bool] | None
) -> Union[
    ResultType,
    Coroutine[ResultType, None, None],
    UdfEvaluationResult,
    Coroutine[UdfEvaluationResult, None, None],
]
```

Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote),
with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and
allows for both synchronous and asynchronous execution. It dynamically determines the execution
path based on the provided parameters.

**Parameters:**

- **udf** (<code>str, Udf or UdfJobStepConfig</code>) – the UDF to execute.
  The UDF can be specified in several ways:
  - A string representing a UDF name or UDF shared token.
  - A UDF object.
  - A UdfJobStepConfig object for detailed execution configuration.
- **x, y, z** (<code>int</code>) – Tile coordinates for tile-based UDF execution.
- **sync** (<code>bool</code>) – If True, execute the UDF synchronously. If False, execute asynchronously.
- **engine** (<code>Optional\[Literal['remote', 'local']\]</code>) – The execution engine to use ('remote' or 'local').
- **instance_type** (<code>Optional[InstanceType]</code>) – The type of instance to use for remote execution ('realtime',
  or 'small', 'medium', 'large' or one of the whitelisted instance types).
  If not specified, gets the default from the UDF (if specified in the
  `@fused.udf()` decorator, and the UDF is not run as a shared token),
  or otherwise defaults to 'realtime'.
- **type** (<code>Optional\[Literal['tile', 'file']\]</code>) – The type of UDF execution ('tile' or 'file').
- **max_retry** (<code>int</code>) – The maximum number of retries to attempt if the UDF fails.
  By default does not retry.
- **cache_max_age** (<code>Optional[str]</code>) – The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. “48h”, “10s”, etc.).
  Default is `None` so a UDF run with `fused.run()` will follow `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) – Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **verbose** – Set to False to suppress any print statements from the UDF.
- **parameters** (<code>Optional\[Dict[str, Any]\]</code>) – Additional parameters to pass to the UDF.
- \*\***kw_parameters** – Additional parameters to pass to the UDF.

**Raises:**

- <code>ValueError</code> – If the UDF is not specified or is specified in more than one way.
- <code>TypeError</code> – If the first parameter is not of an expected type.
- <code>Warning</code> – Various warnings are issued for ignored parameters based on the execution path chosen.

**Returns:**

- <code>Union\[ResultType, Coroutine\[ResultType, None, None\], UdfEvaluationResult, Coroutine\[UdfEvaluationResult, None, None\]\]</code> – The result of the UDF execution, which varies based on the UDF and execution path.

**Examples:**

Run a UDF saved in the Fused system:

```py
fused.run("username@fused.io/my_udf_name")
```

Run a UDF saved in GitHub:

```py
loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

Run a UDF saved in a local directory:

```py
loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
fused.run(loaded_udf, bbox=bbox)
```

This function dynamically determines the execution path and parameters based on the inputs.
It is designed to be flexible and support various UDF execution scenarios.
</details>

---

## fused.submit

```python
submit() -> Union[BaseJobPool, ResultType, pd.DataFrame]
```

Executes a user-defined function (UDF) multiple times for a list of input
parameters, and return immediately a "lazy" JobPool object allowing
to inspect the jobs and wait on the results.

Each individual UDF run will be cached following the standard caching logic as with `fused.run()`
and the specified `cache_max_age`. Additionally, when `collect=True` (the default), the collected results
are cached locally for the duration of `cache_max_age` or 12h by default.

See `fused.run` for more details on the UDF execution.

**Parameters:**

- **udf** (<code>AnyBaseUdf | FunctionType | str</code>) – the UDF to execute.
  See `fused.run` for more details on how to specify the UDF.
- **arg_list** – a list of input parameters for the UDF. Can be specified as:
  - a list of values for parametrizing over a single parameter, i.e.
    the first parameter of the UDF
  - a list of dictionaries for parametrizing over multiple parameters
  - A DataFrame for parametrizing over multiple parameters where each
    row is a set of parameters
- **engine** (<code>Optional\[Literal['remote', 'local']\]</code>) – The execution engine to use. Defaults to 'remote'.
- **max_workers** (<code>Optional[int]</code>) – The maximum number of workers to use. Defaults to 32.
- **max_retry** (<code>int</code>) – The maximum number of retries for failed jobs. Defaults to 2.
- **debug_mode** (<code>bool</code>) – If True, executes only the first item in arg_list directly using
  `fused.run()`, useful for debugging UDF execution. Default is False.
- **collect** (<code>bool</code>) – If True, waits for all jobs to complete and returns the collected DataFrame
  containing the results. If False, returns a JobPool object, which is non-blocking
  and allows you to inspect the individual results and logs.
  Default is True.
- **execution_type** (<code>ExecutionType</code>) – The type of batching to use. Either "thread_pool" (default) for
  ThreadPoolExecutor-based concurrency or "async_loop" for asyncio-based concurrency.
- **cache_max_age** (<code>Optional[str]</code>) – The maximum age when returning a result from the cache.
  Supported units are seconds (s), minutes (m), hours (h), and days (d)
  (e.g. "48h", "10s", etc.).
  Default is `None` so a UDF run with `fused.run()` will follow
  `cache_max_age` defined in `@fused.udf()` unless this value is changed.
- **cache** (<code>bool</code>) – Set to False as a shortcut for `cache_max_age='0s'` to disable caching.
- **ignore_exceptions** (<code>bool</code>) – Set to True to ignore exceptions when collecting results.
  Runs that result in exceptions will be silently ignored. Defaults to False.
- **flatten** (<code>bool</code>) – Set to True to receive a DataFrame of results, without nesting of a
  `results` column, when collecting results. When False, results will be nested
  in a `results` column. If the UDF does not return a DataFrame (e.g. a string
  instead,) results will be nested in a `results` column regardless of this setting.
  Defaults to True.
- \*\***kwargs** – Additional (constant) keyword arguments to pass to the UDF.

**Returns:**

- <code>Union\[BaseJobPool, ResultType, DataFrame\]</code> – JobPool, AsyncJobPool, or DataFrame depending on execution_type and collect parameters

**Examples:**

Run a UDF multiple times for the values 0 to 9 passed to as the first
positional argument of the UDF:

```py
df = fused.submit("username@fused.io/my_udf_name", range(10))
```

Using async batch type:

```py
df = fused.submit(udf, range(10), execution_type="async_loop")
```

Being explicit about the parameter name:

```py
df = fused.submit(udf, [dict(n=i) for i in range(10)])
```

Get the pool of ongoing tasks:

```py
pool = fused.submit(udf, [dict(n=i) for i in range(10)], collect=False)
```

---

## fused.download

```python
download() -> str
```

Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

💡 Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

💡 Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.

**Parameters:**

- **url** (<code>str</code>) – The URL to download.
- **file_path** (<code>str</code>) – The local path where to save the file.
- **storage** (<code>StorageStr</code>) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> – The function downloads the file only on the first execution, and returns the file path.

**Examples:**

```python
@fused.udf
def geodataframe_from_geojson():

    url = "s3://sample_bucket/my_geojson.zip"
    path = fused.core.download(url, "tmp/my_geojson.zip")
    gdf = gpd.read_file(path)
    return gdf
```

---

## fused.ingest

```python
ingest() -> GeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Parameters:**

- **input** (<code>str | Path | Sequence[str | Path] | gpd.GeoDataFrame</code>) – A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.

- **output** (<code>str | None</code>) – Location on S3 to write the `main` table to.

- **output_metadata** (<code>str | None</code>) – Location on S3 to write the `fused` table to.

- **schema** (<code>Schema | None</code>) – Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.

- **file_suffix** (<code>str | None</code>) – filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.

- **load_columns** (<code>Sequence[str] | None</code>) – Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.

- **remove_cols** (<code>Sequence[str] | None</code>) – The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.

- **explode_geometries** (<code>bool</code>) – Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.

- **drop_out_of_bounds** (<code>bool | None</code>) – Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.

- **partitioning_method** (<code>Literal['area', 'length', 'coords', 'rows']</code>) – The method to use for grouping rows into partitions. Defaults to `"rows"`.

  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.

- **partitioning_maximum_per_file** (<code>int | float | None</code>) – Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/10th the total area of all geometries. Defaults to `None`.

- **partitioning_maximum_per_chunk** (<code>int | float | None</code>) – Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will have no more than 1/100th the total area of all geometries. Defaults to `None`.

- **partitioning_max_width_ratio** (<code>int | float</code>) – The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.

- **partitioning_max_height_ratio** (<code>int | float</code>) – The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.

- **partitioning_force_utm** (<code>Literal['file', 'chunk', None]</code>) – Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".

- **partitioning_split_method** (<code>Literal['mean', 'median']</code>) – How to split one partition into children. Defaults to `"mean"` (this may change in the future).

  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.

- **subdivide_method** (<code>Literal['area', None]</code>) – The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).

- **subdivide_start** (<code>float | None</code>) – The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.

- **subdivide_stop** (<code>float | None</code>) – The value below which geometries will not be subdivided into smaller parts, according to `subdivide_method`. Recommended to be equal to subdivide_start. If `None`, geometries will be subdivided up to a recursion depth of 100 or until the subdivided geometry is rectangular.

- **split_identical_centroids** (<code>bool</code>) – If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".

- **target_num_chunks** (<code>int</code>) – The target for the number of files if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
  Defaults to 500, rough default to use in most cases.

- **lonlat_cols** (<code>tuple[str, str] | None</code>) – Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

  ```py
  fused.ingest(
      ...,
      lonlat_cols=("x", "y")
  )
  ```

  This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- **gdal_config** (<code>GDALOpenConfig | dict[str, Any] | None</code>) – Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

  ```py
  fused.ingest(
      ...,
      gdal_config=
      }
  )
  ```

- **overwrite** (<code>bool</code>) – If True, overwrite the output directory if it already exists
  (by first removing all existing content of the directory, i.e. it
  does not only overwrite conflicting files).
  Defaults to False.

- **as_udf** (<code>bool</code>) – Return the ingestion workflow as a UDF that can be executed using
  `fused.run()`. Local files or python objects passed to `input` or
  `partitioning_schema_input` are still uploaded to S3 first such that
  those are available when executing the UDF remotely. Defaults to False.

**Returns:**

- <code>GeospatialPartitionJobStepConfig</code> – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

**Examples:**

For example, to ingest the California Census dataset for the year 2022:

```py
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="s3://fused-sample/census/ca_bg_2022/main/",
    output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
    explode_geometries=True,
    partitioning_maximum_per_file=2000,
    partitioning_maximum_per_chunk=200,
).execute()
```

---

#### `job.run_batch`

```python showLineNumbers
def run_batch(output_table: Optional[str] = ...,
    instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
    *,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: List[str] | None = None,
    image_name: Optional[str] = None,
    ignore_no_udf: bool = False,
    ignore_no_output: bool = False,
    validate_imports: Optional[bool] = None,
    validate_inputs: bool = True,
    overwrite: Optional[bool] = None) -> RunResponse
```

Begin execution of the ingestion job by calling `run_batch` on the job object.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

#### Monitor and manage job

Calling `run_batch` returns a `RunResponse` object with helper methods.

```python showLineNumbers
# Declare ingest job
job = fused.ingest(
  input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
  output="s3://fused-sample/census/ca_bg_2022/main/"
)

# Start ingest job
job_id = job.run_batch()
```

Fetch the job status.

```python showLineNumbers
job_id.get_status()
```

Fetch and print the job's logs.

```python showLineNumbers
job_id.print_logs()
```

Determine the job's execution time.

```python showLineNumbers
job_id.get_exec_time()
```

Continuously print the job's logs.

```python showLineNumbers
job_id.tail_logs()
```

Cancel the job.

```python showLineNumbers
job_id.cancel()
```

---

#### `job.run_remote`

Alias of `job.run_batch` for backwards compatibility. See `job.run_batch` above
for details.

---

## fused.ingest_nongeospatial

```python
ingest_nongeospatial() -> NonGeospatialPartitionJobStepConfig
```

Ingest a dataset into the Fused partitioned format.

**Parameters:**

- **input** (<code>str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame</code>) – A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- **output** (<code>str | None</code>) – Location on S3 to write the `main` table to.
- **output_metadata** (<code>str | None</code>) – Location on S3 to write the `fused` table to.
- **partition_col** (<code>str | None</code>) – Partition along this column for nongeospatial datasets.
- **partitioning_maximum_per_file** (<code>int</code>) – Maximum number of items to store in a single file. Defaults to 2,500,000.
- **partitioning_maximum_per_chunk** (<code>int</code>) – Maximum number of items to store in a single file. Defaults to 65,000.

**Returns:**

- <code>NonGeospatialPartitionJobStepConfig</code> – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

**Examples:**

```py
job = fused.ingest_nongeospatial(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).execute()
```

---

## fused.file_path

```python
file_path() -> str
```

Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF.
It takes a relative file_path, creates the corresponding directory structure,
and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files,
such as when writing intermediary files to disk when processing large datasets.
`file_path` ensures that necessary directories exist.
The directory is kept for 12h.

**Parameters:**

- **file_path** (<code>str</code>) – The relative file path to locate.
- **mkdir** (<code>bool</code>) – If True, create the directory if it doesn't already exist. Defaults to True.
- **storage** (<code>StorageStr</code>) – Set where the cache data is stored. Supported values are "auto", "mount" and "local". Auto will
  automatically select the storage location defined in options (mount if it exists, otherwise local) and
  ensures that it exists and is writable. Mount gets shared across executions where local will only be shared
  within the same execution.

**Returns:**

- <code>str</code> – The located file path.

---

## fused.get_chunks_metadata

```python
get_chunks_metadata() -> gpd.GeoDataFrame
```

Returns a GeoDataFrame with each chunk in the table as a row.

**Parameters:**

- **url** (<code>str</code>) – URL of the table.

---

## fused.get_chunk_from_table

```python
get_chunk_from_table() -> gpd.GeoDataFrame
```

Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

**Parameters:**

- **url** (<code>str</code>) – URL of the table.
- **file_id** (<code>Union[str, int, None]</code>) – File ID to read.
- **chunk_id** (<code>Optional[int]</code>) – Chunk ID to read.

**Other Parameters:**

- **columns** (<code>Optional\[Iterable\[str\]\]</code>) – Read only the specified columns.

---

================================================================================


---

Generated automatically from Fused Python SDK documentation. Last updated: 2025-10-13
Total pages: 11
