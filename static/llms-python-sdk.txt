# Fused Python SDK Documentation

> Complete reference for the Fused Python SDK - a Python library for creating and running User Defined Functions (UDFs) that can be executed via HTTPS requests.

## Python SDK Reference

### fused.api

## Module Functions

The following functions can be called directly from the `fused.api` module:

fused.api.function_name()

`whoami()` - Returns information on the currently logged in user

`delete() -> bool` - Delete the files at the path.
Params:

- path (str)

- max_deletion_depth (int | Literal['unlimited'])

`list() -> list[str] | list[ListDetails]` - List the files at the path.
Params:

- path (str)

- details (bool)

Returns: list\[str\] | list\[ListDetails\]

`get() -> bytes` - Download the contents at the path to memory.
Params:

- path (str)

Returns: bytes

`download() -> None` - Download the contents at the path to disk.
Params:

- path (str)

- local_path (str | Path)

`upload() -> None` - Upload local file to S3.
Params:

- local_path (str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame)

- remote_path (str)

- timeout (float | None)

`sign_url() -> str` - Create a signed URL to access the path.

This function may not check that the file represented by the path exists.
Params:
- path (str)

Returns: str

`sign_url_prefix() -> dict[str, str]` - Create signed URLs to access all blobs under the path.
Params:

- path (str)

Returns: dict\[str, str\]

`get_udfs() -> dict` - Fetches a list of UDFs.
Params:

- n (int | None)

- skip (int)

- by (Literal['name', 'id', 'slug'])

- whose (Literal['self', 'public', 'community', 'team'])

Returns: dict

`job_get_logs() -> list[Any]` - Fetch logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

Returns: list\[Any\]

`job_print_logs() -> None` - Fetch and print logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

- file (IO | None)

Returns: None

`job_tail_logs()` - Continuously print logs for a job
Params:

- job (CoerceableToJobId)

- refresh_seconds (float)

- sample_logs (bool)

- timeout (float | None)

`job_get_status() -> RunResponse` - Fetch the status of a running job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

`job_cancel() -> RunResponse` - Cancel an existing job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

`job_get_exec_time() -> timedelta` - Determine the execution time of this job, using the logs.

Returns: timedelta

`job_wait_for_job() -> RunResponse` - Block the Python kernel until this job has finished
Params:

- poll_interval_seconds (float)

- timeout (float | None)

## FusedAPI Class Methods

The following methods require creating a `FusedAPI` instance first:

from fused.api import FusedAPI
api = FusedAPI()
api.method_name()

`FusedAPI()` - API for running jobs in the Fused service.

Create a FusedAPI instance.

Other Params:

- base_url (str | None)

- shared_udf_base_url (str | None)

- set_global_api (bool)

- credentials_needed (bool)

### create_udf_access_token

`create_udf_access_token() -> UdfAccessToken` - Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.
Params:
- udf_email_or_name_or_id (str | None)

- udf_name (str | None)

Other Params:

- udf_email (str | None)

- udf_id (str | None)

- client_id (str | Ellipsis | None)

- cache (bool)

- metadata_json (dict\[str, Any\] | None)

- enabled (bool)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.create_udf_access_token()`

### upload

`upload() -> None` - Upload a binary blob to a cloud location

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.upload()`

### start_job

`start_job() -> RunResponse` - Execute an operation
Params:

- config (JobConfig | JobStepConfig)

Other Params:

- instance_type (WHITELISTED_INSTANCE_TYPES | None)

- region (str | None)

- disk_size_gb (int | None)

- additional_env (Sequence\[str\] | None)

- image_name (str | None)

- send_status_email (bool | None)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.start_job()`

### get_jobs

`get_jobs() -> Jobs` - Get the job history.
Params:

- n (int)

Other Params:

- skip (int)

- per_request (int)

- max_requests (int | None)

Returns: Jobs

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_jobs()`

### get_status

`get_status() -> RunResponse` - Fetch the status of a running job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_status()`

### get_logs

`get_logs() -> list[Any]` - Fetch logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

Returns: list\[Any\]

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_logs()`

### tail_logs

`tail_logs()` - Continuously print logs for a job
Params:

- job (CoerceableToJobId)

- refresh_seconds (float)

- sample_logs (bool)

- timeout (float | None)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.tail_logs()`

### wait_for_job

`wait_for_job() -> RunResponse` - Block the Python kernel until the given job has finished
Params:

- job (CoerceableToJobId)

- poll_interval_seconds (float)

- timeout (float | None)

### cancel_job

`cancel_job() -> RunResponse` - Cancel an existing job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.cancel_job()`

### auth_token

`auth_token() -> str` - Returns the current user's Fused environment (team) auth token

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.auth_token()`

---

---

### fused.core

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.
Params:
- email (str)
- id (Optional[str])
- x (int)
- y (int)
- z (int)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.
Params:
- token (str)
- id (Optional[str])
- x (int)
- y (int)
- z (int)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.
Params:
- email (str)
- id (Optional[str])
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.
Params:
- token (str)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- `params` - Additional keyword arguments for the operation execution.

---

### fused.h3

`run_ingest_raster_to_h3()` - Run the raster to H3 ingestion process.

This process involves multiple steps:

- extract pixels values and assign to H3 cells in chunks (extract step)
- combine the chunks per partition (file) and prepare metadata (partition step)
- create the metadata `_sample` file and overviews files
Params:
- src_path (str, list)

- output_path (str)

- metrics (str or list of str)

- res (int)

- k_ring (int)

- res_offset (int)

- file_res (int)

- chunk_res (int)

- overview_res (list of int)

- overview_chunk_res (int or list of int)

- max_rows_per_chunk (int)

- include_source_url (bool)

- target_chunk_size (int)

- debug_mode (bool)

- remove_tmp_files (bool)

- tmp_path (str)

- overwrite (bool)

- steps (list of str)

- extract_kwargs (dict)

- partition_kwargs (dict)

- overview_kwargs (dict)

The extract, partition and overview steps are run in parallel using
`fused.submit()`. By default, the function will first attempt to run this using
"realtime" instances, and retry any failed runs using "large" instances.

You can override this behavior by specifying the `engine`, `instance_type`,
`max_workers`, `n_processes_per_worker`, etc parameters as additional
keyword arguments to this function (`kwargs`). If you want to specify
those per step, use `extract_kwargs`, `partition_kwargs`, and `overview_kwargs`.
For example, to run everything locally on the same machine where this
function runs, use:

To run the extract step on realtime and the partition step on medium
instance, you could do:

In contrast to `fused.submit` itself, the ingestion sets `n_processes_per_worker=1`
by default to avoid out-of-memory issues on batch instances. You can
increase this if you know the instance has enough memory to process multiple
chunks in parallel.

---

---

### fused.ingest

## `fused.ingest`

Ingest a dataset into the Fused partitioned format.
Params:
- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

          This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        Returns:

Configuration object describing the ingestion process. Call `.run_batch` on this object to start a job.

---
#### `job.run_batch`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_batch` on this object to start the ingestion job.

Begin job execution.
Params:
- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_batch()` returns a `RunResponse` object which has the following methods:

#### `get_status`

Fetch the status of this job

#### `print_logs`

Fetch and print logs
Params:
- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

---
#### `get_exec_time`

Determine the execution time of this job, using the logs.

---
#### `tail_logs`

Continuously print logs
Params:
- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

Cancel this job

Returns:

  A new job object.

---

### JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### cancel

`cancel()` - Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

### retry

`retry()` - Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

### total_time

`total_time() -> timedelta` - Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

### times

`times() -> list[timedelta | None]` - Time taken for each task.

Incomplete tasks will be reported as None.

### done

`done() -> bool` - True if all tasks have finished, regardless of success or failure.

### all_succeeded

`all_succeeded() -> bool` - True if all tasks finished with success

### any_failed

`any_failed() -> bool` - True if any task finished with an error

### any_succeeded

`any_succeeded() -> bool` - True if any task finished with success

### arg_df

`arg_df()` - The arguments passed to runs as a DataFrame

### status

`status()` - Return a Series indexed by status of task counts

### wait

`wait()` - Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

### tail

`tail()` - Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

### results

`results() -> list[Any]` - Retrieve all results of the job.

Results are ordered by the order of the args list.

### results_now

`results_now() -> dict[int, Any]` - Retrieve the results that are currently done.

Results are indexed by position in the args list.

### df

`df()` - Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

### get_status_df

### get_results_df

### errors

`errors() -> dict[int, Exception]` - Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

### first_error

`first_error() -> Exception | None` - Retrieve the first (by order of arguments) error result, or None.

### logs

`logs() -> list[str | None]` - Logs for each task.

Incomplete tasks will be reported as None.

### first_log

`first_log() -> str | None` - Retrieve the first (by order of arguments) logs, or None.

### success

`success() -> dict[int, Any]` - Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

### pending

`pending() -> dict[int, Any]` - Retrieve the arguments that are currently pending and not yet submitted.

### running

`running() -> dict[int, Any]` - Retrieve the results that are currently running.

### cancelled

`cancelled() -> dict[int, Any]` - Retrieve the arguments that were cancelled and not run.

### collect

`collect()` - Collect all results into a DataFrame

---

---

### fused.options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

### __dir__

`__dir__() -> List[str]` - ### base_url

Fused API endpoint

### shared_udf_base_url

Shared UDF API endpoint

### auth

Options for authentication.

### show

Options for object reprs and how data are shown for debugging.

### max_workers

Maximum number of threads, when multithreading requests

### run_timeout

Request timeout for UDF run requests to the Fused service

### request_timeout

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### metadata_request_timeout

Request timeout for file metadata requests (e.g., /file-metadata endpoint).
These requests may involve processing large parquet files and can take longer.

### request_max_retries

Maximum number of retries for API requests

### request_retry_base_delay

Base delay before retrying a API request in seconds

### realtime_client_id

Client ID for realtime service.

### max_recursion_factor

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

Automatically prompt the user to login when importing Fused.

### no_login

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

The base directory for storing cached results.

### data_directory

The base directory for storing data results. Note: if storage type is 'object', then this path is relative to
fd_prefix.

### temp_directory

The base directory for storing temporary files.

### never_import

Never import UDF code when loading UDFs.

### gcs_secret

Secret name for GCS credentials.

### gcs_filename

Filename for saving temporary GCS credentials to locally or in rt2 instance

### gcp_project_name

Project name for GCS to use for GCS operations.

### logging

Control logging for Fused

### verbose_udf_runs

Whether to print logs from UDF runs by default

### default_run_headers

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

Default transfer type for vector (tabular) data

### default_dtype_out_raster

Default transfer type for raster data

### default_dtype_out_simple

Default transfer type for simple Python data (bool, int, float, str, list)

### fd_prefix

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

Whether to print logs from cache decorated functions by default

### local_engine_cache

Enable UDF cache with local engine

### default_send_status_email

Whether to send a status email to the user when a job is complete.

### cache_storage

Specify the default cache storage type

### use_process_pool_for_local_submit

Use ProcessPoolExecutor instead of ThreadPoolExecutor for local engine submit.
This avoids GDAL thread-safety issues and Python GIL limitations, but has higher
memory overhead and slower startup time.

### row_group_batch_size

Target size in bytes for combining adjacent row group downloads.
Adjacent row groups from the same file will be combined into single downloads
until this threshold is reached. Default is 32KB (32768 bytes), which is
optimized for S3 performance.

### base_web_url

### save

`save()` - Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

---

### Udf

The `Udf` class is the object you get when defining a UDF with the
`@fused.udf` decorator, or when loading
a saved UDF with `fused.load()`.

### to_directory

`to_directory()` - Write the UDF to disk as a directory (folder).
Params:

- where (str | Path | None)

Other Params:

- overwrite (bool)

### to_file

`to_file()` - Write the UDF to disk or the specified file-like object.

The UDF will be written as a Zip file.
Params:
- where (str | Path | BinaryIO)

Other Params:

- overwrite (bool)

---

---

### Authentication

Authenticate the Fused Python SDK in a Python Notebook.

Make sure to have the `fused` package installed.

To use Fused you need to authenticate. The following will store a credentials file in `~/.fused/credentials`:

</Tabs>

Log out the current user. This deletes the credentials saved to disk and resets the global Fused API.

## Get Bearer (Access) token

Get the account's Bearer (sometimes referred to as Access) token.

This can be helpful when calling UDFs via HTTPS requests outside of the Fused Python SDK and Workbench to authenticate with the Fused API.

Do not share your Bearer token with anyone. These allow to impersonate your account and should be treated as such.

:::

---

### Batch jobs

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_batch()` to make kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

---

### index

# Python SDK

> The latest version of `fused-py` is <FusedVersionLive />.

    Installing `fused` is required if you're running `fused` on your end (locally or in a development environment). If you're working in Workbench UDF Builder or App Builder `fused` is already installed for you.

1. Set up a Python environment:

We're using `venv` but you could use `conda` or any other environment manager in Python. 

2. Install the `fused` package:

You can only install the base package, though we recommend still adding the optional dependencies:
Or install with optional dependencies:
</details>

### Authenticate

The first time you use Fused you'll need to authenticate.

Follow the URL in your browser to authenticate.

Some basic examples of how to use `fused` to run UDFs:

#### Hello World UDF

#### Simple data UDF

###

---

### Top-Level Functions

## @fused.udf

`udf() -> Callable[..., Udf]` - A decorator that transforms a function into a Fused UDF.
Params:

- cache_max_age (Optional[str])

- instance_type (Optional[str])

- disk_size_gb (Optional[int])

- default_parameters (Optional\[Dict[str, Any]\])

- headers (Optional\[Sequence\[Union[str, Header]\]\])

    Defaults to None for no headers.

Returns: Callable\..., [Udf\]
- Callable\..., [Udf\] â€“ within GeoPandas workflows to apply the defined operation on geospatial data.

## @fused.cache

`cache() -> Callable[..., Any]` - Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function
to cache its return values. The cache behavior can be customized through
keyword arguments.
Params:
- func (Callable)

- cache_max_age (str | int)

- cache_folder_path (str)

- concurrent_lock_timeout (str | int)

- cache_reset (bool | None)

- cache_storage (StorageStr | None)

- cache_key_exclude (Iterable[str])

- cache_verbose (bool | None)

Returns:
Callable: A decorator that, when applied to a function, caches its
return values according to the specified keyword arguments.

If the output of a cached function changes, for example if remote data is modified,
it can be reset by running the function with the `cache_reset` keyword argument. Afterward,
the argument can be cleared.

`load() -> AnyBaseUdf` - Loads a UDF from various sources including GitHub URLs,
and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused
platform-specific identifier composed of an email and UDF name. It intelligently
determines the source type based on the format of the input and retrieves the UDF
accordingly.
Params:
- url_or_udf (Union[str, Path])

- cache_key (Any)

- import_globals (bool)

Returns: AnyBaseUdf (AnyBaseUdf)

`submit() -> JobPool | ResultType | pd.DataFrame` - Executes a user-defined function (UDF) multiple times for a list of input
parameters, and return immediately a "lazy" JobPool object allowing
to inspect the jobs and wait on the results.

Each individual UDF run will be cached following the standard caching logic as with `fused.run()`
and the specified `cache_max_age`. Additionally, when `collect=True` (the default), the collected results
are cached locally for the duration of `cache_max_age` or 12h by default.

See `fused.run` for more details on the UDF execution.
Params:
- udf (AnyBaseUdf | FunctionType | str)

- arg_list (list | pd.DataFrame)

- engine (Literal['remote', 'local'] | None)

- instance_type (InstanceType | None)

- max_workers (int | None)

- n_processes_per_worker (int | None)

- max_retry (int)

- debug_mode (bool)

- collect (bool)

- execution_type (ExecutionType)

- cache_max_age (str | None)

- cache (bool)

- ignore_exceptions (bool)

- flatten (bool)

- \*\*kwargs â€“ Additional (constant) keyword arguments to pass to the UDF.

Returns: JobPool | ResultType | DataFrame

Run a UDF multiple times for the values 0 to 9 passed to as the first
positional argument of the UDF:

Using async batch type:

Being explicit about the parameter name:

Get the pool of ongoing tasks:

`download() -> str` - Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

ðŸ’¡ Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

ðŸ’¡ Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.
Params:
- url (str)

- file_path (str)

- storage (StorageStr)

Returns: str

`ingest() -> GeospatialPartitionJobStepConfig` - Ingest a dataset into the Fused partitioned format.
Params:

- input (str | Path | Sequence[str | Path] | gpd.GeoDataFrame)

- output (str | None)

- output_metadata (str | None)

- schema (Schema | None)

- file_suffix (str | None)

- load_columns (Sequence[str] | None)

- remove_cols (Sequence[str] | None)

- explode_geometries (bool)

- drop_out_of_bounds (bool | None)

- partitioning_method (Literal['area', 'length', 'coords', 'rows'])

  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.

- partitioning_maximum_per_file (int | float | None)

- partitioning_maximum_per_chunk (int | float | None)

- partitioning_max_width_ratio (int | float)

- partitioning_max_height_ratio (int | float)

- partitioning_force_utm (Literal['file', 'chunk', None])

- partitioning_split_method (Literal['mean', 'median'])

  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.

- subdivide_method (Literal['area', None])

- subdivide_start (float | None)

- subdivide_stop (float | None)

- split_identical_centroids (bool)

- target_num_chunks (int)

- lonlat_cols (tuple[str, str] | None)

  If your point columns are named `"x"` and `"y"`, then pass:

    This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- gdal_config (GDALOpenConfig | dict[str, Any] | None)

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

  - overwrite (bool)

- as_udf (bool)

Returns: GeospatialPartitionJobStepConfig

#### `job.run_batch`

Begin execution of the ingestion job by calling `run_batch` on the job object.
Params:
- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

#### Monitor and manage job

Calling `run_batch` returns a `RunResponse` object with helper methods.

Fetch the job status.

Fetch and print the job's logs.

Determine the job's execution time.

Continuously print the job's logs.

Cancel the job.

#### `job.run_remote`

Alias of `job.run_batch` for backwards compatibility. See `job.run_batch` above
for details.

`ingest_nongeospatial() -> NonGeospatialPartitionJobStepConfig` - Ingest a dataset into the Fused partitioned format.
Params:

- input (str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame)

- output (str | None)

- output_metadata (str | None)

- partition_col (str | None)

- partitioning_maximum_per_file (int)

- partitioning_maximum_per_chunk (int)

Returns: NonGeospatialPartitionJobStepConfig

`file_path() -> str` - Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF.
It takes a relative file_path, creates the corresponding directory structure,
and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files,
such as when writing intermediary files to disk when processing large datasets.
`file_path` ensures that necessary directories exist.
The directory is kept for 12h.
Params:
- file_path (str)

- mkdir (bool)

- storage (StorageStr)

Returns: str

`get_chunks_metadata() -> gpd.GeoDataFrame` - Returns a GeoDataFrame with each chunk in the table as a row.
Params:

- url (str)

`get_chunk_from_table() -> gpd.GeoDataFrame` - Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.
Params:
- url (str)

- file_id (Union[str, int, None])

- chunk_id (Optional[int])

- columns (Optional\[Iterable[str]\])

---

---


---

Generated automatically from Fused Python SDK documentation. Last updated: 2026-01-16
Total pages: 11
