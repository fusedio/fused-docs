# Fused Python SDK Documentation

> Complete reference for the Fused Python SDK - a Python library for creating and running User Defined Functions (UDFs) that can be executed via HTTPS requests.

## Python SDK Reference

### fused.api

## Module Functions

The following functions can be called directly from the `fused.api` module:

fused.api.function_name()

`whoami()` - Returns information on the currently logged in user

`delete() -> bool` - Delete the files at the path.
Params:

- path (str)

- max_deletion_depth (int | Literal['unlimited'])

`list() -> list[str] | list[ListDetails]` - List the files at the path.
Params:

- path (str)

- details (bool)

Returns: list[str] | list[ListDetails]

`get() -> bytes` - Download the contents at the path to memory.
Params:

- path (str)

Returns: `bytes`

`download() -> None` - Download the contents at the path to disk.
Params:

- path (str)

- local_path (str | Path)

`upload() -> None` - Upload local file to S3.
Params:

- local_path (str | Path | bytes | BinaryIO | pd.DataFrame | gpd.GeoDataFrame)

- remote_path (str)

- timeout (float | None)

`sign_url() -> str` - Create a signed URL to access the path.

This function may not check that the file represented by the path exists.
Params:
- path (str)

Returns: str

`sign_url_prefix() -> dict[str, str]` - Create signed URLs to access all blobs under the path.
Params:

- path (str)

Returns: dict\[str, str\]

`get_udfs() -> dict` - Fetches a list of UDFs.
Params:

- n (int | None)

- skip (int)

- by (Literal['name', 'id', 'slug'])

- whose (Literal['self', 'public', 'community', 'team'])

Returns: dict

`job_get_logs() -> list[Any]` - Fetch logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

Returns: list[Any]

`job_print_logs() -> None` - Fetch and print logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

- file (IO | None)

Returns: None

`job_tail_logs()` - Continuously print logs for a job
Params:

- job (CoerceableToJobId)

- refresh_seconds (float)

- sample_logs (bool)

- timeout (float | None)

`job_get_status() -> RunResponse` - Fetch the status of a running job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

`job_cancel() -> RunResponse` - Cancel an existing job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

`job_get_exec_time() -> timedelta` - Determine the execution time of this job, using the logs.

Returns: timedelta

`job_wait_for_job() -> RunResponse` - Block the Python kernel until this job has finished
Params:

- poll_interval_seconds (float)

- timeout (float | None)

`access_token() -> str` - Get an access token for the Fused service.

Returns the Bearer token for the authenticated user. Use this for authenticating API requests outside of the Fused SDK.

Returns: `str`

`logout()` - Log out the current user.

Deletes the credentials saved to disk and resets the global Fused API.

`team_info() -> dict` - Get information about the current user's team.

Returns: `dict`

`schedule_udf() -> CronJob` - Schedule a UDF to run on a cron schedule.
Params:

- udf (`BaseUdf | str`)

- minute (`list[int] | int`)

- hour (`list[int] | int`)

- day_of_month (`list[int] | int | None`)

- month (`list[int] | int | None`)

- day_of_week (`list[int] | int | None`)

- udf_args (`dict[str, Any] | None`)

- enabled (`bool`)

Returns: `CronJob`

Example:

`schedule_list()` - List all cron jobs scheduled for the current user.

Returns: List of `CronJob` objects.

`get_apps() -> dict` - Get apps for the current user or public apps.
Params:

- n (`int | None`)

- skip (`int`)

- by (`Literal['name', 'id', 'slug']`)

- whose (`Literal['self', 'public']`)

Returns: `dict`

`resolve() -> str` - Resolve a path from `fd://` to the full S3 URI.
Params:

- path (`str`)

Returns: `str`

Example:

`enable_gcs()` - Save GCS credentials from AWS secret manager into a temporary local file and set its path to the environment variable.

Use this to enable Google Cloud Storage access in your UDFs.

`job_get_results() -> list[Any]` - Get the results of a completed job.
Params:

- job (`CoerceableToJobId`)

Returns: `list[Any]`

`job_wait_for_results() -> list[UdfEvaluationResult]` - Block until a job completes and return its results.

Combines `job_wait_for_job` and `job_get_results` into a single call.
Params:
- job (`CoerceableToJobId`)

- poll_interval_seconds (`float`)

- timeout (`float | None`)

Returns: `list[UdfEvaluationResult]`

## FusedAPI Class Methods

The following methods require creating a `FusedAPI` instance first:

from fused.api import FusedAPI
api = FusedAPI()
api.method_name()

`FusedAPI()` - API for running jobs in the Fused service.

Create a FusedAPI instance.

Other Params:

- base_url (str | None)

- shared_udf_base_url (str | None)

- set_global_api (bool)

- credentials_needed (bool)

### create_udf_access_token

`create_udf_access_token() -> UdfAccessToken` - Create a token for running a UDF. The token allows anyone who has it to run
the UDF, with the parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.
Params:
- udf_email_or_name_or_id (str | None)

- udf_name (str | None)

Other Params:

- udf_email (str | None)

- udf_id (str | None)

- client_id (str | Ellipsis | None)

- cache (bool)

- metadata_json (dict\[str, Any\] | None)

- enabled (bool)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.create_udf_access_token()`

### upload

`upload() -> None` - Upload a binary blob to a cloud location

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.upload()`

### start_job

`start_job() -> RunResponse` - Execute an operation
Params:

- config (JobConfig | JobStepConfig)

Other Params:

- instance_type (WHITELISTED_INSTANCE_TYPES | None)

- region (str | None)

- disk_size_gb (int | None)

- additional_env (Sequence\[str\] | None)

- image_name (str | None)

- send_status_email (bool | None)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.start_job()`

### get_jobs

`get_jobs() -> Jobs` - Get the job history.
Params:

- n (int)

Other Params:

- skip (int)

- per_request (int)

- max_requests (int | None)

Returns: Jobs

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_jobs()`

### get_status

`get_status() -> RunResponse` - Fetch the status of a running job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_status()`

### get_logs

`get_logs() -> list[Any]` - Fetch logs for a job
Params:

- job (CoerceableToJobId)

- since_ms (int | None)

Returns: list\[Any\]

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.get_logs()`

### tail_logs

`tail_logs()` - Continuously print logs for a job
Params:

- job (CoerceableToJobId)

- refresh_seconds (float)

- sample_logs (bool)

- timeout (float | None)

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.tail_logs()`

### wait_for_job

`wait_for_job() -> RunResponse` - Block the Python kernel until the given job has finished
Params:

- job (CoerceableToJobId)

- poll_interval_seconds (float)

- timeout (float | None)

### cancel_job

`cancel_job() -> RunResponse` - Cancel an existing job
Params:

- job (CoerceableToJobId)

Returns: RunResponse

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.cancel_job()`

### auth_token

`auth_token() -> str` - Returns the current user's Fused environment (team) auth token

Usage: `from fused.api import FusedAPI; api = FusedAPI(); api.auth_token()`

---

---

### fused.core

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.
Params:
- email (str)
- id (Optional[str])
- x (int)
- y (int)
- z (int)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.
Params:
- token (str)
- id (Optional[str])
- x (int)
- y (int)
- z (int)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.
Params:
- email (str)
- id (Optional[str])
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- _client_id (Optional[str])
- `params` - Additional keyword arguments for the UDF execution.

---
Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.
Params:
- token (str)
- _dtype_out_vector (str)
- _dtype_out_raster (str)
- `params` - Additional keyword arguments for the operation execution.

---

### @fused.cache

# @fused.cache

`cache() -> Callable[..., Any]` - Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function to cache its return values. The cache behavior can be customized through keyword arguments.

- func (`Callable`)

- cache_max_age (`str | int`)

- cache_folder_path (`str`)

- concurrent_lock_timeout (`str | int`)

- cache_reset (`bool | None`)

- cache_storage (`StorageStr | None`)

- cache_key_exclude (`Iterable[str]`)

- cache_verbose (`bool | None`)

`Callable` – A decorator that, when applied to a function, caches its return values according to the specified keyword arguments.

Use the `@cache` decorator to cache the return value of a function in a custom path.

If the output of a cached function changes, for example if remote data is modified, it can be reset by running the function with the `cache_reset` keyword argument. Afterward, the argument can be cleared.

---

### fused.context

# context

Access runtime context information from within a UDF. These functions return information about the current execution environment.

### get_headers

`get_headers() -> dict[str, str]` - Get the request headers that were passed to the UDF.

Returns a dictionary of header names to values for all headers passed in the HTTP request.

### get_header

`get_header() -> str | None` - Get a specific request header by name (case-insensitive).
Params:

Returns: The header value, or `None` if not found.

### get_recursion_factor

`get_recursion_factor() -> int | None` - Get the recursion factor from the current context.

Returns the recursion factor when running in a tiled context, or `None` otherwise.

### get_realtime_client_id

`get_realtime_client_id() -> str | None` - Get the realtime client ID from the current context.

Returns the client ID when running in realtime mode, or `None` otherwise.

### get_user_email

`get_user_email() -> str | None` - Get the user email from the current context.

Returns the email of the authenticated user running the UDF, or `None` if not available.

### in_realtime

`in_realtime() -> bool` - Return `True` if the context is in a realtime job.

Use this to conditionally execute code based on execution mode.

### in_batch

`in_batch() -> bool` - Return `True` if the context is in a batch job.

Use this to conditionally execute code based on execution mode.

### get_global_context

`get_global_context() -> ExecutionContextProtocol | None` - Get the global execution context object.

Returns the full context object for advanced use cases.

---

### fused.download

# download

`download() -> str` - Download a file.

May be called from multiple processes with the same inputs to get the same result.

Fused runs UDFs from top to bottom each time code changes. This means objects in the UDF are recreated each time, which can slow down a UDF that downloads files from a remote server.

Downloaded files are written to a mounted volume shared across all UDFs in an organization. This means that a file downloaded by one UDF can be read by other UDFs.

Fused addresses the latency of downloading files with the download utility function. It stores files in the mounted filesystem so they only download the first time.

Because a Tile UDF runs multiple chunks in parallel, the download function sets a signal lock during the first download attempt, to ensure the download happens only once.

- url (`str`)

- file_path (`str`)

- storage (`StorageStr`)

- `str` – The function downloads the file only on the first execution, and returns the file path.

---

### fused.file_path

# file_path

`file_path() -> str` - Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF. It takes a relative file_path, creates the corresponding directory structure, and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files, such as when writing intermediary files to disk when processing large datasets. `file_path` ensures that necessary directories exist. The directory is kept for 12h.

- file_path (`str`)

- mkdir (`bool`)

- storage (`StorageStr`)

- `str` – The located file path.

---

### fused.find_dataset

# find_dataset

Find the dataset that contains a specific location URL.

`fused.find_dataset() -> dict[str, Any]` - Uses hierarchical prefix matching: if the exact path isn't registered, searches progressively shorter prefixes to find the containing dataset.

- location (`str`)

- base_url (`str | None`)

- `dict` – Dataset dict with keys: `id`, `location`, `description`, `storage_type`, `owner`, `public`, `created_at`, `updated_at`, etc.

- `requests.HTTPError` – If dataset not found (404) or request fails.

- `fused.register_dataset` – Register a new dataset

---

### fused.get_chunk_from_table

# get_chunk_from_table

`get_chunk_from_table() -> gpd.GeoDataFrame` - Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

- url (`str`)

- file_id (`Union[str, int, None]`)

- chunk_id (`Optional[int]`)

- columns (`Optional[Iterable[str]]`)

`gpd.GeoDataFrame` – The chunk data as a GeoDataFrame.

---

### fused.get_chunks_metadata

# get_chunks_metadata

`get_chunks_metadata() -> gpd.GeoDataFrame` - Returns a GeoDataFrame with each chunk in the table as a row.

- url (`str`)

`gpd.GeoDataFrame` – A GeoDataFrame where each row represents a chunk in the table.

---

### fused.ingest_nongeospatial

# ingest_nongeospatial

`ingest_nongeospatial() -> NonGeospatialPartitionJobStepConfig` - Ingest a non-geospatial dataset into the Fused partitioned format.

- input (`str | Path | Sequence[str, Path] | pd.DataFrame | gpd.GeoDataFrame`)

- output (`str | None`)

- output_metadata (`str | None`)

- partition_col (`str | None`)

- partitioning_maximum_per_file (`int`)

- partitioning_maximum_per_chunk (`int`)

`NonGeospatialPartitionJobStepConfig` – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

---

### fused.ingest

# ingest

`ingest() -> GeospatialPartitionJobStepConfig` - Ingest a dataset into the Fused partitioned format.

- input (`str | Path | Sequence[str | Path] | gpd.GeoDataFrame`)

- output (`str | None`)

- output_metadata (`str | None`)

- schema (`Schema | None`)

- file_suffix (`str | None`)

- load_columns (`Sequence[str] | None`)

- remove_cols (`Sequence[str] | None`)

- explode_geometries (`bool`)

- drop_out_of_bounds (`bool | None`)

- partitioning_method (`Literal['area', 'length', 'coords', 'rows']`)

- partitioning_maximum_per_file (`int | float | None`)

- partitioning_maximum_per_chunk (`int | float | None`)

- target_num_chunks (`int`)

- lonlat_cols (`tuple[str, str] | None`)

- gdal_config (`GDALOpenConfig | dict[str, Any] | None`)

- overwrite (`bool`)

- as_udf (`bool`)

`GeospatialPartitionJobStepConfig` – Configuration object describing the ingestion process. Call `.execute` on this object to start a job.

Begin execution of the ingestion job by calling `run_batch` on the job object.

### Parameters

- `output_table` – The name of the table to write to. Defaults to None.
- `instance_type` – The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` – The AWS region in which to run. Defaults to None.
- `disk_size_gb` – The disk size to specify for the job. Defaults to None.
- `additional_env` – Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` – Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` – Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` – Ignore validation errors about not specifying output location. Defaults to False.

Calling `run_batch` returns a `RunResponse` object with helper methods.

Alias of `job.run_batch` for backwards compatibility. See `job.run_batch` above for details.

---

### fused.load_async

# load_async

Asynchronously load a UDF from various sources.

This is the async-optimized version of `fused.load()` that uses async HTTP requests to avoid blocking the event loop during UDF metadata fetching.

- url_or_udf (`str | Path`)

- cache_key (`Any`)

- import_globals (`bool`)

- `Udf` – An instance of the loaded UDF.

- `fused.load` – Synchronous version
- `fused.run_async` – Run a UDF asynchronously

---

### fused.load

# load

`load() -> AnyBaseUdf` - Loads a UDF from various sources including GitHub URLs and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused platform-specific identifier composed of an email and UDF name. It intelligently determines the source type based on the format of the input and retrieves the UDF accordingly.

- url_or_udf (`Union[str, Path]`)

- cache_key (`Any`)

- import_globals (`bool`)

- AnyBaseUdf – An instance of the loaded UDF.

- `ValueError` – If the URL or Fused platform-specific identifier format is incorrect or cannot be parsed.
- `Exception` – For errors related to network issues, file access permissions, or other unforeseen errors during the loading process.

Load a UDF from a GitHub URL:

Load a UDF using a Fused platform-specific identifier:

---

### fused.register_dataset

# register_dataset

Register a dataset for indexed queries.

`fused.register_dataset() -> dict[str, Any]` - This function registers a directory in your file storage as a dataset, enabling fast geospatial queries using H3 indexing.

- dataset_path (`str`)

- base_url (`str | None`)

- `dict` – Dictionary with registration results:
  - `dataset_id`: ID of the created/updated dataset
  - `location`: Normalized URL of the dataset
  - `visit_status`: Status of the dataset visit (success/timeout/error)
  - `items_discovered`: Total number of items found
  - `new_items`: Number of new items added

- `requests.HTTPError` – If the API request fails.

- Regular users can use any storage paths they have access to
- Datasets are registered as private (only accessible to your team)
- Files are automatically queued for metadata extraction

- `fused.find_dataset` – Find a registered dataset

---

### fused.run_async

# run_async

Async version of `run()` that executes a UDF asynchronously.

This function provides the same functionality as `fused.run()` but with async execution.

- udf (`str | Udf | UdfJobStepConfig`)

- x, y, z (`int | None`) – Tile coordinates for tile-based UDF execution.
- engine (`Literal['remote', 'local'] | None`)

- instance_type (`str | None`)

- type (`Literal['tile', 'file'] | None`)

- max_retry (`int`)

- cache_max_age (`str | None`)

- cache (`bool`)

- parameters (`dict[str, Any] | None`)

- disk_size_gb (`int | None`)

- kw_parameters – Additional parameters to pass to the UDF.

The result of the UDF execution, which varies based on the UDF and execution path.

- `fused.run` – Synchronous version
- `fused.load_async` – Load a UDF asynchronously

---

### fused.run

# run

`run() -> Union[ResultType, Coroutine[ResultType, None, None]]` - Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote), with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and allows for both synchronous and asynchronous execution.

- udf (`str, Udf or UdfJobStepConfig`)

- x, y, z (`int`) – Tile coordinates for tile-based UDF execution.
- sync (`bool`)

- engine (`Optional[Literal['remote', 'local']]`)

- instance_type (`Optional[InstanceType]`)

- disk_size_gb (`Optional[int]`)

- type (`Optional[Literal['tile', 'file']]`)

- max_retry (`int`)

- cache_max_age (`Optional[str]`)

- cache (`bool`)

- parameters (`Optional[Dict[str, Any]]`)

- \*\*kw_parameters – Additional parameters to pass to the UDF.

- `ValueError` – If the UDF is not specified or is specified in more than one way.
- `TypeError` – If the first parameter is not of an expected type.

The result of the UDF execution, which varies based on the UDF and execution path.

Run a UDF saved in the Fused system:

`fused.run()` - Run a UDF saved in GitHub:

Run a UDF saved in a local directory:

This function dynamically determines the execution path and parameters based on the inputs. It is designed to be flexible and support various UDF execution scenarios.

---

### fused.secrets

# secrets

Access secrets stored in the Fused backend for the current kernel.

`fused.secrets` is a `SecretsManager` object that provides access to secrets stored in your Fused account. Secrets are accessed as attributes using dot notation.

Secrets can be managed in Workbench Settings under the "Secrets" tab, or via the API.

### Accessing secrets

- Secrets are stored encrypted in the Fused backend
- Secrets are only accessible within UDFs running on Fused infrastructure
- Secrets are scoped to your account/organization
- Never log or return secret values from UDFs

- Environment Variables for runtime configuration
- Cloud Storage for connecting to cloud providers

---

### fused.submit

# submit

`submit() -> JobPool | ResultType | pd.DataFrame` - Executes a user-defined function (UDF) multiple times for a list of input parameters, and returns immediately a "lazy" JobPool object allowing to inspect the jobs and wait on the results.

Each individual UDF run will be cached following the standard caching logic as with `fused.run()` and the specified `cache_max_age`. Additionally, when `collect=True` (the default), the collected results are cached locally for the duration of `cache_max_age` or 12h by default.

- udf (`AnyBaseUdf | FunctionType | str`)

- arg_list (`list | pd.DataFrame`)

- engine (`Literal['remote', 'local'] | None`)

- instance_type (`InstanceType | None`)

- max_workers (`int | None`)

- n_processes_per_worker (`int | None`)

- max_retry (`int`)

- debug_mode (`bool`)

- collect (`bool`)

- execution_type (`ExecutionType`)

- cache_max_age (`str | None`)

- cache (`bool`)

- ignore_exceptions (`bool`)

- flatten (`bool`)

- \*\*kwargs – Additional (constant) keyword arguments to pass to the UDF.

`JobPool | ResultType | pd.DataFrame` – JobPool, or DataFrame depending on execution_type and collect parameters.

Run a UDF multiple times for the values 0 to 9:

Using async batch type:

Being explicit about the parameter name:

Get the pool of ongoing tasks:

---

### fused.types

# types

Type annotations for UDF parameters. Use these to get proper type hints and enable Fused-specific behaviors.

### Bounds

A bounding box as a list of `[min_x, min_y, max_x, max_y]`. When used as a UDF parameter type hint, Fused automatically structures the bounds object based on how the UDF is called.

### Bbox [Legacy]

A bounding box as a Shapely Polygon geometry.

### Tile [Legacy]

A GeoDataFrame representing a tile. Alias for `TileGDF`.

### TileGDF [Legacy]

A GeoDataFrame representing a tile with geometry and metadata columns.

### TileXYZ [Legacy]

A mercantile Tile object with `x`, `y`, `z` attributes.

### ViewportGDF [Legacy] 

A GeoDataFrame representing the current viewport bounds.

### UdfTimeoutError

Raised when a UDF execution exceeds the time limit.

### UdfRuntimeError

Raised when a UDF encounters a runtime error during execution.

### UdfSerializationError

Raised when a UDF result cannot be serialized for return.

---

### @fused.udf

# @fused.udf

`udf() -> Callable[..., Udf]` - A decorator that transforms a function into a Fused UDF.

- cache_max_age (`Optional[str]`)

- instance_type (`Optional[str]`)

- disk_size_gb (`Optional[int]`)

- default_parameters (`Optional[Dict[str, Any]]`)

- headers (`Optional[Sequence[Union[str, Header]]]`)

    Defaults to None for no headers.

- `Callable[..., Udf]` – A callable that represents the transformed UDF.

---

### fused.h3

`run_ingest_raster_to_h3()` - Run the raster to H3 ingestion process.

This process involves multiple steps:

- extract pixels values and assign to H3 cells in chunks (extract step)
- combine the chunks per partition (file) and prepare metadata (partition step)
- create the metadata `_sample` file and overviews files
Params:
- src_path (str, list)

- output_path (str)

- metrics (str or list of str)

- res (int)

- k_ring (int)

- res_offset (int)

- file_res (int)

- chunk_res (int)

- overview_res (list of int)

- overview_chunk_res (int or list of int)

- max_rows_per_chunk (int)

- include_source_url (bool)

- target_chunk_size (int)

- debug_mode (bool)

- remove_tmp_files (bool)

- tmp_path (str)

- overwrite (bool)

- steps (list of str)

- extract_kwargs (dict)

- partition_kwargs (dict)

- overview_kwargs (dict)

The extract, partition and overview steps are run in parallel using
`fused.submit()`. By default, the function will first attempt to run this using
"realtime" instances, and retry any failed runs using "large" instances.

You can override this behavior by specifying the `engine`, `instance_type`,
`max_workers`, `n_processes_per_worker`, etc parameters as additional
keyword arguments to this function (`kwargs`). If you want to specify
those per step, use `extract_kwargs`, `partition_kwargs`, and `overview_kwargs`.
For example, to run everything locally on the same machine where this
function runs, use:

To run the extract step on realtime and the partition step on medium
instance, you could do:

In contrast to `fused.submit` itself, the ingestion sets `n_processes_per_worker=1`
by default to avoid out-of-memory issues on batch instances. You can
increase this if you know the instance has enough memory to process multiple
chunks in parallel.

---

---

### fused.ingest

## `fused.ingest`

Ingest a dataset into the Fused partitioned format.
Params:
- `input` - A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
- `output` - Location on S3 to write the `main` table to.
- `output_metadata` - Location on S3 to write the `fused` table to.
- `schema` - Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
- `file_suffix` - filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
- `load_columns` - Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
- `remove_cols` - The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
- `explode_geometries` - Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
- `drop_out_of_bounds` - Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
- `partitioning_method` - The method to use for grouping rows into partitions. Defaults to `"rows"`.
  - `"area"`: Construct partitions where all contain a maximum total area among geometries.
  - `"length"`: Construct partitions where all contain a maximum total length among geometries.
  - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
  - `"rows"`: Construct partitions where all contain a maximum number of rows.
- `partitioning_maximum_per_file` - Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
- `partitioning_maximum_per_chunk` - Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
- `partitioning_max_width_ratio` - The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
- `partitioning_max_height_ratio` - The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
- `partitioning_force_utm` - Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
- `partitioning_split_method` - How to split one partition into children. Defaults to `"mean"` (this may change in the future).
  - `"mean"`: Split each axis according to the mean of the centroid values.
  - `"median"`: Split each axis according to the median of the centroid values.
- `subdivide_method` - The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
- `subdivide_start` - The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
- `subdivide_stop` - The value below which geometries will never be subdivided into smaller parts, according to `subdivide_method`.
- `split_identical_centroids` - If `True`, should split a partition that has
  identical centroids (such as if all geometries in the partition are the
  same) if there are more such rows than defined in "partitioning_maximum_per_file" and
  "partitioning_maximum_per_chunk".
- `target_num_chunks` - The target for the number of chunks if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files and chunks generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
- `lonlat_cols` - Names of longitude, latitude columns to construct point geometries from.

  If your point columns are named `"x"` and `"y"`, then pass:

          This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

- `gdal_config` - Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

  This config is expected to be a dictionary with up to two keys:

  - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
  - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the CSV driver defines these open options you can pass in.

  For example, if you're ingesting a CSV file with two columns
  `"longitude"` and `"latitude"` denoting the coordinate information, pass

        Returns:

Configuration object describing the ingestion process. Call `.run_batch` on this object to start a job.

---
#### `job.run_batch`

`fused.ingest` returns a `GeospatialPartitionJobStepConfig` object. Call `.run_batch` on this object to start the ingestion job.

Begin job execution.
Params:
- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

## Monitor & manage job

Calling `.run_batch()` returns a `RunResponse` object which has the following methods:

#### `get_status`

Fetch the status of this job

#### `print_logs`

Fetch and print logs
Params:
- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to sys.stdout.

---
#### `get_exec_time`

Determine the execution time of this job, using the logs.

---
#### `tail_logs`

Continuously print logs
Params:
- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---
#### `cancel`

Cancel this job

Returns:

  A new job object.

---

### JobPool

The `JobPool` class is used to manage, inspect and retrieve results from
submitted jobs from `fused.submit()`.

### cancel

`cancel()` - Cancel any pending (not running) tasks.

Note it will not be possible to retry on the same JobPool later.

### retry

`retry()` - Rerun any tasks in error or timeout states. Tasks are rerun in the same pool.

### total_time

`total_time() -> timedelta` - Returns how long the entire job took.

If only partial results are available, returns based on the last task to have been completed.

### times

`times() -> list[timedelta | None]` - Time taken for each task.

Incomplete tasks will be reported as None.

### done

`done() -> bool` - True if all tasks have finished, regardless of success or failure.

### all_succeeded

`all_succeeded() -> bool` - True if all tasks finished with success

### any_failed

`any_failed() -> bool` - True if any task finished with an error

### any_succeeded

`any_succeeded() -> bool` - True if any task finished with success

### arg_df

`arg_df()` - The arguments passed to runs as a DataFrame

### status

`status()` - Return a Series indexed by status of task counts

### wait

`wait()` - Wait until all jobs are finished

Use fused.options.show.enable_tqdm to enable/disable tqdm.
Use pool.\_wait_sleep to set if sleep should occur while waiting.

### tail

`tail()` - Wait until all jobs are finished, printing statuses as they become available.

This is useful for interactively watching for the state of the pool.

Use pool.\_wait_sleep to set if sleep should occur while waiting.

### results

`results() -> list[Any]` - Retrieve all results of the job.

Results are ordered by the order of the args list.

### results_now

`results_now() -> dict[int, Any]` - Retrieve the results that are currently done.

Results are indexed by position in the args list.

### df

`df()` - Get a DataFrame of results as they are currently.
The DataFrame will have columns for each argument passed, and columns for:
`status`, `result`, `time`, `logs` and optionally `exception`.

### get_status_df

### get_results_df

### errors

`errors() -> dict[int, Exception]` - Retrieve the results that are currently done and are errors.

Results are indexed by position in the args list.

### first_error

`first_error() -> Exception | None` - Retrieve the first (by order of arguments) error result, or None.

### logs

`logs() -> list[str | None]` - Logs for each task.

Incomplete tasks will be reported as None.

### first_log

`first_log() -> str | None` - Retrieve the first (by order of arguments) logs, or None.

### success

`success() -> dict[int, Any]` - Retrieve the results that are currently done and are successful.

Results are indexed by position in the args list.

### pending

`pending() -> dict[int, Any]` - Retrieve the arguments that are currently pending and not yet submitted.

### running

`running() -> dict[int, Any]` - Retrieve the results that are currently running.

### cancelled

`cancelled() -> dict[int, Any]` - Retrieve the arguments that were cancelled and not run.

### collect

`collect()` - Collect all results into a DataFrame

---

---

### fused.options

List global configuration options.

This object contains a set of configuration options that control global behavior of the library. This object can be used to modify the options.

### __dir__

`__dir__() -> List[str]` - ### base_url

Fused API endpoint

### shared_udf_base_url

Shared UDF API endpoint

### auth

Options for authentication.

### show

Options for object reprs and how data are shown for debugging.

### max_workers

Maximum number of threads, when multithreading requests

### run_timeout

Request timeout for UDF run requests to the Fused service

### request_timeout

Request timeout for the Fused service

May be set to a tuple of connection timeout and read timeout

### metadata_request_timeout

Request timeout for file metadata requests (e.g., /file-metadata endpoint).
These requests may involve processing large parquet files and can take longer.

### request_max_retries

Maximum number of retries for API requests

### request_retry_base_delay

Base delay before retrying a API request in seconds

### realtime_client_id

Client ID for realtime service.

### max_recursion_factor

Maximum recursion factor for UDFs. This is used to limit the number of
recursive calls to UDFs. If a UDF exceeds this limit, an error will be raised.

### save_user_settings

Save per-user settings such as credentials and environment IDs.

### default_udf_run_engine

Default engine to run UDFs, one of: "local" or "remote".

### default_validate_imports

Default for whether to validate imports in UDFs before `run_local`,
`run_batch`.

### prompt_to_login

Automatically prompt the user to login when importing Fused.

### no_login

If set, Fused will not attempt to login automatically when needed.

### pyodide_async_requests

If set, Fused is being called inside Pyodide and should use pyodide
for async HTTP requests.

### cache_directory

The base directory for storing cached results.

### data_directory

The base directory for storing data results. Note: if storage type is 'object', then this path is relative to
fd_prefix.

### temp_directory

The base directory for storing temporary files.

### never_import

Never import UDF code when loading UDFs.

### gcs_secret

Secret name for GCS credentials.

### gcs_filename

Filename for saving temporary GCS credentials to locally or in rt2 instance

### gcp_project_name

Project name for GCS to use for GCS operations.

### logging

Control logging for Fused

### verbose_udf_runs

Whether to print logs from UDF runs by default

### default_run_headers

(Advanced) Default headers to include with UDF run requests.

### default_dtype_out_vector

Default transfer type for vector (tabular) data

### default_dtype_out_raster

Default transfer type for raster data

### default_dtype_out_simple

Default transfer type for simple Python data (bool, int, float, str, list)

### fd_prefix

If set, where fd:// scheme URLs will resolve to. By default will infer this from your user account.

### verbose_cached_functions

Whether to print logs from cache decorated functions by default

### local_engine_cache

Enable UDF cache with local engine

### default_send_status_email

Whether to send a status email to the user when a job is complete.

### cache_storage

Specify the default cache storage type

### use_process_pool_for_local_submit

Use ProcessPoolExecutor instead of ThreadPoolExecutor for local engine submit.
This avoids GDAL thread-safety issues and Python GIL limitations, but has higher
memory overhead and slower startup time.

### row_group_batch_size

Target size in bytes for combining adjacent row group downloads.
Adjacent row groups from the same file will be combined into single downloads
until this threshold is reached. Default is 32KB (32768 bytes), which is
optimized for S3 performance.

### base_web_url

### save

`save()` - Save Fused options to `~/.fused/settings.toml`. They will be automatically
reloaded the next time fused-py is imported.

---

### Udf (class)

The `Udf` class is the object you get when defining a UDF with the
`@fused.udf` decorator, or when loading
a saved UDF with `fused.load()`.

### name

The name of the UDF. Defaults to the function name.

### code

The source code of the UDF as a string.

### headers

Sequence of header files included with the UDF.

### metadata

Optional dictionary of metadata associated with the UDF.

### parameters

Dictionary of default parameters for the UDF.

### cache_max_age

Maximum cache age in seconds. `None` for default caching behavior.

### instance_type

The instance type to use for remote execution (e.g., `'realtime'`, `'small'`, `'medium'`, `'large'`).

### disk_size_gb

Disk size in GB for batch execution.

### region

AWS region for execution.

### entrypoint

The name of the entrypoint function within the UDF code.

### catalog_url

Returns the URL to open this UDF in the Workbench Catalog, or `None` if the UDF is not saved.

### to_directory

`to_directory()` - Write the UDF to disk as a directory (folder).
Params:

- where (`str | Path | None`)

- overwrite (`bool`)

### to_file

`to_file()` - Write the UDF to disk as a zip file.
Params:

- where (`str | Path | BinaryIO`)

- overwrite (`bool`)

### to_fused

`to_fused()` - Save this UDF to the Fused service.
Params:

- overwrite (`bool | None`)

Example:

### run_local

`run_local() -> UdfEvaluationResult` - Evaluate this UDF locally against a sample.
Params:

- inplace (`bool`)

- validate_imports (`bool | None`)

- kwargs – Additional arguments to pass to the UDF.

Example:

### schedule

`schedule() -> CronJob` - Schedule this UDF to run on a cron schedule.
Params:

- minute (`list[int] | int`)

- hour (`list[int] | int`)

- day_of_month (`list[int] | int | None`)

- month (`list[int] | int | None`)

- day_of_week (`list[int] | int | None`)

- udf_args (`dict[str, Any] | None`)

- enabled (`bool`)

Example:

### get_schedule

`get_schedule() -> CronJobSequence` - Retrieve any scheduled runs of this UDF.

Returns: `CronJobSequence`

### set_parameters

`set_parameters() -> Udf` - Set the parameters on this UDF.
Params:

- parameters (`dict[str, Any]`)

- replace_parameters (`bool`)

- inplace (`bool`)

Example:

### create_access_token

`create_access_token() -> UdfAccessToken` - Create an access token for sharing this UDF.
Params:

- client_id (`str | None`)

- public_read (`bool | None`)

- access_scope (`str | None`)

- cache (`bool`)

- metadata_json (`dict[str, Any] | None`)

- enabled (`bool`)

Returns: `UdfAccessToken`

### get_access_tokens

`get_access_tokens() -> UdfAccessTokenList` - Get all access tokens for this UDF.

Returns: `UdfAccessTokenList`

### delete_cache

`delete_cache()` - Delete the cached results for this UDF.

### delete_saved

`delete_saved()` - Delete this UDF from the Fused service.
Params:

- inplace (`bool`)

### eval_schema

`eval_schema() -> Udf` - Reload the schema saved in the code of the UDF.

Note: This will evaluate the UDF function.
Params:
- inplace (`bool`)

### from_gist

Create a UDF from a GitHub gist.
Params:
- gist_id (`str`)

Example:

---

### Batch jobs

This guide shows how to execute a batch job with fused-py from a Jupyter Notebook. It was inspired by a Discord community request.

Running long processes can be computationally expensive, or liable to interruptions from network disconnections. For these cases, `fused-py` can run batch jobs on an EC2 instance.

To illustrate, this guide shows how to trigger and monitor a batch job from a Jupyter Notebook to unzip a large file and upload it to S3.

This UDF downloads a zipped file from S3 file specified with `source_s3_path`, unzips it, then uploads it to the S3 path specified by `destination_s3_path`.

## 1. Define UDF

This UDF downloads a zipped file with `fused.download`, unzips it, and uploads the extracted files to the `destination_s3_path`. For simplicity, you may choose to write to your Fused S3 bucket path which you can find in your Workbench File Explorer - otherwise, you might need to adjust permissions of the target bucket.

## 2. Run UDF on an offline instance

To go beyond the 120s limit of the default `fused.run(udf)` call we'll define a job and use `job.run_batch()` to kick off a call on a large, offline instance.
Get in touch with Fused if your account doesn't have batch-mode enabled.

Note: Make sure to replace `<YOUR_DIR>` with your own directory.

## 3. Monitor job

`job_id` has a number of methods to monitor the job. For example `job_tail_logs` streams logs as the job runs.

---

### API Reference

# API Reference

This is the API reference for the `fused` Python package, which allows you to build and run serverless data workflows.

The reference is intended to be limited to low-level descriptions of various programmatic functionality. If you're just getting started with Fused, we recommend looking at the Guide first or exploring an Example.

| Function | Description |
| --- | --- |
| @fused.udf | Decorator to define a User Defined Function |
| @fused.cache | Decorator to cache function results |

| Function | Description |
| --- | --- |
| fused.run | Execute a UDF and return results |
| fused.run_async | Execute a UDF asynchronously |
| fused.submit | Submit a UDF for parallel execution |
| JobPool | Manage multiple concurrent job executions |

| Function | Description |
| --- | --- |
| fused.load | Load a UDF from file, URL, or Workbench |
| fused.load_async | Load a UDF asynchronously |

| Function | Description |
| --- | --- |
| fused.get_chunk_from_table | Read a chunk from a partitioned table |
| fused.get_chunks_metadata | Get metadata for table chunks |
| fused.find_dataset | Search for datasets in the catalog |
| fused.register_dataset | Register a dataset in the catalog |

| Function | Description |
| --- | --- |
| fused.download | Download files from URLs |
| fused.file_path | Get local file path for cached files |
| fused.ingest | Ingest geospatial data into optimized format |
| fused.ingest_nongeospatial | Ingest non-geospatial data |

## Configuration & Utilities

| Function | Description |
| --- | --- |
| fused.context | Access UDF execution context |
| fused.options | Configure Fused runtime options |
| fused.secrets | Access secrets and credentials |
| fused.types | Type definitions for UDF parameters |
| fused.h3 | H3 hexagonal grid utilities |
| fused.api | Low-level API access |

| Class | Description |
| --- | --- |
| Udf | UDF object with metadata and execution methods |
| JobPool | Pool for managing parallel job execution |

---


---

Generated automatically from Fused Python SDK documentation. Last updated: 2026-01-30
Total pages: 27
