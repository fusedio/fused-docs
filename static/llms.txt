// File: index

# Welcome to Fused

### **Why Fused**

Modern data science applications require datasets measured in 100s or 1,000s of Gb, combining many different sources and formats all together. But the analysis is as slow as the slowest process.

Fused is an end-to-end cloud platform for data analytics, build around [User Defined Functions](/core-concepts/why/) (UDFs): Python functions that can be run via HTTP requests from anywhere, without any install required.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={false} controls height="40vh" url="https://youtu.be/VGAnfhpm0Ok" alignItems='center'/>

### **Fused Features**

To speed up Data Science pipelines we need a few things, all of which we're working on:
- [Ingestion of data](/core-concepts/data-ingestion/) into Cloud Native formats
- [Serverless compute engine](/core-concepts/write/) for User Defined Functions
- [Caching of data](/core-concepts/cache/) for faster iteration & reducing cost of compute
- [Workbench](/workbench/overview/) our browser-based IDE to develop & iterate on UDFs with a map view
- [Batch run](/core-concepts/run-udfs/run_large/) of many User Defined Functions to scale code to any dataset size in a few lines of code

### Getting started

To get your hands dirty with Fused:

- [‚ö° Quickstart Guide](/quickstart/): Learn the basics of Fused with a simple example in just a few minutes
- [Core-Concepts](/core-concepts/) of Fused UDFs (writing & running)
- [Best Practices](/user-guide/best-practices/) for making the most of Fused UDFs
- [Examples](/user-guide/examples/): Building full pipelines with ingestion, writing more complex UDFs, running & deploying them

### Connect with the community

Find inspiration for your next project, ask questions, or share your work with the Fused community.

- [__GitHub__](https://github.com/fusedio/udfs/tree/main)
- [__Discord__](https://bit.ly/fusedslack)
- [__LinkedIn__](https://www.linkedin.com/company/fusedio/)

---

// File: quickstart

# Get started with Fused! üöÄ

Learn the fundamental concepts of working with Fused.

This guide is an introduction to Fused Workbench. It covers the concepts and terminology you will come across as you work with data using the web-based IDE. After reading this page you should have an understanding of the fundamentals to build Fused UDFs and apps.

- [Create a simple UDF](/quickstart/#create-udf-to-read-your-data)
- [Edit a UDF](/quickstart/#edit-a-udf)
- [Share data from your UDF anywhere using HTTP](/quickstart/#get-your-data-from-a-http-endpoint)
- [Create a Fused app](/quickstart/#create-an-app)  

If you get stuck, please ask for help in the [Fused Discord](https://discord.com/invite/BxS5wMzdRk). üòä

## Introduction

A Fused User Defined Function ([UDF](/core-concepts/why/)) is a Python function that interacts with your data and can be called via HTTP requests. You can create UDFs from cloud storage files with the [File Explorer](/workbench/file-explorer/) or clone existing UDFs from the [UDF Catalog](/workbench/udf-catalog/). You edit UDFs in the [UDF Builder](/workbench/udf-builder/) and create & share apps that interact with your UDFs in the [App Builder](/workbench/app-builder/).

## Create UDF to read your data

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import ReactPlayer from 'react-player';

<Tabs className="unique-tabs">
  <TabItem value="workbench" label="Workbench" default>

    ### Option 1: Use the Fused Workbench

    Workbench is Fused browser-based IDE. We have a [whole detailed section](/workbench) about it, but we'll take you through the basics here.

    You can start by opening [Fused Workbench](https://www.fused.io/workbench) in another tab.

    Choose `+ New UDF`, which will start creating a new UDF:

    <ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/add_new_udf.mp4" width="100%" />

    Welcome to workbench! üõ†Ô∏è

    You now have access to a full Python development environment directly in your browser, where you can
    - Edit your UDFs in real time
    - See the geospatial output in the map view
    - Debug & explore data in the `Stdout` tab of the Results page

    :::info

      You can still use Workbench as a read-only user if you aren't signed-up, but to use the full benefit of workbench, [login](https://www.fused.io/workbench/preferences) or [join our waitlist](https://docs.google.com/forms/d/1NVzMjc2tXxlIgnFrxqQPM_NtG1B2AQ0En_pAbZHYSK0)

    :::

    <ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/demo_workbench.mp4" width="100%" />

  </TabItem>
  <TabItem value="locally" label="Local">

    ### Option 2: Use `fused` locally (üöß Docs under construction)

    üöß This section is under construction üöß

    You can also do everything from your local machine (or anywhere else!) using `fused`. To use `fused` locally you first need to install it:

    ```python showLineNumbers
    pip install fused
    ```

    :::info

    We recommend you use a virtual environment to work with `fused` and you'll need `python >=3.9`

    :::

  </TabItem>
</Tabs>

## Edit a UDF

The power of Fused Workbench is the ability to easily edit your code and it executes instantly without you having to hit "Run".
The UDF editor calls [`fused.run(udf)`](/python-sdk/top-level-functions/#run) under the hood for you upon updates to the code. 


Let's give it a try, replace the default UDF with this code into the UDF Builder to render subway stations on a map:

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
    gdf = gpd.read_file(DATASET)
    return gdf
```

You don't need to move your data to a Fused bucket to access it, so we can easily change the input data. Let's do just that:
Change line 6 to a completely different dataset:

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    # highlight-next-line
    DATASET = "https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json"
    gdf = gpd.read_file(DATASET)
    return gdf
```

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/edit_simple_udf.mp4" width="100%" />

  :::info

  The map updates automatically using from the geospatial information returned by `udf()`. 
  This interaction goes both ways as the map can [pass coordinates](/core-concepts/filetile/#tile) into the UDF with `bounds` or XYZ indices, allowing Fused to spatially filter data.

  :::

Let's explore this data a bit more, the map shows us we have US states, let's print out some more info about it, just before the `return`:

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    DATASET = "https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json"
    gdf = gpd.read_file(DATASET)
    # highlight-next-line
    print(gdf)
    return gdf
```

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/print_udf.mp4" width="100%" />

The `stdout` view shows us we have a `name` column in our `GeoDataFrame`, so we can leverage `geopandas` to filter only a few states:

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    DATASET = "https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json"
    gdf = gpd.read_file(DATASET)
    # highlight-next-line
    gdf = gdf[gdf['name'].isin(['California', 'Texas'])]
    print(gdf)
    return gdf
```

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/filter_states.mp4" width="100%" />

You can notice this change in 2 places:
1. The map view only shows 2 states now
2. The `stdout` only has 2 rows

All that, without having to ever press "Run"! 

  :::note

  If you move `print(gdf)` _before_ `gdf = gdf[gdf['name'].isin(['California', 'Texas'])]` you will still see 50 lines in `stdout` and 2 in the map view, because the print statement happens before filtering
  
  Keep this in mind when using `print` to debug

  :::

## Get your data from a HTTP endpoint


The UDF we now have returns a `GeoDataFrame` with 2 rows, that's what we see in the map view. 

We can edit this UDF as much as we want but for now we're going to get this data out of Fused. You might expect us to hit a "Download" button somewhere and save this as a `GeoJSON`, but while you can do this, we're going to do something better: Call this UDF from a HTTP endpoint:
- Make sure to save your UDF by either clicking `üíæ Save` or pressing `Ctrl + S` on Windows / Linux or `Cmd + S` on MacOS
- Click `Share`
- In the `Share modal` click `Share` to view all available sharing options.

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/new_share.mp4" width="100%" />

We've pre-populated a few examples of ways to share these Fused UDFs, but at its core you can *call these UDFs as HTTP endpoints*

UDFs aren't just Python functions you can run in Workbench, they're fully deployed functions that you can call from anywhere. 

To keep it simple to start, we can use the `cURL` option. Open a terminal and the `cURL` command, you'll get something like:

```bash
curl -L -XGET "https://www.fused.io/server/v1/realtime-shared/<YOUR_UDF_ID>/run/file?dtype_out_raster=png&dtype_out_vector=csv"
```

Let's break that down:
- `curl -L XGET` ->  `curl` syntax to GET and automatically follow a redirect
- `https://www.fused.io/server/v1/realtime-shared/` -> When you call a UDF, Fused automatically creates a file from the `return` of the UDF, this is where we host it
- `<YOUR_UDF_ID>` -> Your unique UDF ID. Fused handles this for you
- `run/file?dtype_out_raster=png&dtype_out_vector=csv` -> Tells Fused we want a `file` (more about this in future sections) and what data type. Fused can return Rasters or Vectors, we're telling it give us either here, with rasters as PNG files and vector as CSV

So, if you run this, you'll get something like this:

![cURL UDF output](./index-imgs/Getting-started-curl-output.png) 

That's a CSV, with 2 rows and a lot of coordinates, our geometry! 

This is nice, but these geometries are quite verbose in CSV format. 

Let's go back to our UDF and remove `geometry` from our return:

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    DATASET = "https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json"
    gdf = gpd.read_file(DATASET)
    gdf = gdf[gdf['name'].isin(['California', 'Texas'])]
    print(gdf)
    # highlight-next-line
    return gdf[['id', 'name']]
```

Remember to save, and notice how the geometries disappear from our map view: there's nothing to display anymore. 

Now, in your terminal, rerun the _same_ `curl` request:

![cURL UDF output without geometry](./index-imgs/Getting-started-curl-output-no-geoms.png) 

No more geometry, just 2 rows with `id` and `name`, exactly as we had in our UDF `return` !

That's the real power of Fused: you don't have to deploy anything. Just edit your code, save it and then next time your UDF is called through HTTP, the data created will reflect your UDF!

## Create an app

Sometimes we simply want to show the results of our analysis to someone else, which is what Fused Apps are for.

You can use Streamlit components to create an app in the [App Builder](/workbench/app-builder/), found on the sidebar. You can then move from UDF builder to App Builder. 

Here's a basic example to show our dataframe:

```python
import fused
import streamlit as st

st.write("# Hello World! üëã")
st.write(fused.run("fsh_2kTEwnRARTd9gdDGUbH6ll"))

```

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/share_udf_in_app2.mp4" width="100%" />

As we change our UDF, the output will also change in the app directly, just like we saw with `curl`

You can then share your Fused app to anyone simply by sharing the link at the top of the App Builder:

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/share_app.mp4" width="100%" />

These links work without needing a Fused account or the need to install anything, allowing you to share your work easily to anyone!

## Next steps

Congratulations, you've touched on all the core features of Fused. üéâ

Now you can dive deeper into more advanced topics:
- Learn more about [UDFs](/core-concepts/why/) (We showed how to return static vector files, but we can do a lot more!)
- Take a deeper look at [Workbench](/workbench/overview/)
- Play around with the [Examples](/user-guide/examples) to Fused in action
- Look into the [UDF Catalog](/workbench/udf-catalog/) to find UDFs from the community you could leverage
- Dive into the `fused` [Python SDK](/python-sdk/)
- Learn about how [Fused handles caching](/core-concepts/cache/)
- Join [Discord](https://bit.ly/fused-discord) to connect with the community and discover what's possible

Welcome aboard! üö¢

---

// File: user-guide

{/* Learn the most important concepts through applied examples. */}




import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: workbench

import DocCardList from '@theme/DocCardList';

<DocCardList />

---

// File: faq

## General questions



<details>
<summary>Whom is Fused for?</summary>

Fused is designed for teams seeking to simplify their workflows and accelerate the creation and delivery of data products. It's ideal for organizations that need a scalable solution to handle growing data sizes while minimizing the time spent on data engineering.
</details>



<details>
<summary>Why Python, when there's spatial SQL?</summary>

Python is the go-to language for spatial data science. Although spatial SQL joins and transformations can be efficiently performed using PostGIS in an external database, you may eventually need to convert that data to Pandas and NumPy for further processing and analysis, especially for detailed operations on raster arrays. Additionally, you can run SQL directly on Fused using Python libraries like DuckDB, combining the strengths of both approaches.
</details>

<details>
<summary>What's the benefit of geo partitioning vector tables?</summary>

It enables efficient reading of large datasets by strategically partitioning GeoParquet files. Fused's GeoParquet format includes metadata that allows for spatial filtering of any dataset, loading only the chunks relevant to a specific area of interest. This approach reduces memory usage and allows you to work with any size dataset with just Python.
</details>

<details>
<summary>Which authentication methods do you support?</summary>

Fused currently uses Auth0 to support authentication via Google and GitHub.
</details>

<details>
<summary>What's the best way to create a map from my UDF data and share it with external teams?</summary>

You can use the [App Builder](/workbench/app-builder/app-overview/) create an app that loads the UDF's data then [create a shareable link](/workbench/app-builder/app-overview/#share).

</details>

<details>
<summary>How can I create a set of tiles that cover a given geometry?</summary>

Use cases like creating chips may call for running a UDF across a set of tiles that fall within a given geometry. This can be done by creating a list of tiles with the [mercantile](https://github.com/mapbox/mercantile) library then calling the UDF [in parallel](/user-guide/examples/zonal-stats/#c-parallelization).

```python showLineNumbers
import fused
import mercantile

bounds = [32.4203, -14.0933, 34.6186, -12.42826]

tile_list = list(mercantile.tiles(*bounds,zooms=[15]))
```

</details>


## Troubleshooting

### Status page

Access our [status page](https://fused.instatus.com/) at any time to check on Workbench & API status

### Common Errors

<details>
<summary>Error: `Access is not configured for you in the Fused Workbench. Please refresh the page if you think this is an error, or get in touch if you require further help. Cause: Realtime instance not configured.`</summary>

This error occurs when you try to run a UDF with an account associated with a workspace environment that does not have a realtime instance configured. This means that there are no worker nodes available to run the UDF. To resolve this issue, please get in touch with the Fused team team to ensure your account is associated with an environment with a realtime instance.

When Troubleshooting this error, it may help to navigate to your account's User Profile page to determine if the account is associated with an environment and realtime instance, as shown here.

import ImageSettings from '@site/static/img/user_settings.png';

<div style={{textAlign: 'center'}}>
<img src={ImageSettings} alt="File" style={{}} />
</div>

</details>



<details>
<summary>Error: `No such file or directory: '/mnt/cache/'`</summary>

This error occurs when a UDF attempts to access the [`/mnt/cache` disk](/core-concepts/content-management/file-system/#mntcache-disk) when it is not available for the environment. To resolve this issue, please contact the Fused team to ensure that the cache directory is available for your account.
</details>

<details>
<summary>Error: `No space left on the device: '/tmp/'`</summary>

This error occurs when a UDF attempts to write more data than the `/tmp` directory of the real-time instance can handle. Realtime instances have a limited amount of space available and are ephemeral between runs. You might want to consider writing to [`/mnt/cache` disk](/core-concepts/content-management/file-system/#mntcache-disk) instead.
</details>

<details>
<summary>Error: `Quota limit: Number of running instances`</summary>

Fused batch jobs, which are initiated with [run_remote](/python-sdk/top-level-functions/#jobrun_remote), require a server quota to be enabled for your account. These include data [ingestion](/core-concepts/data-ingestion/) jobs. If you encounter this error, please contact the Fused team to request an increase in the quota allotted to your account.
</details>

<details>
<summary>Error: `Application error: a client-side exception has occurred (see browser console for more information).`</summary>

In the case that you encounter this error, please reset your browser cache and cookies. If the error persists, please contact the Fused team for further assistance.
</details>


<details>
<summary>Error: `MY_UDF... not saved because a UDF or app wit that name already exists.`</summary>

It's possible you might be editing a "local" UDF that has the same name as a Team UDF. To resolve this issue, you can either rename the UDF or, if your intention is to update the Team UDF, [Push it to GitHub](/workbench/udf-catalog/#publish-a-udf-to-a-github-repository).

</details>


<details>
<summary>Error: `Failed to create share token: {"detail":"UDF not found"}`</summary>

To resolve this error, save the UDF again and refreshing your browser window. If the problem persists, rename the UDF and try again.

</details>

---

// File: core-concepts/why

Fused is a data analytics platform to write and deploy Python User Defined Functions (UDFs) behind HTTP endpoints and interactive applications.

- [Read files](/workbench/file-explorer/) in cloud storage with UDFs
- [Write](/core-concepts/write/) and share UDFs with ease
- [Run UDFs](/core-concepts/run-udfs/) from anywhere with simple [HTTP calls](/core-concepts/run-udfs/run-small-udfs/#http-requests)
- Scale and [parallelize](/core-concepts/async) without managing infrastructure
- [Create apps](/workbench/app-builder/) that run UDFs

## The problem

Fused UDFs directly address the problems of productionizing data science and analytical workflows, including:

- Difficulty sharing and reproducing Notebooks
- Slow iteration cycles in development
- Limited discoverability of data within an organization
- Challenging reusability of code snippets across projects
- Friction transpiling code to production
- Managing workflow infrastructure
- Inefficient data delivery to applications
- Slow performance of analytical apps

Fused UDFs address these issues by standardizing how Python code is written, shared, and run.

## What is a UDF?

UDFs are Python functions that can be called from anywhere to apply a specific operation to data. For every UDF, Fused creates an endpoint that can be called to run the function code and return its output. This makes Fused easy to integrate with data applications and deliver dynamically generated data on demand.

import ImageDiagram from '@site/static/img/product_diagram.png';

<div style={{textAlign: 'center'}}>
<img src={ImageDiagram} alt="File" style={{}} />
</div>

You can think of UDFs as versatile building blocks to load and transform data across a range of use cases, including geospatial. They can act, for example, as virtual datasets, file readers, and workflow tasks.

## Virtual Datasets

UDFs can be used as virtual datasets, similar to database views, to deliver data behind an HTTP endpoint. They can return data in formats defined at call time based on the needs of the client application, such as tiffs, Parquet, GeoJSON, and others. This eliminates the need to pre-process or transfer datasets ahead of time.

{/* todo: materialize, caching */}

## File Readers

UDFs can also be used to open files of various formats, like Parquet, CSV, and GeoJSON. This provides a standard interface to easily explore files in cloud object storage and eliminates the need to move, copy, or transform entire datasets.

## Workflow Tasks

UDFs can serve as reusable tasks in analysis pipelines that easily integrate with 3rd party applications to load, process, and write data. Multiple UDFs can be chained and called in parallel to create complex workflows that run and return data on demand via HTTP requests.

---

// File: core-concepts/write

import ImageAnatomy from '@site/static/img/udfanatomy.png';

<div style={{textAlign: 'center'}}>
<img src={ImageAnatomy} alt="File" style={{}} />
</div>

Follow these steps to write a [User Defined Function (UDF)](/core-concepts/why/).

- Decorate a function with [`@fused.udf`](/core-concepts/write/#fusedudf-decorator)
- [Declare the function](/core-concepts/write/#function-declaration) logic
- Optionally [cache](/core-concepts/write/#fusedcache-decorator) parts of the function
- Set [typed parameters](/core-concepts/write/#typed-parameters) to dynamically run based on inputs
- Import [utility modules](/core-concepts/write/#utils-module) to keep your code organized
- [Return](/core-concepts/write/#return-object) a vector table or raster
- [Save](/core-concepts/write/#save-udfs) the UDF

## `@fused.udf` decorator

First decorate a Python function with `@fused.udf` to tell Fused to treat it as a UDF.

## Function declaration

Next, structure the UDF's code. Declare import statements within the function body, express operations to load and transform data, and define a return statement. This UDF is called `udf` and returns a `pd.DataFrame` object.

```python showLineNumbers
@fused.udf # <- Fused decorator
# highlight-start
def udf(name: str = "Fused"): # <- Function declaration
    import pandas as pd
    return pd.DataFrame({'message': [f'Hello {name}!']})
# highlight-end
```
:::info
The UDF Builder in Workbench imports the `fused` module automatically. To write UDFs outside of Workbench, install the [Fused Python SDK](/python-sdk/) with `pip install fused` and import it with `import fused`.
:::

:::note
Placing import statements within a UDF function body (known as "local imports") is not a common Python practice, but there are specific reasons to do this when constructing UDFs. UDFs are distributed to servers as a self-contained units, and each unit needs to import all modules it needs for its execution. UDFs may be executed across many servers (10s, 100s, 1000s), and any time lost to importing unused modules will be multiplied.

An exception to this convention is for modules used for function annotation, which need to be imported outside of the function being annotated.
:::

## `@fused.cache` decorator

Use the [@fused.cache](/core-concepts/cache/) decorator to persist a function's output across runs so UDFs run faster.


```python showLineNumbers
@fused.udf # <- Fused decorator
def udf(bounds: fused.types.Bounds = None, name: str = "Fused"):
    import pandas as pd

    # highlight-start
    @fused.cache # <- Cache decorator
    def structure_output(name):
        return pd.DataFrame({'message': [f'Hello {name}!']})
    # highlight-end

    df = structure_output(name)
    return df
```




## Typed parameters

UDFs resolve input parameters to the types specified in their function annotations.
This example shows the [`bounds` parameter](/core-concepts/filetile/#bounds-object-types) typed as `fused.types.Bounds`
and `name` as a string.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None, # <- Typed parameters
    name: str = "Fused"
):
```

:::tip

To write UDFs that run successfully as both [`File` and `Tile`](/core-concepts/filetile/), set `bounds` as the first parameter, with `None` as its default value. This enables the UDF to be invoked successfully both as `File` (when `bounds` isn't passed) and as `Tile`. For example:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds = None):
    ...
    return ...
```

:::

### Supported types

Fused supports the native Python types `int`, `float`, `bool`, `list`, `dict`, and `list`. Parameters without a specified type are handled as strings by default.

The UDF Builder runs the UDF as a [Map Tile](/core-concepts/filetile/) if the [first parameter](/core-concepts/filetile/#the-bounds-object) is typed as `fused.types.Bounds`.

### `pd.DataFrame` as JSON

Pass tables and geometries as serialized UDF parameters in HTTPS calls. Serialized JSON and GeoJSON parameters can be casted as a `pd.DataFrame` or `gpd.GeoDataFrame`. Note that while Fused requires import statements to be declared within the UDF signature, libraries used for typing must be imported at the top of the file.

```python showLineNumbers
import geopandas as gpd
import pandas as pd

@fused.udf
def udf(
    gdf: gpd.GeoDataFrame = None,
    df: pd.DataFrame = None
):
```

## Reserved parameters

When running a UDF with `fused.run`, it's possible to specify the [map tile](/core-concepts/filetile/#tile) Fused will use to structure the `bounds` object by using the following reserved parameters.

### With `x`, `y`, `z` parameters

```python showLineNumbers
fused.run("UDF_Overture_Maps_Example", x=5241, y=12662, z=15)
```

### Passing a `GeoDataFrame`
```python showLineNumbers
import geopandas as gpd
bounds = gpd.GeoDataFrame.from_features({"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"coordinates":[[[-122.41152460661726,37.80695951427788],[-122.41152460661726,37.80386837460925],[-122.40744576928229,37.80386837460925],[-122.40744576928229,37.80695951427788],[-122.41152460661726,37.80695951427788]]],"type":"Polygon"},"id":1}]})
fused.run("UDF_Overture_Maps_Example", bounds=bounds)
```

### Passing a bounding box list

You can also pass a list of 4 points representing `[min_x, min_y, max_x, max_y]`

```python showLineNumbers
fused.run('UDF_Overture_Maps_Example', bounds=[-122.349, 37.781, -122.341, 37.818])
```


## `utils` Module

Define a UDF's `utils` Module file in the Workbench ["Module" tab](/workbench/udf-builder/code-editor/#module) and import it in the UDF. Use it to modularize code to make it readable, maintainable, and reusable.

```python showLineNumbers
from utils import function
```

### Import utils from other UDFs

UDFs import the `utils` Module from other UDFs with `fused.load` in the [UDFs GitHub repo](https://github.com/fusedio/udfs/tree/main) or [private GitHub repos](/core-concepts/content-management/git/). Here the commit SHA `05ba2ab` pins `utils` to specific commit for version control.

```python showLineNumbers
utils = fused.load(
    "https://github.com/fusedio/udfs/tree/05ba2ab/public/common/"
)
```

`utils` Module are imported from other UDFs in a user's account.

```python showLineNumbers
utils = fused.load("your@email.com/my_udf").utils
```

## `return` object

UDFs can return the following objects:

- Tables: `pd.DataFrame`, `pd.Series`, `gpd.GeoDataFrame`,  `gpd.GeoSeries`, and `shapely geometry`.
- Arrays: `numpy.ndarray`, `xarray.Dataset`, `xarray.DataArray`, and `io.BytesIO`. Fused Workbench only supports the rendering of `uint8` arrays. Rasters without spatial metadata should indicate their [tile bounds](/core-concepts/filetile/#map-tiles).
- Simple Python objects: `str`, `int`, `float`, `bool`. 
- Dictionaries: `dict`. Useful to return dictionaries of raster numpy array for example.

## Save UDFs

UDFs exported from the [UDF Builder](/workbench/udf-builder/) or saved locally are formatted as a `.zip` file containing associated files with the UDFs code, `utils` Module, metadata, and `README.md`.

```
‚îî‚îÄ‚îÄ Sample_UDF
    ‚îú‚îÄ‚îÄ README.MD       # Description and metadata
    ‚îú‚îÄ‚îÄ Sample_UDF.py   # UDF code
    ‚îú‚îÄ‚îÄ meta.json       # Fused metadata
    ‚îî‚îÄ‚îÄ utils.py        # `utils` Module
```

### In Python: `.to_fused()`

When outside of Workbench, save UDF to your local filesystem with `my_udf.to_directory('Sample_UDF')` and to the Fused cloud with `my_udf.to_fused()`.

This will allow you to access your UDF using a [token](/core-concepts/run-udfs/run-small-udfs/#token), from a [Github commit](/core-concepts/run-udfs/run-small-udfs/#git-commit-hash-recommended-for-most-stable-use-cases) or directly importing it in [Workbench from the Github URL](/workbench/udf-catalog/#add-from-github-url)

### In Workbench: Saving through Github

You can also save your UDFs directly through Github as personnal, team or community UDF. Check out the [Contribute to Fused](/workbench/udf-catalog/#contribute-to-fused) to see more.

## Debug UDFs

#### UDF builder

A common approach to debug UDFs is to show intermediate results in the [UDF Builder](/workbench/udf-builder) [results panel](/workbench/udf-builder/results/) with `print` statements.

#### HTTP requests

When using HTTP requests, any error messages are included in the `X-Fused-Metadata` response header. These messages can be used to debug. To inspect the header on a browser, open the [Developer Tools](https://developer.chrome.com/docs/devtools/network) network tab.

import ImageNetwork from '@site/static/img/network.png';

<div style={{textAlign: 'center'}}>
<img src={ImageNetwork} alt="File" style={{}} />
</div>

---

// File: core-concepts/filetile

# Displaying Spatial Data: Tile vs File

When spatial UDFs are called (i.e. that return spatial data like a `GeoDataFrame` or an array of `GeoTiff` tiles), they [run](/core-concepts/run-udfs/) and return the output of the execution like any UDF. However they can be called in two ways that influence how Fused handles them: `File` and `Tile`.

This is an important distinction and can be changed in Workbench at the top of the UDF editor:

![File or Tile](https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/core-concepts/file-tile/File_Tile_Viewport.gif)

We'll demonstrate the differences with a simple UDF that takes a [`bounds` object](/core-concepts/filetile/#the-bounds-object) as input and display how it behaves differently in each mode:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds):
    import geopandas as gpd
    from shapely.geometry import box
    
    # Returning bounds to gdf
    gdf = gpd.GeoDataFrame(geometry=[box(*bounds)], crs=4326)
    return gdf
```

### File (Viewport)

Selecting `File (Viewport)` mode, the UDF runs within a single call.

If you run the example UDF, make sure to select `File (Viewport)` mode in the UDF editor, execute it with "Shift + Enter" and zoom out you'll see that `gdf` covers the viewport you had:

![File (Viewport)](/img/core-concepts/file_tile/File_bounds_udf.png)

**We generally recommend using `File (Viewport)` mode when :**
- Working with smaller datasets that fit into memory
- Wanting to load data that doesn‚Äôt move each time you pan around
- Wanting 1 seamless layer without tiling artifacts
- Wanting data at a specific resolution

### Tile

By contrast, `Tile` mode runs your UDF multiple times over a grid of [Mercator Tiles](https://en.wikipedia.org/wiki/Tiled_web_map) that cover the viewport. We aim to use anywhere from 2 to 15 tiles to cover the current viewport. 

Looking at the same UDF and selecting `Tile` mode, we get many different tiles:

![Tile](https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/core-concepts/file-tile/Tile_bounds_udf.gif)

You can see a few other differences:
- This UDF is called each time you move around the Map Viewport
- We need to "Freeze" the viewport for it to stop rendering. This is in contrast to `File (Viewport)` mode where the UDF is called once and panning in Map View doesn't re-run the UDF

**We generally recommend using `Tile` mode when :**
- Loading a lot of data at once (since the UDF is called multiple times over a smaller extent each time)
- Wanting a more dynamic, responsive panning & scrolling experience
- Wanting a dynamic resolution to be calculated for your image rendering based on your current zoom level

:::tip

The mode you select for a UDF is saved with the UDF. You can decide which mode you prefer people to use for your UDF by selecting the mode in the UDF editor.
:::

## The `bounds` object

A UDF may use the `bounds` parameter to spatially filter datasets and load into memory only the data that corresponds to the `bounds` spatial bounds. This reduces latency and data transfer costs.

[Cloud-optimized formats](/core-concepts/data-ingestion/file-formats/) are particularly suited for these operations - they include [Cloud Optimized GeoTiff](https://www.cogeo.org/), [Geoparquet](https://geoparquet.org/), and [GeoArrow](https://geoarrow.org/format.html).

### `bounds` object types

The `bounds` object defines the spatial bounds of the Tile, which can be represented as a geometry object or XYZ index.

#### `fused.types.Bounds`

This is a `list` of 4 points representing the bounds (extent) of a geometry. The 4 points represent `[xmin, ymin, xmax, ymax]` of the bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    print(bounds)

>>> [-1.52244399, 48.62747869, -1.50004107, 48.64359255]
```

:::note
`fused.types.Bounds` is a list of 4 points so it might be helpful to convert it to a `GeoDataFrame` when returning spatial data:

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    import shapely
    import geopandas as gpd
    box = shapely.box(*bounds)
    return gpd.GeoDataFrame(geometry=[box], crs=4326)
```

The `fused` module also comes with many [handy utils functions](https://github.com/fusedio/udfs/blob/e74035a1/public/common/utils.py#L1891) that allow you to quickly access these common operations. For example, you can use the `bounds_to_gdf` utility function to perform the same operation as above. You can also use `estimate_zoom` to estimate the zoom level that matches the bounds.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):
    utils = fused.load("https://github.com/fusedio/udfs/tree/e74035a1/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    zoom = utils.estimate_zoom(bounds)
    print(zoom)
    return bounds
```


:::

### Legacy types

These types are still currently supported in `fused` though only for legacy reasons and will soon be deprecated.

#### [Legacy] `fused.types.Tile`

This is a [geopandas.GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html) with `x`, `y`, `z`, and `geometry` columns.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Tile=None):
    print(bounds)

>>>      x    y   z                                           geometry
>>> 0  327  790  11  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `fused.types.TileGDF`

This behaves the same as [`fused.types.Tile`](/core-concepts/filetile/#legacy-fusedtypestile).

#### [Legacy] `fused.types.ViewportGDF`

This is a [geopandas.geodataframe.GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html) with a `geometry` column corresponding to the Polygon geometry of the current viewport in the Map.
```python showLineNumbers
@fused.udf
def udf(bbox: fused.types.ViewportGDF=None):
    print(bbox)
    return bbox

>>>  geometry
>>>  POLYGON ((-122.0 37.0, -122.0 37.1, -122.1 37.1, -122.1 37.0, -122.0 37.0))
```

#### [Legacy] `bbox` object

UDFs defined using the legacy keyword `bbox` are automatically now mapped to `bounds`. Please update your code to use `bounds` directly as this alias will be removed in a future release.

## Call HTTP endpoints

A UDF called via an [HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) is invoked as `File` or `Tile`, depending on the URL structure.

### File endpoint

This endpoint structure runs a UDF as a `File`. See implementation examples with [Felt](/user-guide/out/felt/#vector) and [Google Sheets](/user-guide/out/googlesheets/#2-create-a-url-for-the-udf) for vector.

```bash
https://www.fused.io/server/.../run/file?dtype_out_vector=csv
```

:::info
   In some cases, `dtype_out_vector=json` may return an error. This can happen when a GeoDataFrame without a `geometry` column is being return or a Pandas DataFrame. You can bypass this by using `dtype_out_vector=geojson`.
:::

### Tile endpoint

This endpoint structure runs a UDF as a `Tile`. The `{z}/{x}/{y}` templated path parameters correspond to the Tile's XYZ index, which Tiled web map clients dynamically populate. See implementation examples for Raster Tiles with [Felt](/user-guide/out/felt/#raster-tiles) and [DeckGL](/user-guide/out/deckgl/#raster-tile-layers), and for Vector Tiles with [DeckGL](/user-guide/out/deckgl/#vector-tile-layers) and [Mapbox](/user-guide/out/mapbox/#a-vector-tile-layers).

```bash
https://www.fused.io/server/.../run/tiles/{z}/{x}/{y}?&dtype_out_vector=csv
```

## Tile UDF behavior in `fused.run()`

UDFs behave different when using `fused.types.Tile` than in any other case. When passing a `gpd.GeoDataFrame`. `shapely.Geometry` to bounds:

```python showLineNumbers
fused.run(tile_udf, bounds=bounds)
```

Or passing `X Y Z`:

```python showLineNumbers
fused.run(tile_udf, x=1, y=2, z=3)
```

The `tile_udf` gets tiled and run on [Web mercator XYZ tiles](https://en.wikipedia.org/wiki/Tiled_web_map) and then combined back together to speed up processing rather than executing a single run. This is in contrast to most other UDFs (either using no `bounds` input at all or using `bounds: fused.types.Bounds`) which run a single run across the given input.

---

// File: core-concepts/cache

# Caching

_This pages explains how caching makes Fused more responsive & some best practices for making the best use of it_

## Caching Basics

The goal of Fused is to make developing & running code faster for data scientists. This is done by using [efficient file formats](/core-concepts/data-ingestion/file-formats/) and making [UDFs simple to run](/core-concepts/run-udfs/). On top of those, Fused relies heavily on caching to make recurring calls much faster.

At a high level, caching is storing the output of a function run with some input so we can directly access the result next time that function is called with the same input, rather than re-computing it to save time & processing cost.

![Function + Input run](/img/core-concepts/caching/function_input_run_cache.png)

_The first run of a [Function + Input] is processed, but the next time that same combination is called, the result is retrieved much faster_

As soon as either the function or the inputs change however, the output needs to be processed (as the result of this new combination has not been computed before)

![Different Function + Input run](/img/core-concepts/caching/running_different_function.png)

Fused uses a few different types of cache, but they all work in this same manner


## Caching any Python function: [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache)

### Locally

Any Python function, either inside a UDF or even locally on your machine can be cached using the [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) decorator around it:

```python {5} showLineNumbers
# This works locally on your machine
import python
from datetime import datetime

@fused.cache(cache_max_age='30s')
def telling_time():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return current_time

telling_time()
```

![Fused_cache_function_locally](/img/core-concepts/caching/fused_cache_function.png)

As seen in the debug logs, your cached data will be saved under `/tmp/cached_data/tmp/` locally.

:::tip
Similar to how this works with [`fused.run()`](/python-sdk/top-level-functions/#run), you can overwrite [`cache_max_age`](/core-concepts/cache/#defining-your-cache-lifetime-cache_max_age) when executing your function directly:

```python showLineNumbers
telling_time(cache_max_age="0s") # Overwrite cache duration to be 0s, i.e. no caching
```
:::

### Inside a UDF

This also works inside a UDF by passing [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) decorator around any function:

```python {5} showLineNumbers
@fused.udf
def udf():
    import pandas as pd

    @fused.cache
    def load_data(i):
        # Do heavy processing here
        return pd.DataFrame({"id": [i]})

    df_first = load_data(i=1)
    df_first_repeat = load_data(i=1)
    df_second = load_data(i=2)

    return pd.concat([df_first, df_first_repeat, df_second])
```

Under the hood:
- The first time Fused sees the function code and parameters, Fused runs the function and stores the return value in a cache.
    - This is what happens in our example above, line 10: `load_data(i=1)`
- The next time the function is called with the same parameters and code, Fused skips running the function and returns the cached value
    - Example above: line 11, `df_first_repeat` is the same call as `df_first` so the function is simply retrieved from cache, not computed
- As soon as the function _or_ the input changes, Fused re-computes the function
    - Example above: line 12 as `i=2`, which is different from the previous calls

**Implementation Details**

A function cached with [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) is:
- Cached for [12h by default](/python-sdk/top-level-functions/#fusedcache) (can be changed with [`cache_max_age`](/core-concepts/cache/#defining-your-cache-lifetime-cache_max_age))
- Stored as pickle file on `mount/`

### Benchmark: With / without [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache)

Using [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) is mostly helpful to cache functions that have long, repetitive calls like for example loading data from slow file formats.

Here are 2 simple UDFs to demonstrate the impact:
- `without_cache_loading_udf` -> Doesn't use cache
- `with_cache_loading_udf` -> Caches the loading of a CSV

```python {6} showLineNumbers
@fused.udf
def without_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    # @fused.cache
    def load_ais_data(ais_path: str):
        import pandas as pd
        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

and the same:
```python {6} showLineNumbers
@fused.udf
def with_cache_loading_udf(
    ship_length_meters: int = 100,
    ais_path: str = "s3://fused-users/fused/file_format_demo/AIS_2024_01_01_100k_points.csv"
):
    @fused.cache
    def load_ais_data(ais_path: str):
        import pandas as pd
        return pd.read_csv(ais_path)

    ais = load_ais_data(ais_path)

    return ais[ais.Length > ship_length_meters]
```

Comparing the 2:

![Caching benchmark](/img/core-concepts/caching/caching_benchmark.png)

### Best Practices: [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache)

Caching a local function or inside a UDF works best for:
- Loading data from slow formats (CSV, Shapefile)
- Repetitive operations that can take a long amount of processing

However, be wary of relying on [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) to load very large (>10Gb) datasets as cache is only stored for a few hours by default and is over-written each time you change the cached function or inputs.

Look into [ingesting your data](/core-concepts/data-ingestion/) in partitioned [cloud native formats](/core-concepts/data-ingestion/file-formats/) if you're working with large datasets.

:::tip
The line between when to ingest your data or use `@fused.cache` to load data inside a UDF is a bit blurry. Check [this section](/core-concepts/data-ingestion/why-ingestion/#using-cache-as-a-single-use-ingester) for more
:::

## Caching a UDF

While [`@fused.cache`](/python-sdk/top-level-functions/#fusedcache) allows you to cache functions locally or _inside_ UDFs, UDFs ran with [`fused.run()`](/python-sdk/top-level-functions/#run) are cached by default on Fused server.

You can create a token for your UDF in Python by first saving your UDF to Fused server:

```python showLineNumbers
@fused.udf
def slow_caching_udf():
    import time
    import pandas as pd
    
    time.sleep(5)
    
    return pd.DataFrame({"output": ["I'm done running my long task!"]})

fused.run(slow_caching_udf)
```

We can demonstrate this caching with a UDF that has a `time.sleep(5)` in it. Running this same UDF twice:

![Cached_fused_run_udf](/img/core-concepts/caching/cached_fused_run_udf.png)

This means that UDFs that are repeatably called with `fused.run()` become much more responsive. Do remember once again that UDFs are recomputed each time either anything in the UDF function or the inputs change!

**Implementation Details**

Cached UDF are:
- Stored for 90d by default (see [Python SDK](/python-sdk/top-level-functions/#fusedudf) for more details)
- Stored on S3
- You can overwrite the cache age by passing [`cache_max_age`](/core-concepts/cache/#defining-your-cache-lifetime-cache_max_age) either when defining the UDF with [`@fused.udf(cache_max_age)`](/python-sdk/top-level-functions/#fusedudf) or when running the UDF with [`fused.run(udf, cache_max_age)`](/python-sdk/top-level-functions/#run)

## Advanced

### Caching & [`bounds`](/core-concepts/filetile/#the-bounds-object)

Pass [`bounds`](/core-concepts/filetile/#the-bounds-object) to make the output unique to each [Tile](/core-concepts/filetile/#tile).

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def fn(bounds):
        # convert bounds to tile
        common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
        zoom = common_utils.estimate_zoom(bounds)
        tile = common_utils.get_tiles(bounds, zoom=zoom)
        return tile

    return fn(bounds)
```

Note that this means that if you're running your Tile UDF in Workbench, every time you pan around on the [map](/workbench/udf-builder/map/) you will cache a new file

For this reason, it's recommend to keep cache for tasks that _aren't_ dependent on your `bounds` when possible, for example:

```python {5} showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds=None):

    @fused.cache
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    # convert bounds to tile
    common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = common_utils.estimate_zoom(bounds)
    tile = common_utils.get_tiles(bounds, zoom=zoom)

    # Loading of our slow data does not depend on bounds so can be cached even if we pan around
    gdf = loading_slow_geodataframe()
    gdf_in_bounds = gdf[gdf.geometry.within(tile.iloc[0].geometry)]

    return gdf_in_bounds
```

{/* TODO: Commenting this out for now, will add back once these are implemented */}
### Defining your cache lifetime: `cache_max_age`

You can define how long to keep your cache data for with `cache_max_age`. Valid time units include:
- Seconds (`s`)
- Minutes (`m`)
- Hours (`h`)
- Days (`d`)

Examples: `24h` (24 hours), `30m` (30 minutes), `10s` (10 seconds)


:::note
**Cache Behavior:** UDF executions are cached by default. To bypass caching and ensure fresh results, pass `cache_max_age="0s"` in your `fused.run()` call.
:::

```python showLineNumbers
@fused.udf
def udf():

    @fused.cache(
        cache_max_age="24h" # Your cache will stay available for 24h
    )
    def loading_slow_geodataframe(data_path):
        ...
        return gdf

    return gdf
```

This also works with [`@fused.udf()`](/python-sdk/top-level-functions/#fusedudf) & [`fused.run()`](/python-sdk/top-level-functions/#run):
```python showLineNumbers
@fused.udf(cache_max_age="24h") # This UDF will be cached for 24h after its initial run
def udf(path):

    gdf = gpd.read_file(path)

    return gdf
```

This UDF will be cached from the moment it's executed with `fused.run(udf)` for as long as is defined in `cache_max_age`:

```python showLineNumbers
fused.run(udf)
```

If you run `fused.run(udf)` again with no changes to `udf`, then for the next 24h `fused.run(udf)` will return a cached result. This is both faster & cheaper (saving on compute) while giving you control over how long to keep your cache for.

{/* NOTE: This only works with a token for now! */}
You can also overwrite the `cache_max_age` defined in `udf` when running your UDF:

```python showLineNumbers
fused.run(udf, cache_max_age="12h")
```

`udf` results will now only be cached for `12h`, even if `udf` was defined with a `cache_max_age` of `24h`:

The age of your cache is defined as follows:
- By default a UDF is cached for 90 days.
- If `@fused.udf(cache_max_age)` is defined, this new cache age overwrites the default.
- If `fused.run(udf, cache_max_age)` is passed, then this cache age takes priority over default & `@fused.udf(cache_max_age)`

{/* TODO: This needs to be linked to the Python SDK page once its updated */}
{/* :::tip
    Read more about this in the [Python SDK page on `@fused.cache`](/python-sdk/top-level-functions/#fusedcache)
::: */}

{/* TODO: Don't want to expose this just yet  */}
{/* ### Caching on `local` or `mount` storage

Your cache is by default saved to `mount/`. This means if you or a team mate runs the same function with the same input they can also leverage your previously cached functions

You can however decide to only have this available to each individual instance by passing `storage="local"`. There are a few benefits / reasons to do this:
- Using `@fused.cache` on regular Python functions when developing locally:
    - You don't need to even use [`fused.run()`](/core-concepts/run-udfs/run-small-udfs/#fusedrun), you could use `@fused.cache` without ever using any other Fused functionality

```python {5} showLineNumbers
@fused.cache(
    storage="local"
)
def local_function_load(data_path):
    ...
    return gdf

gdf = local_function_load()
```

:::tip
    Read more about this in the [Python SDK page on `@fused.cache`](/python-sdk/top-level-functions/#fusedcache)
::: */}

---

// File: core-concepts/async

import LinkButtons from "@site/src/components/LinkButtons.jsx";
import CellOutput from "@site/src/components/CellOutput.jsx";
import {BokehFigure, PlotlyFigure} from "@site/src/components/Plotting.jsx";
import Tag from '@site/src/components/Tag'


A UDF can be called asynchronously using the [async/await](https://docs.python.org/3/library/asyncio.html) syntax. A common implementation is to call a UDF multiple times in parallel with different parameters then combine the results.

:::note
    Setting `sync=False` in `fused.run` is intended for asynchronous calls when running in the cloud with `engine='remote'`. The parameter has no effect if the UDF is ran in the local environment with `engine='local'`.
:::

To illustrate this concept, let's create a simple UDF and save it as `udf_to_run_async` in the workbench:

```python showLineNumbers
@fused.udf
def udf(date: str='2020-01-01'):
    import pandas as pd
    import time
    time.sleep(2)
    return pd.DataFrame({'selected_date': [date]})
```
:::note
    We can not pass a UDF object directly to `fused.run`. Asynchronous execution is only supported for saved UDFs specifed by name or token.
:::

We can now invoke the UDF asynchronously for each date in the `dates` list and concatenate the results:

```python showLineNumbers
async def parent_fn():
    import pandas as pd
    import asyncio

    # Parameter to loop through
    dates = ['2020-01-01', '2021-01-01', '2022-01-01', '2023-01-01']

    # Invoke the UDF as coroutines
    promises_dfs = []
    for date in dates:
        df = fused.run("udf_to_run_async", date=date, engine='remote', sync=False)
        promises_dfs.append(df)

    # Run concurrently and collect the results
    dfs = await asyncio.gather(*promises_dfs)
    return pd.concat(dfs)
```



:::note
[nest_asyncio](https://pypi.org/project/nest-asyncio/) might be required to run UDFs async from Jupyter Notebooks.
```python showLineNumbers
!pip install nest-asyncio -q
import nest_asyncio
nest_asyncio.apply()
```
:::

---

// File: core-concepts/onprem

Fused offers an on-prem version of the application in a Docker container. The container runs in your computing environment (such as AWS, GCP, or Azure) and your data stays under your control.

The container image is currently distributed via a private release. Email `info@fused.io` for access.

## Fused On-Prem Docker Installation Guide

![On prem](/img/advanced/on_prem_diagram.png)

_Diagram of the System Architecture_

### 1. Install Docker

Follow these steps to install Docker on a bare-metal environment:

Step 1: Update System Packages

Ensure your system is up-to-date:
```bash
sudo apt update && sudo apt upgrade -y
```

Step 2: Start & Enable Docker
```bash
sudo apt install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable docker
sudo systemctl start docker
```

Step 4: Add Docker Permission to local user (after this command is run, the shell session must be restarted)
```bash
sudo usermod -G docker $(whoami)
```

Step 5: Configure Artifact Registry
```bash
gcloud auth configure-docker us-west1-docker.pkg.dev
```

### 2. Install Dependencies and Create Virtual Environment

Step 1: Install pip
```bash
sudo apt install python3-pip python3.11-venv
```

Step 2: Create virtual environment
```bash
python3 -m venv venv
```

Step 3: Activate virtual environment
```bash
source venv/bin/activate
```

Step 4: Install Fused and dependencies
```bash
pip install pandas ipython https://fused-magic.s3.us-west-2.amazonaws.com/fused-1.14.1.dev2%2B2c8d59a-py3-none-any.whl
```

### 3. Configure Fused on the Docker container

Run the following in a Python environment within the container to configure the on-prem profile. The Fused team will provide values specific to your account via secure communication.

```python showLineNumbers
import fused

fused.options.base_url = "***"
fused.options.auth.client_id = "***"
fused.options.auth.client_secret = "***"
fused.options.auth.audience = "***"
fused.options.auth.oauth_token_url = "***"
fused.options.auth.authorize_url = "***"

fused.options.save()
```

The code above only needs to be run once. After this is complete, Fused will use the local configuration for future batch jobs.

Note: if Fused has already been configured for batch jobs, you may need to remove the local `~/.fused` directory before running the above code.


### 4. Authenticate an individual Fused user account

Step 1: Start a Python shell
```bash
python
```
Step 2: Obtain credentials URL

```python showLineNumbers
import fused
credentials = fused.api.NotebookCredentials()
credentials.url
```

Step 3:
Go to the credentials URL from the prior step in a web browser. Copy the code that is generated and paste into Python.
```python showLineNumbers
credentials.finalize(code="xxxxxxxxxxxxxxx")
```

### 5. Create Google Cloud service account key and add to Fused

Step 1:
In Google Cloud Console, go to `IAM & Admin > Service Accounts`. Select the service account you want to use, click on the three dots on the right, and select `Manage Keys`. Choose JSON and download the key.

Step 2:
Login to the [Fused workbench environment settings](https://www.fused.io/workbench/settings/environment). Click `Add new secret`. For name use `gcs_fused` and for value paste the contents of the JSON key file.

### 6. Run Fused API: Test UDF

{/* TODO: We need to add section that this writes in S3, so if using this on GCS, then we need communicate to user that they shouldn't return anything rather write their data somewhere else */}

Step 1: Open [Fused Workbench](/workbench/udf-builder/code-editor/), create a "New UDF" and copy this UDF to Workbench:

```python showLineNumbers
@fused.udf
def udf(datestr=0):
  import loguru
  loguru.logger.info(f'hello world {datestr}')
```

Step 2: Rename this UDF to "hello_world_udf" & Save

![Hello World UDF](/img/advanced/hello_world_udf.png)

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF from Python

```python showLineNumbers
import fused

fused.api.FusedAPI()

my_udf = fused.load("hello_world_udf") # Make sure this is the same name as the UDF you saved
job = my_udf(arg_list=[1, 2])
fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1"
  ]
)

job_status = job.run_remote()
job_status.run_and_tail_output()
```

Optionally, to mount a filestore volume to the node that runs the job, add the following to the `additional_docker_args`. This assumes that filestore is mounted at `/mnt/cache` on the host machine.
```python showLineNumbers
additional_docker_args=["-v", "/mnt/cache:/mnt/cache"]
```

### 7. Run Fused API: Example with ETL Ingest UDF

Now that we've tested a simple UDF we can move to a more useful UDF

Step 1: Open [Fused Workbench](/workbench/udf-builder/code-editor/), create a "New UDF" and copy this UDF to Workbench:

:::note
You'll need a GCS Bucket to save this to, pass it to `bucket_name` in the UDF definition for now
:::

```python {2,13} showLineNumbers
@fused.udf
def udf(datestr: str='2001-01-03', res:int=15, var='t2m', row_group_size:int=20_000, bucket_name:str):
  import pandas as pd
  import h3
  import xarray
  import io
  import pyarrow.parquet as pq
  import pyarrow as pa
  import gcsfs
  import json

  path_in=f'https://storage.googleapis.com/gcp-public-data-arco-era5/raw/date-variable-single_level/{datestr.replace("-","/")}/2m_temperature/surface.nc'
  path_out=f"gs://{bucket_name}/data/era5/t2m/datestr={datestr}/0.parquet"

  if len(fused.api.list(path_out))>0:
    df = pd.DataFrame([{'status':'Already Exist.'}])
    print("Already exists")
    return None

  def get_data(path_in, path_out):
    path = fused.download(path_in, path_in)
    xds = xarray.open_dataset(path)
    df = xds[var].to_dataframe().unstack(0)
    df.columns = df.columns.droplevel(0)
    df['hex'] = df.index.map(lambda x:h3.api.basic_int.latlng_to_cell(x[0],x[1],res))
    df = df.set_index('hex').sort_index()
    df.columns=[f'hour{hr}' for hr in range(24)]
    df['daily_min'] = df.iloc[:,:24].values.min(axis=1)
    df['daily_max'] = df.iloc[:,:24].values.max(axis=1)
    df['daily_mean'] = df.iloc[:,:24].values.mean(axis=1)
    return df

  df = get_data(path_in, path_out)

  memory_buffer = io.BytesIO()
  table = pa.Table.from_pandas(df)
  pq.write_table(table, memory_buffer, row_group_size=row_group_size, compression='zstd', write_statistics=True)
  memory_buffer.seek(0)

  gcs = gcsfs.GCSFileSystem(token=json.loads(fused.secrets['gcs_fused']))
  with gcs.open(path_out, "wb") as f:
    f.write(memory_buffer.getvalue())

  print(df.shape)
  return None
```

Step 2: Rename this UDF to "ETL_Ingest"

![Ingest ETL in workbench](/img/advanced/ETL_udf.png)

Step 3: Start a Python shell
```bash
python
```

Step 4: Run UDF

```python showLineNumbers
import fused
import pandas as pd
fused.api.FusedAPI()

udf = fused.load("ETL_ingest")
start_datestr='2020-02-01'; end_datestr='2020-03-01';
arg_list = pd.date_range(start=start_datestr, end=end_datestr).strftime('%Y-%m-%d').tolist()
job = udf(arg_list=arg_list)

fused.api.FusedDockerAPI(
  set_global_api=True,
  is_gcp=True,
  repository="us-west1-docker.pkg.dev/daring-agent-375719/fused-job2/fused-job2",
  additional_docker_args=[
    "-e","FUSED_SERVER_ROOT=https://app.fused.io/server/v1", "-v", "./.fused:/root/.fused"
  ]
)

job_status = job.run_remote()
job_status.run_and_tail_output()
```



## Commands

### `run-config`

`run-config` runs the user's jobs. The job configuration can be specified either on the command line, as a local file path, or as an S3/GCS path. In all cases the job configuration is loaded as JSON.

```
Options:
  --config-from-gcs FILE_NAME   Job step configuration, as a GCS path
  --config-from-s3 FILE_NAME    Job step configuration, as a S3 path
  --config-from-file FILE_NAME  Job step configuration, as a file name the
                                application can load (i.e. mounted within the
                                container)
  -c, --config JSON             Job configuration to run, as JSON
  --help                        Show this message and exit.
```

### `version`

Prints the container version and exits.

## Environment Variables

The on-prem container can be configured with the followin environment variables.

- `FUSED_AUTH_TOKEN`: Fused token for the licensed user or team. When using the FusedDockerAPI, this token is automatically retrieved.
- `FUSED_DATA_DIRECTORY`: The path to an existing directory to be used for storing temporary files. This can be the location a larger volume is mounted inside the container. Defaults to Python's temporary directory.
- `FUSED_GCP`: If "true", enable GCP specific features. Defaults to false.
- `FUSED_AWS`: If "true", enable AWS specific features. Defaults to false.
- `FUSED_AWS_REGION`: The current AWS region.
- `FUSED_LOG_MIN_LEVEL`: Only logs with this level of severity or higher will be emitted. Defaults to "DEBUG".
- `FUSED_LOG_SERIALIZE`: If "true", logs will be written in serialized, JSON form. Defaults to false.
- `FUSED_LOG_AWS_LOG_GROUP_NAME`: The CloudWatch Log Group to emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_LOG_AWS_LOG_STREAM_NAME`: The CloudWatch Log Stream to create and emit logs to. Defaults to not using CloudWatch Logs.
- `FUSED_PROCESS_CONCURRENCY`: The level of process concurrency to use. Defaults to the number of CPU cores.
- `FUSED_CREDENTIAL_PROVIDER`: Where to obtain AWS credentials from. One of "default" (default to ec2 on AWS, or none otherwise), "none", "ec2" (use the EC2 instance metadata), or "earthdata" (use EarthData credentials in `FUSED_EARTHDATALOGIN_USERNAME` and `FUSED_EARTHDATALOGIN_PASSWORD`).
- `FUSED_EARTHDATALOGIN_USERNAME`: Username when using earthdata credential provider, above.
- `FUSED_EARTHDATALOGIN_PASSWORD`: Password when using earthdata credential provider, above.
- `FUSED_IGNORE_ERRORS`: If "true", continue processing even if some computations throw errors. Defaults to false.
- `FUSED_DISK_SPACE_GB`: Maximum disk space available to the job, e.g. for temporary files on disk, in gigabytes.

## Connecting an encrypted S3 bucket

To connect an encrypted S3 bucket, access to both the bucket and the KMS key is required. The KMS key must be in the same region as the bucket. The following steps are required to connect an encrypted S3 bucket:

- Configure KMS policy
```json
{
  "Sid": "AllowCrossAccountUseOfKMS",
  "Effect": "Allow",
  "Principal": {
    "AWS": "arn:aws:iam::<FUSED_ACCOUNT>:role/<FUSED_ROLE_NAME>"
  },
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:GenerateDataKey*",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}
```

- Configure S3 bucket policy
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::<FUSED_ACCOUNT>:role/<FUSED_ROLE_NAME>"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::<BUCKET_NAME>",
        "arn:aws:s3:::<BUCKET_NAME>/*"
      ]
    }
  ]
}
```

---

// File: core-concepts/content-management/download

# Download

Download remote files to the local system to make them available to UDFs across runs. Files are written to a [disk](/core-concepts/content-management/file-system/#mntcache-disk) shared across all UDFs in an organization.

## `fused.download`

Call [`fused.download`](/python-sdk/top-level-functions/#download) with the file's endpoint in the `url` parameter and the local file name in the `file_path` parameter. The function will download the file and return the file path, which other functions can reference.

This example downloads a `.zip` file then returns it as a `GeoDataFrame`.

```python showLineNumbers
@fused.udf
def udf(url='https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip'):
    import fused
    import geopandas as gpd

    # Download zip file
    out_path = fused.download(url=url, file_path='out.zip')

    # Show path to file
    print(out_path)

    return gpd.read_file(out_path)
```

:::info
The `download` function sets a lock to ensure the download happens only once, in case the UDF is called concurrently.
:::

---

// File: core-concepts/content-management/environment-variables

Save constants to an `.env` file to make them available to UDFs as environment variables. You should use the [secrets manager](/workbench/account/#secrets-management) for sensitive information like API keys.

First, run a [File UDF](/core-concepts/filetile/#single-file) that sets variables in an `.env` file in the [`/mnt/cache/` directory](/core-concepts/content-management/file-system/#mntcache-disk).

```py
@fused.udf
def udf():
    env_vars = """
    MY_ENV_VAR=123
    """

    # Path to .env file in disk file system
    env_file_path = '/mnt/cache/.env'

    # Write the environment variables to the .env file
    with open(env_file_path, 'w') as file:
        file.write(env_vars)
```

Now, any UDF can load the values from `.env` as environment variables with the [`load_dotenv`](https://pypi.org/project/python-dotenv/) and access them with [os.getenv](https://www.geeksforgeeks.org/python-os-getenv-method/).

```py
@fused.udf
def udf():
    from dotenv import load_dotenv

    # Load environment variable
    env_file_path = '/mnt/cache/.env'
    load_dotenv(env_file_path, override=True)

    # Access environment variable
    print(f"Updated MY_ENV_VAR: {os.getenv('MY_ENV_VAR')}")
```

---

// File: core-concepts/content-management/file-system

Fused provides two file systems to make files accessible to all UDFs: an [S3 bucket](/core-concepts/content-management/file-system/#fd-s3-bucket) and a [disk](/core-concepts/content-management/file-system/#mntcache-disk). Access is scoped at the organization level.

## `fd://` S3 bucket

Fused provisions a private S3 bucket namespace for your organization. It's ideal for large-scale, cloud-native, or globally accessible datasets, such as ingested tables, GeoTIFFs, and files that need to be read outside of Fused.

Use the [File explorer](/workbench/file-explorer/) to browse the bucket and see its full path.

import ImageCm from '@site/static/img/workbench_s3file.png';

<div style={{textAlign: 'center'}}>
<img src={ImageCm} alt="File" style={{width: '60%'}} />
</div>

Fused utility functions may reference it with the `fd://` alias.

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
    output="fd://census/ca_bg_2022/",
).run_remote()
```

## `/mnt/cache` disk

`/mnt/cache` is the path to a mounted disk to store files shared between UDFs. This is where `@fused.cache` and [`fused.download`](/core-concepts/content-management/download/) write data. It's ideal for files that UDFs need to read with low-latency, downloaded files, the output of cached functions, access keys, `.env`, and ML model weights.

UDFs may interact with the disk as with a local file system. For example, to list files in the directory:

```python showLineNumbers
@fused.udf
def udf():
    import os
    for each in os.listdir('/mnt/cache/'):
        print(each)
```

### Troubleshooting

If you encounter the following error, it means `/mnt/cache` is not yet configured for your environment. To resolve this issue, please contact the Fused team to enable it.

```text
Error: No such file or directory: '/mnt/cache/'
```

---

// File: core-concepts/content-management/git

import Tag from '@site/src/components/Tag'

<Tag color="#3399ff">Enterprise</Tag> _This feature is accessible to organizations with a Fused Enterprise subscription._

Teams use the GitHub integration to share UDFs and load [util Modules](/core-concepts/write/#utils-module) from private GitHub repos. When an organization sets up GitHub integration, team members will see UDFs in the linked repos under the "Team UDFs" tab of the [UDF Catalog](/workbench/udf-catalog/). Users may "duplicate" the UDFs to make changes.

import gh1 from '@site/static/img/workbench/workbench_catalog.png';

<div style={{textAlign: 'center'}}>
<img src={gh1} alt="File" style={{}} />
</div>


## Configuring Github integration

The Fused GitHub app activates a webhook to listen to changes to the `main` branch of the target repository. When a change is detected, Fused syncs UDFs into the UDF Catalog.

1. Create a repo. There's no enforced repo structure because Fused scans the entire repo for UDFs, although the [Public UDFs repo](https://github.com/fusedio/udfs) may serve as guideline.

2. Install the Fused GitHub app for your organization. Navigate to [this GitHub URL](https://github.com/apps/fused-io), click "Configure", and select the GitHub organization that contains the repository.

import gh2 from '@site/static/img/gh2.png';

<div style={{textAlign: 'center'}}>
<img src={gh2} alt="File" style={{width: 400}} />
</div>

3. Scope the app to the target repository. It's recommended to select only the specific repo.

import gh3 from '@site/static/img/gh3.png';

<div style={{textAlign: 'center'}}>
<img src={gh3} alt="File" style={{width: 400}} />
</div>


4. Once the above is complete, let the Fused team know the full repo path so they configure it for your organization.

5. Confirm the integration is enabled by checking that repo UDFs appear under the "Team UDFs" tab in the UDF Catalog.

## Enabling GitHub integration

To enable the GitHub integration, click the "Preferences" icon in the top left corner, and enable "GitHub Integration".

import ImageGit from '@site/static/img/core-concepts/enable_git.png';

<div style={{textAlign: 'center'}}>
<img src={ImageGit} alt="File" style={{width: 700}} />
</div>


<br></br>

Once it's enabled, the "Push" option should appear in the UDF's dropdown menu.

<br></br>

import ImageGitDropdown from '@site/static/img/core-concepts/github_dropdown.png';

<div style={{textAlign: 'center'}}>
<img src={ImageGitDropdown} alt="File" style={{width: 500}} />
</div>


{/* Note: Image not within collapsible component, for consistency */}
{/* TODO: Update screenshot when UI capitalization is fixed */}

---

// File: core-concepts/data-ingestion/why-ingestion

# Why we need data Ingestion

_This page will give you all the tools to make your data fast to read to make your UDFs more responsive._

## What is this page about?

The whole purpose of Fused is to speed up data science pipelines.
To make this happen we need the data we're working with to be responsive, regardless of the dataset. The ideal solution is to have all of our data sitting in RAM right next to our compute, but in real-world applications:

- Datasets (especially geospatial data) can be in the Tb or Pb range which rarely fit in storage, let alone RAM
- Compute needs to be scaled up and down depending on workloads.

One solution to this is to build data around **Cloud Optimized formats**: Data lives in the cloud but also leverages file formats that are fast to access. Just putting a `.zip` file that needs to be uncompressed at every read on an S3 bucket is still very slow. Our ingested data should be:

- **On the cloud** so dataset size doesn't matter (AWS S3, Google Cloud Storage, etc.)
- **Partitioned** (broken down into smaller pieces that are fast to retrieve so we can load only sections of the dataset we need)

This makes it fast to read for any UDF (and any other cloud operation), so developing UDFs in [Workbench UDF Builder](/workbench/udf-builder/) & [running UDFs](/core-concepts/run-udfs/) is a lot faster & responsive!

## Benchmark & Demo

We're going to use a real-world example to show the impact of using different file formats & partitioning to make data a lot faster to access. For this demo, we'll be using AIS (Automatic Identification System) data as for our [Dark Vessel Detection example](/user-guide/examples/dark-vessel-detection/).
These are points which represent the location of boats at any given time. We'll be using free & open data from [NOAA Digital Coast](https://www.coast.noaa.gov/digitalcoast/tools/ais.html).

import ImgAISNoaa from '@site/docs/user-guide/examples/AIS_noaa_coast_portal.png';

<div style={{textAlign: 'center'}}>
<img src={ImgAISNoaa} alt="Dark Vessel Detection data pipeline" style={{width: 600}} />
</div>

The NOAA Digital Coast platform gives us 1 zip file per day with the location of every boat with an AIS transponder as CSV (once unzipped).

We'll download 1 day and upload it as a CSV to Fused server with [`fused.upload()`](/python-sdk/api-reference/api/#upload):

```python showLineNumbers
@fused.udf
def save_ais_csv_to_fused_udf():
    """Downloading a single day of data to Fused server"""
    import requests
    import zipfile
    import pandas as pd

    response = requests.get("https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/AIS_2024_01_01.zip")

    with open("data.zip", "wb") as f:
        f.write(response.content)

    with zipfile.ZipFile("data.zip", "r") as zip_ref:
        zip_ref.extractall("./data")

    csv_df = fused.api.upload("./data/AIS_2024_01_01.csv", "fd://demo_reading_ais/AIS_2024_01_01.csv")
    print(f"Saved data to fd://demo_reading_ais/")

    return pd.DataFrame({"status": 'Done'})
```

And simply running this UDF:

```python
fused.run(save_ais_csv_to_fused_udf)
```

We can check that our CSV was properly ingested with File Explorer by navigating to `fd://demo_reading_ais/`:

import ImgFileExplVerification from '@site/docs/core-concepts/data-ingestion/FileExplorer_Demo_AIS.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFileExplVerification} alt="Our AIS CSV properly uploaded on File Explorer" style={{width: 600}} />
</div>

That's one big CSV.

But opening it on its own doesn't do all that much for us. We're going to create 3 [UDFs](/core-concepts/write/) to showcase a more real-world application: Opening the dataset and returning a subset inside a bounding box. We'll do this 3 different ways to compare their execution time:
- 1. From the default CSV
- 2. From the same data but saved a `.parquet`
- 3. Ingesting this data with `fused.ingest()` and reading it from our ingested data

Since our AIS data covers the waters around the US, we'll use a simple bounding box covering a small portion of water:

```python showLineNumbers
import shapely

bounds = gpd.GeoDataFrame(geometry=[shapely.box(-81.47717632893792,30.46235012285108,-81.33723531132267,30.58447317149745)])
```

This `bounds` is purposefully small (`print(smaller_bounds.iloc[0].geometry.area)` returns `0.02`) to highlight loading a very large dataset and recover only a small portion of data.

### 1. Reading directly from CSV

Here's a simple UDF to read our CSV in memory and return only points in within our bounding box:

```python showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):
    import geopandas as gpd
    import pandas as pd
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    df = pd.read_csv(path)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 2. Reading from Parquet

First we need to save our AIS data a `.parquet`:

```python showLineNumbers
@fused.udf
def ais_to_parquet():
    import pandas as pd
    # S3 bucket & dir for demo purpose here. Replace with your own
    csv_df = pd.read_csv("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv")
    csv_df.to_parquet("s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet")
    print(f"Saved data (as parquet) to fd://demo_reading_ais/")

    return pd.DataFrame({"status": ['Done']})
```

Here's our updated UDF to read a `.parquet` file:

```python {4,8-9} showLineNumbers
@fused.udf
def from_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.parquet"
):
    import geopandas as gpd
    import pandas as pd
    df = pd.read_parquet(path)
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]
    return bounds_ais
```

### 3. Reading from Fused Ingested GeoParquet

Now, we're going to ingest the parquet file we have to create geo-partitioned files, using `fused.ingest()`. We'll go in more details on how to ingest your own data in [the following section](/core-concepts/data-ingestion/ingestion-your-data/).

```python showLineNumbers
job = fused.ingest(
    "fd://demo_reading_ais/AIS_2024_01_01.parquet",
    "fd://demo_reading_ais/ingested/",
    target_num_chunks=500, # 500 is a rough default to use in most cases. We're not optimizing this value for now
    lonlat_cols=('LON','LAT')
)
```

Ingestion jobs are quite memory hungry so we'll run this on a remote machine as a [large run](/core-concepts/run-udfs/run_large/):

```python
job.run_remote(
    instance_type='r5.8xlarge', # 256GiB RAM machine
)
```

Again using [File Explorer](/workbench/file-explorer/) to inspect our data we can see `fused.ingest()` didn't create 1 file but rather multiple:

import ImgFileExplorerIngested from '@site/docs/core-concepts/data-ingestion/ingested_file_explorer.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFileExplorerIngested} alt="File Explorer view of ingested files" style={{width: 600}} />
</div>

Our ingestion process broke down our dataset into smaller files (making each file easier to access) and a `_sample.parquet` file containing the bounding box of each individual file. This allows us to first intersect our `bounds` with `_sample` and then only open the smaller `.parquet` files we need:

```python showLineNumbers
@fused.udf
def read_ingested_parquet_udf(
    bounds: fused.types.Bounds,
    path: str = "s3://your-bucket/your-dir/file_format_demo/ingested/"
):
    import fused
    import pandas as pd

    # convert bounds to tile
    common_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = common_utils.estimate_zoom(bounds)
    tile = common_utils.get_tiles(bounds, zoom=zoom)

    # Built in fused method to reach the `_sample` file and return only bounding box of each parquet holding our points
    df = fused.get_chunks_metadata(path)

    # Only keeping the tiles where our bounds is -> Only need to load actual data inside / touching our bounds
    df = df[df.intersects(tile.geometry.iloc[0])]

    # This is based on Fused's ingestion process
    chunk_values = df[["file_id", "chunk_id"]].values
    rows_df = pd.concat([
        fused.get_chunk_from_table(path, fc[0], fc[1], columns=['geometry'])
        for fc in chunk_values
    ])
    df = rows_df[rows_df.intersects(tile.geometry[0])]
    df.crs = tile.crs
    return df
```

:::tip
    We've implemented a utils function that allows you to more simply read Fused ingested data: [table_to_tile](https://github.com/fusedio/udfs/blob/ca262bef10b2774c4b4b58fc04548863efff2364/public/common/utils.py#L272)

    So instead of re-implementing the above in 2 lines of code you can read your ingested data:
    ```python showLineNumbers
    @fused.udf
    def udf(
        bounds: fused.types.Bounds, path: str='s3://your-bucket/your-dir/demo_reading_ais/ingested/'):
        utils = fused.load('https://github.com/fusedio/udfs/tree/bb712a5/public/common/').utils
        df = utils.table_to_tile(bounds, table=path)
        return df
    ```
:::

### Comparing all 3 runs

We'll run each of these 3 methods in a Jupyter notebook using the [`%%time` magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time) to compare their run time:

import ImgComparingAISReadMethods from '@site/docs/core-concepts/data-ingestion/comparing_opening_ais_methods.png';

<div style={{textAlign: 'center'}}>
<img src={ImgComparingAISReadMethods} alt="Comparing all 3 methods of reading AIS" style={{width: 600}} />
</div>

There are a few conclusions to draw here:
- simply saving a CSV to `.parquet` makes files smaller & significantly faster to read just by itself (4x speed gain in this example)
- But proper geo-partitioning takes those gains ever higher (additional 8x speed gain in this specific example)

In short, by ingesting our data with `fused.ingest()` we're trading some up-front processing time for much faster read time. We only need to ingest our data once and every subsequent read will be fast and responsive.


## When is ingestion needed?

You don't _always_ need to ingest your file into a cloud, geo-partitioned format. There are a few situation when it might be simpler & faster to just load your data.
Small files (< 100Mb ) that are fast to open (already in `.parquet` for example) that you only read once (note that it might be read 1x in your UDF but your UDF might be run many times)

Example of data you should ingest: 1Gb `.zip` of shapefile
- `.zip` means you need to unzip your file each time you open it and then read it. This slows down working with the data _every minute_. This results in each individual files (a CSV when unzipped) containing millions of points.
- shapefile contains multiple files, it isn't the fastest to read

Example of data you don't need to ingest: 50Mb `.parquet`
- Even if the data isn't geo-partitioned, loading this data should be fast enough to make any UDF fast

### Using cache as a single-use "ingester"

We could actually significantly speed up the above example where we loaded the AIS data as a CSV without running `fused.ingest()`, by using [cache](/core-concepts/cache/):

```python {9-13} showLineNumbers
@fused.udf
def from_csv_df_udf(
    bounds: fused.types.ViewportGDF,
    path: str = "s3://your-bucket/your-dir/demo_reading_ais/AIS_2024_01_01.csv"
):
    import geopandas as gpd
    import pandas as pd

    @fused.cache
    def load_csv(path):
        import pandas as pd
        return pd.read_csv(path)
    df = load_csv(path)

    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LON, df.LAT))
    bounds_ais = gdf[gdf.geometry.within(bounds.iloc[0].geometry)]

    return bounds_ais
```

The first run of this would still be very slow, but running this a second time would give us a much faster result:

import ImgCSVCached from '@site/docs/core-concepts/data-ingestion/cached_csv_comparison.png';

<div style={{textAlign: 'center'}}>
<img src={ImgCSVCached} alt="Comparing first CSV read to cached CSV read" style={{width: 600}} />
</div>

We're using [`@fused.cache`](/core-concepts/cache/) to cache the result of `load_csv()` which is the same regardless of our `bounds`, so this allows us to save an 'intermediate' result on disk.
There are some limitations to this approach though:
- This cache is emptied after 24h.
- This cache is overwritten any time you change the cached function or its inputs

This approach is only helpful if you want to 1 time explore a new dataset in [UDF Builder](/workbench/udf-builder/) and don't want to wait around for the ingestion run to be done. Beyond that, this will end up being slower (and more frustrating)

---

// File: core-concepts/data-ingestion/Ingest-your-data

import LinkButtons from "@site/src/components/LinkButtons.jsx";
import CellOutput from "@site/src/components/CellOutput.jsx";
import {BokehFigure, PlotlyFigure} from "@site/src/components/Plotting.jsx";
import Tag from '@site/src/components/Tag'

_This guide explains how to use `fused.ingest` to geopartition and load vector tables into an S3 bucket so they can quickly be queried with Fused._

## Ingest vector data

To run an ingestion job on vector data we need:
1. **Input data** - This is could be CSV files, a `.zip` containing shapely files or any other sort of non partitioned data
2. **A cloud directory** - This is where we will save our ingested data and later access it through UDFs

We've built our own ingestion pipeline at Fused that partitions data based on dataset size & location.
Our ingestion process:

1. Uploads the `input`
2. Creates geo-partitions of the input data

This is defined with [`fused.ingest()`](/python-sdk/top-level-functions/#ingest):

```python showLineNumbers
job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract",
)
```

Ingestion jobs often take more than a few seconds and require a lot of RAM (we open the whole dataset & re-partition it), which makes [this a large run](/core-concepts/run-udfs/run_large/#defining-large-jobs) so we're going to use `run_remote()` so our ingestion job can take as long as needed.

To start an ingestion job, call [`run_remote`](/python-sdk/top-level-functions/#jobrun_remote) on the job object returned by [`fused.ingest`](/python-sdk/top-level-functions/#ingest).

```python showLineNumbers
job_id = job.run_remote()
```

:::tip
    Refer to the [dedicated documentation page for `fused.ingest()`](/python-sdk/top-level-functions/#ingest) for more details on all the parameters
:::

{/* TODO: This is a tip on how to read ingested data, useful but should come after */}
Ingested tables can easily be read with the Fused utility function `table_to_tile`, which spatially filters the dataset and reads only the chunks within a specified polygon.

```python showLineNumbers
@fused.udf
def udf(bounds, table="s3://fused-asset/infra/building_msft_us/"):
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    return utils.table_to_tile(bounds, table)
```

The following sections cover common ingestion implementations. It's recommended to run ingestion jobs from a Python Notebook or [this web app](https://www.fused.io/workbench#app/s/i/fa_3sY9SPDK9sqyHoR8NYm14H).

### Ingest a table from a URL

Ingests a table from a URL and writes it to an S3 bucket specified with [`fd://`](/core-concepts/content-management/file-system/#fd-s3-bucket).

```python showLineNumbers
import fused

job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract",
).run_remote()
```

:::info

If you encounter the message
`HTTPError: {'detail': 'Quota limit: Number of running instances'}`, please contact Fused to increase the number of workers in your account.

:::


### Ingest multiple files

```python showLineNumbers
import fused

job_id = fused.ingest(
    input=["s3://my-bucket/file1.parquet", "s3://my-bucket/file2.parquet"],
    output=f"fd://census/dc_tract",
).run_remote()
```

:::warning

To ingest multiple local files, first upload them to S3 with [fused.upload](/python-sdk/top-level-functions/#upload) then specify an array of their S3 paths as the input to ingest.

:::

### Row-based ingestion

Standard ingestion is row-based, where the user sets the maximum number of rows per chunk and file.

Each resulting table has one or more _files_ and each file has one or more _chunks_, which are spatially partitioned. By default, ingestion does a best effort to create the number of files specified by `target_num_files` (default `20`), and the number of rows per file and chunk can be adjusted to meet this number.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    explode_geometries=True,
    partitioning_method="rows",
    partitioning_maximum_per_file=100,
    partitioning_maximum_per_chunk=10,
).run_remote()
```

### Area-based ingestion

Fused also supports area-based ingestion, where the number of rows in each partition is
determined by the sum of their area.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_area",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
).run_remote()
```

### Geometry subdivision

Subdivide geometries during ingestion. This keeps operations efficient when geometries have many vertices or span large areas.

```python showLineNumbers
job_id = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/TRACT/tl_rd22_11_tract.zip",
    output=f"fd://census/dc_tract_geometry",
    explode_geometries=True,
    partitioning_method="area",
    partitioning_maximum_per_file=None,
    partitioning_maximum_per_chunk=None,
    subdivide_start=0.001,
    subdivide_stop=0.0001,
    subdivide_method="area",
).run_remote()
```

### Ingest GeoDataFrame

Ingest a [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html) directly.

```python showLineNumbers
job_id = fused.ingest(
    input=gdf,
    output="s3://sample-bucket/file.parquet",
).run_remote()
```

### Ingest non-geospatial

Ingest a table that doesn't have a spatial component.

```python showLineNumbers
job_id = fused.ingest_nongeospatial(
    input=df,
    output="s3://sample-bucket/file.parquet",
).run_remote()
```

### Ingest with a predefined bounding box schema

Here is an example of an ingestion using an existing partition schema which comes from a previously ingested dataset. This assumes you've already ingested a previous dataset with [`fused.ingest()`](/python-sdk/top-level-functions/#ingest).
This may be useful if you are analyzing data across a time series and want to keep the bounding boxes consistent throughout your analysis.

```python showLineNumbers
@fused.udf
def read_ingested_parquet_udf(path: str = "s3://sample-bucket/ingested_data/first_set/"):
    import fused
    import pandas as pd

    # Built in fused method to reach the `_sample` file and return the bounding boxes of each parquet
    df = fused.get_chunks_metadata(path)

    # Since we want our `partition_schema_input` specified in `ingest()` to be a link to a parquet file containing bounds coordinates, we will save this metadata as a parquet file
    partition_schema_path = path + 'boxes.parquet'
    df.to_parquet(partition_schema_path)

    return partition_schema_path

partition_schema_path = fused.run(read_ingested_parquet_udf)

job_id = fused.ingest(
    input="s3://sample-bucket/file.parquet",
    output="s3://sample-bucket/ingested_data/second_set/",
    partition_schema_input=partition_schema_path
).run_remote()
```

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FirstIngest from './AIS_first_date.png';
import SecondIngest from './AIS_second_date_no_schema.png';
import SecondIngestSchema from './AIS_second_date_schema.png';

<details>
  <summary>Example: Comparing bounding boxes from [ingested AIS](/user-guide/examples/dark-vessel-detection/#3-ingesting-ais-data) data on different days </summary>

    <Tabs className="unique-tabs">
    <TabItem value="first-parquet" label="First Ingestion" default>
        <img src={FirstIngest} alt="The bounding boxes from ingestion of AIS data from 2024-01-01" style={{width: 800}} />
    </TabItem>
    <TabItem value="second-parquet" label="Second Ingestion" default>
        <img src={SecondIngest} alt="The bounding boxes from ingestion of AIS data from 2024-08-01" style={{width: 800}} />
    </TabItem>
    <TabItem value="second-parquet-schema" label="Second Ingestion with Defined Partition Schema" default>

        <img src={SecondIngestSchema} alt="The bounding boxes from ingestion of AIS data from 2024-08-01 with a defined partition schema" style={{width: 800}} />

        :::info
            Some of the bounding boxes have changed sizes because of the absence of data in certain areas.
        :::

    </TabItem>
    </Tabs>
</details>



{/*
TODO: Still need to show people how to read their ingested data
For now we're showing how to ingest, but not how to read ingested data
*/}


## Troubleshooting

As with other Fused batch jobs, ingestion jobs require server allocation for the account that initiates them. If you encounter the following error message, please contact the Fused team to request an increase.

```text
Error: `Quota limit: Number of running instances`
```

---

// File: core-concepts/data-ingestion/file_formats

# File Formats

_This page specifies which File Formats for both raster & vector data we prefer working with at Fused, and why_

## What this page is about

Fused works with any file formats you might have, as all [UDFs](/core-concepts/write/) are running pure Python. This means you can use any file formats you want to process your data.
That being said the goal of Fused is to significantly speed up the workflow for data scientists, by leveraging modern cloud compute infrastructure and simplify it.

Some formats like Shapefile, CSV, JSON, while incredibly versatile, aren't the most appropriate for large datasets (even above a few Gb) and are slow to read / write (we consider anything above 10s of seconds to read to be extremely slow).

:::note
Take a look at [our benchmark](/core-concepts/data-ingestion/why-ingestion/#benchmark--demo) to see a comparison between loading a CSV, GeoParquet & Fused-partitioned GeoParquet to see a concrete example of this
:::

To make the most out of Fused, we recommend [ingesting your data](/core-concepts/data-ingestion/ingestion-your-data/) into the following file formats:

### For rasters (images)

For images (like satellite images) we recommend using **Cloud Optimized GeoTiffs** (COGs). To paraphrase [the Cloud Native Geo guide on them](https://guide.cloudnativegeo.org/cloud-optimized-geotiffs/intro.html):

> Cloud-Optimized GeoTIFF (COG), a raster format, is a variant of the TIFF image format that specifies a particular layout of internal data in the GeoTIFF specification to allow for optimized (subsetted or aggregated) access over a network for display or data reading

:::warning
    Fused does not (yet) have a build-in tool to ingest raster data. We suggest you create COGs yourself, for example by using `gdal`'s [built-in options](https://gdal.org/en/stable/drivers/raster/cog.html#examples) or [`rio-cogeo`](https://cogeotiff.github.io/rio-cogeo/)
    {/* TODO: Link to how to run CLI commands in Fused once we have this up and running */}
:::

Cloud Optimized GeoTiffs have multiple different features making them particularly interesting for cloud native applications, namely:
- **Tiling**: Images are split into smaller tiles that can be individually accessed, making getting only parts of data a lot faster.
- **Overviews**: Pre-rendered images of lower zoom levels of images. This makes displaying images at different zoom levels a lot faster

import ImgCOGE84 from '@site/docs/core-concepts/data-ingestion/e84_cog_smiley_tiled.png';

<div style={{textAlign: 'center'}}>
<img src={ImgCOGE84} alt="A simple overview of Geoparquet benefits" style={{width: 400}} />
</div>
_A simple visual of COG tiling: If we only need the top left part of the image we can fetch only those tiles (green arrows). Image courtesy of [Element 84's blog on COGs](https://element84.com/software-engineering/remote-sensing/cloud-optimized-geotiff-vs-the-meta-raster-format)_

- Element84 wrote a [simple explainer of what Cloud Optimized GeoTiffs are](https://element84.com/software-engineering/remote-sensing/cloud-optimized-geotiff-vs-the-meta-raster-format) with great visuals
- [Cloud Optimized Geotiff spec dedicated website](https://cogeo.org/)
- [Cloud Optimized Geotiff page on Cloud Native Geo guide](https://guide.cloudnativegeo.org/cloud-optimized-geotiffs/intro.html)

### For vectors (tables)

To handle vector data such as `pandas` `DataFrames` or `geopandas` `GeoDataFrames` we recommend using **[GeoParquet](https://github.com/opengeospatial/geoparquet)** files. To (once again) paraphrase the [Cloud Native Geo guide](https://guide.cloudnativegeo.org/geoparquet/):

> GeoParquet is an encoding for how to store geospatial vector data (point, lines, polygons) in Apache Parquet, a popular columnar storage format for tabular data.

import ImgGeoParquet from '@site/docs/user-guide/examples/geoparquet_overview.png';

<div style={{textAlign: 'center'}}>
<img src={ImgGeoParquet} alt="A simple overview of Geoparquet benefits" style={{width: 800}} />
</div>

_Image credit from the [Cloud Native Geo slideshow](https://guide.cloudnativegeo.org/overview.html#/geoparquet)_

:::tip
    Refer to the [next section](/core-concepts/data-ingestion/ingestion-your-data/) to see all the details of how to ingest your data with Fused's built-in `fused.ingest()` to make the most out of geoparquet
:::

- [`geoparquet` Github repo](https://github.com/opengeospatial/geoparquet)
- [`geoparquet` 1 page website](https://geoparquet.org/#intro) with a list of companies & projects involved
- [GeoParquet page on Cloud Native Geo guide](https://guide.cloudnativegeo.org/geoparquet/)

### Additional resources

- Read the [Cloud-Optimized Geospatial Formats Guide](https://guide.cloudnativegeo.org/) written by the [Cloud Native Geo Org](https://cloudnativegeo.org/) about why we need Cloud Native formats
- Friend of Fused Kyle Barron did an [interview about Cloud Native Geospatial Formats](https://cloudnativegeo.org/blog/2024/12/interview-with-kyle-barron-on-geoarrow-and-geoparquet-and-the-future-of-geospatial-data-analysis/). Kyle provides simple introductions to some cloud native formats like `GeoParquet`

---

// File: core-concepts/data-ingestion/index

# Data Ingestion

UDFs work at their best when data is fast to access. This section is on why this is needed, and how to prepare any data you'd like to use to be as efficient as possible when working with Fused.

## Documentation overview

import DocCardList from '@theme/DocCardList';

<DocCardList />

---

// File: core-concepts/run-udfs/run

Fused UDF functions really shine once you start calling them from anywhere. You can call small jobs in 2 main ways:
1.  [`fused.run()`](/python-sdk/top-level-functions/#run) in Python. All you need is the [`fused` Python](/python-sdk/#install) package installed
    - Useful when wanting to run UDF as part of another pipeline, inside another UDF or anywhere in Python / code.
2.  [**HTTP call**](/core-concepts/run-udfs/run-small-udfs/#http-requests) from *anywhere*
    - Useful when you want to call a UDF outside of Python. For example receiving a [dataframe into Google Sheets](/user-guide/out/googlesheets/) or plotting points and images in a [Felt map](/user-guide/out/felt/)
{/* TODO: Might want to mention `fused.submit()` here to be able to run multiple jobs? */}

### Defining "Small" job

"Small" jobs are defined as any job being:
- Less than 120s to execute
- Using less than a few Gb of RAM to run

These jobs run in "real-time" with no start-up time so are quick to run, but with limited resources and time-out if taking too long.

## [`fused.run()`](/python-sdk/top-level-functions/#run)

[`fused.run()`](/python-sdk/top-level-functions/#run) is the simplest & most common way to execute a UDF from any Python script or notebook.

The simplest way to call a public UDF is using a [public UDF name](/core-concepts/run-udfs/run-small-udfs/#public-udf-name) and calling it as: `UDF_` + name. Let's take this UDF that returns [the location of the Eiffel Tower](https://github.com/fusedio/udfs/tree/main/public/Single_point_Eiffel_Tower) in a `GeoDataFrame` as an example:

```python showLineNumbers
import fused
fused.run("UDF_Single_point_Eiffel_Tower")
```


![Simple UDF fused.run() returning a geodataframe](/img/core-concepts/run-udfs/running_simple_eiffel_tower_udf.png)

There are a few other ways to run a UDF:

- [By name from your account](/core-concepts/run-udfs/run-small-udfs/#name-from-your-account)
- [By public UDF name](/core-concepts/run-udfs/run-small-udfs/#public-udf-name)
- [Using a token](/core-concepts/run-udfs/run-small-udfs/#token)
- [Using a `udf` object](/core-concepts/run-udfs/run-small-udfs/#udf-object)
- [From Github URL](/core-concepts/run-udfs/run-small-udfs/#github-url)
- [From git commit hash (most recommended for teams)](/core-concepts/run-udfs/run-small-udfs/#git-commit-hash-recommended-for-most-stable-use-cases)

### _Name_ (from your account)

_When to use: When calling a UDF you made, from your own account._

You can call any UDFs you have made simply by calling it by name (given when you [save a UDF](/core-concepts/write/#save-udfs)).

(Note: This requires authentication)


![Hello World UDF](/img/core-concepts/run-udfs/hello_world_bbox_udf.png)

This UDF can then be run in a notebook locally (granted that you have authenticated):

```python showLineNumbers
fused.run("Hello_World_bbox")
```


![Running Hello World UDF](/img/core-concepts/run-udfs/running_hello_world_bbox_udf.png)

### _Public UDF Name_

_When to run: Whenever you want to run a public UDF for free from anywhere_

Any UDF saved in the [public UDF repo](https://github.com/fusedio/udfs/tree/main) can be run for free.

Reference them by prefixing their name with `UDF_`. For example, [the public UDF `Get_Isochrone`](https://github.com/fusedio/udfs/tree/main/public/Get_Isochrone) is run with `UDF_Get_Isochrone`:

```python showLineNumbers
fused.run('UDF_Get_Isochrone')
```

### _Token_

_When to use: Whenever you want someone to be able to execute a UDF but might not want to share the code with them._

You can get the token from a UDF either in [Workbench](/workbench/) (Save your UDF then click "Share") or returning the token in Python.

Here's a toy UDF that we want others to be able to run, but we don't want them to see the code:

```python showLineNumbers
import fused

@fused.udf()
def my_super_duper_private_udf(my_udf_input):
    import pandas as pd
    # This code is so private I don't want anyone to be able to read it
    return pd.DataFrame({"input": [my_udf_input]})
```

We then need to [save this UDF to Fused server](/core-concepts/write/#save-udfs) to make it accessible from anywhere.

```python showLineNumbers
my_super_duper_private_udf.to_fused()
```

:::note
`my_udf.to_fused()` saves your UDF to your personal user UDFs. These are private to you and your team. You can create a token than anyone (even outside your team) can use to run your UDF but by default these UDFs are private.
:::

We can create a token for this `my_super_duper_private_udf` and share it:

```python showLineNumbers
from fused.api import FusedAPI
api = FusedAPI()
token = api.create_udf_access_token("my_super_duper_private_udf").token
print(token)
```

This would return something like: `'fsh_**********q6X'` (You can recognise this to be a shared token because it starts with `fsh_`)

```python showLineNumbers
fused.run(token, my_udf_input="I'm directly using the token object")
```

or directly:

```python showLineNumbers
fused.run('fsh_**********q6X', my_udf_input="I can't see you're private UDF but can still run it")
```

### _UDF_ object

_When to run: When you're writing your UDF in the same Python file / jupyter notebook and want to refer to the Python object directly. You might want to do this to test your UDF works locally for example_

You may also pass a UDF Python object to `fused.run`:

```python showLineNumbers
# Running a local UDF
@fused.udf
def local_udf():
    import pandas as pd
    return pd.DataFrame({})

# Note that by default fused.run() will run your UDF on the Fused serverless server so we pass engine='local' to run this as a normal Python function
fused.run(local_udf, engine='local')
```

### _Github URL_

{/* Need to emphasise why this isn't recommended  */}
_When to use: [Not recommended] This is useful if you're working on a branch that you control over. This method always points to the last commit on a branch so your UDF can break without you knowing if someone else pushes a new commit or merges & deletes your branch_

```python showLineNumbers
gh_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
fused.run(gh_udf)
```

:::warning
We do NOT recommend you use this approach as your UDF might break if changes are done to it

    Especially using a URL pointing to a `main` branch means that your UDF will change if someone else pushes towards it, in a way that isn't visible to you.

    For that reason we recommend using [git commit hash](/core-concepts/run-udfs/run-small-udfs/#git-commit-hash-recommended-for-most-stable-use-cases) instead

:::

### _Git commit hash_ (recommended for most stable use cases)

_When to use: Whenever you want to rely on a UDF such as in production or when using a UDF as a building block for another UDF._

This is the safest way to use a UDF. Since you're pointing to a specific git commit hash you won't end up with changes breaking your UDF.

Using a git commit hash is the safest, and thus recommended way to call UDFs from Github.

This does mean you need to update the commit where your UDFs are being called if you want to propagate updates. But this gives you the most amount of control.

Let's again take the example of the Simple Eiffel Tower UDF:

import ImgCommitHash from "@site/static/img/core-concepts/run-udfs/commit_hash_demo.png";

![Running Hello World UDF](/img/core-concepts/run-udfs/commit_hash_demo.png)

```python showLineNumbers
commit_hash = "bdfb4d0"
commit_udf = fused.load(f"https://github.com/fusedio/udfs/tree/{commit_hash}/public/Single_point_Eiffel_Tower/")
fused.run(commit_udf)
```

## Execution engines

{/* Need more details for this */}

`fused.run` can run the UDF in various execution modes, as specified by the `engine` parameter either local, realtime, or batch mode.

- `local`: Run in the current process.
- `realtime`: Run in the serverless Fused cloud engine.
- `batch`: Run a long-running job in a Fused server. This must first be enabled for the account.

```python showLineNumbers
fused.run(my_udf, engine="remote")
```

Set `sync=False` to run a UDF [asynchronously](/core-concepts/async/).

### Passing arguments in [`fused.run()`](/python-sdk/top-level-functions/#run)

A typical [`fused.run()`](/python-sdk/top-level-functions/#run) call of a UDF looks like this:

```python showLineNumbers
@fused.udf
def my_udf(inputs: str):
    import pandas as pd
    return pd.DataFrame({"output": [inputs]})

fused.run(my_udf, inputs="hello world")
```

A [`fused.run()`](/python-sdk/top-level-functions/#run) call will require the following arguments:

1. [Mandatory] The first argument needs to be the UDF to run (name, object, token, etc [as seen above](/core-concepts/run-udfs/run-small-udfs/#fusedrun))
2. [Optional] Any arguments of the UDF itself (if it has any). In the example above that's `inputs` because `my_udf` takes `inputs` as argument.
3. [Optional] Any protected arguments as seen in the dedicated [API docs page](/python-sdk/top-level-functions/#run) (if applicable). These include for example:
   - `bounds` -> A geographical bounding box (as a list of 4 point: `[min_x, min_y, max_x, max_y]`) defining the area of interest.
   - `x`, `y`, `z` -> Tile coordinates for tile-based UDF execution.

## Running multiple jobs in parallel

Sometimes you want to run a UDF over a list of inputs (for example running a [UDF that unzips a file over a list of files](/user-guide/examples/dark-vessel-detection/#33---run-this-udf-over-a-year-of-ais-data)). If each run itself is quite small, than you can run all a batch of UDFs over a list of inputs.

Let's use a simple UDF to demonstrate:

```python showLineNumbers
@fused.udf
def udf(val):
    import pandas as pd
    return pd.DataFrame({'val':[val]})
```

Say we wanted to run this `udf` 10 times over `inputs = [0,1,2,3,4,5,6,7,8,9]`:

### [`fused.submit()`](/python-sdk/top-level-functions/#submit)

Fused is built to help you scale your processing to huge datasets and the core of this ability is [`fused.submit()`](/python-sdk/top-level-functions/#submit). You can run 1 UDF over a large amount of arguments:

```python showLineNumbers
job_pool = fused.submit(udf, inputs)
```

This example will quick-off 10 UDF runs in parallel:

```python showLineNumbers
job_pool

>>> <JobPool with 10 jobs [status: 0/10 done]>
```

### Getting [`fused.submit()`](/python-sdk/top-level-functions/#submit) results

A few seconds after, once all our jobs are run we can get our results as a `pd.DataFrame`, using the same example as above:

```python showLineNumbers
results = job_pool.collect_df()
```

![job pool results](/img/core-concepts/run-udfs/job_pool_results.png)

:::tip
Have a look at the dedicated [`fused.submit()`](/python-sdk/top-level-functions/#submit) Python SDK page to read more about all the available methods to access your multi-runs
:::

<details>
  <summary>Simple [`fused.submit()`](/python-sdk/top-level-functions/#submit) Benchmark</summary>

    `fused.submit(udf)` runs all the UDF calls in parallel, making it a helpful tool to run multiple UDFs all at once.

    We can demonstrate this by adding a simple `time.sleep(1)` in our original UDF:
    ```python {4-5} showLineNumbers
    @fused.udf
    def udf(val):
        import pandas as pd
        import time
        time.sleep(1)
        return pd.DataFrame({'val':[val]})
    ```

    In a notebook, we can time how long each cell takes to execute with the [`%%time` magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time)

    ```python showLineNumbers
    # In a jupyter notebook
    %%time
    fused.run(udf, val=1)
    ```

    ![Singe run](/img/core-concepts/run-udfs/fused_run_single_job.png)

    This takes 1.7s: A few ms of overhead to send the UDF to Fused server & run + 1s of `time.sleep(1)`

    Now using `fused.submit()` to run this over 30 UDFs:

    ![30 runs](/img/core-concepts/run-udfs/submit_30jobs.png)

    This takes a few more seconds, but not 30s. `fused.submit()` is a helpful way to scale a single UDF to many inputs in a timely manner.


</details>

## HTTP requests

{/* Need to showcase this in video */}

In the UDF Builder, you can create an HTTP endpoint for a UDF in the ["Snippets"](/workbench/udf-builder/navigation/#share-snippets) section. This generates a unique URL to call the UDF via HTTP requests. The URL is scoped to that UDF only and it can be revoked to disable access. The same can be done with the [Fused Python SDK](/python-sdk/).

### Shared token

To run a UDF via HTTP request, generate a [shared token](/workbench/udf-builder/navigation/#share) and use the provided URL. Manage your account's shared tokens in [fused.io/profile#tokens](https://www.fused.io/workbench/tokens).

<ReactPlayer playsinline className="video__player" loop playing={false} muted controls height="100%" width="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/shared_token.mp4" />

Structure the URL with the `file` path parameter to run as a single batch operation.

```
https://www.fused.io/server/v1/realtime-shared/******/run/file?dtype_out_raster=png
```

To integrate with a tiling service, structure the URL with the `tiles` path parameter, followed by templated `/{z}/{x}/{y}` path parameters. See [Lonboard](/user-guide/out/lonboard/) for an example.

```
https://www.fused.io/server/v1/realtime-shared/******/run/tiles/{z}/{x}/{y}?dtype_out_raster=png
```

### Private token

Calling UDFs with [Bearer authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) requires an account's private token. The URL structure to run UDFs with the private token varies slightly, as the URL specifies the UDF's name and the owner's user account.

```bash
curl -XGET "https://app.fused.io/server/v1/realtime/fused/api/v1/run/udf/saved/user@fused.io/caltrain_live_location?dtype_out_raster=png" -H "Authorization: Bearer $ACCESS_TOKEN"
```

### Specify parameters

When UDF endpoints are called via HTTP requests argument values are specified with [query parameters](https://www.branch.io/glossary/query-parameters/), which require input parameters to be serializable. As such, the UDF should specify the types to cast them to. Read more about [supported types for UDF parameters](/core-concepts/write/#supported-types).

### Response data types

The `dtype_out_vector` and `dtype_out_raster` parameters define the output data type for vector tables and raster arrays, respectively.

- The supported types for vector tables are `parquet`, [`geojson`](/user-guide/out/deckgl/#vector-tile-layers), [`json`](/user-guide/out/deckgl/#h3hexagonlayer), `feather`, [`csv`](/user-guide/out/googlesheets/), [`mvt`](/user-guide/out/mapbox/#2-create-a-mapbox-html-map), `html`, `excel`, and `xml`.
- For raster array: [`png`](/user-guide/out/deckgl/#raster-tile-layers), `gif`, `jpg`, `jpeg`, `webp`, `tif`, and `tiff`.

```
https://www.fused.io/server/v1/realtime-shared/****/run/file?dtype_out_raster=png
```

Read how to structure HTTP endpoints to call the UDF as a [Map Tile & File](/core-concepts/filetile/#call-http-endpoints).

## Caching responses

If a UDF's [cache](/core-concepts/cache/) is enabled, its endpoints cache outputs for each combination of code and parameters. The first call runs and caches the UDF, and subsequent calls return cached data.

---

// File: core-concepts/run-udfs/index

# Running UDFs

There are 2 main ways to run UDFs:

1. On [short](/core-concepts/run-udfs/run-small-udfs/#defining-small-job), small resources requirements (RAM / CPU / storage) jobs 
2. On [longer](/core-concepts/run-udfs/run_large/#defining-large-jobs), higher resources requirements jobs 

## Documentation overview

import DocCardList from '@theme/DocCardList';

<DocCardList />

---

// File: core-concepts/run-udfs/large_jobs

Some jobs require more resources than a few Gb of RAM or take more than a few seconds to run. This section will show how to run such larger jobs
{/* TODO: This should link to a "Best Practices" section about how big should the data processed by a single UDF be: 1 big large jobs, or many small jobs? */}

### Defining "Large" jobs

Large jobs are jobs to run UDFs that are:
- Longer than 120s to run
- Require large resources (more than a few Gb of RAM)
{/* TODO: Do these also include GPU instances with `run_remote`? */}

To run these we use higher-latency instances than for small jobs, but with the ability to specific RAM, CPU count & storage depending on the needs.

This is useful for example when running large ingestion jobs which can require a lot of RAM & storage
{/* TODO: Link to ingestion page once we have it up */}

### A Simple UDF to demonstrate

We'll use the same UDF as in the [running multiple small jobs section](/core-concepts/run-udfs/run-small-udfs/#running-multiple-jobs-in-parallel):

```python showLineNumbers
@fused.udf
def udf(val):
    import pandas as pd
    return pd.DataFrame({'val':[val]})
```

As mentioned in the [Small UDF job section](/core-concepts/run-udfs/run-small-udfs/#fusedrun), to call it 1 time we can use `fused.run()`:

```python showLineNumbers
fused.run(udf, val=1)
```

{/* We then have 2 options to run this multiple times: */}


{/* ## Using "offline" instances (`run_remote()`)

_When to use: This is for high-latency (anything more than a few seconds) parallel processing of longer or more complex jobs_ */}


## Running a large job: `job.run_remote()`

{/*
Topics:
- Need to define a job: job = udf(arg_list=range(10))
- Can be used for parallel processing
- Or for specifying running with larger instance
 */}

Running a UDF over a large instance is done in 2 steps:
1. Creating a job with the UDF to run and [passing input parameters](/core-concepts/run-udfs/run_large/#passing-udf-parameters-with-arg_list) to run the UDF over
2. Send the job to a remote instance and (optionally) [defining the instance arguments](/core-concepts/run-udfs/run_large/#run_remote-instance-arguments)

```python showLineNumbers
# We'll run this UDF 5 times with 5 different inputs
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

### Passing UDF parameters with `arg_list`

**Single Parameter**

As mentioned [above](/core-concepts/run-udfs/run_large/#running-a-large-job-job-jobrun_remote) to pass UDF arguments to a remote job, use `arg_list` to specify a `list` of inputs to run your job over:

```python showLineNumbers
job = udf(arg_list=[0,1,2,3,4])
job.run_remote()
```

**Multiple Parameters**

Currently `arg_list` only supports giving 1 input variables to each UDF. We can work around this by aggregating multiple variables into a `dict` and having a UDF take a `dict` as input:

```python showLineNumbers
@fused.udf
def udf(variables: dict = {'val1':1, 'val2':2}):
    import pandas as pd
    import fused

    # Retrieving each variables from the dictionary
    val1 = variables['val1']
    val2 = variables['val2']

    # Some simple boilerplate logic
    output_value = int(val1)*int(val2)
    df = pd.DataFrame(data={'output':[output_value]})

    # Saving output to shared file location to access results later
    # `/mnt/cache` is the shared file location for all small & large jobs
    df.to_csv(f"/mnt/cache/demo_multiple_arg_list_run/output_{str(output_value)}.csv")

    return df
```

:::note
  You do need to type the input variable for this to work. If we had defined `variables` without typing it as a `dict`:
  ```python showLineNumbers
  @fused.udf
  def udf(variables = {}):
    # Notice the lack of `variables: dict = {}`
    ...
    return df
  ```
  our remote job run would have fail as Fused server has no way of knowing what to expect from `variables`
:::

We can then call this UDF as a remote job by passing a list of dictionaries to `arg_list`:
```python showLineNumbers
job = udf(arg_list=[{"val1": 5, "val2": 2}, {"val1": 3, "val2": 4}])
job.run_remote()
```

We can confirm this worked by viewing our results by browsing [File Explorer](/workbench/file-explorer/):

import ImgRunRemoteMultiArgList from '@site/docs/core-concepts/run-udfs/efs_results_run_remote_multiple_args.png';

<div style={{textAlign: 'center'}}>
  <img src={ImgRunRemoteMultiArgList} alt="Multiple arg list run remote output on File Explorer" style={{width: 800}} />
</div>

### `run_remote` instance arguments

{/* NOTE: Realized this is a bit of a repeat over dedicated section in Python SDK section: /python-sdk/top-level-functions/#jobrun_remote */}

With `job.run_remote()` you also have access to a few other arguments to make your remote job fit your needs:

- `instance_type`: Decide which type of machine to run your job on (see below for which ones we support)
- `disk_size_gb`: The amount of disk space in Gb allocated to your instance (between `16` and `999` Gb)

For example if you want a job with 16 vCPUs, 64Gb of RAM and 100Gb of storage you can call:
```python showLineNumbers
job.run_remote(instance_type="m5.4xlarge", disk_size_gb=100)
```

<details>
  <summary>Currently supported `instance_type`</summary>

  Fused `run_remote()` `instance_type` are based around [AWS General Purpose instance types](https://aws.amazon.com/ec2/instance-types/). We do support the following:

  ```python showLineNumbers
  supported_instance_types = [
    "m5.large",
    "m5.xlarge",
    "m5.2xlarge",
    "m5.4xlarge",
    "m5.8xlarge",
    "m5.12xlarge",
    "m5.16xlarge",
    "r5.large",
    "r5.xlarge",
    "r5.2xlarge",
    "r5.4xlarge",
    "r5.8xlarge",
    "r5.12xlarge",
    "r5.16xlarge",
  ]
  ```

</details>

## Accessing job logs

While your job is running you can access monitor & manage it with the following:

```python showLineNumbers
# View the job status
job.status

# Follow the job logs
job.tail_logs()

# Get the job logs
job.print_logs()

# Cancel the job
job.cancel()
```

These logs can also be accessed:

import ReactPlayer from 'react-player';
import ImgRunRemoteLogs from '@site/docs/core-concepts/run-udfs/run_remote_output.png';
import ImgRunRemoteEmail from '@site/docs/core-concepts/run-udfs/run-remote-email.png';

<details>
  <summary>In a notebook</summary>

    Running `job.run_remote()` in a notebook gives you a clickable link:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteLogs} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>

<details>
  <summary>In Fused Workbench</summary>

    Under the "Jobs" tab, on the bottom left of Workbench:

    <ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/core-concepts/run-udfs/run-remote-job-logs.mp4" width="100%" />

</details>

<details>
  <summary>By email (you'll receive 1 email for each job)</summary>

    Each job leads to an email summary with logs upon completion:

    <div style={{textAlign: 'center'}}>
    <img src={ImgRunRemoteEmail} alt="Dark Vessel Detection workflow" style={{width: 800}} />
    </div>

</details>


## Getting results

To get data back from your `run_remote()` jobs is a bit more complicated than for ["real-time"](/core-concepts/run-udfs/run-small-udfs/#getting-real-time-results). Our recommendation is to have your UDF write data directly to shared storage `/mount/` or cloud storage and access it after

<details>
  <summary>Example job: saving to shared storage `/mount/`</summary>

    A common use case for offline jobs is as a "pre-ingestion" process. You can find a real-life example of this in our [dark vessel detection example](/user-guide/examples/dark-vessel-detection/#32---writing-a-udf-to-open-each-ais-dataset)

    Here all we're returning is a status information in a pandas dataframe, but the our data in unzipped, read and saved to S3:

    ```python showLineNumbers
    import fused

    @fused.udf()
    def read_ais_from_noaa_udf(datestr='2023_03_29'):
        import os
        import requests
        import io
        import zipfile
        import pandas as pd

        url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'
        # This is our local mount file path,
        path=f'/mount/AIS_demo/{datestr[:7]}/'
        daily_ais_parquet = f'{path}/{datestr[-2:]}.parquet'

        # Download ZIP file to mounted disk
        r=requests.get(url)
        if r.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
                with z.open(f'AIS_{datestr}.csv') as f:
                    df = pd.read_csv(f)

                    # highlight-next-line
                    df.to_parquet(daily_ais_parquet)
            return pd.DataFrame({'status':['Done']})
        else:
            return pd.DataFrame({'status':[f'read_error_{r.status_code}']})
    ```

    Data written to `/mount/` can be accessed by any other instance used by anyone on your team so it can be used by any other UDF you run after.

    :::tip
      You can use [File Explorer](/workbench/file-explorer/) to easily see your outputs! In this case of the above example typing `efs://AIS_{datestr}.csv` (and replacing `datestr` with your date) will show the results directly in File Explorer!
    :::

</details>

{/* TODO: Need example here */}
{/* TODO: Show output on File Explorer */}


### Large jobs trade-offs

- Takes a few seconds to startup machine
- Can run as long as needed

---

// File: core-concepts/run-udfs/dependencies

# Dependencies

To keep things simple, Fused maintains a single runtime image. This means that any UDF you [run](/core-concepts/run-udfs/) will be executed with these dependencies by default

## UDF Dependencies

The Python packages are listed below and can also be found in [this](https://github.com/fusedio/fused-docs/blob/main/static/requirements_fused.txt) public `.txt` file.

import txt from 'raw-loader!/requirements_fused.txt';

<details>
    <summary>UDF Dependency Python packages</summary>
    <pre>
      <code>{`${txt}`}</code>
    </pre>
</details>

{/* TODO: We need a way to for people to, well, get in touch. Discord? Slack? Specific email? */}
Get in touch to have a package added to the list of dependencies or to learn about private runtime images for your organization.

## [BETA] Install your own dependencies

The simplest way to add your own library is to run the dedicated [Package Management Fused app](https://www.fused.io/workbench/apps/catalog/Package_Management-688daddc-dd51-4173-9003-d2bb5ddee72d). This app allows you to create a different environment and add any module you'd like

:::note
You need to make sure you have access to [Fused Apps](/workbench/app-builder/app-overview/) to be able to run this.
:::

![Beta package management app](/img/core-concepts/run-udfs/beta_package_management_app.png)

You'll then need to import the environment path in your UDF:

```python showLineNumbers
@fused.udf
def udf():
  import sys;
  sys.path.append(f"/mount/envs/demo_env/lib/python3.11/site-packages/")

  # Logic using your dedicated package
  return
```

---

// File: core-concepts

# Core concepts



{/* Learn the Fused internals and how to leverage tools in the Fused ecosystem. */}



import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: python-sdk/changelog

# Changelog

## v1.16.3 (2025-04-03)

**[`fused-py`](/python-sdk/)**

- It is now possible to return general `list`s and `tuple`s from UDFs. (Note: a tuple of a raster and bounds will be treated as a raster return type.)

**[Workbench](/workbench/)**

- Workbench will now prompt you when loading large UDF results that could slow down or overwhelm your browser. The threshold for this prompt is configurable in your Workbench preferences.
- Fixed bugs with loading large UDF results.
- UDF list will show an error if a UDF has an empty name.
- Fixed running some public UDFs in Workbench.

## v1.16.2 (2025-04-01)

**[`fused-py`](/python-sdk/)**

- It is now possible to return dictionaries of objects from a UDF, for example a dictionary of a raster numpy array, a DataFrame, and a string.
- Whitespace in a UDF will be considered as changes when determining whether to return cached data. (a UDF with different whitespace will be rerun rather than cached)
- Fixed calling `fused.run` in [large jobs](/core-concepts/run-udfs/run_large/).

**[Workbench](/workbench/)**

- Added experimental AI agent builder.
- Workbench will now prompt you to replace an existing UDF when adding the same UDF (by name) from the catalog.
- Added ability to download & upload an entire collection.
- Fixed saving collections with empty names.

[Visualization](/workbench/udf-builder/styling/):
- Added an H3-only visualization preset.
- Fixed a bug where changing TileLayer visualization type could result in a crash.

[App Builder](/workbench/app-builder/app-overview/):
- Updated the runtime.

## v1.16.0 (2025-03-27)

**[`fused-py`](/python-sdk/)**

- The result object of running in `batch` now has `logs_url` property.
- Fixed `fused.submit` raising an error if some run failed.

**[Workbench](/workbench/)**

- Added a Download UDFs button for downloading an entire collection.
- Results will show a message at the top if UDF execution was cached.
- Non-visible UDFs will have a different highlight color on them in the UDF list.
- Collections will show as modified if the order of UDFs has been changed.
- Fixes for Collections saving the ordering and visibility of UDFs.
- Fixed the Team Jobs page in Workbench crashing in some cases.

**Shared tokens**

- Shared token URLs can be called with an arbitrary (ignored) file extension in the URL.

## v1.15.0 (2025-03-20)

**[`fused-py`](/python-sdk/)**

- Loading UDFs now behaves like importing a Python module, and attributes defined on the UDF can be accessed.
- The `fused.submit()` keyword `wait_on_result` has been renamed to `collect`, with a default of `collect=True` returning the collected results (pass `collect=False` to get the JobPool object to inspect individual results).
- New UDFs default to using `fused.types.Bounds`.
- Upgraded `duckdb` to v1.2.1.
- UDFs can now return simple types like `str`, `int`, `float`, `bool`, and so on.
- Files in `/mount/` can be listed through the API.
- UDFs from publicly accessible GitHub repositories can be loaded through `fused.load`.
- `fused.load` now supports loading a UDF from a local .py file or directory
- The `x`, `y` and `z` aren't protected arguments when running a UDF anymore (previously protected to pass X/Y/Z mercantile tiles).

**[Workbench](/workbench/)**

New:
- Added a new account page and redesigned preferences page.
- You can now customize the code formatter settings (available under Preferences > Editor preferences).
- UDFs can optionally be shared with their code when creating a share token.

General:
- Moved shared token page to bottom left bar, and adjusted the icons.
- The ordering of UDFs in collections is now saved.

[App Builder](/workbench/app-builder/app-overview/):
- Updated app list UI.
- Fixed bugs with shared apps showing the wrong URL in the browser.

## v1.14.0 (2025-02-25)

v1.14.0 introduces a lot of new changes across `fused-py` and Workbench

**[`fused-py`](/python-sdk/)**

- Introducing [`fused.submit()`](/python-sdk/top-level-functions/#submit) method for multiple job run
- Improvement to [UDF caching](/core-concepts/cache/#caching-a-udf)
    - All UDFs are now cached for 90 days by default
    - Ability to customize the age of cached data & UDFs with the new [`cache_max_age`](/core-concepts/cache/#defining-your-cache-lifetime-cache_max_age) argument when defining UDFs, running UDFs or when caching regular Python functions
- `pandas` & `geopandas` are now optional for running non-spatial UDF locally
- Removed hardcoded `nodata=0` value for serializing raster data

**[Workbench](/workbench/)**

New:
- Introducing [Collections](/workbench/udf-builder/collections/) to organize & aggregate UDFs together
- Redesigned "Share" button & page: All the info you need to share your UDFs to your team or the world

General:
- Improvements to Navigation in [Command Pallette](/user-guide/best-practices/workbench-best-practices/#using-keyboard-shortcuts-command-palette). Try it out in Workbench by doing `Cmd + K` (`Ctrl + K` on Windows / Linux)
- Autocomplete now works with `Tab` in [Code Editor](/workbench/udf-builder/code-editor/) with `Tab`
- Added a Delete Button in the Shared Tokens page (under [Account page](/workbench/account/))
- Ability to upload images for UDF Preview in [Settings Page](/workbench/udf-builder/code-editor/#settings)
- Adding ‚ÄúFullscreen‚Äù toggle in [Map View](/workbench/udf-builder/map/)
- Improved `colorContinuous` in [Visualize Tab](/workbench/udf-builder/styling/)
- Allowing users to configure public/team access scopes for share tokens 
- No longer able to edit UDF & App name in read-only mode
- Fixing job loading logs

[File Explorer](/workbench/file-explorer/):
- Download directories as `zip`
- Adding favorites to file path input search results 
- Ability to open `.parquet` files with Kepler.gl


## v1.13.0 (2025-01-22)

- Fixed shared UDFs not respecting the Cache Enabled setting.
- Added a cache TTL (time-to-live) setting when running a UDF via a shared token endpoint.
- Tags you or your team have already used will be suggested when editing a UDF's tags.
- Team UDFs will be shown as read-only in Workbench, similar to Public UDFs.
- File Explorer shows deletion in progress.
- File Explorer can accept more S3 URLs, and uses `/mount/` instead of `/mnt/cache`.
- UDF Builder will no longer select a UDF when clicking to hide it.
- Fixed how Push to Github chooses the directory within a repository to push to.
- Fixed the browser location bar in Workbench updating on a delay.
- Fixed writing Shapefile or GPKG files to S3.
- (Beta) New [fusedio/apps](https://github.com/fusedio/apps) repository for public Fused Apps.
- Navigating to Team UDFs or Saved UDFs in the UDF Catalog will now prompt for login.
- Fixed the "Select..." environment button in Workbench settings.
- UDF Builder will no longer replace all unaccepted characters with `_` (underscore).
- Fixed loading team UDFs when running a UDF with a shared token.
- Batch jobs that use `print` will now have that output appear in the job logs.
- Apps in the shared token list show an app icon.
- Removed some deprecated batch job options.
- Installed `vega-datasets` package.

## v1.12.0 (2025-01-10)

- (Beta) Added an App catalog in Workbench, and a new type of URL for sharing apps.
- Added `/mount` as an alias for `/mnt/cache`.
- More consistently coerce the type of inputs to UDFs.
- Added more visualization presets to UDF builder in Workbench.
- Fixed an issue where the tab icon in Workbench could unintentionally change.
- Fixed bugs in Workbench File Explorer for `/mnt/cache` when browsing directories with many files.
- Fixed bugs in `fused` Python API not being able to list all files that should be accessible.
- Fixed bugs in the Github integration, command palette, and file explorer in Workbench.
- Fixed bugs in caching some UDF outputs.
- The shareable URL for public and community UDFs will now show in the settings tab for those UDFs.
- UDFs can customize their data return with `Response` objects.

## v1.11.9 (2024-12-19)

- Accounts now have a *handle* assigned to them, which can be used when loading UDFs and pushing to community UDFs
- Account handle can be changed once by the user (for more changes please contact the Fused team.)
- Added a command palette to the Workbench, which can be opened with Cmd-k or Ctrl-k.
- When creating a PR for a community UDF or to update a public UDF, it will be under your account if you log in to Fused with Github.
- Bug fixes for pushing to Github, e.g. when pushing a saved UDF, and for listing the Fused bot account as an author.
- Batch (`run_remote`) jobs can call back to the Fused API.
- Team UDFs can be pinned to the end of the featured list.
- Speed improvements in ingestion.
- Ingestion will detect `.pq` files as Parquet.
- Format code shortcut in Workbench is shown in the keyboard shortcut list and command palette.
- Workbench will hide the map tooltip when dragging the map by default.
- Workbench will now look for a `hexLayer` visualization preset for tabular results that do not contain `geometry`.
- Workbench file explorer can now handle larger lists of files.
- Fix for browsing disk cache (`/mnt/cache`) in Workbench file explorer.
- Teams with multiple realtime instances can now set one as their default.
- Fix for saving UDFs with certain names. Workbench will show more descriptive error messages in more cases for issues saving UDFs.

## v1.11.8 (2024-12-04)

- New File Explorer interface, with support for managing Google Cloud Storage (GCS) and `/mnt/cache` files.
- Workbench will show an error when trying to save a UDF with a duplicate name.
- Fixed a few bugs with Github integration, including the wrong repository being selected by default when creating a PR.
- Updated `fsspec` and `pyogrio` packages.

## v1.11.7 (2024-11-27)

- Decluttered the interface on mobile browsers by default.
- Fixed redo (Cmd-Shift-z or Ctrl-Shift-z) sometimes being bound to the wrong key.
- Tweaked the logic for showing the selected object in Workbench.

## v1.11.6 (2024-11-26)

- Added Format with Black (Alt+Shift+f) to Workbench.
- Fix the CRS of DataFrame's returned by get_chunk_from_table.
- Added a human readable ID to batch jobs.
- Fused will send an email when a batch job finishes.
- Fix for opening larger files in Kepler.gl.
- Fix for accessing UDFs in a team.
- Improved messages for UDF recursion, UDF geometry arguments, and returning geometry columns.
- Adjusted the UDF list styling and behavior in Workbench.
- Fix for secrets in shared tokens.

## v1.11.5 (2024-11-20)

- Show message for keyword arguments in UDFs that are reserved.
- Added reset kernel button.
- Workbench layers apply visualization changes immediately when the map is paused.
- Show the user that started a job for the team jobs list.
- Fix for running nested UDFs with utils modules.
- Fix for returning xarray results from UDFs.
- Fix for listing files from within UDFs.
- Upgraded to GeoPandas v1.

## v1.8.0 (2024-06-25) :package:

- Added Workbench tour for first-time users.
- Undo history is now saved across UDFs and persists through reloads.
- Added autocomplete when writing UDFs in Workbench.
- Added `colorBins`, `colorCategories`, and `colorContinuous` functions to Workbench's Visualize tab.
- Migrated SDK to Pydantic v2 for improved data validation and serialization.
- Fixed a bug causing NumPy dependency conflicts.

## v1.7.0 (2024-06-04) :bird:

- Execution infrastructure updates.
- Update DuckDB package to v1.0.0.
- Improve responsivity of Workbench allotments.
- Crispen Workbench UI.

## v1.6.1 (2024-05-06) :guardsman:

_GitHub integration_

- Updates to team GitHub integration.
- Users are now able to create shared UDF token from a team UDF both in Workbench and Python SDK.

## v1.6.0 (2024-04-30) :checkered_flag:

- The Workbench file explorer now shows UDFs contributed by community members.
- Team admins can now set up a GitHub repository with UDFs that their team members can access from Workbench.

## v1.5.4 (2024-04-15) :telescope:

- Button to open slice of data in Kepler.gl.
- Minor UI design and button placement updates.

## v1.5.3 (2024-04-08) :duck:

- Improved compatibility with DuckDB requesting data from shared UDFs.
- Geocoder in Workbench now supports coordinates and H3 cell IDs.
- GeoDataFrame arguments to UDFs can be passed as bounding boxes.
- The package ibis was upgraded to 8.0.0.
- Utils modules no longer need to import fused.

## v1.5.2 (2024-04-01) :tanabata_tree:

- File browser can now preview images like TIFFs, JPEGs, PNGs, and more.
- Users can now open Parquet files with DuckDB directly from the file browser.

## v1.5.0 (2024-03-25) :open_file_folder:

- The upload view in Workbench now shows a file browser.
- Users can now preview files in the file browser using a default UDF.

## v1.4.1 (2024-03-19) :speech_balloon:

- UDFs now support typed function annotations.
- Introduced special types  `fused.types.TileXYZ`, `fused.types.TileGDF`, `fused.types.Bbox`.
- Workbench now autodetects Tile or File outputs based on typing.
- Added button to Workbench to autodetect UDF parameters based on typing.

## v1.1.1 (2024-01-17) :dizzy:

- Renamed `fused.utils.run_realtime` and `fused.utils.run_realtime_xyz` to `fused.utils.run_file` amd `fused.utils.run_tile`.
- Removed `fused.utils.run_once`.

## v1.1.0 (2024-01-08) :rocket:

- Added functions to run the UDFs realtime.

## v1.1.0-rc2 (2023-12-11) :bug:

- Added `fused.utils.get_chunk_from_table`.
- Fixed bugs in loading and saving UDFs with custom metadata and headers.

## v1.1.0-rc0 (2023-11-29) :cloud:

- Added cloud load and save UDFs.
- `target_num_files` is replaced by `target_num_chunks` in the ingest API.
- Standardize how a decorator's headers are preprocesses to set `source_code` key.
- Fixed a bug loading UDFs from a job.

## v1.0.3 (2023-11-7) :sweat_drops:

_Getting chunks_

- Added `fused.utils.get_chunks_metadata` to get the metadata GeoDataFrame for a table.
- `run_local` now passes a copy of the input data into the UDF, to avoid accidentally persisting state between runs.
- `instance_type` is now shown in more places for running jobs.
- Fixed a bug where `render()`ing UDFs could get cut off.
- Fixed a bug with defining a UDF that contained an internal `@contextmanager`.

## v1.0.2 (2023-10-26) :up:

_Uploading files_

- Added `fused.upload` for uploading files to Fused storage.
- Added a warning for UDF parameter names that can cause issues.
- Fixed some dependency validation checks incorrectly failing on built-in modules.

## v1.0.1 (2023-10-19) :ant:

- Added `ignore_chunk_error` flag to jobs.
- Added warning when sidecar table names are specified but no matching table URL is provided.
- Fixed reading chunks when sidecars are requested but no sidecar file is present.
- Upgraded a dependency that was blocking installation on Colab.

## v1.0.0 (2023-10-13) :ship:

_Shipping dependencies_

- Added `image_name` to `run_remote` for customizing the set of dependencies used.
- Added `fused.delete` for deleting files or tables.
- Renamed `output_main` and `output_fused` to `output` and `output_metadata` respectively in ingestion jobs.
- Adjusted the default instance type for `run_remote`.
- Fixed `get_dataframe` sometimes failing.
- Improved tab completion for `fused.options` and added a repr.
- Fixed a bug where more version migration messages were printed.
- Fixed a bug when saving `fused.options`.

---

// File: python-sdk/api-reference/api

## `whoami`

```python
def whoami()
```

Returns information on the currently logged-in user.

---

## `delete`

```python
def delete(path: str,
           max_deletion_depth: Union[int, Literal["unlimited"]] = 2) -> bool
```

Delete the files at the path.

**Arguments**:

- `path` - Directory or file to delete, like `fd://my-old-table/`
- `max_deletion_depth` - If set (defaults to 2), the maximum depth the operation will recurse to. This helps avoid accidentally deleting more data than intended. Pass `"unlimited"` for unlimited.

**Examples**:

```python
fused.api.delete("fd://bucket-name/deprecated_table/")
```

---

## `list`

```python
def list(path: str, *, details: bool = False)
```

List the files in the path.

**Arguments**:

- `path` - Parent directory URL, like `fd://bucket-name/`

**Arguments**:

- `details` - If True, return additional metadata about each record.

**Returns**:

A list of paths as URLs, or as metadata objects.

**Examples**:

```python
fused.api.list("fd://bucket-name/")
```

---

## `get`

```python
def get(path: str) -> bytes
```

Download the contents at the path to memory.

**Arguments**:

- `path` - URL to a file, like `fd://bucket-name/file.parquet`

**Returns**:

Bytes of the file.

**Examples**:

```python
fused.api.get("fd://bucket-name/file.parquet")
```

---

## `download`

```python
def download(path: str, local_path: Union[str, Path]) -> None
```

Download the contents at the path to disk.

**Arguments**:

- `path` - URL to a file, like `fd://bucket-name/file.parquet`
- `local_path` - Path to a local file.

---

## `upload`

```python
def upload(local_path: Union[str, Path, bytes, BinaryIO],
           remote_path: str) -> None
```

Upload local file to S3.

**Arguments**:

- `local_path` - Either a path to a local file (`str`, `Path`) or the contents to upload.
  Any string will be treated as a Path, if you wish to upload the contents of
  the string, first encode it: `s.encode("utf-8")`
- `remote_path` - URL to upload to, like `fd://new-file.txt`

**Examples**:

To upload a local JSON file to your Fused-managed S3 bucket:

```py
fused.api.upload("my_file.json", "fd://my_bucket/my_file.json")
```

---

## `sign_url`

```python
def sign_url(path: str) -> str
```

Create a signed URL to access the path. This function may not check that the file represented by the path exists.

**Arguments**:

- `path` - URL to a file, like `fd://bucket-name/file.parquet`

**Returns**:

HTTPS URL to access the file using signed access.

**Examples**:

```python
fused.api.sign_url("fd://bucket-name/table_directory/file.parquet")
```

---

## `sign_url_prefix`

```python
def sign_url_prefix(path: str) -> Dict[str, str]
```

Create signed URLs to access all blobs under the path.

**Arguments**:

- `path` - URL to a prefix, like `fd://bucket-name/some_directory/`

**Returns**:

Dictionary mapping from blob store key to signed HTTPS URL.

**Examples**:

``` python
fused.api.sign_url_prefix("fd://bucket-name/table_directory/")
```

---

## `get_udfs`

```python
def get_udfs(n: int = 10,
             *,
             skip: int = 0,
             per_request: int = 25,
             max_requests: Optional[int] = None,
             by: Literal["name", "id", "slug"] = "name",
             whose: Literal["self", "public"] = "self")
```

Fetches a list of UDFs.

**Arguments**:

- `n` - The total number of UDFs to fetch. Defaults to 10.
- `skip` - The number of UDFs to skip before starting to collect the result set. Defaults to 0.
- `per_request` - The number of UDFs to fetch in each API request. Defaults to 25.
- `max_requests` - The maximum number of API requests to make.
- `by` - The attribute by which to sort the UDFs. Can be "name", "id", or "slug". Defaults to "name".
- `whose` - Specifies whose UDFs to fetch. Can be "self" for the user's own UDFs or "public" for
  UDFs available publicly. Defaults to "self".

**Returns**:

A list of UDFs.

**Examples**:

Fetch UDFs under the user account:

```py
fused.api.get_udfs()
```

---

## `job_get_logs`

```python
def job_get_logs(job: CoerceableToJobId,
                 since_ms: Optional[int] = None) -> List[Any]
```

Fetch logs for a job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns**:

Log messages for the given job.

---

## `job_print_logs`

```python
def job_print_logs(job: CoerceableToJobId,
                   since_ms: Optional[int] = None,
                   file: Optional[IO] = None) -> None
```

Fetch and print logs for a job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.
- `file` - Where to print logs to. Defaults to `sys.stdout`.

---

## `job_tail_logs`

```python
def job_tail_logs(job: CoerceableToJobId,
                  refresh_seconds: float = 1,
                  sample_logs: bool = True,
                  timeout: Optional[float] = None,
                  get_logs_retries: int = 1)
```

Continuously print logs for a job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---

## `job_get_status`

```python
def job_get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.

**Returns**:

The status of the given job.

---

## `job_cancel`

```python
def job_cancel(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.

**Returns**:

A new job object.

---

## `job_get_exec_time`

```python
def job_get_exec_time(job: CoerceableToJobId) -> timedelta
```

Determine the execution time of this job, using the logs.

**Returns**:

Time the job took. If the job is in progress, time from first to last log message is returned.

---

## `job_wait_for_job`

```python
def job_wait_for_job(job: CoerceableToJobId,
                     poll_interval_seconds: float = 5,
                     timeout: Optional[float] = None) -> RunResponse
```

Block the Python kernel until this job has finished.

**Arguments**:

- `poll_interval_seconds` - How often (in seconds) to poll for status updates. Defaults to 5.
- `timeout` - The length of time in seconds to wait for the job. Defaults to None.

**Raises**:

- `TimeoutError` - if waiting for the job timed out.

**Returns**:

The status of the given job.

---

## `FusedAPI`

```python
class FusedAPI()
```

API for running jobs in the Fused service.

#### `__init__`

```python
def __init__(*,
             base_url: Optional[str] = None,
             set_global_api: bool = True,
             credentials_needed: bool = True)
```

Create a FusedAPI instance.

**Arguments**:

- `base_url` - The Fused instance to send requests to. Defaults to `https://www.fused.io/server/v1`.
- `set_global_api` - Set this as the global API object. Defaults to True.
- `credentials_needed` - If True, automatically attempt to log in. Defaults to True.

---

#### `create_udf_access_token`

```python
def create_udf_access_token(udf_email_or_name_or_id: Optional[str] = None,
                            udf_name: Optional[str] = None,
                            *,
                            udf_email: Optional[str] = None,
                            udf_id: Optional[str] = None,
                            client_id: Union[str, Ellipsis, None] = ...,
                            cache: bool = True,
                            metadata_json: Optional[Dict[str, Any]] = None,
                            enabled: bool = True) -> UdfAccessToken
```

Create a token for running a UDF. Anyone with the token can run the UDF with any parameters they choose. The UDF will run under your environment.

The token does not allow running any other UDF on your account.

**Arguments**:

- `udf_email_or_name_or_id` - A UDF ID, email address (for use with udf_name), or UDF name.
- `udf_name` - The name of the UDF to create the

**Arguments**:

- `udf_email` - The email of the user owning the UDF, or, if udf_name is None, the name of the UDF.
- `udf_id` - The backend ID of the UDF to create the token for.
- `client_id` - If specified, overrides which realtime environment to run the UDF under.
- `cache` - If True, UDF tiles will be cached.
- `metadata_json` - Additional metadata to serve as part of the tiles metadata.json.
- `enable` - If True, the token can be used.

---

#### `upload`

```python
def upload(path: str, data: Union[bytes, BinaryIO]) -> None
```

Upload a binary blob to a cloud location.

---

#### `start_job`

```python
def start_job(config: Union[JobConfig, JobStepConfig],
              *,
              instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
              region: Optional[str] = None,
              disk_size_gb: Optional[int] = None,
              additional_env: Optional[Sequence[str]] = (
                  "FUSED_CREDENTIAL_PROVIDER=ec2", ),
              image_name: Optional[str] = None) -> RunResponse
```

Execute an operation.

**Arguments**:

- `config` - the configuration object to run in the job.

**Arguments**:

- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job, each in the form KEY=value. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.

---

#### `get_jobs`

```python
def get_jobs(n: int = 5,
             *,
             skip: int = 0,
             per_request: int = 25,
             max_requests: Optional[int] = 1) -> Jobs
```

Get jobs history with `get_jobs`.

**Arguments**:

- `n` - The number of jobs to fetch. Defaults to 5.

**Arguments**:

- `skip` - Where in the job history to begin. Defaults to 0, which retrieves the most recent job.
- `per_request` - Number of jobs per request to fetch. Defaults to 25.
- `max_requests` - Maximum number of requests to make. May be None to fetch all jobs. Defaults to 1.

**Returns**:

The job history.

---

#### `get_status`

```python
def get_status(job: CoerceableToJobId) -> RunResponse
```

Fetch the status of a running job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.

**Returns**:

The status of the  job.

---

#### `get_logs`

```python
def get_logs(job: CoerceableToJobId,
             since_ms: Optional[int] = None) -> List[Any]
```

Fetch logs for a job

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `since_ms` - Timestamp, in milliseconds since epoch, to get logs for. Defaults to None for all logs.

**Returns**:

Log messages for the given job.

---

#### `tail_logs`

```python
def tail_logs(job: CoerceableToJobId,
              refresh_seconds: float = 1,
              sample_logs: bool = True,
              timeout: Optional[float] = None,
              get_logs_retries: int = 1)
```

Continuously print logs for a job

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `refresh_seconds` - how frequently, in seconds, to check for new logs. Defaults to 1.
- `sample_logs` - if true, print out only a sample of logs. Defaults to True.
- `timeout` - if not None, how long to continue tailing logs for. Defaults to None for indefinite.
- `get_logs_retries` - Number of additional retries for log requests. Defaults to 1.

---

#### `wait_for_job`

```python
def wait_for_job(job: CoerceableToJobId,
                 poll_interval_seconds: float = 5,
                 timeout: Optional[float] = None) -> RunResponse
```

Block the Python kernel until the given job has finished.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.
- `poll_interval_seconds` - How often (in seconds) to poll for status updates. Defaults to 5.
- `timeout` - The length of time in seconds to wait for the job. Defaults to None.

**Raises**:

- `TimeoutError` - if waiting for the job timed out.

**Returns**:

The status of the given job.

---

#### `cancel_job`

```python
def cancel_job(job: CoerceableToJobId) -> RunResponse
```

Cancel an existing job.

**Arguments**:

- `job` - the identifier of a job or a `RunResponse` object.

**Returns**:

A new job object.

---

#### `auth_token`

```python
def auth_token() -> str
```

Returns the current user's Fused environment (team) auth token.

---

// File: python-sdk/api-reference/core

## `run_tile`

```python showLineNumbers
def run_tile(email: str,
             id: Optional[str] = None,
             *,
             x: int,
             y: int,
             z: int,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.


**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_shared_tile`

```python showLineNumbers
def run_shared_tile(token: str,
                    *,
                    x: int,
                    y: int,
                    z: int,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    _client_id: Optional[str] = None,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared tile-based UDF.

This function constructs a URL to run a UDF on a specific tile defined by its x, y, and z coordinates, and
sends a request to the server. It supports customization of the output data types for vector and raster data,
as well as additional parameters for the UDF execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a pre-defined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a pre-defined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.


**Returns**:

  The response from the server after executing the UDF on the specified tile.

---
## `run_file`

```python showLineNumbers
def run_file(email: str,
             id: Optional[str] = None,
             *,
             _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
             _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
             _client_id: Optional[str] = None,
             **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a private file-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to run a UDF associated with the given email and ID, allowing for output data type customization for both vector and raster outputs. It also supports additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - Email address of user account associated with the UDF.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.


**Returns**:

  The response from the server after executing the UDF.

---
## `run_shared_file`

```python showLineNumbers
def run_shared_file(token: str,
                    *,
                    _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
                    _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
                    **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Executes a shared file-based UDF.

This function constructs a URL for running an operation on a file accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.


**Returns**:

  The response from the server after executing the operation on the file.


**Raises**:

- `Exception` - Describes various exceptions that could occur during the function execution, including but not limited to invalid parameters, network errors, unauthorized access errors, or server-side errors.


:::note

This function is designed to access shared operations that require a token for authorization. It requires network access to communicate with the server hosting these operations and may incur data transmission costs or delays depending on the network's performance.

:::

---
## `run_tile_async`

```python showLineNumbers
async def run_tile_async(
        email: str,
        id: Optional[str] = None,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a private tile-based UDF indexed under the specified email and ID. The calling user must have the necessary permissions to execute the UDF.

This function constructs a URL to asynchronously run a UDF on a specific tile defined by its x, y, and z coordinates. It supports customization of the output data types for vector and raster data, and accommodates additional parameters for the UDF execution.

**Arguments**:

- `email` _str_ - User's email address. Used to identify the user's saved UDFs. If the ID is not provided, the email is also used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the user's email is used as the ID.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output. Defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output. Defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.


**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF on the specified tile and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_tile_async`

```python showLineNumbers
async def run_shared_tile_async(
        token: str,
        *,
        x: int,
        y: int,
        z: int,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared tile-based UDF using a specific access token.

This function constructs a URL for running an operation on a tile, defined by its x, y, and z coordinates, accessible via a shared token. It allows for customization of the output data types for vector and raster data and supports additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation on the specified tile.
- `x` _int_ - The x coordinate of the tile.
- `y` _int_ - The y coordinate of the tile.
- `z` _int_ - The zoom level of the tile.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.


**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the specified tile and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---
## `run_file_async`

```python showLineNumbers
async def run_file_async(
        email: str,
        id: Optional[str] = None,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        _client_id: Optional[str] = None,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a file-based UDF associated with the specific email and ID.

This function constructs a URL to run a UDF on a server, allowing for output data type customization for vector and raster outputs and supporting additional parameters for the UDF execution. If no ID is provided, the user's email is used as the identifier.

**Arguments**:

- `email` _str_ - The user's email address, used to identify the user's saved UDFs. If the ID is not provided, this email will also be used as the ID.
- `id` _Optional[str]_ - Unique identifier for the UDF. If None, the function fetches the user's email as the ID.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `_client_id` _Optional[str]_ - Client identifier for API usage. If None, a default or global client ID may be used.
- `**params` - Additional keyword arguments for the UDF execution.


**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the UDF and returns the server's response. The format and content of the response depend on the UDF's implementation and the server's response format.

---
## `run_shared_file_async`

```python showLineNumbers
async def run_shared_file_async(
        token: str,
        *,
        _dtype_out_vector: str = DEFAULT_DTYPE_VECTOR,
        _dtype_out_raster: str = DEFAULT_DTYPE_RASTER,
        **params) -> Optional[Union["pd.DataFrame", "xr.Dataset"]]
```

Asynchronously executes a shared file-based UDF using the specific access token.

Constructs a URL to run an operation on a file accessible via a shared token, enabling customization of the output data types for vector and raster data. It accommodates additional parameters for the operation's execution.

**Arguments**:

- `token` _str_ - A shared access token that authorizes the operation.
- `_dtype_out_vector` _str_ - Desired data type for vector output, defaults to a predefined type.
- `_dtype_out_raster` _str_ - Desired data type for raster output, defaults to a predefined type.
- `**params` - Additional keyword arguments for the operation execution.


**Returns**:

  A coroutine that, when awaited, sends a request to the server to execute the operation on the file and returns the server's response. The format and content of the response depend on the operation's implementation and the server's response format.

---

// File: python-sdk/authentication

import LinkButtons from "@site/src/components/LinkButtons.jsx";
import CellOutput from "@site/src/components/CellOutput.jsx";
import {BokehFigure, PlotlyFigure} from "@site/src/components/Plotting.jsx";

## Authenticate

Authenticate the Fused [Python SDK](/python-sdk/) in a Python Notebook.

Start by installing [`fused-py`](https://pypi.org/project/fused/).

```python showLineNumbers
# !pip install fused -q
```

When running Fused in a Notebook, you might be prompted to authenticate. Call `NotebookCredentials` in a cell.

```python showLineNumbers
from fused.api import NotebookCredentials

credentials = NotebookCredentials()
```

Complete the authentication flow and paste the provided credentials snippet in a new cell.

```
credentials.finalize(code="...")
```

Fused will write credentials to disk (in `~/.fused/credentials`) and `fused-py` will have access to the Fused API.

:::info
When running code from a Python environment without browser access, you may copy the `~/.fused/credentials` file to the same location in the target system.
:::

## Log out

Log out the current user. This deletes the credentials saved to disk and resets the global Fused API.

```python showLineNumbers
import fused
fused.api.logout()
```

## Get access token

Get the account's access token.

```python showLineNumbers
import fused
fused.api.access_token()
```

---

// File: python-sdk/index

# Python SDK

The Fused Python SDK ([`fused-py`](https://pypi.org/project/fused/)) exposes utility functions to write, manage, and run Fused UDFs. Use it in the [UDF Builder](/workbench/udf-builder/) or install it in your preferred Python environment.

## Documentation overview

import DocCardList from '@theme/DocCardList';

<DocCardList />

## Install

:::note
    This step is only required if you're running `fused` on your end (locally or in a development environment). If you're working in [Workbench UDF Builder](/workbench/udf-builder/) or [App Builder](/workbench/app-builder/) `fused` is already installed for you.
:::

1. Set up a Python environment:

We're using `venv` but you could use `conda` or any other environment manager in Python. 

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install the `fused` package:

Install the base package:
```bash
pip install fused
```

Or install with optional dependencies:
```bash
# For raster data processing
pip install "fused[raster]"

# For vector data processing
pip install "fused[vector]"

# Install all optional dependencies
pip install "fused[all]"
```

3. Authenticate:

{/* Need a way to authenticate locally, in terminal */}

**In a notebook**
```py
from fused.api import NotebookCredentials
credentials = NotebookCredentials()
```

Run this snippet from a Notebook Cell and follow the authentication flow, which will store a credentials file in `~/.fused/credentials`.

---

// File: python-sdk/top-level-functions

## `@fused.udf`

```python showLineNumbers
def udf(
    fn: Optional[Callable] = None,
    *,
    schema: Union[Schema, Dict, None] = None,
    name: Optional[str] = None,
    cache_max_age: Optional[str] = None,
    default_parameters: Optional[Dict[str, Any]] = None,
    headers: Optional[Sequence[Union[str, Header]]] = None,
) -> Callable[..., GeoPandasUdfV2]:
```

 A decorator that transforms a function into a Fused UDF.

Args:
    schema: The schema for the DataFrame returned by the UDF. The schema may be either
        a string (in the form `"field_name:DataType field_name2:DataType"`, or as JSON),
        as a Python dictionary representing the schema, or a `Schema` model object.

        Defaults to None, in which case a schema must be evaluated by calling `run_local`
        for a job to be able to write output. The return value of `run_local` will also
        indicate how to include the schema in the decorator so `run_local` does not need
        to be run again.
    name: The name of the UDF object. Defaults to the name of the function.
    cache_max_age: The maximum age when returning a result from the cache.
    default_parameters: Parameters to embed in the UDF object, separately from the arguments
        list of the function. Defaults to None for empty parameters.
    headers: A list of files to include as modules when running the UDF. For example,
        when specifying `headers=['my_header.py']`, inside the UDF function it may be
        referenced as:

        ```py
        import my_header
        my_header.my_function()
        ```

        Defaults to None for no headers.
Returns:
    A callable that represents the transformed UDF. This callable can be used
    within GeoPandas workflows to apply the defined operation on geospatial data.

Examples:
    To create a simple UDF that calls a utility function to calculate the area of geometries in a GeoDataFrame:

    ```py
    @fused.udf
    def udf(bbox, table_path="s3://fused-asset/infra/building_msft_us"):
        ...
        gdf = table_to_tile(bbox, table=table_path)
        return gdf
    ```

---

## `@fused.cache`

```python showLineNumbers
DEFAULT_CACHE_MAX_AGE = "12h"
```

```python showLineNumbers
def cache(
    func: Optional[Callable[..., Any]] = None,
    cache_max_age: str | int = DEFAULT_CACHE_MAX_AGE,
    path: str = "tmp",
    concurrent_lock_timeout: str | int = 120,
    reset: bool = False,
) -> Callable[..., Any]:
```
Decorator to cache the return value of a function.

This function serves as a decorator that can be applied to any function
to cache its return values. The cache behavior can be customized through
keyword arguments.

Args:
    func (Callable, optional): The function to be decorated. If None, this
        returns a partial decorator with the passed keyword arguments.
    cache_max_age: A string with a numbered component and units. Supported units are seconds (s), minutes (m), hours (h), and
        days (d) (e.g. "48h", "10s", etc.).
    path: Folder to append to the configured cache directory.
    concurrent_lock_timeout: Max amount of time in seconds for subsequent concurrent calls to wait for a previous
        concurrent call to finish execution and to write the cache file.

Returns:
    Callable: A decorator that, when applied to a function, caches its
    return values according to the specified keyword arguments.

Examples:

    Use the `@cache` decorator to cache the return value of a function in a custom path.

    ```py
    @cache(path="/tmp/custom_path/")
    def expensive_function():
        # Function implementation goes here
        return result
    ```

    If the output of a cached function changes, for example if remote data is modified,
    it can be reset by running the function with the `reset` keyword argument. Afterward,
    the argument can be cleared.

    ```py
    @cache(path="/tmp/custom_path/", reset=True)
    def expensive_function():
        # Function implementation goes here
        return result
    ```
---

## `load`

```python showLineNumbers
def load(url_or_udf: Union[str, Path], /, *, cache_key: Any = None) -> AnyBaseUdf:
```

Loads a UDF from various sources including GitHub URLs,
and a Fused platform-specific identifier.

This function supports loading UDFs from a GitHub repository URL, or a Fused
platform-specific identifier composed of an email and UDF name. It intelligently
determines the source type based on the format of the input and retrieves the UDF
accordingly.

Args:
    url_or_udf: A string representing the location of the UDF, or the raw code of the UDF.
        The location can be a GitHub URL starting with "https://github.com",
        a Fused platform-specific identifier in the format "email/udf_name",
        or a local file path pointing to a Python file.
    cache_key: An optional key used for caching the loaded UDF. If provided, the function
        will attempt to load the UDF from cache using this key before attempting to
        load it from the specified source. Defaults to None, indicating no caching.

Returns:
    AnyBaseUdf: An instance of the loaded UDF.

Raises:
    ValueError: If the URL or Fused platform-specific identifier format is incorrect or
        cannot be parsed.
    Exception: For errors related to network issues, file access permissions, or other
        unforeseen errors during the loading process.

Examples:
    Load a UDF from a GitHub URL:
    ```py
    udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/REM_with_HyRiver/")
    ```

    Load a UDF using a Fused platform-specific identifier:
    ```py
    udf = fused.load("username@fused.io/REM_with_HyRiver")
    ```

---

## `run`

```python showLineNumbers
def run(
    udf: Union[str, None, UdfJobStepConfig, GeoPandasUdfV2, UdfAccessToken] = None,
    *args,
    x: Optional[int] = None,
    y: Optional[int] = None,
    z: Optional[int] = None,
    sync: bool = True,
    engine: Optional[Literal["remote", "local"]] = None,
    type: Optional[Literal["tile", "file"]] = None,
    max_retry: int = 0,
    cache_max_age: Optional[str] = None,
    parameters: Optional[Dict[str, Any]] = None,
    _include_log: Optional[bool] = False,
    _return_response: Optional[bool] = False,
    **kw_parameters,
):
```

Executes a user-defined function (UDF) with various execution and input options.

This function supports executing UDFs in different environments (local or remote),
with different types of inputs (tile coordinates, geographical bounding boxes, etc.), and
allows for both synchronous and asynchronous execution. It dynamically determines the execution
path based on the provided parameters.

Args:
    udf (str, GeoPandasUdfV2 or UdfJobStepConfig): the UDF to execute.
        The UDF can be specified in several ways:
        - A string representing a UDF name or UDF shared token.
        - A UDF object.
        - A UdfJobStepConfig object for detailed execution configuration.
    x, y, z: Tile coordinates for tile-based UDF execution.
    sync: If True, execute the UDF synchronously. If False, execute asynchronously.
    engine: The execution engine to use ('remote' or 'local').
    type: The type of UDF execution ('tile' or 'file').
    max_retry: The maximum number of retries to attempt if the UDF fails.
        By default does not retry.
    cache_max_age: The maximum age when returning a result from the cache.
        Supported units are seconds (s), minutes (m), hours (h), and days (d) (e.g. ‚Äú48h‚Äù, ‚Äú10s‚Äù, etc.).
        Default is `None` so a UDF run with `fused.run()` will follow `cache_max_age` defined in `@fused.udf()` unless this value is changed.
    parameters: Additional parameters to pass to the UDF.
    **kw_parameters: Additional parameters to pass to the UDF.

Raises:
    ValueError: If the UDF is not specified or is specified in more than one way.
    TypeError: If the first parameter is not of an expected type.
    Warning: Various warnings are issued for ignored parameters based on the execution path chosen.

Returns:
    The result of the UDF execution, which varies based on the UDF and execution path.

Examples:


    Run a UDF saved in the Fused system:
    ```py
    fused.run("username@fused.io/my_udf_name")
    ```

    Run a UDF saved in GitHub:
    ```py
    loaded_udf = fused.load("https://github.com/fusedio/udfs/tree/main/public/Building_Tile_Example")
    fused.run(loaded_udf, bbox=bbox)
    ```

    Run a UDF saved in a local directory:
    ```py
    loaded_udf = fused.load("/Users/local/dir/Building_Tile_Example")
    fused.run(loaded_udf, bbox=bbox)
    ```

Note:
    This function dynamically determines the execution path and parameters based on the inputs.
    It is designed to be flexible and support various UDF execution scenarios.

## `submit`

```python showLineNumbers
def submit(
    udf,
    arg_list,
    /,
    *,
    engine: Optional[Literal["remote", "local"]] = "remote",
    max_workers: Optional[int] = None,
    max_retry: int = 2,
    debug_mode: bool = False,
    collect: bool = True,
    **kwargs,
) -> Union[JobPool, ResultType, "pd.DataFrame"]:
```

Executes a user-defined function (UDF) multiple times for a list of input
parameters, and return immediately a "lazy" JobPool object allowing
to inspect the jobs and wait on the results.

See `fused.run` for more details on the UDF execution.

Args:
    udf: the UDF to execute.
        See `fused.run` for more details on how tos specify the UDF.
    arg_list: a list of input parameters for the UDF. Can be specified as:
        - a list of values for parametrizing over a single parameter, i.e.
            the first parameter of the UDF
        - a list of dictionaries for parametrizing over multiple parameters
        - A DataFrame for parametrizing over multiple parameters where each
            row is a set of parameters

    engine: The execution engine to use. Defaults to 'remote'.
    max_workers: The maximum number of workers to use. Defaults to 32.
    max_retry: The maximum number of retries for failed jobs. Defaults to 2.
    debug_mode: If True, executes only the first item in arg_list directly using
        `fused.run()`, useful for debugging UDF execution. Default is False.
    collect: If True, waits for all jobs to complete and returns the collected DataFrame
        containing the results. If False, returns a JobPool object, which is non-blocking
        and allows you to inspect the individual results and logs.
        Default is True.

Returns:
    JobPool

Examples:

    Run a UDF multiple times for the values 0 to 9 passed to as the first
    positional argument of the UDF:
    ```py
    pool = fused.submit("username@fused.io/my_udf_name", range(10))
    ```

    Being explicit about the parameter name:
    ```py
    pool = fused.submit(udf, [dict(n=i) for i in range(10)])
    ```

---

## `download`

```python showLineNumbers
def download(path: str, local_path: Union[str, Path]) -> None:
```

Download the contents at the path to disk.

Args

`path:` URL to a file, like `fd://bucket-name/file.parquet`  
`local_path:` Path to a local file.

## `ingest`

```python showLineNumbers
def ingest(
    input: Union[str, Path, Sequence[Union[str, Path]], "gpd.GeoDataFrame"],
    output: Optional[str] = None,
    *,
    output_metadata: Optional[str] = None,
    schema: Optional[Schema] = None,
    file_suffix: Optional[str] = None,
    load_columns: Optional[Sequence[str]] = None,
    remove_cols: Optional[Sequence[str]] = None,
    explode_geometries: bool = False,
    drop_out_of_bounds: Optional[bool] = None,
    partitioning_method: Literal["area", "length", "coords", "rows"] = "rows",
    partitioning_maximum_per_file: Union[int, float, None] = None,
    partitioning_maximum_per_chunk: Union[int, float, None] = None,
    partitioning_max_width_ratio: Union[int, float] = 2,
    partitioning_max_height_ratio: Union[int, float] = 2,
    partitioning_force_utm: Literal["file", "chunk", None] = "chunk",
    partitioning_split_method: Literal["mean", "median"] = "mean",
    subdivide_method: Literal["area", None] = None,
    subdivide_start: Optional[float] = None,
    subdivide_stop: Optional[float] = None,
    split_identical_centroids: bool = True,
    target_num_chunks: int = 5000,
    lonlat_cols: Optional[Tuple[str, str]] = None,
    partitioning_schema_input: Optional[Union[str, "pd.DataFrame"]] = None,
    gdal_config: Union[GDALOpenConfig, Dict[str, Any], None] = None,
) -> GeospatialPartitionJobStepConfig:
```

Ingest a dataset into the Fused partitioned format.

Args:
    input: A GeoPandas `GeoDataFrame` or a path to file or files on S3 to ingest. Files may be Parquet or another geo data format.
    output: Location on S3 to write the `main` table to.
    output_metadata: Location on S3 to write the `fused` table to.
    schema: Schema of the data to be ingested. This is optional and will be inferred from the data if not provided.
    file_suffix: filter which files are used for ingestion. If `input` is a directory on S3, all files under that directory will be listed and used for ingestion. If `file_suffix` is not None, it will be used to filter paths by checking the trailing characters of each filename. E.g. pass `file_suffix=".geojson"` to include only GeoJSON files inside the directory.
    load_columns: Read only this set of columns when ingesting geospatial datasets. Defaults to all columns.
    remove_cols: The named columns to drop when ingesting geospatial datasets. Defaults to not drop any columns.
    explode_geometries: Whether to unpack multipart geometries to single geometries when ingesting geospatial datasets, saving each part as its own row. Defaults to `False`.
    drop_out_of_bounds: Whether to drop geometries outside of the expected WGS84 bounds. Defaults to True.
    partitioning_method: The method to use for grouping rows into partitions. Defaults to `"rows"`.

        - `"area"`: Construct partitions where all contain a maximum total area among geometries.
        - `"length"`: Construct partitions where all contain a maximum total length among geometries.
        - `"coords"`: Construct partitions where all contain a maximum total number of coordinates among geometries.
        - `"rows"`: Construct partitions where all contain a maximum number of rows.

    partitioning_maximum_per_file: Maximum value for `partitioning_method` to use per file. If `None`, defaults to _1/10th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/10th the total area of all geometries. Defaults to `None`.
    partitioning_maximum_per_chunk: Maximum value for `partitioning_method` to use per chunk. If `None`, defaults to _1/100th_ of the total value of `partitioning_method`. So if the value is `None` and `partitioning_method` is `"area"`, then each file will be have no more than 1/100th the total area of all geometries. Defaults to `None`.
    partitioning_max_width_ratio: The maximum ratio of width to height of each partition to use in the ingestion process. So for example, if the value is `2`, then if the width divided by the height is greater than `2`, the box will be split in half along the horizontal axis. Defaults to `2`.
    partitioning_max_height_ratio: The maximum ratio of height to width of each partition to use in the ingestion process. So for example, if the value is `2`, then if the height divided by the width is greater than `2`, the box will be split in half along the vertical axis. Defaults to `2`.
    partitioning_force_utm: Whether to force partitioning within UTM zones. If set to `"file"`, this will ensure that the centroid of all geometries per _file_ are contained in the same UTM zone. If set to `"chunk"`, this will ensure that the centroid of all geometries per _chunk_ are contained in the same UTM zone. If set to `None`, then no UTM-based partitioning will be done. Defaults to "chunk".
    partitioning_split_method: How to split one partition into children. Defaults to `"mean"` (this may change in the future).

        - `"mean"`: Split each axis according to the mean of the centroid values.
        - `"median"`: Split each axis according to the median of the centroid values.

    subdivide_method: The method to use for subdividing large geometries into multiple rows. Currently the only option is `"area"`, where geometries will be subdivided based on their area (in WGS84 degrees).
    subdivide_start: The value above which geometries will be subdivided into smaller parts, according to `subdivide_method`.
    subdivide_stop: The value below which geometries will not be subdivided into smaller parts, according to `subdivide_method`. Recommended to be equal to subdivide_start. If `None`, geometries will be subdivided up to a recursion depth of 100 or until the subdivided geometry is rectangular.
    split_identical_centroids: If `True`, should split a partition that has
        identical centroids (such as if all geometries in the partition are the
        same) if there are more such rows than defined in "partitioning_maximum_per_file" and
        "partitioning_maximum_per_chunk".
    target_num_chunks: The target for the number of files if `partitioning_maximum_per_file` is None. Note that this number is only a _target_ and the actual number of files generated can be higher or lower than this number, depending on the spatial distribution of the data itself.
    lonlat_cols: Names of longitude, latitude columns to construct point geometries from.

        If your point columns are named `"x"` and `"y"`, then pass:

        ```py
        fused.ingest(
            ...,
            lonlat_cols=("x", "y")
        )
        ```

        This only applies to reading from Parquet files. For reading from CSV files, pass options to `gdal_config`.

    gdal_config: Configuration options to pass to GDAL for how to read these files. For all files other than Parquet files, Fused uses GDAL as a step in the ingestion process. For some inputs, like CSV files or zipped shapefiles, you may need to pass some parameters to GDAL to tell it how to open your files.

        This config is expected to be a dictionary with up to two keys:

        - `layer`: `str`. Define the layer of the input file you wish to read when the source contains multiple layers, as in GeoPackage.
        - `open_options`: `Dict[str, str]`. Pass in key-value pairs with GDAL open options. These are defined on each driver's page in the GDAL documentation. For example, the [CSV driver](https://gdal.org/drivers/vector/csv.html) defines [these open options](https://gdal.org/drivers/vector/csv.html#open-options) you can pass in.

        For example, if you're ingesting a CSV file with two columns
        `"longitude"` and `"latitude"` denoting the coordinate information, pass

        ```py
        fused.ingest(
            ...,
            gdal_config={
                "open_options": {
                    "X_POSSIBLE_NAMES": "longitude",
                    "Y_POSSIBLE_NAMES": "latitude",
                }
            }
        )
        ```
Returns:

    Configuration object describing the ingestion process. Call `.execute` on this object to start a job.


Examples:
    For example, to ingest the California Census dataset for the year 2022:
    ```py
    job = fused.ingest(
        input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
        output="s3://fused-sample/census/ca_bg_2022/main/",
        output_metadata="s3://fused-sample/census/ca_bg_2022/fused/",
        explode_geometries=True,
        partitioning_maximum_per_file=2000,
        partitioning_maximum_per_chunk=200,
    ).execute()
    ```
---

#### `job.run_remote`

```python showLineNumbers
def run_remote(output_table: Optional[str] = ...,
    instance_type: Optional[WHITELISTED_INSTANCE_TYPES] = None,
    *,
    region: str | None = None,
    disk_size_gb: int | None = None,
    additional_env: List[str] | None = None,
    image_name: Optional[str] = None,
    ignore_no_udf: bool = False,
    ignore_no_output: bool = False,
    validate_imports: Optional[bool] = None,
    validate_inputs: bool = True,
    overwrite: Optional[bool] = None) -> RunResponse
```

Begin execution of the ingestion job by calling `run_remote` on the job object.

**Arguments**:

- `output_table` - The name of the table to write to. Defaults to None.
- `instance_type` - The AWS EC2 instance type to use for the job. Acceptable strings are `m5.large`, `m5.xlarge`, `m5.2xlarge`, `m5.4xlarge`, `m5.8xlarge`, `m5.12xlarge`, `m5.16xlarge`, `r5.large`, `r5.xlarge`, `r5.2xlarge`, `r5.4xlarge`, `r5.8xlarge`, `r5.12xlarge`, or `r5.16xlarge`. Defaults to None.
- `region` - The AWS region in which to run. Defaults to None.
- `disk_size_gb` - The disk size to specify for the job. Defaults to None.
- `additional_env` - Any additional environment variables to be passed into the job. Defaults to None.
- `image_name` - Custom image name to run. Defaults to None for default image.
- `ignore_no_udf` - Ignore validation errors about not specifying a UDF. Defaults to False.
- `ignore_no_output` - Ignore validation errors about not specifying output location. Defaults to False.

#### Monitor and manage job

Calling `run_remote` returns a `RunResponse` object with helper methods.

```python showLineNumbers
# Declare ingest job
job = fused.ingest(
  input="https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/06_CALIFORNIA/06/tl_rd22_06_bg.zip",
  output="s3://fused-sample/census/ca_bg_2022/main/"
)

# Start ingest job
job_id = job.run_remote()
```

Fetch the job status.

```python showLineNumbers
job_id.get_status()
```

Fetch and print the job's logs.

```python showLineNumbers
job_id.print_logs()
```

Determine the job's execution time.

```python showLineNumbers
job_id.get_exec_time()
```

Continuously print the job's logs.

```python showLineNumbers
job_id.tail_logs()
```

Cancel the job.

```python showLineNumbers
job_id.cancel()
```


---
## `file_path`

```python showLineNumbers
def file_path(file_path: str, mkdir: bool = True) -> str:
```

Creates a directory in a predefined temporary directory.

This gives users the ability to manage directories during the execution of a UDF. It takes a relative file_path,
creates the corresponding directory structure, and returns its absolute path.

This is useful for UDFs that temporarily store intermediate results as files,
such as when writing intermediary files to disk when processing large datasets.
file_path ensures that necessary directories exist.

Args:
    file_path: The file path to locate.
    mkdir: If True, create the directory if it doesn't already exist. Defaults to True.

Returns:
    The located file path.
  
---
## `get_chunks_metadata`

```python showLineNumbers
def get_chunks_metadata(url: str) -> "gpd.GeoDataFrame":
```

Returns a GeoDataFrame with each chunk in the table as a row.

Args:
    url: URL of the table.

---
## `get_chunk_from_table`

```python showLineNumbers
def get_chunk_from_table(
    url: str,
    file_id: Union[str, int, None],
    chunk_id: Optional[int],
    *,
    columns: Optional[Iterable[str]] = None,
) -> "gpd.GeoDataFrame":
```

Returns a chunk from a table and chunk coordinates.

This can be called with file_id and chunk_id from `get_chunks_metadata`.

Args:
    url: URL of the table.
    file_id: File ID to read.
    chunk_id: Chunk ID to read.

---

// File: user-guide/in

Learn to load data into UDFs from the following sources and formats.


import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/examples

Hands-on examples you can follow to begin using Fused.

import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/transform

Learn how to implement transformations and operations on data with UDFs.


import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/out

Learn how to call UDFs from 3rd party applications to load data.


import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/best_practices

Making the most out of all the features Fused offers you

import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/best-practices/udf-best-practices

# Build & Running UDFs

_An opinionated guide to making the most out of Fused UDFs_

[Fused UDFs](/core-concepts/write/) are Python functions that run on serverless compute and can be called from anywhere with [`fused.run(udf)`](/core-concepts/run-udfs/run-small-udfs/#fusedrun). This guide is a resource meant to help you on your way to making the most out of UDFs.

## A short reminder: The anatomy of a UDF

```python showLineNumbers
@fused.udf
def udf(my_awesome_input: int = 1):
    import pandas as pd

    return pd.DataFrame({"Look at my output: ": [my_awesome_input]})
```

Each UDF has a few specific elements:
- The [`@fused.udf` decorator](/core-concepts/write/#fusedudf-decorator)
- Arguments -ideally typed-
- Imports _inside_ the function
- Some logic
- A [supported `return` object](/core-concepts/write/#return-object)

:::note
All of this is explained in the ["Write UDF"](/core-concepts/write/) section in much more details.
:::

You can then run UDFs from _anywhere_ with [`fused.run(udf)`](/core-concepts/run-udfs/run-small-udfs/#fusedrun). These are still Python functions, giving you a lot of flexibility on what oyu can do, but we have some recommendations for keeping them fast & efficient.

## Writing efficient UDFs

### Keep things small

The main benefit of Fused UDFs is how responsive they are. They achieve this by running on Python serverless compute. They can [time out](/core-concepts/run-udfs/run-small-udfs/#defining-small-job), so the best way to keep workflows fast is to keep them small:

- Break pipelines into single-task UDFs
- Leverage [`fused.run()`](/core-concepts/run-udfs/run-small-udfs/#fusedrun) to chain UDFs together
- Or [run small tasks in parallel](/user-guide/best-practices/udf-best-practices/#run-tasks-in-parallel)

<details>
    <summary>Example: Breaking down a complex pipeline into smaller UDFs</summary>

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):
        import pandas as pd

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):
        import pandas as pd
        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):
        import pandas as pd

        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ```python showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):
        import pandas as pd

        df = fused.run(load_data_udf, data_path=data_path)
        processed_df = fused.run(process_data_udf, df=df)

        return processed_df
    ```
</details>

### Run often, Iterate quickly

Just like writing short cells when developing in a Jupyter Notebook, we recommend you keep your UDFs short & fast to execute

‚ö°Ô∏è Aim for **UDFs that take up to 1min to run**

UDFs run with [`fused.run()`](/core-concepts/run-udfs/run-small-udfs/#fusedrun) [time out after 120s](/core-concepts/run-udfs/run-small-udfs/#defining-small-job) so we recommend you keep a buffer in case your UDF takes a bit longer to execute

<details>
    <summary>Visual: UDF timing guideline</summary>

    This is a breakdown of what happens when you run a UDF with `fused.run()` and why we recommend you keep your UDFs at the 30s-1min mark:

    ![UDF Design Guidelines](/img/user-guide/best-practices/udf_design_timing.png)

</details>

### Run tasks in parallel

Sometimes you need to run a UDF over a large amount of inputs that takes [longer than 120s to run](/core-concepts/run-udfs/run-small-udfs/#defining-small-job), for example loading a large dataset or running a more complex process.

In this case you can use [`fused.submit()`](/core-concepts/run-udfs/run-small-udfs/#fusedsubmit) to run a small UDF over a set of inputs in parallel.

<details>
    <summary>Example: Parallelizing a UDF with [`fused.submit()`](/core-concepts/run-udfs/run-small-udfs/#fusedsubmit)</summary>

    Let's imagine we have a `fetch_single_data` that loads data from an API for a large amount of inputs `input_data=[0,1,2,3,4,5,7,8,9]`:

    ```python showLineNumbers
    @fused.udf
    def fetch_single_data(single_input: int):
        import pandas as pd
        import time

        # Considering this as our API call, sleeping to simulate the time it takes to get the data
        time.sleep(3)

        return pd.DataFrame({"data": [f"processed_{str(single_input)}"]})
    ```
    If we were to run this UDF in [Workbench](/workbench/overview/) we would only be able to run it for 1 input at a time, so we could edit our UDF to loop over the inputs:
    ```python showLineNumbers
    @fused.udf
    def fetch_data(inputs: list):
        import pandas as pd
        import time

        fetched_data = []
        for i in inputs:
            # Considering this as our API call
            time.sleep(3)
            fetched_data.append(f"processed_{str(i)}")

        return pd.DataFrame({"data": fetched_data})
    ```
    However, running this UDF with `fused.run(fetch_data, inputs=input_data)` will take longer as we add inputs, we could even quickly go over [the 120s limit](/core-concepts/run-udfs/run-small-udfs/#defining-small-job). We still do want to fetch data across all our inputs which is where [`fused.submit()`](/core-concepts/run-udfs/run-small-udfs/#fusedsubmit) comes in:

    Going back to our original UDF, we can now run it with `fused.submit()` to run it in parallel:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(input_data):

        results = fused.submit(
            fetch_single_data,
            input_data,
            engine='local', # This ensures the UDF is run in our local server rather than spinning up new instances.
        )
        fetched_data = results.collect_df()

        return fetched_data
    ```

    This is of course a simplified example, but it shows how you can use `fused.submit()` to run a UDF in parallel for each input.

    This now runs a lot faster by running the `fetch_single_data` UDF in parallel for each input.

    :::note
    The example here blocks the main thread until all the `fused.submit()` calls have finished. This means you might have to wait longer in [Workbench](/workbench/overview/) for the results to show up.
    :::

    Comparison of both approaches in Workbench:

    Running with `fused.run()`, 31.5s:
    ![10 inputs fused.run](/img/user-guide/best-practices/10_udfs_fused_run.png)

    Running with `fused.submit()`, 4.6s:
    ![10 inputs fused.submit](/img/user-guide/best-practices/10_udfs_fused_submit.png)

</details>

### Use [offline instances](/core-concepts/run-udfs/run_large/) for your large jobs

You might not be able to run your UDF within the 120s limit of the default [`fused.run(udf)`](/core-concepts/run-udfs/run-small-udfs/#fusedrun) call or break it down into smaller UDFs and parallelize it with [`fused.submit()`](/core-concepts/run-udfs/run-small-udfs/#fusedsubmit). In that case you can [use offline instances](/core-concepts/run-udfs/run_large/). These also allow you to:
- [Choose instances](/core-concepts/run-udfs/run_large/#run_remote-instance-arguments) with more memory & CPU
- Run your UDF for longer than 120s (at the expense of slower startup time and [requiring to save your data to storage](/core-concepts/run-udfs/run_large/#getting-results) to retrieve it)

You can find an example of using offline instances in [one of our end to end examples](/user-guide/examples/dark-vessel-detection/#34---ingest-1-month-of-ais-data-into-a-geo-partitioned-format) when ingesting data into portioned cloud native formats

### Cache as much as you can

Fused relies heavily on [caching](/core-concepts/cache/) repetitive tasks to make recurring calls much faster (and more compute efficient)

‚úÖ You want to use caching for functions with inputs that are recurring:
- Loading a dataset
- Computing a recurring operation with default variables
- Intermediate results you'll reuse soon

‚ùå When not to use caching:
- In most cases, for functions taking `bounds` as an argument -> your function + input cache would get re-generated for each new `bounds` (which changes each time you pan around in [Workbench Map](/workbench/udf-builder/map/) view for example)
- Data you want others in your team or external to Fused to use. You're better off writing your data to cloud storage like `s3` or `gcs`

<details>
    <summary>Example: Caching a repetitive task</summary>

    Re-using the example from [keeping things small](/user-guide/best-practices/udf-best-practices/#keep-things-small):

    ‚ùå Not recommended:

    ```python showLineNumbers
    @fused.udf
    def inefficient_pipeline_udf(data_path):
        import pandas as pd

        df = pd.read_csv(data_path)
        # Some complicated processing logic to create df_processed
        processed_df = ...

        return processed_df
    ```

    ‚úÖ Instead, break it down AND cache the calls:

    ```python showLineNumbers
    @fused.udf
    def load_data_udf(data_path):
        import pandas as pd
        return pd.read_csv(data_path)
    ```

    ```python showLineNumbers
    @fused.udf
    def process_data_udf(df):
        import pandas as pd
        # Some complicated processing logic to create df_processed
        # ...
        return processed_df
    ```

    {/* NOTE: This might actually have a hit on performance as df needs to go from UDF -> Fused server -> UDF. If this is too much of a hit we need to update these guidelines */}
    ```python {5-7,9-11} showLineNumbers
    @fused.udf
    def pipeline_udf(data_path):
        import pandas as pd

        @fused.cache
        def load_data(data_path):
            return fused.run(load_data_udf, data_path=data_path)

        @fused.cache
        def process_data(df):
            return fused.run(process_data_udf, df=df)

        df = load_data(data_path)
        processed_df = process_data(df)

        return processed_df
    ```
</details>

:::tip
Read more about the caching details:
- [in the dedicated section](/core-concepts/cache/)
- How you can use cache to [speed up exploration of slow to read datasets](/core-concepts/data-ingestion/why-ingestion/#using-cache-as-a-single-use-ingester)
:::

### Prepare your large datasets

Fused works at its best with data that is fast to read and can be read in tiles or chunks. We know that most of the data out there isn't in the most [efficient file formats](/core-concepts/data-ingestion/file-formats/) which is why we provide tools to [ingest your own data](/core-concepts/data-ingestion/ingestion-your-data/) into cloud-optimized, partitioned formats.

We have a [dedicated page](/core-concepts/data-ingestion/why-ingestion/#when-is-ingestion-needed) for when you should consider ingesting your own data. As a rule of thumb you want to consider ingesting your data when:
- Files are read multiple times and >100MB
- Files that are slow or require some processing to open (`.zip` for example)

### Don't start from scratch: [UDF Catalog](/workbench/udf-catalog/)

Just like using libraries in Python to leverage existing tools, you don't need to start from scratch in Fused. We have a [Catalog of existing UDFs](/workbench/udf-catalog/) built & maintained by us and the community.

You can find a host of different UDFs that can serve as a starting point or as inspiration to create your own UDFs:
- Open datasets from common open repository [like STAC catalogs](https://www.fused.io/workbench/catalog/Sentinel_Tile_Example_2-ddb4c495-40e3-48ba-849c-256487f8a9cb)
- Run [on the fly ML prediction](https://www.fused.io/workbench/catalog/Dl4eo_Airplane_Detection_Global-91f81c0c-08f6-4a0c-82f9-bd2e0322b4e7) on satellite images
- Compute a [spatial join](https://www.fused.io/workbench/catalog/Overture_Nsi-dd89972c-ce30-4544-ba0f-81fc09f5bbef) between 2 datasets

![UDF Catalog](/img/user-guide/best-practices/udf_catalog.png)

You can also [contribute your own UDFs](/workbench/udf-catalog/#contribute-to-fused) to the community!

## Debugging UDFs

The reality of writing code is that stuff breaks, often and sometimes in mysterious ways. Here's some of our recommendations for how to debug your UDFs

### Use `print()`

UDFs return `stdout` either in [Workbench Code Editor](/workbench/udf-builder/code-editor/) or locally when running `fused.run(udf)` so the easiest way to get info about your UDFs is to use good old `print`:

```python showLineNumbers
@fused.udf
def udf(n: int = 1):
    print(f"{n=}")
    return
```

Since Python 3.8 you can use [f-string debugging](https://docs.python.org/3/whatsnew/3.8.html#f-strings-support-for-self-documenting-expressions-and-debugging) which is what we recommend you use:
```python showLineNumbers
print(f"{my_fancy_variable=}")
```

This allows you to print many variables without getting lost with what is what

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs className="unique-tabs">
  <TabItem value="workbench" label="Workbench" default>
    ![UDF Catalog](/img/user-guide/best-practices/stdout.png)
  </TabItem>
  <TabItem value="notebook" label="notebook" default>
    ![UDF Catalog](/img/user-guide/best-practices/notebook_print_stdout.png)
  </TabItem>
</Tabs>

### Type all your inputs

We strongly recommend you [type](https://docs.python.org/3/library/typing.html) all your inputs with the appropriate type:

```python {3} showLineNumbers
@fused.udf
def udf(
    bounds:fused.types.Bounds=None, n:int=1
):
    ...
    return
```

This has 2 effects:
- It makes your code more readable to others
- Fused only supports a [few types](/core-concepts/write/#supported-types) at the moment. Any non-typed or unsupported types will be passed as `str`

### Use `time.time()`

Sometimes you're not sure what's taking so long. The simplest way to figure this out is to use [`time.time()`](https://docs.python.org/3/library/time.html#time.time):

<details>
    <summary>Example: finding a slow process</summary>

    ```python {4,9-11,17-19} showLineNumbers
    @fused.udf
    def udf():
        import time
        beginning_time = time.time()

        # long processing step #1
        time.sleep(5)
        end_process_1 = time.time()
        process_time_1 = round(
            end_process_1 - beginning_time, 2
        )
        print(f"{process_time_1=}")


        # short processing step
        time.sleep(0.2)
        process_time_2 = round(
            time.time() - end_process_1, 2
        )
        print(f"{process_time_2=}")

        return
    ```

    Would give us:

    ```
    >>> process_time_1=5.0
    >>> process_time_2=0.2
    ```
</details>

### Join the [Discord](https://discord.com/invite/BxS5wMzdRk) for support

We host & run a Discord server where you can ask any questions! We or the community will do our best to help you out!

![Discord](/img/user-guide/best-practices/discord_server.png)

---

// File: user-guide/best-practices/workbench-best-practices

# Workbench best practices

_Tips & Tricks for making [Fused Workbench](/workbench/) work for you_

import ReactPlayer from 'react-player';

![Workbench Overview](/img/user-guide/best-practices/new_workbench.png)

[Workbench](/workbench/) is a web-IDE built to make working with Fused UDFs even faster! 

## Experimenting with UDFs, fast

In [UDF Builder](/workbench/udf-builder/) you have access to a [Code Editor](/workbench/udf-builder/code-editor/) that runs your UDFs and outputs results directly on the [Map View](/workbench/udf-builder/map/) for you. As soon as you make changes they show up in Map View! 

### üí° Leverage all the [UDF Best Practices](/user-guide/best-practices/udf-best-practices/)

While this page is for Workbench, it builds on top of all the [Best Practices](/user-guide/best-practices/udf-best-practices/) that make your UDF fast & efficient. So if you haven't yet, take a look at our [dedicated UDF Best Practices](/user-guide/best-practices/udf-best-practices/).

### Use `return` to quickly explore data

Your UDF will stop at the first `return` it sees, which you can use to your advantage to return an intermediate result and explore it directly on the map:

<details>
    <summary>Example: Exploring `bounds` mid-UDF</summary>

    In this example, we're using the [Overture Maps Example UDF](https://www.fused.io/workbench/catalog/Overture_Maps_Example-77970462-8c4d-44dd-87ad-3520f5d7bd83) but not sure exactly what our `bounds` object looks like.

    The easiest way to check is simply to return it inside our UDF before any other logic:

    <ReactPlayer className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/user-guide/best-practices/workbench_return_early_1080p_socials.mp4" width="100%" />

</details>

### Use Workbench to explore data on top of Python

While you might be tempted to explore a specific row of a `GeoDataFrame` by filtering it and printing it, sometimes it faster to just click on it in [Map View](/workbench/udf-builder/map/) and explore in the [Selected Object section](/workbench/udf-builder/results/#selected-object)

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/eiffel_tower.mp4" width="100%" />

### Format your code for more visibility

You can hit `Opt + Shift + F` (or `Alt + Shift + F` on Windows/Linux) to format your code with a smaller line-length. This comes in handy if you don't want to scroll left and right to read your code, at the expense of having a bit more up and down scrolling to do


## üó∫Ô∏è Visualizing results

### Changing map view

Map View supports [multiple base maps](/workbench/udf-builder/map/#map-controls) that might be better suited for different uses cases.

- ‚ßâ Sometimes you just want a neutral basemap to focus on your data -> "Light" or "Dark" or even "Blank" basemaps are best
- üåç Other times you might want some context, in which case using the "Satellite" basemap will suit your need best

![Changing Map View](/img/user-guide/best-practices/eiffel_tower.png)

### Using the Visualize Tab 

You can easily change the styling of your data in the [Visualization Tab](/workbench/udf-builder/styling/):
- [Changing the color of your data](/workbench/udf-builder/styling/#color-styling) (especially for vector files)
- Using Visualize presets under "Surprised Me"
- [Creating your own custom `loadingLayer`](/workbench/udf-builder/styling/#custom-loadinglayer-and-errorlayer)

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/visualization_new.mp4" width="100%" />


### Adjusting Opacity for Better Insights (Advanced Tips)

Opacity control is essential for analyzing layered or dense data:

- **Detecting data density**: Lower opacity (20-50%) helps identify areas where multiple points or features overlap, appearing as darker regions
- **Visualizing through layers**: Set opacity between 30-70% to see underlying basemaps or other data layers
- **Highlighting confidence levels**: Use opacity to represent confidence or importance of different features

##### Example: Vegetation Segmentation with Semi-Transparent Overlay

The [Vegetation Segmentation UDF](https://github.com/fusedio/udfs/tree/b87a6638603d96f0eb13a1d259474817227b9245/public/Vegetation_Segmentation) demonstrates how reducing opacity helps visualize segmentation results while still seeing the underlying imagery:

```json
"rasterLayer": {
  "@@type": "BitmapLayer",
  "opacity": 0.3,
  "pickable": true
}
```

This visualization uses:
- 30% opacity to show the vegetation segmentation results as a semi-transparent overlay
- The underlying satellite imagery remains visible for context
- The pickable property allows users to hover over areas to see detailed values

<div style={{ textAlign: 'center' }}>
<img src="https://fused-magic.s3.us-west-2.amazonaws.com/thumbnails/udfs-staging/veg_segmentation_v2.png" alt="Vegetation segmentation with opacity" style={{ width: '80%'}} />
</div>


#### Detecting Feature Overlaps

Finding overlapping features is crucial for data validation and analysis:

- **Fill vs. stroke emphasis**: For overlap detection, use filled polygons with 40-60% opacity and thin strokes
- **Contrasting colors**: Use complementary colors when you need to distinguish overlapping features
- **Using H3 hexagons**: Convert irregular polygons to H3 hexagons for a regularized comparison
- **Color-coding categories**: Assign different colors to intersection vs. difference areas

##### Example: Advanced Overlap Analysis with H3 Hexagons

The [Overture East Asian Buildings IOU UDF](https://github.com/fusedio/udfs/tree/b87a6638603d96f0eb13a1d259474817227b9245/public/Overture_East_Asian_Buildings_IOU) compares 2 datasets together. Here's the visualisation we used:

```json
{
  "tileLayer": {
    "@@type": "TileLayer",
    "minZoom": 0,
    "maxZoom": 14,
    "tileSize": 256,
    "extrude": true,
    "pickable": true
  },
  "hexLayer": {
    "opacity": 0.5,
    "@@type": "H3HexagonLayer",
    "stroked": true,
    "filled": true,
    "pickable": true,
    "extruded": true,
    "getFillColor": {
      "@@function": "colorCategories",
      "attr": "how",
      "domain": [
        "intersection",
        "symmetric_difference"
      ],
      "colors": "TealRose"
    },
    "getHexagon": "@@=properties.hex",
    "getElevation": "@@=properties.ratio",
    "elevationScale": 50
  }
}
```

This visualization uses:
- H3 hexagons to normalize irregular polygon shapes
- Different colors to show where datasets overlap vs. differ
- Elevation to represent the overlap ratio
- 50% opacity to see through overlapping features

<div style={{ textAlign: 'center' }}>
<img src="https://fused-magic.s3.us-west-2.amazonaws.com/thumbnails/udfs-staging/overture_iou.png" alt="H3 hexagon overlap analysis" style={{ width: '80%'}} />
</div>

### Visual Debugging Techniques

- **Elevation as a debugging tool**: Use 3D elevation (`extruded: true`) to add an extra dimension to your analysis
- **Multi-dimensional visualization**: Combine opacity, color, and height to encode different aspects of your data
- **Color by data attributes**: Map colors to categorical attributes and height to quantitative metrics

### Optimizing Color Use for Analysis

- **Color categories for types**: Use distinct color schemes like `TealRose` to clearly distinguish between categories
- **Sequential colors for metrics**: Use color gradients for representing continuous values like ratios or densities
- **Diverging color schemes**: Highlight values above and below an interesting mid-point in quantitative data - the middle color is assigned to the critical value with contrasting colors on either end
- **Visibility first**: Choose colors that maintain visibility over your basemap (darker colors for light basemaps, lighter colors for dark basemaps)

For a complete reference of available color schemes, check out [CARTO color schemes](https://carto.com/carto-colors/) which are implemented in DeckGL and available in Fused. These include categorical schemes like `Bold` and `Pastel`, sequential schemes like `BluYl` and `Sunset`, and diverging schemes like `TealRose` and `Tropic`.



### Tilt Map view to explore 3D datasets

Map View gives you a top-level view of the world by default. But hold `Cmd` (or `Ctrl` on Windows / Mac) to tilt the view!

![Tilted View](/img/user-guide/best-practices/3dmap.png)

üèòÔ∏è This can be really helpful to explore 3D datasets like in this [DSM Zonal Stats UDF](https://www.fused.io/workbench/catalog/DSM_Zonal_Stats-0c801e56-d0c4-47e8-a5ce-90d37703bdb7).

## Navigating Workbench

### Using Keyboard Shortcuts: Command Palette

Workbench has built-in keyboard shortcuts & quick navigation features: Hit `Cmd + K` (or `Ctrl + K` on Windows / Linux) to bring up Command Palette or use the search bar in the header for quick access:

![Command Palette](/img/user-guide/best-practices/command_pallete.png)

Without lifting your hands from the keyboard you can:
- Open a New UDF
- Search the Docs, directly in Workbench!
- See some of the most helpful Keyboard Shortcuts

You'll find a more extended list of Keyboard Shortcuts under [preferences](/workbench/preferences):

![Preferences - Keyboard Shortcuts](/img/user-guide/best-practices/workbench_keyboard_shortcuts.png)

### Quickly jump from UDF Builder to File Explorer

UDF Builder & File Explorer work well together, so we've made easy to jump from one to the other

 {/* TODO: need video showing this once its in production*/}
- In UDF Builder, `Cmd + Click` on a `s3://...` path will open it directly in File Explorer
- In File Explorer double clicking on a file will prompt Fused to do its best at guessing which Catalog UDF to use to load this file in Code Editor

## Troubleshooting

If things feel a bit off, for example your UDF output looks suspicious here are a few things you can do:
- Manually rerun the UDF with `Shift + Enter`
- Check how much RAM your tab is using (in Chrome can easily do so by hovering the tab). Sometimes too much data is brought in to your browser and while we do our best to manage it properly it can get out of hand. A good old tab refresh goes a long way

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/refresh.mp4" width="100%" />

---

// File: user-guide/examples/dark-vessel-detection

_A complete example show casing how to use Fused to ingest data into a geo-partitioned, cloud friendly format, process images & vectors and use UDFs to produce an analysis_


### Requirements
- [access to Fused](/user-guide/login/)
- [access to a Jupyter Notebook](https://jupyter.org/)
- the following Python packages installed locally:
  - [`fused`](/python-sdk/#install)
  - [`pandas`](https://pandas.pydata.org/)
  - [`geopandas`](https://geopandas.org/en/stable/index.html)


## 1. The problem: Detecting illegal boats

{/* Start by showing what the end result looks like: "this is what we're going to do in this example"*/}

Monitoring what happens at sea isn't the easiest task. Shores are outfitted with radars and each ship has a transponder to publicly broadcast their location (using [Automatic Identification System, AIS](https://en.wikipedia.org/wiki/Automatic_identification_system)), but ships sometimes want to hide their location when taking part in illegal activities.

Global Fishing Watch [has reported on "dark vessels"](https://globalfishingwatch.org/research-project-dark-vessels/) comparing Sentinel 1 radar images to public AIS data and matching the two to compare where boats report being versus where they _actually_ are.

In this example, we're going to showcase a basic implementation of a similar analysis to identify _potential_ dark vessels, all in Fused.

import ImageWorkflow from '@site/docs/user-guide/examples/dark_vessel_methodology.png';

<div style={{textAlign: 'center'}}>
<img src={ImageWorkflow} alt="Dark Vessel Detection workflow" style={{width: 1000}} />
</div>

Here's the result of our analysis, running in real time in Fused:

import ReactPlayer from 'react-player'

{/* <ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/output_analysis_S1_ais.mp4" width="100%" /> */}
<ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/analysis_walkthrough_dark_vessel_detection.mp4" width="100%" />

{/* Need more context around how the analysis is done */}

Here are the steps we'll produce:
- Getting Sentinel 1 radar images over our Area of Interest and within our Time of Interest
- Run a simple algorithm to detect bright spots in radar images -> Return boats outline
- Fetch AIS data over the same Area & Time of Interest
- Merge the 2 datasets together
- Keep boats that appear in Sentinel 1 but not in AIS -> Those are our potential Dark Vessels.

import ImageDataPipeline from '@site/docs/user-guide/examples/dark_vessel_data_breakdown.png';

<div style={{textAlign: 'center'}}>
<img src={ImageDataPipeline} alt="Dark Vessel Detection data pipeline" style={{width: 1000}} />
</div>


:::note
    This is crude analysis, mostly meant to demonstrate some of the major components of Fused, not to expose any dark vessels. The analysis has some major limitations and itself should be taken with a grain of salt.

    That being said, we encourage you to use it as a starting point for your own work!
:::



## 2. Data for our analysis

{/* Need a blogpost / docs page about "why cloud native" or something along those lines */}

We will be using radar images and AIS data from free and open data sources for this study. Since we want to process and visualize this data, and iterate quickly, we want our datasets in a Cloud Native format.
At the highest level and in practice this means that our data is:
- On a cloud storage (i.e. on S3 buckets)
- In a format that's fast to read and allows loading only small areas at a time ([Cloud Optimized GeoTiff](https://cogeo.org/) or [GeoParquet](https://geoparquet.org/))

The datasets we'll be accessing:
- [Sentinel 1](https://sentinel.esa.int/web/sentinel/copernicus/sentinel-1) radar images from [Microsoft Planetary Computer Data Catalog](https://planetarycomputer.microsoft.com/dataset/sentinel-1-grd)


<details>
    <summary>Specifics of the Sentinel 1 Dataset</summary>

     For the nerds out there, we're using the Ground Range Detected product, not the [Radiometrically Terrain Corrected](https://planetarycomputer.microsoft.com/dataset/sentinel-1-rtc) because we're looking at boats in the middle of the ocean, so terrain shouldn't be any issue.
    - This dataset is available as Cloud Optimized GeoTiff through a [STAC Catalog](https://stacspec.org/en/), meaning we can directly use this data as is.

</details>

- AIS data from the [NOAA Digital Coast](https://www.coast.noaa.gov/digitalcoast/tools/ais.html). We have data all around the continental US [per day as individual `.zip` files](https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/index.html)
    - This dataset is not in a cloud native format. If we were to use this directly, every time we were to make a change to our analysis or look at a new area we'd have to find the correct `.zip` file, decompress it, read the entire AIS data for any given day then query only around our area of interest. This is possible, but brings out iteration speed from seconds to minutes.


import ImgAISNoaa from '@site/docs/user-guide/examples/AIS_noaa_coast_portal.png';

<div style={{textAlign: 'center'}}>
<img src={ImgAISNoaa} alt="Dark Vessel Detection data pipeline" style={{width: 600}} />
</div>

## 3. Ingesting AIS data

{/*
Topics:
- Sentinel 1 already in COG + STAC -> No need to do ingest it ourselves
- AIS is in zip files on a server somewhere -> we're going to re-ingest it ourselves to make it cloud native (i.e. on cloud on S3 bucket) + geo-partitioned (i.e. in a format that's fast to read - parquet + chunked accordingly)
 */}

Since our AIS dataset is not in a geo-partitioned, cloud native format, our first step is to [ingest](/core-concepts/data-ingestion/) it into a tiled format and put it on a cloud bucket. Thankfully we can [do all of this in Fused](/core-concepts/data-ingestion/ingestion-your-data/).
This will give us our 2014 AIS data on cloud storage in a GeoParquet format, allowing us to read data from small portions at a time.

To get this done we'll:
1. Get a location to store our partitioned AIS data on (this can be any AWS S3 bucket, we'll use one managed by Fused to make this simpler)
{/* Link to UDF docs */}
2. Write a simple UDF to unzip each AIS data over a given date range, read it and save it as a parquet file on S3
{/* Link to batch job doc */}
3. Run this UDF over all of the 2024 AIS archive
4. Ingest all of the monthly AIS `.parquet` files into geo-partitioned files to speed up their read time over small areas

### 3.1 - Deciding on where to store out partitioned data

We first need to unzip & read our AIS data before ingesting in as a geo-partitioned cloud native format. One simple way to do this is to write a UDF that reads a zip file and saves it as `.parquet` file on a mounted disk.
AIS datapoint from NOAA's platform are available per day but we'll aggregate them per month to make it simpler to manage. We can run all of the code in this section in a notebook locally, since we don't need to visualize any data.

:::note
    Fused UDFs by default run on serverless instances, so their local storage changes at every run. To keep data persistent across runs we use shared mounted storage across all the instances of your team
:::

`fused.file_path()` returns the mount path of any file we'd like to create. When run locally, it returns the local `/tmp/fused/` directory on your machine.
If this function is called inside a UDF, and that UDF is run on Fused servers, the returned path will be of the shared mounted storage `/mount/`. Specifying [`engine='local'`](/python-sdk/top-level-functions/#run) runs the UDF on your local machine.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import WorkbenchFiles from '@site/docs/user-guide/examples/workbench_files.png';

<Tabs className="unique-tabs">
  <TabItem value="local" label="Local" default>

        ```python showLineNumbers
        import fused

        datestr='2024_09_01'
        path=fused.file_path(f'/AIS/{datestr[:7]}')
        print(path)

        >>> /tmp/fused/AIS/2024_09
        ```
  </TabItem>
  <TabItem value="udf" label="In a UDF" default>
        If you run this code in the Fused Workbench or locally with `fused.run()` with default parameters, you'll see the `/mount/` directory.

        ```python showLineNumbers
        @fused.udf
        def see_file_path():
            datestr='2024_09_01'
            path=fused.file_path(f'/AIS/{datestr[:7]}')
            print(path)
            return

        fused.run(see_file_path)

        >>> /mount/AIS/2024_09
        ```
        You can see this directory in the [File Explorer](/workbench/file-explorer/) in [Workbench](/workbench/).

        <div style={{textAlign: 'center'}}>
        <img src={WorkbenchFiles} alt="The /mount directory in your Fused Workbench file explorer" style={{width: 500}} />
        </div>
  </TabItem>
  <TabItem value="local_udf" label="In a UDF run locally" default>

        ```python showLineNumbers
        @fused.udf
        def see_file_path():
            datestr='2024_09_01'
            path=fused.file_path(f'/AIS/{datestr[:7]}')
            print(path)
            return

        #highlight-next-line
        fused.run(see_file_path, engine='local')

        >>> /tmp/AIS/2024_09
        ```
  </TabItem>
</Tabs>


{/* NOTE: As of Jan 2024, fused.file_path() doesn't work reliably across real time, local & run_remote() */}


### 3.2 - Writing a UDF to open each AIS dataset

The rest of the logic is [write a UDF](/core-concepts/write/) to open each file, read it as a CSV and write it to parquet.

```python showLineNumbers
@fused.udf()
def read_ais_from_noaa_udf(datestr:str='2023_03_29', overwrite:bool=False):
    import os
    import requests
    import io
    import zipfile
    import pandas as pd
    import s3fs

    # This is the specific URL where daily AIS data is available
    url=f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{datestr[:4]}/AIS_{datestr}.zip'

    # This is our local mount file path
    path=fused.file_path(f'/AIS/{datestr[:7]}/')
    daily_ais_parquet = f'{path}/{datestr[-2:]}.parquet'

    # Skipping any existing files
    if os.path.exists(daily_ais_parquet) and not overwrite:
        print(f'{daily_ais_parquet} exist')
        return pd.DataFrame({'status':['exist']})

    # Download ZIP file to mounted disk
    r=requests.get(url)
    if r.status_code == 200:
        with zipfile.ZipFile(io.BytesIO(r.content), 'r') as z:
            with z.open(f'AIS_{datestr}.csv') as f:
                df = pd.read_csv(f)
                # MMSI is the unique identifier of each boat. This is a simple clean up for demonstration
                df['MMSI'] = df['MMSI'].astype(str)
                df.to_parquet(daily_ais_parquet)
                print(f"Saved {daily_ais_parquet}")
        return pd.DataFrame({'status':['Done'], "file_path": [daily_ais_parquet]})
    else:
        return pd.DataFrame({'status':[f'read_error_{r.status_code}']})
```

We can run this UDF a single time to make sure it works:

```python showLineNumbers
single_ais_month = fused.run(read_ais_from_noaa_udf, datestr="2024_09_01")

>>> Saved /mnt/cache/AIS/2024_09/01.parquet
```

To recap what we've done so far:
- Build a UDF that takes a date, fetches a `.zip` file from NOAA's AISDataHandler portal and saves it to our UDFs' mount (so other UDFs can access it)
- Run this UDF 1 time for a specific date

### 3.3 - Run this UDF over a month of AIS data

Next step: Run this for a whole period!

Since each UDF takes a few seconds to run per date, we're going to use the experimental `PoolRunner` to call a large pool of UDFs all at once with a large date range.
You can also [run multiple small UDFs](/core-concepts/run-udfs/run-small-udfs/#running-multiple-jobs-in-parallel) in parallel.

With a bit of Python gymnastics we can create a list of all the dates we'd like to process. Preparing to get all of September 2024 would look like this:

```python showLineNumbers
import pandas as pd
range_of_ais_dates = [str(i)[:10].replace('-','_') for i in pd.date_range('2024-09','2024-10')[:-1]]
print(range_of_ais_dates)

>>> ['2024_09_01', '2024_09_02', '2024_09_03', '2024_09_04', '2024_09_05', '2024_09_06', '2024_09_07', '2024_09_08', '2024_09_09', '2024_09_10', '2024_09_11', '2024_09_12', '2024_09_13', '2024_09_14', '2024_09_15', '2024_09_16', '2024_09_17', '2024_09_18', '2024_09_19', '2024_09_20', '2024_09_21', '2024_09_22', '2024_09_23', '2024_09_24', '2024_09_25', '2024_09_26', '2024_09_27', '2024_09_28', '2024_09_29', '2024_09_30']
```

We can create a simple `lamda` function that takes each value and sends it to a [`PoolRunner` job](/core-concepts/run-udfs/run-small-udfs/#experimental-using-run_pool-and-poolrunner):

```python showLineNumbers
utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
runs = utils.PoolRunner(lambda datestr: fused.run(read_ais_from_noaa_udf(datestr=datestr, overwrite=False)), range_of_ais_dates)

runs.get_result_all()
```

Since `PoolRunner` is running on a "real-time" instance, we can also query the status in a notebook as the job is executing:

import ImgGetResultsAll from '@site/docs/user-guide/examples/run_get_results_all_notebook_output.png';

<div style={{textAlign: 'center'}}>
<img src={ImgGetResultsAll} alt="Pool Runner live results" style={{width: 400}} />
</div>

We've now unzipped, opened & saved 30 days of data!

One handy way to make sure our data is in the right place is to check it in the Workbench File Explorer. In the search bar type: `file:///mount/AIS/2024_09/`:

import ImgEFSWorkbench from '@site/docs/user-guide/examples/fused_workbench_file_explorer_efs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgEFSWorkbench} alt="Pool Runner live results" style={{width: 500}} />
</div>

You'll see all our daily files! Notice how each file is a few 100 Mb. These files are still big individual files, i.e. would take a little while to read.

### 3.4 - Ingest 1 month of AIS data into a geo-partitioned format

{/* TODO: This section (and the docs in general) are missing info about number of chunks*/}

These individual parquet files are now store on our mount disk. We could save them directly onto cloud storage but before that we can geo-partition them to make them even faster to read. This will make us reduce the time it takes to access our data from [minutes to seconds](/core-concepts/data-ingestion/why-ingestion/).
Fused provides a simple way to do this with the [ingestion process](/core-concepts/data-ingestion/).

import ImgGeoParquet from '@site/docs/user-guide/examples/geoparquet_overview.png';

<div style={{textAlign: 'center'}}>
<img src={ImgGeoParquet} alt="A simple overview of Geoparquet benefits" style={{width: 800}} />
</div>

_Image credit from the [Cloud Native Geo slideshow](https://guide.cloudnativegeo.org/overview.html#/geoparquet)_

To do this we need a few things:
- **Our input dataset**: in this case our month of AIS data.
   {/*- We need to refer to the files directly so we'll use the */}
- **A target cloud bucket**: We're going to create a bucket to store our month of geo-partitioned AIS data in parquet files
- A target number of chunks to partition our data in. For now we're going to keep it at 500
- Latitude / Longitude columns to determine the location of each point

```python showLineNumbers
ais_daily_parquets = [f'file:///mnt/cache/AIS/{day[:-3]}/{day[-2:]}.parquet' for day in range_of_ais_dates]

job = fused.ingest(
    ais_daily_parquets,
    's3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09',
    target_num_chunks=500,
    lonlat_cols=('LON','LAT')
)
```

We'll send this job to a [large instance using `job.run_remote()`](/core-concepts/run-udfs/run_large/) as we latency doesn't matter much (we can wait a few extra seconds) and we'd rather have a larger machine & a larger storage:

```python showLineNumbers
job.run_remote(
    instance_type='r5.8xlarge', # We want why big beefy machine to do the partitioning in parallel, so large amounts of CPUs
    disk_size_gb=999 # Set a large amount of disk because this job will open each output parquet file to calculate centroid
)
```

Running this in a notebook gives us a link to logs so we can follow the progress of the job on the offline machine:

import ImgRunRemoteLogs from '@site/docs/user-guide/examples/ingest_run_remote_job_logs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgRunRemoteLogs} alt="Notebook run remote print" style={{width: 800}} />
</div>

Following the link shows us the live logs of what our job is doing:

import ImgWorkbenchRunRemoteLogs from '@site/docs/user-guide/examples/workbench_run_remote_logs.png';

<div style={{textAlign: 'center'}}>
<img src={ImgWorkbenchRunRemoteLogs} alt="Workbench run remote logs" style={{width: 800}} />
</div>

We can once again check that our geo-partitioned images are available using [File Explorer](/workbench/file-explorer/). This time because our files are a lot faster to read we can even see the preview in the map view:

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="60%" width="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/geopartioned_AIS_file_explorer.mp4"/>


Our AIS data is ready to be use for the entire month of September 2024. To narrow down our search, we now need to get a Sentinel 1 image. Since these images are taken every 6 to 12 days depending on the region, we'll find a Sentinel 1 image and then narrow our AIS data to just a few minutes before and after the acquisition time of the Sentinel 1 image.

## 4. Retrieving Sentinel 1 images

Sentinel 1 images are free & open, so thankfully for us others have already done the work of turning the archive into cloud native formats (and continue to maintain the ingestion as new data comes in).

We're going to use the [Microsoft Planetary Computer Sentinel-1 Ground Range Detected](https://planetarycomputer.microsoft.com/dataset/sentinel-1-grd) dataset, because it offers:
- Access through a [STAC Catalog](https://stacspec.org/en/) helping us only get the data we need and nothing else
- Images are in [Cloud Optimized Geotiff](https://cogeo.org/) giving us tiled images that load even faster
- [Examples of how to access the data](https://planetarycomputer.microsoft.com/dataset/sentinel-1-grd#Example-Notebook) so most of our work will be copy pasta

:::tip
    Most of the following section was written in Workbench's [UDF Builder](/workbench/udf-builder/) rather than in Jupyter Notebooks.

    We'll have the code in code blocks, you can run these anywhere but as we're looking at images, it's helpful to have UDF Builders' [live map](/workbench/udf-builder/map/) updated as you write your code.

    {/* TODO: Should also add a link to the best practices for writing UDFs in Workbench once we have that section */}
:::

Let's start with a basic UDF just returning our area of interest:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    # Convert bounds to GeoDataFrame using Fused utility function
    bounds = utils.bounds_to_gdf(bounds)
    return bounds
```

:::info
The code above demonstrates a basic User-Defined Function (UDF) that utilizes [Fused's utility functions](https://github.com/fusedio/udfs/blob/eeb22486da7b3a073371b278e3473c24438ec715/public/common/utils.py#L1891). These utilities provide a some general functions for geospatial operations and transformations. You can also define your own utils in the [module](http://localhost:3000/workbench/udf-builder/code-editor/#module) section of the workbench.
The utility function `bounds_to_gdf` is mentioned above is defined as:

```python
def bounds_to_gdf(bounds_list, crs=4326):
    import geopandas as gpd
    import shapely
    box = shapely.box(*bounds_list)
    return gpd.GeoDataFrame(geometry=[box], crs=crs)
```

This function converts a bounding box into a GeoDataFrame .
:::

This UDF simply returns our [Map viewport](/core-concepts/filetile/#fusedtypesviewportgdf) as a `gpd.GeoDataFrame`, this is a good starting point for our UDF returning Sentinel 1 images

While you can do this anywhere around the continental US (our AIS dataset covered shores around the US, so we want to limit ourselves there), if you want to follow along this is the area of interest we'll be using. You can overwrite this in the UDF directly:

```python {6-7} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    # Define our specific bounds
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    bounds = utils.bounds_to_gdf(specific_bounds)
    return bounds
```

Following the [Microsoft Planetary Computer examples for getting Sentinel-1](https://planetarycomputer.microsoft.com/dataset/sentinel-1-grd#Example-Notebook) we can add a few of the imports we need and call the STAC catalog:

{/* TODO: explain the bounds to gdf */}

```python {5-8,13-17} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    import planetary_computer
    import pystac_client
    import geopandas as gpd
    import shapely
    # Define our specific bounds and convert to GeoDataFrame
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    bounds = utils.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    return bounds
```

We already have a bounding box, but let's narrow down our search to the first week of September:

```python {10,20-26} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    import planetary_computer
    import pystac_client
    import geopandas as gpd
    import shapely

    time_of_interest="2024-09-03/2024-09-04"
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    bounds = utils.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"{len(items)=}")
    return bounds
```

This print statement should return something like:

```bash
Returned 15 Items
```

This will be the number of unique Sentinel 1 images covering our `bounds` and `time_of_interest`.

We can now use the [`odc` package](https://odc-stac.readthedocs.io/en/latest/intro.html) to load the first image & we'll use the VV polarisation from Sentinel 1 (VH could also work, and it would be good to iterate on this to visually assess which one would work best. We're keeping it simple for now, but feel free to test out both!).

We'll get an [`xarray.Dataset` object](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html) back that we can simply open & return as a `uint8` array:

```python {5,13,33-39,41-43} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
):
    import odc.stac
    import planetary_computer
    import pystac_client
    import geopandas as gpd
    import shapely


    time_of_interest="2024-09-03/2024-09-04"
    bands = ["vv"]

    # Convert bounds to GeoDataFrame for STAC operations
    specific_bounds = [-93.90425364, 29.61879782, -93.72767384, 29.7114987]
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    bounds = utils.bounds_to_gdf(specific_bounds)

    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-1-grd"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()
    print(f"{len(items)=}")

    ds = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=bands,
        resolution=10, # We want to use the native Sentinel 1 resolution which is 10m
        bbox=bounds.total_bounds,
    ).astype(float)

    da = ds[bands[0]].isel(time=0)
    image = da.values * 1.0
    return image.astype('uint8')
```

Which gives us a Sentinel 1 image over our area of interest:

import ImgS1Uint8 from '@site/docs/user-guide/examples/S1_da_return.png';

<div style={{textAlign: 'center'}}>
<img src={ImgS1Uint8} alt="Notebook run remote print" style={{width: 800}} />
</div>

We've simplified the process quite a bit here, you could also:
- Instead of loading `image.astype('uint8')`, do a more [controlled calibration and conversation to dB](https://www.mdpi.com/2504-3900/18/1/11)
- Select a more specific image rather than the 1st one in our stack
- Use a different band or band combination
- Use [Radiometrically Terrain Corrected](https://planetarycomputer.microsoft.com/dataset/sentinel-1-rtc) images

### 4.1 Cleaning our Sentinel 1 UDF

Before adding any new functionality, we're going to clean our UDF up a bit more:
- Move some of the functionality into separate functions
- Adding common error catching (so our UDF doesn't fail if no Sentinel 1 images are found within our AOI + date range if it's too narrow)
- Add a [cache decorator](/core-concepts/cache/) to code functions that retrieve data to speed up the UDF & reduce costs.

This will allow us to keep our UDF more readable (by abstracting code away) and more responsive. Cached functions store their result to disk, which makes a common query a lot more responsive and less expensive by using less compute

Here's our cleaned up UDF:

{/* TODO: explain the parameters */}


```python {4-6,14,33-36,51-54} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    bands: list=["vv"],
    resolution: int=10,
    time_of_interest: str="2024-09-03/2024-09-10",
):
    import pandas as pd
    da = get_data(bounds, time_of_interest, resolution, bands)

    image = da.values * 1.0
    return image.astype('uint8')

@fused.cache
def get_data(
    bounds: fused.types.Bounds,
    time_of_interest: str,
    resolution: int,
    bands: list,
    data_name: str="sentinel-1-grd"
):
    """
    Getting Sentinel Data from MPC
    Resolution is defined in meters as we're using EPSG:3857
    """
    import odc.stac
    import planetary_computer
    import pystac_client

    # Convert bounds to GeoDataFrame
    utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)

    if resolution < 10:
        print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
        resolution = 10
        print(f"{resolution=}")

    catalog = pystac_client.Client.open(
            "https://planetarycomputer.microsoft.com/api/stac/v1",
            # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
            modifier=planetary_computer.sign_inplace,
        )

    items = catalog.search(
        collections=[data_name],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query=None,
    ).item_collection()

    if len(items) < 1:
        print(f'No items found. Please either zoom out or move to a different area')
    else:
        print(f"Returned {len(items)} Items")

        def odc_load(bbox,resolution, time_of_interest):
            ds = odc.stac.load(
                    items,
                    crs="EPSG:3857",
                    bands=bands,
                    resolution=resolution,
                    bbox=bounds.total_bounds,
                ).astype(float)
            return ds

        ds=odc_load(bounds,resolution, time_of_interest)
        da = ds[bands[0]].isel(time=0)
        return da
```

We can make this even simpler by moving the `get_data` function under [the `Module` tab in the Workbench UDF Editor](/workbench/udf-builder/code-editor/#module):

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/using_utils_module.mp4" width="100%" />


<Tabs className="unique-tabs">
  <TabItem value="editor" label="Editor" default>
    ```python {8} showLineNumbers
   @fused.udf
   def udf(
        bounds: fused.types.Bounds,
        bands: list=["vv"],
        resolution: int=10,
        time_of_interest: str="2024-09-03/2024-09-10",
   ):
        from local_utils import get_data
        da = get_data(bounds, time_of_interest, resolution, bands)
        image = da.values * 1.0

        return image.astype('uint8')
    ```
  </TabItem>
  <TabItem value="module" label="Module" default>
    Rename the "Module name" to `local_utils`:

    ```python showLineNumbers
    @fused.cache
    def get_data(
        bounds: fused.types.Bounds,
        time_of_interest: str,
        resolution: int,
        bands: list,
        data_name: str="sentinel-1-grd"
    ):
        """
        Getting Sentinel Data from MPC
        Resolution is defined in meters as we're using EPSG:3857
        """
        import odc.stac
        import planetary_computer
        import pystac_client
        import geopandas as gpd

        # Convert bounds to GeoDataFrame if needed
        if not isinstance(bounds, gpd.GeoDataFrame):
            utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
            bounds = utils.bounds_to_gdf(bounds)

        if resolution < 10:
            print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
            resolution = 10
            print(f"{resolution=}")

        catalog = pystac_client.Client.open(
                "https://planetarycomputer.microsoft.com/api/stac/v1",
                # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
                modifier=planetary_computer.sign_inplace,
            )

        items = catalog.search(
            collections=[data_name],
            bbox=bounds.total_bounds,
            datetime=time_of_interest,
            query=None,
        ).item_collection()

        if len(items) < 1:
            print(f'No items found. Please either zoom out or move to a different area')
        else:
            print(f"Returned {len(items)} Items")

            def odc_load(bounds,resolution, time_of_interest):
                ds = odc.stac.load(
                        items,
                        crs="EPSG:3857",
                        bands=bands,
                        resolution=resolution,
                        bbox=bounds.total_bounds,
                    ).astype(float)
                return ds

            ds=odc_load(bounds,resolution, time_of_interest)
            da = ds[bands[0]].isel(time=0)
            return da
    ```
  </TabItem>
</Tabs>


## 5. Simple boat detection in Sentinel 1 radar images

Now that we have a Sentinel 1 image over a Area of Interest and time range, we can write a simple algorithm to return bounding boxes of the boats in the image. We're going to keep this very basic as we're optimizing for:
- **Speed of execution**: We want our boat detection algorithm to run in a few seconds at most while we're iterating. Especially at first when we're developing our pipeline, we want a fast feedback loop
- **Simplicity**: We're focused on demo-ing how to build an end-to-end pipeline with Fused in this example, not making the most thorough analysis possible. This should be a baseline for you to build upon!

Radar images over calm water tend to look black (as all the radar signal is reflect _away_ from the sensor), while (mostly metallic) boats reflect back to the sensor appearing like bright spots in our image. A simple "boat detecting" algorithm is thus to do a 2D convolution and threshold the output to a certain pixel value:


<Tabs className="unique-tabs">
  <TabItem value="editor" label="Editor" default>
    ```python {8,14,16} showLineNumbers
    @fused.udf
    def udf(
        bounds: fused.types.Bounds,
        bands: list=["vv"],
        resolution: int=10,
        time_of_interest: str="2024-09-03/2024-09-10",
    ):
        import numpy as np
        from local_utils import get_data, quick_convolution

        da = get_data(bounds, time_of_interest, resolution, bands)
        image = da.values * 1.0

        convoled_image = quick_convolution(np.array(image), kernel_size=2)

        return convoled_image.astype('uint8')
    ```
  </TabItem>
  <TabItem value="module" label="Module" default>

    ```python {58-76} showLineNumbers
   @fused.cache
   def get_data(
        bounds: fused.types.Bounds,
        time_of_interest: str,
        resolution: int,
        bands: list,
        data_name: str="sentinel-1-grd"
   ):
        """
        Getting Sentinel Data from MPC
        Resolution is defined in meters as we're using EPSG:3857
        """
        import odc.stac
        import planetary_computer
        import pystac_client


        # Convert bounds to GeoDataFrame
        utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
        bounds = utils.bounds_to_gdf(bounds)

        if resolution < 10:
            print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
            resolution = 10
            print(f"{resolution=}")

        catalog = pystac_client.Client.open(
                "https://planetarycomputer.microsoft.com/api/stac/v1",
                # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
                modifier=planetary_computer.sign_inplace,
            )

        items = catalog.search(
            collections=[data_name],
            bbox=bounds.total_bounds,
            datetime=time_of_interest,
            query=None,
        ).item_collection()

        if len(items) < 1:
            print(f'No items found. Please either zoom out or move to a different area')
        else:
            print(f"Returned {len(items)} Items")

            def odc_load(bounds,resolution, time_of_interest):
                ds = odc.stac.load(
                        items,
                        crs="EPSG:3857",
                        bands=bands,
                        resolution=resolution,
                        bbox=bounds.total_bounds,
                    ).astype(float)
                return ds

            ds=odc_load(bounds,resolution, time_of_interest)
            da = ds[bands[0]].isel(time=0)

            return da

    def quick_convolution(input_array, kernel_size):
        import numpy as np
        shifted_images = []

        # Shifting the image in all combinations of directions (x, y) with padding
        for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
            for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
                shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
                shifted_images.append(shifted)

        stacked_image = np.stack(shifted_images)
        return stacked_image.std(axis=0)

    def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):
        import numpy as np
        """Pad and shift an image by x_shift and y_shift with specified pad_value."""

        padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
        shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

        return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]
    ```
  </TabItem>
</Tabs>

This returns a filtered image highlighting the brightest spots and reducing the natural speckle of the radar image

import ImgConvolvedS1 from '@site/docs/user-guide/examples/convoled_s1_image.png';

<div style={{textAlign: 'center'}}>
<img src={ImgConvolvedS1} alt="Workbench run remote logs" style={{width: 800}} />
</div>

This is now relatively simple to vectorise (turn into a vector object, from image to polygons):

<Tabs className="unique-tabs">
  <TabItem value="editor" label="Editor" default>
    ```python {8-10,17-31} showLineNumbers
    @fused.udf
    def udf(
        bounds: fused.types.Bounds,
        bands: list=["vv"],
        resolution: int=10,
        time_of_interest: str="2024-09-03/2024-09-10",
    ):
        import geopandas as gpd
        import numpy as np
        from local_utils import get_data, quick_convolution, vectorise_raster
        utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
        bounds_gdf = utils.bounds_to_gdf(bounds)
        da = get_data(bounds, time_of_interest, resolution, bands)
        image = da.values * 1.0

        convoled_image = quick_convolution(np.array(image), kernel_size=2)

        gdf_predictions = vectorise_raster(
            raster=convoled_image.astype("uint8"),
            bounds=bounds_gdf,
            threshold=200 # Taking a high threshold in 0-255 range to keep only very bright spots
        )

        # Merging close polygons together into a single polygon so 1 polygon <-> 1 boat
        buffer_distance = 0.0001  # eyeballed a few meters in EPSG:4326 (degrees are annoying to work with ¬Ø\_(„ÉÑ)_/¬Ø)
        merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
            -buffer_distance/2
        )
        merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

        return merged_gdf
    ```
  </TabItem>
  <TabItem value="module" label="Module" default>
    ```python {77-97} showLineNumbers
    @fused.cache
    def get_data(
        bounds: fused.types.Bounds,
        time_of_interest: str,
        resolution: int,
        bands: list,
        data_name: str="sentinel-1-grd"
    ):
        """
        Getting Sentinel Data from MPC
        Resolution is defined in meters as we're using EPSG:3857
        """
        import odc.stac
        import planetary_computer
        import pystac_client

        utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
        bounds = utils.bounds_to_gdf(bounds)
        if resolution < 10:
            print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
            resolution = 10
            print(f"{resolution=}")

        catalog = pystac_client.Client.open(
                "https://planetarycomputer.microsoft.com/api/stac/v1",
                # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
                modifier=planetary_computer.sign_inplace,
            )
        items = catalog.search(
            collections=[data_name],
            bbox=bounds.total_bounds,
            datetime=time_of_interest,
            query=None,
        ).item_collection()

        if len(items) < 1:
            print(f'No items found. Please either zoom out or move to a different area')
        else:
            print(f"Returned {len(items)} Items")

            def odc_load(bounds,resolution, time_of_interest):
                ds = odc.stac.load(
                        items,
                        crs="EPSG:3857",
                        bands=bands,
                        resolution=resolution,
                        bbox=bounds.total_bounds,
                    ).astype(float)
                return ds

            ds=odc_load(bounds,resolution, time_of_interest)
            da =  ds[bands[0]].isel(time=0)

            return da

    def quick_convolution(input_array, kernel_size):
        import numpy as np
        shifted_images = []

        # Shifting the image in all combinations of directions (x, y) with padding
        for x in [-kernel_size, 0, kernel_size]: # Shift left (kernel_size), no shift (0), right (kernel_size)
            for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
                shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
                shifted_images.append(shifted)

        stacked_image = np.stack(shifted_images)
        return stacked_image.std(axis=0)

    def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):
        import numpy as np
        """Pad and shift an image by x_shift and y_shift with specified pad_value."""

        padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
        shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

        return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

    @fused.cache
    def vectorise_raster(raster, bounds, threshold: float = 0.5):
        from rasterio import features
        import rasterio
        import geopandas as gpd
        import shapely
        import numpy as np

        transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

        shapes = features.shapes(
            source=raster.astype(np.uint8),
            mask = (raster > threshold).astype('uint8'),
            transform=transform
        )

        gdf = gpd.GeoDataFrame(
            geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
            crs=bounds.crs
        )
        return gdf
    ```
  </TabItem>
</Tabs>


And that's how we have turned our Sentinel 1 image into a vector `gpd.GeoDataFrame` of bright objects:

import ImgVectorBoats from '@site/docs/user-guide/examples/vector_boats.png';

<div style={{textAlign: 'center'}}>
<img src={ImgVectorBoats} alt="Workbench run remote logs" style={{width: 800}} />
</div>


## 6. Retrieving AIS data for our time of Interest

To get our AIS data, we now need to retrieve the exact moment our Sentinel 1 images were acquired. We can use this information to only keep AIS points within just a few minutes around that time.

<Tabs className="unique-tabs">
  <TabItem value="editor" label="Editor" default>
    ```python {31-32} showLineNumbers@fused.udf
    def udf(
        bounds: fused.types.Bounds,
        bands: list=["vv"],
        resolution: int=10,
        time_of_interest: str="2024-09-03/2024-09-10",
    ):
        import geopandas as gpd
        import numpy as np
        from local_utils import get_data, quick_convolution, vectorise_raster

        utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
        bounds_gdf = utils.bounds_to_gdf(bounds)

        da = get_data(bounds, time_of_interest, resolution, bands)
        image = da.values * 1.0

        convoled_image = quick_convolution(np.array(image), kernel_size=2)

        gdf_predictions = vectorise_raster(
            raster=convoled_image.astype("uint8"),
            bounds=bounds_gdf,
            threshold=200 # Using higher threshold to make sure only bright spots are taken
        )

        # Merging close polygons together
        buffer_distance = 0.0001  # eyeballed value in EPSG:4326 so need to use degrees. I don't like degrees
        merged = gdf_predictions.geometry.buffer(buffer_distance).unary_union.buffer(
            -buffer_distance/2
        )
        merged_gdf = gpd.GeoDataFrame(geometry=[merged], crs=bounds.crs).explode()

        # Keeping metadata close by for merging with AIS data
        merged_gdf['S1_acquisition_time'] = da['time'].values

        return merged_gdf
    ```
  </TabItem>
  <TabItem value="module" label="Module" default>
    ```python showLineNumbers
    @fused.cache
    def get_data(
        bounds: fused.types.Bounds,
        time_of_interest: str,
        resolution: int,
        bands: list,
        data_name: str="sentinel-1-grd"
    ):
        """
        Getting Sentinel Data from MPC
        Resolution is defined in meters as we're using EPSG:3857
        """
        import odc.stac
        import planetary_computer
        import pystac_client

        utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
        bounds = utils.bounds_to_gdf(bounds)

        if resolution < 10:
            print("Resolution shouldn't be lower than Sentinel 1 or 2 native resolution. Bumping to 10m")
            resolution = 10
            print(f"{resolution=}")

        catalog = pystac_client.Client.open(
                "https://planetarycomputer.microsoft.com/api/stac/v1",
                # Details as to why we need to sign it are addressed here: https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/#Searching
                modifier=planetary_computer.sign_inplace,
            )
        items = catalog.search(
            collections=[data_name],
            bbox=bounds.total_bounds,
            datetime=time_of_interest,
            query=None,
        ).item_collection()

        if len(items) < 1:
            print(f'No items found. Please either zoom out or move to a different area')
        else:
            print(f"Returned {len(items)} Items")

            def odc_load(bounds,resolution, time_of_interest):
                ds = odc.stac.load(
                        items,
                        crs="EPSG:3857",
                        bands=bands,
                        resolution=resolution,
                        bbox=bounds.total_bounds,
                    ).astype(float)
                return ds

            ds=odc_load(bounds,resolution, time_of_interest)
            da =  ds[bands[0]].isel(time=0)

            return da

    def quick_convolution(input_array, kernel_size):
        import numpy as np
        shifted_images = []

        # Shifting the image in all combinations of directions (x, y) with padding
        for x in [-kernel_size, 0, kernel_size]:  # Shift left (kernel_size), no shift (0), right (kernel_size)
            for y in [-kernel_size, 0, kernel_size]:  # Shift up (kernel_size), no shift (0), down (kernel_size)
                shifted = pad_and_shift_image(input_array, x, y, pad_value=0, kernel_size=kernel_size)
                shifted_images.append(shifted)

        stacked_image = np.stack(shifted_images)
        return stacked_image.std(axis=0)

    def pad_and_shift_image(img, x_shift, y_shift, pad_value=0, kernel_size: int = 3):
        import numpy as np
        """Pad and shift an image by x_shift and y_shift with specified pad_value."""

        padded_img = np.pad(img, pad_width=kernel_size, mode='constant', constant_values=pad_value)
        shifted_img = np.roll(np.roll(padded_img, y_shift, axis=0), x_shift, axis=1)

        return shifted_img[kernel_size:-kernel_size, kernel_size:-kernel_size]

    @fused.cache
    def vectorise_raster(raster, bounds, threshold: float = 0.5):
        from rasterio import features
        import rasterio
        import geopandas as gpd
        import shapely
        import numpy as np

        transform = rasterio.transform.from_bounds(*bounds.total_bounds, raster.shape[1], raster.shape[0])

        shapes = features.shapes(
            source=raster.astype(np.uint8),
            mask = (raster > threshold).astype('uint8'),
            transform=transform
        )

        gdf = gpd.GeoDataFrame(
            geometry=[shapely.geometry.shape(shape) for shape, shape_value in shapes],
            crs=bounds.crs
        )
        return gdf
    ```
  </TabItem>
</Tabs>


`merged_gdf` now returns a column called `S1_acquisition_time` with the time the Sentinel 1 image was taken.

If we save and call this UDF with a [token](/core-concepts/run-udfs/run-small-udfs/#token) we can call it from anywhere, from a Jupyter Notebook or from another UDF. Let's create a new UDF in Workbench:

```python {14-18} showLineNumbers
# This is a new UDF
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: int="2024-09-03/2024-09-10",
):
    import fused

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found {s1_detections.shape[0]} Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : {s1_acquisition_month_day_hour_min}")

    return s1_detections
```

This prints out:

```bash
Found 16 Unique Sentinel 1 detections
Sentinel 1 image was acquired at : 2024-09-04 00:19:09
```

We can now create another UDF that will take this `s1_acquisition_month_day_hour_min` date + a bounding box in input and returns all the AIS points in that time + area.

We're going to leverage code from the community for this part, namely reading the AIS data from a geo-partitioned GeoParquet. Fused allows us to easily re-use any code we want and freeze it to a specific commit so it doesn't break our pipelines ([read more about this here](/core-concepts/run-udfs/run-small-udfs/#git-commit-hash-recommended-for-most-stable-use-cases))

We can use [this bit of code called `table_to_tile`](https://github.com/fusedio/udfs/blob/f8f0c0f5f5565c4f8be599c9fcb43368352f145f/public/common/utils.py#L163) which will either load the AIS data or the bounding box depending on our zoom level to keep our UDF fast & responsive.

:::note
    You could write a GeoParquet reader from scratch or call a UDF that you have that already does this, you don't have to use this option. But we want to show you how you can re-use bits of code from others here.
:::

```python {26-33} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    s1_acquisition_month_day_hour_min:str = '2024-09-04T00:19:09.175874',
    time_delta_in_hours: float = 0.1, # by default 6min (60min * 0.1)
    min_zoom_to_load_data: int = 14,
    ais_table_path: str = "s3://fused-users/fused/demo_user/AIS_2024_ingester/prod_2024_09", # This is the location where we had ingested our geo-partitioned AIS data
    ):
    """Reading AIS data from Fused partitioned AIS data from NOAA (data only available in US)"""
    import pandas as pd
    from datetime import datetime, timedelta

    # Load the utils module
    local_utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = local_utils.estimate_zoom(bounds)

    sentinel1_time = pd.to_datetime(s1_acquisition_month_day_hour_min)
    time_delta_in_hours = timedelta(hours=time_delta_in_hours)

    month_date = sentinel1_time.strftime('%Y_%m')
    monthly_ais_table = f"{ais_table_path}prod_{month_date}/"
    print(f"{monthly_ais_table=}")

    @fused.cache
    def getting_ais_from_s3(bounds, monthly_table):
        return local_utils.table_to_tile(
            bounds,
            table=monthly_ais_table,
            use_columns=None,
            min_zoom=min_zoom_to_load_data
        )

    ais_df = getting_ais_from_s3(bounds, monthly_ais_table)

    if ais_df.shape[0] == 0:
        print("No AIS data within this bounds & timeframe. Change bounds or timeframe")
        return ais_df

    if zoom > min_zoom_to_load_data:
        print(f"Zoom is {zoom=} | Only showing bounds")
        return ais_df

    print(f"{ais_df['BaseDateTime'].iloc[0]=}")
    ais_df['datetime'] = pd.to_datetime(ais_df['BaseDateTime'])
    mask = (ais_df['datetime'] >= sentinel1_time - time_delta_in_hours) & (ais_df['datetime'] <= sentinel1_time + time_delta_in_hours)
    filtered_ais_df = ais_df[mask]
    print(f'{filtered_ais_df.shape=}')
    return filtered_ais_df
```

In workbench UDF builder we can now see the output of both of our UDF:

import ImgAISandS1 from '@site/docs/user-guide/examples/AIS_and_Sentinel1.png';

<div style={{textAlign: 'center'}}>
<img src={ImgAISandS1} alt="Notebook run remote print" style={{width: 800}} />
</div>

We can now see that 1 of these boats doesn't have an associated AIS point (in red).

:::tip
    You can change the styling of your layers in [the Visualize tab](/workbench/udf-builder/styling/) to make them look like the screenshot above
:::

Now all we need to do is merge these 2 datasets together and keep all the boats that don't match an AIS point.

## 7. Merging the 2 datasets together

We can expand the UDF we had started in [section 6.](/user-guide/examples/dark-vessel-detection/#6-retrieving-ais-data-for-our-time-of-interest) to call our AIS UDF by passing a bounding box + `s1_acquisition_month_day_hour_min`.

We'll get the AIS data and join it with the Sentinel 1 detected boats by using [geopandas `sjoin_nearest`](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin_nearest.html) to get the nearest distance of each boat to an AIS point.

Any point with the closest AIS point >100m from the Sentinel 1 boat will be considered a potentiel "dark vessel".

```python {5,29-53} showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=None,
    time_of_interest: str="2024-09-03/2024-09-10",
    ais_search_distance_in_meters: int=10,
):
    import fused

    @fused.cache()
    def get_s1_detection(
        time_of_interest=time_of_interest,
        bounds=bounds):

        return fused.run(
            "fsh_673giUH9R6KqWFCOQtRfb3",
            time_of_interest=time_of_interest,
            bounds=bounds,
        )

    s1_detections = get_s1_detection()
    print(f"Found {s1_detections.shape[0]} Unique Sentinel 1 detections")

    # We want to keep AIS data only right around the time the S1 image was acquired
    s1_acquisition_date = s1_detections['S1_acquisition_time'].values[0]
    s1_acquisition_month = str(s1_acquisition_date.astype('datetime64[M]'))
    s1_acquisition_month_day_hour_min = s1_acquisition_date.astype('datetime64[s]').astype(str).replace('T', ' ')
    print(f"Sentinel 1 image was acquired at : {s1_acquisition_month_day_hour_min}")

    @fused.cache()
    def get_ais_from_s1_date(s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds):
        return fused.run("fsh_FI1FTq2CVK9sEiX0Uqakv", s1_acquisition_month_day_hour_min=s1_acquisition_month_day_hour_min, bounds=bounds)

    ais_gdf = get_ais_from_s1_date()

    # Making sure both have the same CRS
    s1_detections.set_crs(ais_gdf.crs, inplace=True)

    # Buffering AIS points to leverage spatial join
    ais_gdf['geometry'] = ais_gdf.geometry.buffer(0.005)

    joined = s1_detections.to_crs(s1_detections.estimate_utm_crs()).sjoin_nearest(
        ais_gdf.to_crs(s1_detections.estimate_utm_crs()),
        how="inner", # Using left, i.e. s1 as keys
        distance_col='distance_in_meters',
    )

    # Dark vessels will be unique S1 points that don't have an AIS point within 10m
    potential_dark_vessels = joined[joined['distance_in_meters'] > ais_search_distance_in_meters]
    print(f"Found {potential_dark_vessels.shape[0]} potential dark vessels")

    # back to EPSG:4326
    potential_dark_vessels.to_crs(s1_detections.crs, inplace=True)
    return potential_dark_vessels
```

And now we have a UDF that takes a `time_of_interest` and bounding box and returns potential dark vessel:

import ImgPotDarKvessel from '@site/docs/user-guide/examples/potential_dark_vessel.png';

<div style={{textAlign: 'center'}}>
<img src={ImgPotDarKvessel} alt="Potential dark vessel" style={{width: 800}} />
</div>

## Limitations & Next steps

This is a simple analysis, that makes a lot of relatively naive assumptions (for ex: all bright spots in SAR are boats for example, which only works in open water and not near the shore or around solid structures like ocean wind mills or oil rigs). There's a lot of ways in which it could be improved but provides a good starting point.

This could be improved in a few ways:
- Masking out any shore or known areas with static infrastructure (to limit potential false positives around coastal wind mill farms)

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="80%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/examples/dark-vessel-detection/false_positive_wind_mills.mp4" width="80%" />
Example of the [Block Island Wind Farm](https://en.wikipedia.org/wiki/Block_Island_Wind_Farm) in Rhode Island showing up as false positive "potential dark vessel": Wind mills appear as bright spots but don't have any AIS data associated to them
(`bounds=[-71.08534386325134,41.06338103121641,-70.89011235861962,41.15718153299125]` & `s1_acquisition_month_day_hour_min = "2024-09-03T22:43:33"`)

- Using a more sophisticated algorithm to detect boats. The current algorithm is naive, doesn't return bounding boxes but rather general shapes.
- Return more information derived from AIS data: Sometimes boats go dark for a certain period of time only, making it possible to tie a boat that was dark for a certain time to a known ship when it does have it's AIS on.
- Running this over all the coast of the continental US and/or over a whole year. This would be closer to the [Global Fishing Watch dark vessel detection project](https://globalfishingwatch.org/research-project-dark-vessels/).

If you want to take this a bit further check out:
- Running [UDFs at scale with `run_remote()`](/core-concepts/run-udfs/run_large/#running-a-large-job-job-jobrun_remote). You could use this to run this pipeline over a larger area or over a much longer time series (or both!) to find out more potentiel dark vessels
- More about Fused core concepts like chosing between [running UDFs based on Tile or File](/core-concepts/filetile/)

---

// File: user-guide/examples/zonal-stats

{/* TODO:
- Rename this example as it's not very clear it's a zonal stats, uses parallelization
- Intro doesn't need to explain FUsed workbench, already done in other sections. Should demo what we're going to build in this doc
*/}

_A step-by-step guide for data scientists._


### Requirements

- [access to Fused](/user-guide/login/)
- [access to a Jupyter Notebook](https://jupyter.org/)
- the following Python packages installed locally:
  - [`fused`](/python-sdk/#install)
  - [`pandas`](https://pandas.pydata.org/)
  - [`geopandas`](https://geopandas.org/en/stable/index.html)


## 1. Using Fused for a Zonal Statistics Example

In this guide, we'll estimate how much alfalfa grows in zones defined by polygon geometries.
This will show you how to:
- Bring in your data
- Write a UDF to process the data
- Run the UDF remotely & in parallel
- Create an app that shows your results and can be shared with anyone

```mermaid
---
title: Overview of Zonal Stats
---
graph LR
    A("Continuous Field<br/>(raster image)") --> D{Zonal Stats}
    B("Areas of Interest<br/>(vector polygons)") --> D
    D --> E(Areas of Interest<br/>w/ Attributes)
```


## 2. Bring in your data

{/* TODO:
- This doesn't explain _why_ we need to use `fused.ingest` -> What if my vector file is already on a S3 bucket?
- We should also provide users with some sample data so they can replicate the whole process. Then explain how to reproduce with their own data
*/}

You'll first upload your own vector table with `fused.ingest`. This spatially partitions the table and writes it in your specified S3 bucket as a [GeoParquet](/core-concepts/data-ingestion/file-formats/#for-vectors-tables) file. You'll then calculate zonal stats over a raster array of alfalfa crops in the [USDA Cropland Data Layer](https://catalog.data.gov/dataset/cropscape-cropland-data-layer) (CDL) dataset.

This example shows how to geo partition polygons of [Census Block Groups](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html) for Utah, which is a Parquet table with a `geometry` column. You can follow along with this file or any vector table you'd like. Read about other supported formats in [Ingest your own data](/core-concepts/data-ingestion/).

{/* TODO:
- We assume here that we're doing this locally? We need to explain this if so, not clear until now
- Why not do this in workbench? I like offering people to do it locally, but then we should explain why, and that they can do it in workbench
 */}

First, set up a local Python environment, install the latest Fused SDK with `pip install fused`, and [authenticate](/python-sdk/authentication/).

Now, write the following script to geo partition your data. Pass the URL of the table to `fused.ingest`. When you kick off an ingest job with `run_remote`, Fused spins up a server to geo partition your table and writes the output to the path specified by the `output` parameter. In the codeblock below, `fd://tl_2023_49_bg/` is the base path to your account's S3 bucket.

```python showLineNumbers
  import fused

  job = fused.ingest(
    input="https://www2.census.gov/geo/tiger/TIGER2023/BG/tl_2023_49_bg.zip",
    output="fd://tl_2023_49_bg/"
  )
  job_id = job.run_remote()
  ```

{/* TODO: Needs visuals / screenshots. Tell people, and show them. */}
After running the preceeding code, open [fused.io/jobs](https://www.fused.io/jobs) to view the job status and logs.

Once the job is complete, you can preview the output dataset in the [File Explorer](/workbench/file-explorer/).

:::note

You can also ingest data without installing anything by using this [Fused App](https://www.fused.io/workbench#app/s/i/fa_6XEw7ACaNo7Qtxggt6Cm9e).

:::

For the next step you can use the path to the data you just ingested or, if you prefer, this public sample table: `s3://fused-asset/data/tl_2023_49_bg/`.

import ImageDataPipeline from '@site/docs/user-guide/examples/zonal_stats_file_explorer.png';

<div style={{textAlign: 'center'}}>
  <img src={ImageDataPipeline} alt="Zonal Stats Parquet Files in Workbench File Explorer" style={{width: 1000}} />
</div>


## 3. Write a UDF to process the data

To see the data as we process it, we will write a UDF in the [Fused Workbench](https://www.fused.io/workbench). As you write code in the [UDF Builder](/workbench/udf-builder/) you'll see how visualization results, logs, and errors show up immediately.

To [write a UDF](/core-concepts/write/) simply wrap a Python function with the decorator `@fused.udf`.

The first parameter of this UDF, `bounds`. It is reserved for Fused to pass a `GeoDataFrame` which the UDF may use to spatially filter datasets, and usually corresponds to a web map [tile](/core-concepts/filetile/). This enables Fused to run the UDF for each tile in the viewport to distribute processing across multiple workers.

The `year` parameter is used to structure the S3 path of the CDL GeoTiff which the utility function `read_tiff` reads for the area defined by `bounds`. The `crop_id` parameter 36 corresponds to alfalfa the CDL [colormap](https://storage.googleapis.com/earthengine-stac/catalog/USDA/USDA_NASS_CDL.json), which the UDF uses to mask the raster array.

Fused lets you import utility Modules from other UDFs with `fused.load`. Their code lives in the [public UDFs repo](https://github.com/fusedio/udfs/blob/main/public/common/utils.py).

- `read_tiff` loads an array of the CDL dataset for the specified `bounds` extent and `year`
- `table_to_tile` loads the table you geo partitioned for the specified `bounds` extent
- `geom_stats` calculates zonal statistics by aggregating the `arr` variable over the geometries specified by the `gdf`


```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    year: int = 2020,
    crop_id: int = 36
):
    import numpy as np

    # Convert bounds to tile
    utils = fused.load("https://github.com/fusedio/udfs/tree/bb712a5/public/common/").utils
    zoom = utils.estimate_zoom(bounds)
    tile = utils.get_tiles(bounds, zoom=zoom)

    # Load CDLS data
    arr = utils.read_tiff(
      tile,
      input_tiff_path=f"s3://fused-asset/data/cdls/{year}_30m_cdls.tif"
    )

    # Mask for crop
    arr = np.where(np.isin(arr, [crop_id]), 1, 0)

    # Load polygons
    gdf = utils.table_to_tile(
      bounds,
      table='s3://fused-asset/data/tl_2023_49_bg/',
      min_zoom=5,
      use_columns=['NAMELSAD', 'GEOID', 'MTFCC', 'FUNCSTAT', 'geometry']
    )
    gdf.crs = 4326

    # Calculate zonal stats
    return utils.geom_stats(gdf, arr)
```

Try running the UDF in the [UDF Builder](/workbench/udf-builder/) and visually inspect the output on the map. See what happens when you change `year`. Try introducing print statements such as `print(arr)` and `print(gdf)` to show logs in the console.


import SLC_stats from '@site/docs/user-guide/examples/zonal_stats_map.png';

<div style={{textAlign: 'center'}}>
  <img src={SLC_stats} alt="Zonal Stats Map near Heliotrope Mountain" style={{width: 1000}} />
</div>

:::note

You might receive a [timeout error](/core-concepts/run-udfs/run-small-udfs/#defining-small-job) in the `stdout` tab. Try zooming into Utah on the map where the zonal areas are highlighted, to reduce the [size of the tile](/core-concepts/filetile/#tile) being passed into the UDF from the map.

:::


import Details from '@theme/MDXComponents/Details';

<details>
  <summary>You can also [style the map layer](/workbench/udf-builder/styling/) by setting this JSON in the [Visualize tab](/workbench/udf-builder/code-editor/#visualize)</summary>

```json showLineNumbers
{
  "tileLayer": {
    "@@type": "TileLayer",
    "minZoom": 0,
    "maxZoom": 19,
    "tileSize": 256,
    "pickable": true
  },
  "rasterLayer": {
    "@@type": "BitmapLayer",
    "pickable": true
  },
  "vectorLayer": {
    "@@type": "GeoJsonLayer",
    "stroked": true,
    "filled": true,
    "pickable": true,
    "lineWidthMinPixels": 1,
    "pointRadiusMinPixels": 1,
    "getFillColor": {
      "@@function": "colorContinuous",
      "attr": "count",
      "domain": [
        0,
        100
      ],
      "colors": "Tropic",
      "nullColor": [
        184,
        14,
        184
      ]
    },
    "getLineColor": [
      208,
      208,
      208,
      40
    ]
  }
}
```
</details>

## 4. Run the UDF remotely and in parallel

Now that you have a UDF, let's see three ways you can [Run UDFs](/core-concepts/run-udfs/) remotely:
- via an HTTP endpoint,
- in a Python application,
- in parallel.

### a. HTTP endpoint

Save the UDF and greate an HTTP endpoint for the UDF by clicking "Share" in the UDF Settings tab. You'll see snippets to call the UDF from anywhere using a [token](/core-concepts/run-udfs/run-small-udfs/#token). Try this by changing the HTTP endpoint's query parameter as shown. You should see the output in GeoJSON format.

```javascript
https://www.fused.io/server/v1/realtime-shared/fsh_46eSFZaR3q3SnoVB28pN0g/run/tiles/12/778/1548?dtype_out_vector=geojson&crop_id=36&year=2020
```

#### What just happened?

When you called the HTTP endpoint, Fused ran the UDF then sent back the output table and [debug logs](/core-concepts/write/#http-requests). In the URL above, `/12/778/1548` specifies the ZXY tile index to structure `bounds` and `dtype_out_vector=geojson` specifies the output format. Fused passes the `year` & `crop_type` parameters to the UDF as `int` based on their types in the function definition.

#### Why does this matter?

You called the HTTP endpoint with a shared token, which means any application may call the UDF and get data back without needing to configure credentials. You also passed parameters to the UDF, which enables you to dynamically generate data and define its output format.

To start seeing the full power of Fused, change the UDF code, save it, and call the endpoint again. You'll see the UDF automatically updates. When you call the UDF again it should run even faster.

### b. Python SDK

The share token also enables you to run the UDF in a Python environment. You can specify `bounds` as the same map tile as above by passing `x`, `y`, and `z` parameters.

```python showLineNumbers
import fused
fused.run("fsh_46eSFZaR3q3SnoVB28pN0g", x=778, y=1548, z=12)
```

:::note

If you haven't installed GeoPandas at this point, the data in the `geometries` column may appear as unparsed binary values from the GeoParquet file.

:::

You can also pass a `GeoDataFrame` to explicitly define a custom `bounds` and other parameters specific to the UDF.

```python showLineNumbers
import geopandas as gpd

# Square AOI near Utah Lake
bounds = gpd.GeoDataFrame.from_features({"type":"FeatureCollection","features":[{"type":"Feature","properties":{"shape":"Rectangle"},"geometry":{"type":"Polygon","coordinates":[[[-112.01315665811222,40.13628586159681],[-111.89330615564467,40.13628586159681],[-111.89330615564467,40.004073791892196],[-112.01315665811222,40.004073791892196],[-112.01315665811222,40.13628586159681]]]}}]})
fused.run("fsh_46eSFZaR3q3SnoVB28pN0g", bounds=bounds, year=2020, crop_id=36)
```

### c. Parallelization

Invoke the UDF in parallel with `run_pool` to increase performance when you want to run it repeatedly with different input parameters. In this case, we'll use it to call the UDF across a set of years. For a deeper dive, read how to [Call UDFs asynchronously](/core-concepts/async/).

```python showLineNumbers
import pandas as pd

def run_udf(year):
    gdf = fused.run("fsh_46eSFZaR3q3SnoVB28pN0g", bounds=bounds, year=year, crop_id=36)
    gdf['year'] = year
    return gdf

utils = fused.load("https://github.com/fusedio/udfs/tree/e1fefb7/public/common/").utils
gdfs = utils.run_pool(run_udf, [2019, 2020, 2021])
gdf_final = pd.concat(gdfs)
gdf_final
```

These examples show how you can easily integrate Fused into your analytics workflows. For example, you can group `gdf_final` by `GEOID` and `year` and calculate aggregates of the `count` and `stats` columns.

```python showLineNumbers
gdfs.groupby(['GEOID', 'year']).agg({'count':'sum', 'stats': 'mean'}).reset_index()
```

## 5. Create an app

Now that you've created a UDF and explored different ways to invoke it, you can create a data app to share your results. We'll structure the HTTP endpoint you created above to act as a Tile server (with `/run/tiles/{z}/{x}/{y}`), allowing it to be called from a [pydeck](https://deckgl.readthedocs.io/en/latest/index.html) `TileLayer` within the Fused App Builder. We'll also create a [Streamlit dropdown](https://docs.streamlit.io/develop/api-reference/widgets/st.selectbox) for users to set the year parameter.

import Iframe from "@site/src/components/Iframe";
import CODE from "@site/src/app-iframe/python/example_zstats.py";

<div style={{marginTop: '2rem'}}>
<Iframe
  id="iframe-1"
  code={CODE}
  requirements={[
    "/pyarrow/pyarrow-17.0.0-cp312-cp312-pyodide_2024_0_wasm32.whl",
    "micropip",
    "pyodide-unix-timezones", // needed by pyarrow
    "geopandas",
    "requests",
    "xarray",
    "yarl",
    // Commonly used in product:
    "pydeck",
  ]}
>
</Iframe>
</div>

\
Click "Copy shareable link" to share the app with others or [embed it in Notion](/user-guide/out/notion/#2-embed-the-map-into-notion)!

## 6. Conclusion and next steps

We've shown how you can use Fused to develop a distributed Python workflow to power an app. Through a simple sequence of steps we loaded data, wrote analytics code, and created an app to interact with the data. With a single click you went from experimental development code to a live application.

We hope this overview gives you a glimpse of what you can build with Fused. You can continue to learn how to [get data in](/user-guide/in/), [transform data](/user-guide/transform/), and [integrate](/user-guide/out/) with other applications.

Find inspiration for your next project, ask questions, or share your work with the Fused community.

- [GitHub](https://github.com/fusedio/udfs/tree/main)
- [Discord](https://bit.ly/fusedslack)
- [LinkedIn](https://www.linkedin.com/company/fusedio/)
- [Twitter](https://twitter.com/Fused_io)

---

// File: user-guide/in/awsearth

Fused loads data from the [Earth on AWS](https://aws.amazon.com/earth/) data catalog with the Python [`pystac_client`](https://pystac-client.readthedocs.io/en/stable/) library.

This UDF loads data from an endpoint by [Element 84](https://aws.amazon.com/marketplace/seller-profile?id=seller-ndxivtlrwqtuc). It first searches for items in the `cop-dem-glo-30` collection within the area specified by [`bounds`](/core-concepts/filetile/#the-bounds-object) then loads their data with [`odc.stac.load`](https://odc-stac.readthedocs.io/en/latest/_api/odc.stac.load.html).

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.Bounds):
    import odc.stac
    import pystac_client
    from pystac.extensions.eo import EOExtension as eo

    utils = fused.load(
        "https://github.com/fusedio/udfs/tree/bb712a5/public/common/"
    ).utils
    zoom = utils.estimate_zoom(bounds)
    tile = utils.get_tiles(bounds, zoom=zoom)

    odc.stac.configure_s3_access(aws_unsigned=True)
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

    items = catalog.search(
        collections=["cop-dem-glo-30"],
        bbox=bounds.total_bounds,
    ).item_collection()

    resolution = int(20 * 2 ** (13 - zoom))

    ds = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=["data"],
        resolution=resolution,
        bbox=bounds,
    ).astype(float)
    arr = ds["data"].max(dim="time")
    return utils.arr_to_plasma(arr.values, min_max=(0, 500), reverse=False)
```

---

// File: user-guide/in/bigquery

Fused integrates with [Google BigQuery](https://cloud.google.com/bigquery/docs/introduction) with the Python [`bigquery`](https://cloud.google.com/python/docs/reference/bigquery/latest) library.

## 1. Authenticate with a Google Service Account

Create a UDF to set your Google [Service Account credentials](https://cloud.google.com/bigquery/docs/use-service-accounts) in your Fused runtime [disk](/core-concepts/content-management/file-system/#mntcache-disk) in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    import os, json

    # Google Key as JSON
    data = {
        'type': 'service_account',
        'project_id': 'MYPROJECT',
        'private_key_id': '1234',
        'private_key': '-----BEGIN PRIVATE KEY-----...\n-----END PRIVATE KEY-----\n',
        'client_email': 'fused-account@MYPROJECT.iam.gserviceaccount.com',
        'client_id': '1234567',
        'auth_uri': 'https://accounts.google.com/o/oauth2/auth',
        'token_uri': 'https://oauth2.googleapis.com/token',
        'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs',
        'client_x509_cert_url': 'https://www.googleapis.com/robot/v1/metadata/x509/fused-pg%40MYPROJECT.iam.gserviceaccount.com',
        'universe_domain': 'googleapis.com'
    }

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/bq_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

## 2. Load data from BigQuery

Create a UDF to perform a query on a BigQuery dataset and return the results as a DataFrame or GeoDataFrame. Authenticate by passing the key file path to `service_account.Credentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, geography_column=None):
    from google.cloud import bigquery
    from google.oauth2 import service_account

    # This UDF will only work on runtime with mounted EFS
    key_path = "/mnt/cache/bq_creds.json"

    # Authenticate BigQuery
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )

    # Create a BigQuery client
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # Structure spatial query
    query = f"""
        SELECT * FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`
        LIMIT 10
    """

    if geography_column:
        return client.query(query).to_geodataframe(geography_column=geography_column)
    else:
        return client.query(query).to_dataframe()
```

---

// File: user-guide/in/csv

Load CSV files in a Fused UDF using pandas. Upload a CSV file to your Fused environment's [S3 bucket](/core-concepts/content-management/file-system/#fd-s3-bucket) through the [file browser](/workbench/file-explorer/) or link to a public CSV file.


```python showLineNumbers
@fused.udf
def udf():
    import pandas as pd

    # Read csv file
    df = pd.read_csv("https://pasteur.epa.gov/uploads/10.23719/1528686/SupplyChainGHGEmissionFactors_v1.2_NAICS_CO2e_USD2021.csv")
    return df
```

---

// File: user-guide/in/duckdb

import Tag from '@site/src/components/Tag'

Fused interfaces [DuckDB](https://duckdb.org/docs/) with the Python [`duckdb`](https://duckdb.org/docs/api/python/overview) library.

## 1. Create a DuckDB client and set `home_directory`



```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, agg_factor=3, min_count=5):
    import duckdb

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils
    con = duckdb.connect()
    con.sql("""
    SET home_directory='/tmp/';
    install 'httpfs'; load 'httpfs';
    """)
```


## 2. Write a query

This example UDF shows how to query the [NYC Taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) dataset with a [DuckDB](https://duckdb.org/).

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, agg_factor=3, min_count=5):
    import duckdb

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils
    con = duckdb.connect()
    con.sql("""
    SET home_directory='/tmp/';
    install 'httpfs'; load 'httpfs';
    """)

    df = con.sql(f"""
    SELECT
        round(pickup_longitude*{agg_factor},3)/{agg_factor} lng,
        round(pickup_latitude*{agg_factor},3)/{agg_factor} lat,
        count(1) cnt
    FROM
        read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet')
    GROUP BY
        round(pickup_longitude*{agg_factor},3),
        round(pickup_latitude*{agg_factor},3)
    HAVING
        cnt>{min_count} AND
        lat>40 AND
        lat<41
    """).df()

    print("Number of trips:", df.cnt.sum())
    gdf = utils.geo_convert(df)
    return gdf
```

---

// File: user-guide/in/excel

Read an [Excel](https://www.microsoft.com/en-ca/microsoft-365/excel) file from a URL with [`pd.read_excel`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html).

```python showLineNumbers
@fused.udf
def udf():
    import pandas as pd

    # Read census .xlsx file
    df = pd.read_excel(
        "https://www2.census.gov/programs-surveys/acs/summary_file/2021/sequence-based-SF/documentation/tech_docs/ACS_2021_SF_5YR_Appendices.xlsx"
    )

    return df
```

---

// File: user-guide/in/gee

import CellOutput from "@site/src/components/CellOutput.jsx";
import Tag from '@site/src/components/Tag'

Fused interfaces [Google Earth Engine](https://earthengine.google.com/) with the Python [`earthengine-api`](https://github.com/google/earthengine-api) library. This example shows how to load data from GEE [datasets](https://developers.google.com/earth-engine/datasets) into Fused UDFs and read it with xarray.

## 1. Authenticate with a Google Service Account

Create a UDF to set your Google [Service Account credentials](https://developers.google.com/earth-engine/guides/service_account) in your Fused runtime [disk](/core-concepts/content-management/file-system/#mntcache-disk) in a file in the `/mnt/cache` directory.

```python showLineNumbers
@fused.udf
def udf():

    import os, json

    # Google Key as JSON
    data = {
        'type': 'service_account',
        'project_id': 'MYPROJECT',
        'private_key_id': '1234',
        'private_key': '-----BEGIN PRIVATE KEY-----...\n-----END PRIVATE KEY-----\n',
        'client_email': 'fused-account@MYPROJECT.iam.gserviceaccount.com',
        'client_id': '1234567',
        'auth_uri': 'https://accounts.google.com/o/oauth2/auth',
        'token_uri': 'https://oauth2.googleapis.com/token',
        'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs',
        'client_x509_cert_url': 'https://www.googleapis.com/robot/v1/metadata/x509/fused-pg%40MYPROJECT.iam.gserviceaccount.com',
        'universe_domain': 'googleapis.com'
    }

    # Define the target path for the new GEE credentials file
    key_path = '/mnt/cache/gee_creds.json'

    # Write the loaded JSON data to the new file
    with open(key_path, 'w') as file:
        json.dump(data, file)
```

## 2. Load data from Google Earth Engine

Create a UDF to load data from a GEE ImageCollection and open it with xarray. Authenticate by passing the key file path to `ee.ServiceAccountCredentials`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, n=10):
    import ee
    import xarray

    utils = fused.load("https://github.com/fusedio/udfs/tree/f928ee1/public/common/").utils

    # Authenticate GEE
    key_path = '/mnt/cache/gee_creds.json'
    credentials = ee.ServiceAccountCredentials("fused-account@fused-gee.iam.gserviceaccount.com", key_path)
    ee.Initialize(opt_url="https://earthengine-highvolume.googleapis.com", credentials=credentials)

    # Generate GEE bounding box for spatial filter
    geom = ee.Geometry.Rectangle(*bounds.total_bounds)
    scale = 1 / 2 ** max(0, bounds.z[0])  # A larger scale will increase your resolution per z but slow down the loading

    # Load data from a GEE ImageCollection
    ic = ee.ImageCollection("MODIS/061/MOD13A2").filter(
        ee.Filter.date("2023-01-01", "2023-06-01")
    )

    # Open with xarray (the `xee` package must be present for engine="ee" to work)
    ds = xarray.open_dataset(ic, engine="ee", geometry=geom, scale=scale).isel(time=0)

    # Transform image color with a utility function
    arr = utils.arr_to_plasma(ds["NDVI"].values.squeeze().T, min_max=(0, 8000))
    return arr

```

---

// File: user-guide/in/geojson

Load a [GeoJSON](https://geojson.org/) file to create a GeoDataFrame with [GeoPandas](https://geopandas.org/en/stable/).

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd
    import requests

    data = requests.get("https://raw.githubusercontent.com/python-visualization/folium-example-data/main/us_states.json").json()
    print(data)

    gdf = gpd.GeoDataFrame.from_features(data)
    return gdf
```

---

// File: user-guide/in/geotiff

Read a GeoTIFF with the Fused [`utils.read_tiff`](https://github.com/fusedio/udfs/blob/main/public/common/utils.py#L213) utility function, which reads a portion of a tiff file based on the spatial extent of a provided `bounds`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, year="2022", chip_len=256):
    import numpy as np

    utils = fused.load(
        "https://github.com/fusedio/udfs/tree/f928ee1/public/common/"
    ).utils

    input_tiff_path = f"s3://fused-asset/data/cdls/{year}_30m_cdls.tif"
    arr, color_map = utils.read_tiff(
        bounds, input_tiff_path, output_shape=(chip_len, chip_len), return_colormap=True
    )

    colored_array = (
        np.array([color_map[value] for value in arr.flat], dtype=np.uint8)
        .reshape(arr.shape + (4,))
        .transpose(2, 0, 1)
    )
    return colored_array
```

---

// File: user-guide/in/pc

import Tag from '@site/src/components/Tag'

Fused loads data from the [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/) data [catalog](https://planetarycomputer.microsoft.com/catalog) with the Python [`pystac_client`](https://pystac-client.readthedocs.io/en/stable/) library.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None, time_of_interest="2023-11-01/2023-12-30"):

    import odc.stac
    import planetary_computer
    import pystac_client

    utils = fused.load(
        "https://github.com/fusedio/udfs/tree/f928ee1/public/common/"
    ).utils

    red_band = "B04"
    nir_band = "B08"
    catalog = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        modifier=planetary_computer.sign_inplace,
    )

    items = catalog.search(
        collections=["sentinel-2-l2a"],
        bbox=bounds.total_bounds,
        datetime=time_of_interest,
        query={"eo:cloud_cover": {"lt": 10}},
    ).item_collection()

    resolution = int(5 * 2 ** (15 - bounds.z[0]))
    ds = odc.stac.load(
        items,
        crs="EPSG:3857",
        bands=[nir_band, red_band],
        resolution=resolution,
        bbox=bounds.total_bounds,
    ).astype(float)

    # Calculate NDVI
    ndvi = (ds[nir_band] - ds[red_band]) / (ds[nir_band] + ds[red_band])

    arr = ndvi.max(dim="time")
    return utils.arr_to_plasma(arr.values, min_max=(0, 0.8), reverse=False)
```

---

// File: user-guide/in/shapefile

Load [shapefiles](https://en.wikipedia.org/wiki/Shapefile) with [GeoPandas](https://geopandas.org/en/stable/).

```python showLineNumbers
@fused.udf
def udf():
    import geopandas as gpd

    # Shape file as zip
    url = "https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/11_DISTRICT_OF_COLUMBIA/11/tl_rd22_11_bg.zip"
    gdf = gpd.read_file(url)
    return gdf
```

---

// File: user-guide/in/wmstile

import WMS1 from '@site/static/img/wms1.png';

<div style={{textAlign: 'center'}}>
<img src={WMS1} alt="File" style={{width: 600}} />
</div>

This UDF fetches WMS tiles from [Terrestris OpenStreetMap WMS](https://ows.terrestris.de/osm/service), using the `OSM-WMS` layer, which provides standard OpenStreetMap imagery. This function requests tiles based on bounding box coordinates ```bounds```, coordinate reference system ```CRS```, and image size ```256x256 pixels```. The result is a NumPy array that can be displayed as an image on the tiles.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    wms_url: str = "https://ows.terrestris.de/osm/service",
    layer: str = "OSM-WMS",
    width: int = 256,
    height: int = 256
):  
    import numpy as np
    import utils  
    from utils import fetch_wms
    

    z = utils.common.estimate_zoom(bounds)
    print(f"Estimated zoom level: {z}")
    
    if z < 4:
        print("WARNING: Please zoom in more for better visualization. Zoom level should be at least 4.")

        return np.zeros((4, height, width), dtype=np.uint8)
    
    data = fetch_wms(
        wms_url=wms_url,
        layer=layer,
        bbox_coords=bounds,
        width=width,
        height=height 
    )
    
    return data
```

The ```fetch_wms``` Utils function looks like this. 

```python showLineNumbers
common = fused.load("https://github.com/fusedio/udfs/blob/main/public/common/utils.py").utils

def fetch_wms(
    wms_url: str, 
    layer: str, 
    bbox_coords: tuple, 
    width: int, 
    height: int, 
    version: str = "1.3.0", 
    format: str = "image/png", 
    crs: str = "EPSG:4326", 
    transparent: bool = True
):
    import requests
    from io import BytesIO
    from PIL import Image
    import numpy as np
    try:
        minx, miny, maxx, maxy = bbox_coords
        
        # Handle bbox order based on WMS version and CRS
        if version == "1.3.0" and crs.upper() in ["EPSG:4326", "CRS:84"]:

            bbox_str = f"{miny},{minx},{maxy},{maxx}"
        else:

            bbox_str = f"{minx},{miny},{maxx},{maxy}"
        
        # Construct WMS parameters
        params = {
            'SERVICE': 'WMS',
            'VERSION': version,
            'REQUEST': 'GetMap',
            'LAYERS': layer,
            'STYLES': '',
            'CRS' if version == "1.3.0" else 'SRS': crs,
            'BBOX': bbox_str,
            'WIDTH': width,
            'HEIGHT': height,
            'FORMAT': format,
            'TRANSPARENT': 'TRUE' if transparent else 'FALSE'
        }

        response = requests.get(wms_url, params=params)

        if response.status_code != 200:
            print(f"Error: HTTP status {response.status_code}")
        
        # Check content type
        content_type = response.headers.get('Content-Type', '')
        
        # Process image response
        if 'image' in content_type.lower() or response.content[:4] in [b'\xff\xd8\xff\xe0', b'\x89PNG']:
            img = Image.open(BytesIO(response.content))
            array = np.array(img)
            
            # Convert to channels-first format
            if len(array.shape) == 3:  # RGB or RGBA
                array = array.transpose(2, 0, 1)
            else:  # Grayscale
                array = array[np.newaxis, :, :]
            
            return array
        else:
            print(f"Error: Response not an image. Content type: {content_type}")

            
    except Exception as e:
        print(f"Error fetching WMS: {e}")

    

```

## WMS Examples to try out in the UDF

The wms_url and layer parameters can be set to any WMS service and layer you want to visualize. Here are some examples to get you started:

- **wms_url** (str): The base URL of the WMS service
- **layer** (str): The layer name to request

## 1. NASA GIBS (Global Imagery)
**URL:** [https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi](https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi)  
**Layers:**
- `MODIS_Terra_CorrectedReflectance_TrueColor` - Satellite imagery
- `MODIS_Terra_Snow_Cover` - Snow cover data
- `MODIS_Terra_NDVI_8Day` - Vegetation index

---

import WMS2 from '@site/static/img/wms2.png';

<div style={{textAlign: 'center'}}>
<img src={WMS2} alt="File" style={{width: 600}} />
</div>

## 2. OpenStreetMap
**URL:** [https://ows.terrestris.de/osm/service](https://ows.terrestris.de/osm/service)  
**Layers:**
- `OSM-WMS` - OpenStreetMap base map
- `TOPO-WMS` - Topographic view

---


<div style={{textAlign: 'center'}}>
<img src={WMS1} alt="File" style={{width: 600}} />
</div>

## 3. Swiss Federal Geoportal
**URL:** [https://wms.geo.admin.ch/](https://wms.geo.admin.ch/)  
**Layers:**
- `ch.swisstopo.pixelkarte-farbe` - Swiss national map
- `ch.swisstopo.swissimage` - Aerial imagery

---

import WMS3 from '@site/static/img/wms3.png';

<div style={{textAlign: 'center'}}>
<img src={WMS3} alt="File" style={{width: 600}} />
</div>

---

// File: user-guide/out/deckgl

[DeckGL](https://deck.gl/) is a highly framework to create interactive map visualizations that handle large datasets.

This guide shows how to load data from Fused into DeckGL maps created from a single standalone HTML page with the following layer types:

- [H3HexagonLayer](/user-guide/out/deckgl/#h3hexagonlayer)
- [Vector Tile Layer](/user-guide/out/deckgl/#vector-tile-layers)
- [Raster Tile Layer](/user-guide/out/deckgl/#raster-tile-layers)

## 1. Generate a signed URL for a UDF

First create a UDF and [generate an HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests).

## 2. Create a DeckGL HTML map

Create an `.html` file following this template. This code creates a DeckGL map then introduces a layer that renders data from the specified Fused endpoint.

```html
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Fused DeckGL</title>
    <meta
      name="viewport"
      content="initial-scale=1,maximum-scale=1,user-scalable=no"
    />

    <script src="https://unpkg.com/@deck.gl/core@^9.0.0/dist.min.js"></script>
    <script src="https://unpkg.com/@deck.gl/layers@^9.0.0/dist.min.js"></script>
    <script src="https://unpkg.com/@deck.gl/geo-layers@^9.0.0/dist.min.js"></script>
    <script src="https://unpkg.com/@deck.gl/carto@^9.0.0/dist.min.js"></script>
    <script src="https://unpkg.com/h3-js"></script>
    <script src="https://unpkg.com/deck.gl@latest/dist.min.js"></script>
    <script src="https://api.mapbox.com/mapbox-gl-js/v3.2.0/mapbox-gl.js"></script>
    <link
      href="https://api.mapbox.com/mapbox-gl-js/v3.2.0/mapbox-gl.css"
      rel="stylesheet"
    />
    <style>
      body {
        width: 100vw;
        height: 100vh;
        margin: 0;
      }
    </style>
  </head>
  <body>
    <div id="map"></div>
    <script>
      const { DeckGL, H3HexagonLayer, GeoJsonLayer, BitmapLayer, TileLayer } = deck;

      new DeckGL({
        mapboxApiAccessToken:
          "pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA",
        mapStyle: "mapbox://styles/mapbox/dark-v10",
        initialViewState: {
          longitude: -122.417759,
          latitude: 37.776452,
          zoom: 12,
        },
        controller: true,
        layers: [
          new H3HexagonLayer({
            id: "H3HexagonLayer",
            data: "https://www.fused.io/server/v1/realtime-shared/f393efed9c75425365f2f00254d37cb15166e22fc5defabcc7ee6fd9e2d7a3b4/run/file?dtype_out_vector=json",
            extruded: true,
            getElevation: (d) => d.count,
            elevationScale: 20,
            filled: true,
            stroked: true,
            getFillColor: (d) => [255, (1 - d.count / 500) * 255, 0],
            getHexagon: (d) => d.hex,
            getLineColor: [255, 255, 255],
            getLineWidth: 2,
            lineWidthUnits: "pixels",
          }),
        ],
      });
    </script>
  </body>
</html>
```

### H3HexagonLayer

Create an [`H3HexagonLayer`](https://deck.gl/docs/api-reference/geo-layers/h3-hexagon-layer).

<iframe src="/img/deckgl_h3.html"  height="400px" width="100%" scrolling="no"></iframe>

```js
new H3HexagonLayer({
    id: "H3HexagonLayer",
    data: "https://www.fused.io/server/v1/realtime-shared/f393efed9c75425365f2f00254d37cb15166e22fc5defabcc7ee6fd9e2d7a3b4/run/file?dtype_out_vector=json",
    extruded: true,
    getElevation: (d) => d.count,
    elevationScale: 20,
    filled: true,
    stroked: true,
    getFillColor: (d) => [255, (1 - d.count / 500) * 255, 0],
    getHexagon: (d) => d.hex,
    getLineColor: [255, 255, 255],
    getLineWidth: 2,
    lineWidthUnits: "pixels",
}),
```

### Vector Tile Layer

Vector Tile layers are created by placing a [`GeoJsonLayer`](https://deck.gl/docs/api-reference/layers/geojson-layer) sublayer within a [`TileLayer`](https://deck.gl/docs/api-reference/geo-layers/tile-layer). Use the following snippet to introduce a vector layer.

The layer in the sample map comes from [Overture Buildings UDF](https://github.com/fusedio/udfs/tree/main/public/Overture_Maps_Example).

<iframe src="/img/deckgl_vector.html"  height="400px" width="100%" scrolling="no"></iframe>

```js
new TileLayer({
  // Use geojsonlayer inside of tilelayer. This is instead of MVT Layer, which has optimizations that can cause clipping when polygon extends beyond Tile area.
  id: "VectorTileLayer",
  data: "https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/tiles/{z}/{x}/{y}?dtype_out_vector=geojson",
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) => {
    const { boundingBox } = props.tile;

    return new GeoJsonLayer(props, {
      data: props.data,
      stroked: true,
      getLineColor: [0, 255, 10],
      getLineWidth: 10,
      getFillColor: [0, 40, 0, 0.5],
      getPointRadius: 4,
      getLineWidth: 5,
      pointRadiusUnits: "pixels",
      bounds: [
        boundingBox[0][0],
        boundingBox[0][1],
        boundingBox[1][0],
        boundingBox[1][1],
      ],
    });
  },
});
```

### Raster Tile Layer

Raster Tile layers are created by placing a [`BitmapLayer`](https://deck.gl/docs/api-reference/layers/bitmap-layer) sublayer within a [`TileLayer`](https://deck.gl/docs/api-reference/geo-layers/tile-layer). Use the following snippet to introduce a raster layer. The sample layer below was created from the [NAIP Tile UDF](https://github.com/fusedio/udfs/tree/main/public/NAIP_Tile_Example).

<iframe src="/img/deckgl_raster.html"  height="300px" width="100%" scrolling="no"></iframe>

```js
new TileLayer({
  id: "RasterTileLayer",
  data: `https://www.fused.io/server/v1/realtime-shared/UDF_Arcgis_Rgb/run/tiles/{z}/{x}/{y}?dtype_out_raster=png`,
  maxZoom: 19,
  minZoom: 0,

  renderSubLayers: (props) => {
    const { boundingBox } = props.tile;

    return new BitmapLayer(props, {
      data: null,
      image: props.data,
      bounds: [
        boundingBox[0][0],
        boundingBox[0][1],
        boundingBox[1][0],
        boundingBox[1][1],
      ],
    });
  },
  pickable: true,
});
```

---

// File: user-guide/out/duckdb

# DuckDB

[DuckDB](https://duckdb.org/) is an open source, in-process, analytical database. Its can natively read several of output formats that Fused HTTP Endpoints return.


## 1. Generate a signed URL for a UDF

First create a UDF and generate an [HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests).

## 2. Install and load `httpfs`

To load Parquet files from remote endpoints from within DuckDB, you can install the `httpfs` extension.

```sql
INSTALL httpfs;
LOAD httpfs;
```

## 3. Query using `read_parquet`

Now you can make a query using the UDF URL, with the dtype_out_vector set to `parquet`:

```sql
SELECT *
FROM read_parquet('https://www.fused.io/server/v1/realtime-shared/221aa65f3d96f1a320ed0f4eea0d320724c0ddc0c75cbf70df711def11e2ecc5/run/file?dtype_out_vector=parquet');
```

You can pass parameters into the URL from the query:

```sql
SELECT *
FROM read_parquet('https://www.fused.io/server/v1/realtime-shared/221aa65f3d96f1a320ed0f4eea0d320724c0ddc0c75cbf70df711def11e2ecc5/run/file?dtype_out_vector=parquet&resolution=13');
```

---

// File: user-guide/out/felt

# Felt

[Felt](https://felt.com/) is an interactive and collaborative map making platform. Teams use it to quickly create stunning map visualizations - without needing to move data or install software. Sign up for a free account [here](https://felt.com/login).

{/* This is incredibly slow, needs to be a video instead of GIF */}
![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/felt_fused.gif)

Felt supports several [file formats](https://www.youtube.com/watch?v=zgadqdMMYp0&ab_channel=Felt), some of which can originate from hosted URLs. Fused UDFs can be configured to return data in some of those formats - all it takes is a simple URL call.

import ImgFelt3 from '@site/static/img/felt-3.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFelt3} alt="File" style={{width: 600}} />
</div>

## 1. Generate a signed URL for a UDF

First, create a UDF and [generate an HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests).

## 2. Load data into Felt

There are 2 ways to load data from Fused into Felt, depending on whether the UDF returns a raster or vector data type. Both are passed through the "Upload from URL" modal, but require particular URL structures. Read the [Felt docs](https://feltmaps.notion.site/Upload-Anything-b26d739e80184127872faa923b55d232#3e37f06bc38c4971b435fbff2f4da6cb) for a deeper dive into this feature.

import ImgFelt0 from '@site/static/img/felt-0.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFelt0} alt="File" style={{width: 600}} />
</div>

### Raster Tiles

Dynamically render XYZ raster tiles on the Felt map.

As an example, the ["Sentinel Tile Example"](https://github.com/fusedio/udfs/tree/main/public/Sentinel_Tile_Example) public UDF returns an XYZ tile URL of NDVI from the Sentinel dataset. Create a new instance of it on your workbench and ensure it successfully renders data under `Tile` mode. Next, generate a shared URL and modify it as such:

- Set `dtype_out_raster` to `png`.
- Set replace the values after `/run/tiles` with the `/{z}/{x}/{y}` template.
- Set UDF parameters as needed.

For example:

```bash
https://www.fused.io/server/v1/realtime-shared/da71c7bf79376f2e88b37be9ecd2679f2e40a5b79f9b0f2c75e7ea9a3f0c5171/run/tiles/{z}/{x}/{y}?dtype_out_raster=png
```

This should render the raster output tiles on the map. This example shows NDVI on the area surrounding the Panama Canal.

import ImgFelt2 from '@site/static/img/felt-2.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFelt2} alt="File" style={{width: 600}} />
</div>

### Vector

Load vector data in a one-off manner onto the Felt map.

As an example, the ["Single Route"](https://github.com/fusedio/udfs/tree/main/public/single_route) public UDF returns a composite vector line of driving directions between an origin and destination locations. Create a new instance of it on your workbench and ensure it successfully renders data under `File` mode. Next, generate a shared URL and modify it as such:

- Set `dtype_out_vector` to `csv` or `parquet`.
- Set UDF parameters as needed.

For example:

```bash
https://www.fused.io/server/v1/realtime-shared/4f8fe6b81811ce011d7045b9fffbcfb43663637a4c626955821566fe62e6fb57/run/file?dtype_out_vector=csv&lat_start=35.0154145&lng_start=-114.2253804&lat_end=33.9422&lng_end=-114.4036
```

Click "Upload from URL", paste the URL, and click "Add to map".

This should load the data and render it on the map.

import ImgFelt from '@site/static/img/felt.png';

<div style={{textAlign: 'center'}}>
<img src={ImgFelt} alt="File" style={{width: 600}} />
</div>

Note that once the data is loaded, it's cached in the Felt platform so it won't be affected by modifications to the UDF.

---

// File: user-guide/out/googlesheets

The Google Sheets [importData](https://support.google.com/docs/answer/3093335?hl=en)
command imports data at from a given url in `.csv` format. You can use it to load data
from a UDF into a Google Sheets cell.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/google_sheets.gif)



## 1. Create a UDF in Fused Hosted API

Create a UDF that returns a table then save it on Workbench to automatically create an
endpoint.

This example retrieves Caltrain live location data from GTFS realtime feed, and returns
it as a dataframe.

```python showLineNumbers
@fused.udf
def udf():
    import pandas as pd
    import requests

    r = requests.get(f"https://www.caltrain.com/files/rt/vehiclepositions/CT.json")
    j = r.json()
    df = pd.json_normalize(j["Entities"])
    return df
```

## 2. Create a URL for the UDF

Now, create a [shareable Tile HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) for the UDF.

Append this query parameter to the end of the URL to structure the response as a CSV
type: `?dtype_out_vector=csv`.

The generated URL should look like this:

`https://www.fused.io/server/v1/realtime-shared/940c7d75bb3f12f2c411beeee7293729d35a9429ebb3760df29fa84c3166b7b9/run/file?dtype_out_vector=csv`

## 3. Call the UDF in a cell

Paste the `importData` function with the UDF endpoint in a cell.

`=importData("https://www.fused.io/server/v1/realtime-shared/940c7d75bb3f12f2c411beeee7293729d35a9429ebb3760df29fa84c3166b7b9/run/file?dtype_out_vector=csv")`

When you enter this formula into a cell, Google Sheets will call the UDF, and the
returned dataframe will be loaded into the spreadsheet.



:::warning

Google Sheets' `importData` imposes data size constraints that will require the UDF to
return a conservative number of rows. Otherwise it may show the following error message.

import sl from '@site/static/img/sheets_limit.png';

<div style={{textAlign: 'center'}}>
<img src={sl} alt="File" style={{width: 600}} />
</div>

:::

---

// File: user-guide/out/http

{/* We need to explain where to get the HTTP endpoint in the first place, this just assumes you already know where to get them */}

Call a UDF via [HTTP requests](/core-concepts/run-udfs/run-small-udfs/#http-requests) and pass geometries as query parameters or as XYZ tiles.

## Specify geometry

There are 2 ways to specify geometries. The first method involves specifying XYZ tiles as path parameters and the second method entails passing a geometry query parameter.

### With an XYZ Tile

```python showLineNumbers
!curl -L -XGET "https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/tiles/16/11242/26162?dtype_out_vector=csv"
```

## With parameters for latitude and longitude pairs

This sample URL passes a geometry's polygon pairs in the `lng_list` and `lat_list`
parameters. It also passes parameters for `census_variable`, `costing`, `duration`,
`count`, and `buffer`.

```python showLineNumbers
!curl -L -XGET "https://app-staging.fused.io/server/v1/realtime-shared/efff19cb8e3e12d0df1f307c0198384c746b961635d404eb1302cf15ad031485/run/file?dtype_out_vector=csv&census_variable=median household income&lng_list=-74.01, -74.010901, -74.000008, -73.98255, -73.9830327, -73.99468, -73.9905&lat_list=40.71021, 40.714353, 40.728349, 40.731949, 40.7150147, 40.7252, 40.730013&costing=auto&duration=20&count=TRUE&buffer=5"
```

## With a GeoJSON

When parameters are passed to a User Defined Function (UDF) via HTTP requests, they must be encodable. This is because HTTP is a text-based protocol, and any data transmitted over it, including parameters, must be represented in a format that is compatible with the protocol.

The best way to pass a geometry as a query parameter is to use a URL-encoded GeoJSON. It's possible to encode a GeoJSON with `jq`.

```python showLineNumbers
# !apt-get install jq -q
```

```python showLineNumbers
!\
    export GEOJSON_ENCODED=$(printf '{"type": "FeatureCollection", "features": [{"id": "0", "type": "Feature", "properties": {"x": 11243, "y": 26163, "z": 16}, "geometry": {"type": "Polygon", "coordinates": [[[-118.23486328125, 34.075412438417395], [-118.23486328125, 34.070862323766306], [-118.2403564453125, 34.070862323766306], [-118.2403564453125, 34.075412438417395], [-118.23486328125, 34.075412438417395]]]}}]}' | jq --slurp --raw-input --raw-output @uri) && \
    echo $GEOJSON_ENCODED && \
    curl -L -XGET "https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/file?dtype_out_vector=geojson&bbox=${GEOJSON_ENCODED}"
```

---

// File: user-guide/out/kepler

# Kepler

[Kepler](https://kepler.gl/) is a powerful open source too to visualize large geospatial datasets. Data analysts use it to create data-driven maps, gain insights from location data, and embed dynamic maps into custom applications.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/kepler.gif)

The UDF Builder can load the data returned by a UDF call into Kepler. When visualizing a vector dataset, click "Open in Kepler.gl" button located on the top-right menu of the map.

---

// File: user-guide/out/leaflet

Display responsive Tile maps in Jupyter Notebooks with the [`ipyleaflet`](https://ipyleaflet.readthedocs.io/en/latest/) library.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/leaflet.gif)

## 1. Create UDF in Workbennch

Create a signed [HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) for a raster Tile UDF.

## 2. Create a Leaflet map

### Raster Tile layers

Modify the HTTP endpoint to introduce templatized XYZ Tile parameters (`{{z}}/{{x}}/{{y}}`) as well as query strings for other UDF-specific parameters (`?crop_type={crop_type}`), and pass it to the `TileLayer`.

```python showLineNumbers
import ipyleaflet

crop_type = "almond"
m = ipyleaflet.Map(
    center=(37.316, -120.69),
    zoom=10,
    basemap=ipyleaflet.basemaps.CartoDB.PositronOnlyLabels,
)
l = ipyleaflet.TileLayer(
    url=f"https://www.fused.io/server/v1/realtime-shared/8110ef6e0c66f07f0c73f39843db27ece3960f98f268f38ef2f79f3623faae01/run/tiles/{{z}}/{{x}}/{{y}}?crop_type={crop_type}",
    tile_size=512,
    zoom_offset=-1,
    cross_origin=True,
    show_loading=True,
)
m.add_layer(l)
m
```

### Vector Tile layers

Similarly, Vector Tiles are rendered by passing the HTTP endpoint to a `VectorTileLayer`.

```python showLineNumbers
import ipyleaflet

m = ipyleaflet.Map(center=(37.7749, -122.4194), zoom=17)

l = ipyleaflet.VectorTileLayer(
    url="https://www.fused.io/server/v1/realtime-shared/fsh_5M8jCBuswIF8PcFvZBP9k9/run/tiles/{z}/{x}/{y}?dtype_out_vector=mvt"
)
m.add_layer(l)
m
```

:::note

If on Colab, Enable the ipyleaflet widget (might require restarting the kernel).

```python showLineNumbers
# !pip install ipywidgets==7.7.1 -q
# from google.colab import output
# output.enable_custom_widget_manager()
```

:::

---

// File: user-guide/out/lonboard

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/lonboard_cdl.png" alt="overture" width="600"/>


## 1. Create UDF in Workbench

Create a UDF in Workbench that works as a `Tile`, and generate a
[Tile HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#token)
for the UDF.

## 2. Create a Lonboard map


Render the [CDLs Tile Example](https://github.com/fusedio/udfs/tree/main/public/CDLs_Tile_Example) UDF as a Lonboard BitmapTileLayer.

```python showLineNumbers
import lonboard

crop_type = "almond"
url = f"https://www.fused.io/server/v1/realtime-shared/8110ef6e0c66f07f0c73f39843db27ece3960f98f268f38ef2f79f3623faae01/run/tiles/{{z}}/{{x}}/{{y}}?crop_type={crop_type}"
layer = lonboard.BitmapTileLayer(
    data=url, tile_size=256, max_requests=-1, min_zoom=0, max_zoom=19
)
map = lonboard.Map(
    layers=[layer],
    view_state={"longitude": -121.4, "latitude": 37.7, "zoom": 10},
    basemap_style=lonboard.basemap.CartoBasemap.Voyager,
    show_tooltip=True,
)
map
```

---

// File: user-guide/out/mapbox

Create [Mapbox GL JS](https://docs.mapbox.com/mapbox-gl-js/example/) maps that load data from UDF [HTTP endpoints](/core-concepts/run-udfs/run-small-udfs/#http-requests).

You'll first generate a signed UDF URL and render it on an HTML map. You can then use the HTML map in a low-code app like Retool or render it as an `iframe` in an app such as Notion.

## 1. Generate a signed URL for a UDF

First, create a UDF and [generate an HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests).

## 2. Create a Mapbox HTML map

Create an `.html` file following this template. This code creates a mapbox map, adds a source, and then a layer that renders data from that source. Supported layer types are:

- [Vector Tiles](/user-guide/out/mapbox/#vector-tile-layers)
- [Raster Tiles](/user-guide/out/mapbox/#raster-tile-layers)

```html
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Add a vector tile source</title>
<meta name="viewport" content="initial-scale=1,maximum-scale=1,user-scalable=no">
<link href="https://api.mapbox.com/mapbox-gl-js/v3.2.0/mapbox-gl.css" rel="stylesheet">
<script src="https://api.mapbox.com/mapbox-gl-js/v3.2.0/mapbox-gl.js"></script>
<style>
body { margin: 0; padding: 0; }
#map { position: absolute; top: 0; bottom: 0; width: 100%; }
</style>
</head>
<body>
<div id="map"></div>
<script>
	mapboxgl.accessToken = 'pk.eyJ1IjoiaXNhYWNmdXNlZGxhYnMiLCJhIjoiY2xicGdwdHljMHQ1bzN4cWhtNThvbzdqcSJ9.73fb6zHMeO_c8eAXpZVNrA';
    const map = new mapboxgl.Map({
        container: 'map',
        // Choose from Mapbox's core styles, or make your own style with Mapbox Studio
        style: 'mapbox://styles/mapbox/dark-v10',
        zoom: 13,
        center: [-122.447303, 37.753574]
    });

    // Optionally, pass parameters to the tile source
    const model = {
        theme: 'building'
    }

    map.on('load', () => {
        map.addSource('fused-vector-source', {
            'type': 'vector',
            'tiles': [ // Vector Tile URL that returns mvt (https://docs.mapbox.com/data/tilesets/guides/vector-tiles-standards/)
                `https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/tiles/{z}/{x}/{y}?dtype_out_vector=mvt&type=${model.theme}`
            ],
            'minzoom': 6,
            'maxzoom': 14
        });
        map.addLayer(
            {
                'id': 'fused-vector-layer', // Layer ID
                'type': 'line',
                'source': 'fused-vector-source', // ID of the tile source created above
                'source-layer': 'udf', // Important! The source-layer name is 'udf' for all Fused vector tiles
                'layout': {
                    'line-cap': 'round',
                    'line-join': 'round'
                },
                'paint': {
                    'line-opacity': 0.6,
                    'line-color': 'rgb(53, 175, 109)',
                    'line-width': 2
                }
            }
        );
    })
</script>

</body>
</html>
```

### Vector Tile layers

Use the following snippet to create a vector layer. The layer in the sample map comes from [Overture Buildings UDF](https://github.com/fusedio/udfs/tree/main/public/Overture_Maps_Example).

:::info

When rendering `mvt` data types, Mapbox may clip features that exceed the bounds of the tile. A workaround is for the UDF to [clip](https://geopandas.org/en/stable/docs/reference/api/geopandas.clip.html) the output at the `bounds` bounds or to only return features [within](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.within.html) the `bounds`. See [Discord thread](https://discord.com/channels/1199097729243152434/1261105758360305775/1261105758360305775) for more information.

:::

<iframe src="/img/mapbox_vector.html"  height="300px" width="100%" scrolling="no"></iframe>

```html
<script>
    map.addSource('fused-vector-source', {
        'type': 'vector',
        'tiles': [ // Vector Tile URL that returns mvt (https://docs.mapbox.com/data/tilesets/guides/vector-tiles-standards/)
            `https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/tiles/{z}/{x}/{y}?dtype_out_vector=mvt&type=${model.theme}`
        ],
        'minzoom': 6,
        'maxzoom': 14
    });
    map.addLayer(
        {
            'id': 'fused-vector-layer', // Layer ID
            'type': 'line',
            'source': 'fused-vector-source', // ID of the tile source created above
            'source-layer': 'udf', // Important! The source-layer name is 'udf' for Fused vector tiles
            'layout': {
                'line-cap': 'round',
                'line-join': 'round'
            },
            'paint': {
                'line-opacity': 0.6,
                'line-color': 'rgb(53, 175, 109)',
                'line-width': 2
            }
        }
    );
</script>
```

### Raster Tile layers

Use the following snippet to create a raster layer. The layer in the sample map comes from the [Solar Irradiance UDF](https://github.com/fusedio/udfs/tree/main/public/Solar_Irradiance).

<iframe src="/img/mapbox_raster.html"  height="300px" width="100%" scrolling="no"></iframe>

```html
<script>
    map.addSource('fused-irradiation-source', {
        'type': 'raster',
        'tiles': [ // Raster Tile URL that returns png
            'https://www.fused.io/server/v1/realtime-shared/af0bc71e64075233b731f316988b323ac28658059db9e87388393fe187752501/run/tiles/{z}/{x}/{y}?dtype_out_raster=png'
        ],
        'tileSize': 256,
        'minzoom': 10,
        'maxzoom': 18
    });
    map.addLayer(
        {
            'id': 'wms-test-layer',
            'type': 'raster',
            'source': 'fused-irradiation-source',
            'paint': {}
        },
        'building' // Place layer under labels, roads and buildings.
    );
</script>
```

---

// File: user-guide/out/microsoft_excel

Microsoft [Excel](https://www.microsoft.com/en-ca/microsoft-365/excel) can load data from a UDF's [HTTP Endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) that returns a `.csv`.

## 1. Create a UDF

This example retrieves Caltrain live location data from GTFS real-time feed and returns it as a `DataFrame`.

```python showLineNumbers
@fused.udf
def udf(bounds: fused.types.TileGDF=None):
    import requests
    import pandas as pd

    r = requests.get(f'https://www.caltrain.com/files/rt/vehiclepositions/CT.json')
    j = r.json()
    df = pd.json_normalize(j['Entities'])
    return df
```

## 2. Create a URL for the UDF

Now, [create an HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) for the UDF.

Append this query parameter to the end of the URL to structure the response as a CSV type: `?dtype_out_vector=csv`.

The generated URL should look like this:

`https://www.fused.io/server/v1/realtime-shared/ccd781317018362a6966c9f12b27e95f1fe2fd88ff339de90eb9ac35b87cf439/run/file?dtype_out_vector=csv`

## 3. Import Data to Excel

Open Excel, then click the `Data` tab in the top ribbon. Click `From Web`.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/excel_start.png)

In the From Web dialog box, paste in the URL from your UDF, and click `OK`.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/excel_load.png)

A preview of the data will be shown. Click `Load` to import the data to your Excel sheet. Optionally, you can click `Transform` to transform the data if needed.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/excel_preview.png)

The UDF will run and load its output data in a new Excel sheet.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/excel_loaded.png)

**_Note:_** The desktop version of Microsoft Excel is required. Microsoft Excel Online does not support loading data from web sources.

---

// File: user-guide/out/motherduck

[MotherDuck](https://motherduck.com/) is a serverless SQL analytics platform. It's popular for its hybrid DuckDB execution engine, simplified database sharing, diverse integrations ecosystem, and SQL notebook-like UI. It runs DuckDB, so it supports several of the table output formats that Fused can serve - particularly Parquet files.

To follow along, you'll need a Motherduck account - which you can [create for free](https://motherduck.com/docs/getting-started/).

## 1. Generate a signed URL for a UDF

First, create an [HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests) for a UDF. Set [`dtype_out_vector`](/core-concepts/run-udfs/run-small-udfs/#http-requests) to `parquet`. You can optionally pass UDF parameters as URL-encoded strings, which can be configured to change based on query input.

This example uses an instance of the [DEM Raster to Vector](https://github.com/fusedio/udfs/blob/main/public/DEM_Raster_to_Vector_Example/DEM_Raster_to_Vector_Example.py) public UDF.

## 2. Run a query

Now you can make a query using the UDF URL.

![alt text](motherduck_fused.png)

### Parquet

With the `dtype_out_vector` query parameter set to `parquet`:

```sql
SELECT wkt, area
FROM read_parquet('https://www.fused.io/server/v1/realtime-shared/' ||
  '1e35c9b9cadf900265443073b0bd99072f859b8beddb72a45e701fb5bcde807d' ||
  '/run/file?dtype_out_vector=parquet&min_elevation=500')
LIMIT 10;
```

### CSV

With the `dtype_out_vector` query parameter set to `csv`:

```sql
SELECT wkt, area
FROM read_csv('https://www.fused.io/server/v1/realtime-shared/' ||
  '1e35c9b9cadf900265443073b0bd99072f859b8beddb72a45e701fb5bcde807d' ||
  '/run/file?dtype_out_vector=csv&min_elevation=500',
  AUTO_DETECT=TRUE)
LIMIT 10;
```

---

// File: user-guide/out/notion

# Notion

Embed responsive apps in Notion pages.



## 1. Create an app with Fused App Builder

[Create an app](/quickstart/#create-an-app) and click "Copy shareable link" to get the app URL. It should look like this `https://staging.fused.io/workbench#app/s/i/fa_5GEbndYpfpxwbJxer5JjKN`.

import Iframe from "@site/src/components/Iframe";
import CODE from "@site/src/app-iframe/python/basic.py";

<div style={{marginTop: '2rem'}}>
<Iframe
  id="iframe-1"
  code={CODE}
  requirements={[
    "/pyarrow/pyarrow-17.0.0-cp312-cp312-pyodide_2024_0_wasm32.whl",
    "micropip",
    "pyodide-unix-timezones", // needed by pyarrow
    "requests",
    "yarl",
    "pydeck",

  ]}
  height="250px"
/>
</div>
## 2. Embed the map into Notion

On a Notion page, type `/embed` to create an [embed component](https://www.notion.so/help/embed-and-connect-other-apps). Paste the app URL in the menu that appears. You may publish the page as a [Notion site](https://www.notion.so/help/public-pages-and-web-publishing).

import ImgNotion from '@site/static/img/notion_basics.png';

<div style={{textAlign: 'center'}}>
<img src={ImgNotion} alt="File" style={{width: '100%'}} />
</div>



[This Notion Page](https://www.notion.so/fusedio/Fused-App-Gallery-82acae5d0d454527b6c881b6b10c5cfa) shows what a sample Notion page looks like with embedded apps.

---

// File: user-guide/out/qgis

[QGIS](https://www.qgis.org/en/site/) is an Open Source Desktop GIS platform. Since its inception in 2002, it's become a staple in the geospatial data stack. [Download QGIS here](https://www.qgis.org/en/site/index.html).

Fused UDFs can be configured to return raster tile and vector data in formats that QGIS can load. The QGIS [User Guide](https://docs.qgis.org/3.34/en/docs/user_manual/index.html) describes how QGIS handles different data types. This walkthrough shows how to generate URL endpoints to easily load data into QGIS.

## 1. Generate a signed URL for a UDF

First, create a UDF and [generate an HTTP endpoint](/core-concepts/run-udfs/run-small-udfs/#http-requests).

## 2. Load data into QGIS

QGIS supports loading data from UDFs called as both File and Tile.

### Raster Tiles

Dynamically render XYZ raster tiles on the QGIS map.

As an example, the ["Sentinel Tile Example"](https://github.com/fusedio/udfs/tree/main/public/Sentinel_Tile_Example) public UDF returns an XYZ tile URL of NDVI from the Sentinel dataset. Create a new instance of it on your workbench, generate a shared URL, then modify the URL query parameters:

- Set `dtype_out_raster` to `png`.
- Set replace the values after `/run/tiles` with the `/{z}/{x}/{y}` template.
- Set UDF parameters as needed.

For example:

```bash
https://www.fused.io/server/v1/realtime-shared/da71c7bf79376f2e88b37be9ecd2679f2e40a5b79f9b0f2c75e7ea9a3f0c5171/run/tiles/{z}/{x}/{y}?dtype_out_raster=png
```

Open the modal to create a new raster XYZ layer by right-clicking: `XYZ Tiles` -> `New Connection`. Now paste the UDF URL in the URI field and configure the layer as shown.

import ImgQgis9 from '@site/static/img/qgis-9.png';

<div style={{textAlign: 'center'}}>
<img src={ImgQgis9} alt="File" style={{width: 600}} />
</div>

This should render the raster output tiles on the map. This example shows NDVI in the area surrounding Santa Rosa Island, off the coast of Southern California.

import ImgQgis10 from '@site/static/img/qgis-10.png';

<div style={{textAlign: 'center'}}>
<img src={ImgQgis10} alt="File" style={{width: 600}} />
</div>

Note that the QGIS client is liable to timeout errors if it encounters a high number of concurrent requests and might show a warning similar to this one. Issues generally resolve after waiting some time.

```bash
2024-04-12T12:10:06     WARNING    Network request https://www.fused.io/server/v1/realtime-shared/da71c7bf79376f2e88b37be9ecd2679f2e40a5b79f9b0f2c75e7ea9a3f0c5171/run/tiles/14/2722/6554?dtype_out_raster=png timed out`
```

### Vector File

Load vector data onto the QGIS map.

As an example, the ["Single Route"](https://github.com/fusedio/udfs/tree/main/public/single_route) public UDF returns a composite vector line of driving directions between an origin and destination locations. Create a new instance of it on your workbench, generate a shared URL, and then modify the URL query parameters:

- Set `dtype_out_vector` to `geojson`.
- Set UDF parameters as needed.

For example:

```bash
https://www.fused.io/server/v1/realtime-shared/4f8fe6b81811ce011d7045b9fffbcfb43663637a4c626955821566fe62e6fb57/run/file?dtype_out_vector=csv&lat_start=37.7954425&lng_start=-122.3961885&lat_end=37.8592408&lng_end=-122.5349321
```

Open the modal to create a new vector layer by clicking: `Layer` -> `New Layer` -> `Add Vector Layer`. Now paste the UDF URL in the URI field and configure the layer as shown.

import ImgQgis5 from '@site/static/img/qgis-5.png';

<div style={{textAlign: 'center'}}>
<img src={ImgQgis5} alt="File" style={{width: 600}} />
</div>

This should load the data and render it on the map.

import ImgQgis6 from '@site/static/img/qgis-6.png';

<div style={{textAlign: 'center'}}>
<img src={ImgQgis6} alt="File" style={{width: 600}} />
</div>

Note that QGIS might cache the response of endpoint calls, which means UDF changes might not propagate immediately.

---

// File: user-guide/out/retool

[Retool](https://retool.com/) is a low-code platform to build internal software. This guide shows how to create a vector map from a UDF on a [Mapbox GL JS](https://docs.mapbox.com/mapbox-gl-js/example/) map with a [custom Retool component](https://retool.com/components/custom-component). You'll first generate a signed UDF URL then introduce it into a custom map component that can input and output data across other Retool components.

## 1. Create an HTML map

Follow this [guide](/user-guide/out/mapbox/) to create a mapbox `.html` map that renders vector tiles from a UDF.

## 2. Create a custom Retool component

In a Retool app, create a custom Retool component. In the `IFrame Code` box, paste the code Mapbox HTML map created in the first step.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/retool-1.png)

## 3. Pass data from a UI component to the map

Create a component that accepts user input which will be passed as a query parameter to the Fused endpoint.

This example uses a `select` component. Add options to the component - in this case `building`, `water`, and `place` because in the sample UDF these are passed in the `type` parameter to select different "theme" layers of the Overture dataset.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/retool-2.png)

In Retool custom components, variables are passed through the Model. The IFrame code receives updates from Retool via a subcribe function.

In the custom component's `Model` box, paste a snippet like this one to pass data from the `select` component.

```json
{
  "theme": {{select1.value}}
}
```

Now subscribe the sections of code that will make use of the model values by wrapping all map components within a `window.Retool.subscribe` function. In this example, the value of `model.theme` is passed to a query parameter via string interpolation. When the value changes in the UI element, the value will update for the map.

```js
window.Retool.subscribe(function (model) {

    const map = new mapboxgl.Map({
        ...
    });

    map.on('load', () => {
        map.addSource('fused-vector-source', {
            'type': 'vector',
            'tiles': [ // Vector Tile URL that returns mvt (https://docs.mapbox.com/data/tilesets/guides/vector-tiles-standards/)
                `https://www.fused.io/server/v1/realtime-shared/UDF_Overture_Maps_Example/run/tiles/{z}/{x}/{y}?dtype_out_vector=mvt&type=${model.theme}`
            ],
            'minzoom': 6,
            'maxzoom': 14
        });

        map.addLayer(
            ...
        );
    })
})
```

The result should look like this.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/retool_in.gif)

## 4. Pass data from the map to a UI component

Passing data from the map component to another Retool component follows a similar process. This example introduces a drawing tool to draw custom polygons on the preceding map, then pass the polygons' geojson to a `jsonExplorer` Retool component.

First, update the custom component's model to include a `data` key with an empty dictionary as a value. This is where the map component will pass data.

```json
{
  "theme": {{select1.value}},
  "data": {}
}
```

Then, introduce these headers and snippet to the map IFrame.

```html
<script src="https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-draw/v1.2.1/mapbox-gl-draw.js"></script>
<link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-draw/v1.2.1/mapbox-gl-draw.css" type="text/css">
```

This is a [MapboxDraw](https://github.com/mapbox/mapbox-gl-draw) component. Ensure it's wrapped within the same `window.Retool.subscribe` function introduced in step 3 so Retool updates the `data` field when a change in the draw component triggers the `updateData` function.

```js
var draw = new MapboxDraw({
    displayControlsDefault: false,
    controls: {
        polygon: true,
        trash: true
    }
});
map.addControl(draw);

map.on('draw.create', updateData);
map.on('draw.delete', updateData);
map.on('draw.update', updateData);

function updateData(e) {
    var data = draw.getAll();
    window.Retool.modelUpdate({ data })
}
```

Finally, create a `jsonExplorer` component with the following value `{{customComponent1.model.data}}` which will receive the GeoJSON when a polygon is drawn on the map.

![alt text](https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/retool_out.gif)

This GeoJSON can be used in subsequent operations. For example, it could be passed as a parameter to downstream UDF calls as explained [here](/user-guide/out/http/#with-a-geojson).

---

// File: user-guide/transform/generic/duckdb

Fused runs [User Defined Functions (UDFs)](/core-concepts/why/) behind HTTP endpoints. DuckDB can call those endpoints and run SQL on their output.

This guide provides support material for the blog post [DuckDB + Fused: Fly beyond the serverless horizon](https://medium.com/@fused/duckdb-fused-fly-beyond-the-serverless-horizon-886d892834aa). It can be followed in this [Jupyter Notebook](https://github.com/fusedio/fused-docs/blob/main/docs/user-guide/transform/generic/duckdb.ipynb).

## 1. Run DuckDB in a Fused UDF

As an example of running DuckDB within a Fused UDF, take the case of loading a geospatial Parquet dataset. The "DuckDB H3" sample UDF runs an SQL query with DuckDB on the NYC Taxi Trip Record [Dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). It uses the [bounds](/core-concepts/filetile/#the-bounds-object) argument to spatially filter the dataset and automatically parallelize the operation.

To try this example, you can run the cell below. You can find the code of the UDF in the Fused public UDF [repo](https://github.com/fusedio/udfs/tree/main/public/DuckDB_H3_Example_Tile). Alternatively, you can import the "DuckDB H3 Example Tile" UDF into your Fused Workbench environment.

This pattern gives DuckDB easy parallel operations. Fused spatially filters the dataset via the \`bounds parameter, runs the operation for each encompassing tile, and stitches the results together. Because Fused breaks down operations to only a fraction of the dataset, it's easy to transition between SQL and Python.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/nyc.png" alt="overture" width="600"/>

```python showLineNumbers
import fused

udf = fused.load("https://github.com/fusedio/udfs/tree/ea99f07/public/DuckDB_H3_Example_Tile")
gdf = fused.run(udf=udf, x=2412, y=3078, z=13, engine='local')
gdf.head()
```

## 2. Call Fused UDFs from DuckDB

Any database that supports querying data via HTTP can call and load data from Fused UDF
endpoints using common formats like Parquet or CSV. This means that DuckDB can dispatch
operations to Fused that otherwise would be too complex or impossible to express with
SQL, or would be unsupported in the local runtime.

As an example of calling a Fused endpoint from within DuckDB, take an operation to
vectorize a raster dataset. This might be necessary to determine the bounds of areas
with pixel value within a certain threshold range in an Earth observation image‚Ää-‚Ääsuch
as a Digital Elevation Model. SQL is not geared to support raster operations, but these
are easy to do in Python.

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sql.gif" alt="overture" width="600"/>

In this example, a Fused UDF returns a table where each record is a polygon generated
from the contour of a raster provided by the
[Copernicus Digital Elevation Model](https://spacedata.copernicus.eu/collections/copernicus-digital-elevation-model)
as a [Cloud Optimized GeoTIFF](https://www.cogeo.org/). DuckDB can easily trigger a UDF
and load its output with this simple query, which specifies that the UDF endpoint
returns a Parquet file.

This SQL query uses DuckDB's
[read_parquet](https://duckdb.org/docs/data/parquet/overview.html) function to call an
endpoint of a UDF instance of the "DEM Raster to Vector" UDF.

You can find the code of the UDF in the Fused Public UDF [repo](https://github.com/fusedio/udfs/tree/main/public/DEM_Raster_to_Vector_Example).

To try this example, simply run the following SQL query on the cell below or in a
[DuckDB shell](<https://shell.duckdb.org/#queries=v0,SELECT-wkt%2C-area%0AFROM-read_csv('https%3A%2F%2Fwww.fused.io%2Fserver%2Fv1%2Frealtime%20shared%2F'-%7C%7C%0A'1e35c9b9cadf900265443073b0bd99072f859b8beddb72a45e701fb5bcde807d'-%7C%7C%0A'%2Frun%2Ffile%3Fdtype_out_vector%3Dcsv'-%7C%7C%0A'%26min_elevation%3D500')%0ALIMIT-10~>).
Change the `min_elevation` parameter to run the UDF for parts of California at different
elevations. (Note: for DuckDB WASM, the file will be requested as CSV.)

```python showLineNumbers
import duckdb

con = duckdb.connect()

con.sql("""
    SELECT
        wkt,
        ROUND(area,1) AS area
    FROM read_parquet('https://www.fused.io/server/v1/realtime-shared/1e35c9b9cadf900265443073b0bd99072f859b8beddb72a45e701fb5bcde807d/run/file?min_elevation=500&dtype_out_vector=parquet')
    LIMIT 5
""")
```

This pattern enables DuckDB to perform operations and handle data formats that would otherwise be challenging or impossible, such as raster operations, API calls, and control flow logic.

## 3. Integrate DuckDB in applications using Fused

Fused can act like the glue layer between DuckDB and other 3rd party apps. It enables seamless integrations that
trigger Fused UDFs and load their results with simple parameterized HTTP calls.

DuckDB is an embedded database engine and doesn't have built-in capability to share
results other than writing out files. With Fused, DuckDB can query and transform data and seamlessly integrate the results into any workflow or app.

As an example, take the case of loading the output of a DuckDB query into Google Sheets.
Sheets can easily structure the Fused UDF endpoint to pass parameters defined in
specific cells as URL query parameters. In this example, the
[importData](https://support.google.com/docs/answer/3093335?hl=en) command calls the
endpoint for the UDF above and loads its output data in CSV format. It constructs the
endpoint string by concatenating a base URL with the `B2`, `B3`, and `B4` cell values as
query parameters.

```bash
=importData(CONCATENATE("https://www.fused.io/server/v1/realtime-shared/aba7b238d9445d576e15b2d6b780dc353bfdee55f02a285a85a3917b72835600/run/file?dtype_out_vector=csv&resolution=", B2, "&min_count=", B3, "&head=", B4))
```

<img src="https://fused-magic.s3.us-west-2.amazonaws.com/docs_assets/gifs/sheets.gif" alt="overture" width="600"/>

To try this example simply make a copy of
[this](https://docs.google.com/spreadsheets/d/1iufyjEct5bQjYAI8v1Mb5e29yG8ukzH4X8MD6oP1xLQ/edit?usp=sharing)
Google Sheets spreadsheet (File > Make a copy) and click, and modify the parameters in
`B2:4` to trigger the Fused UDF endpoint and load data.

This pattern brings the power of DuckDB no-code tools like Google Sheets, Retool, and beyond‚Ää-‚Ääwithout the need to build bespoke integrations with closed-source systems. With this, a Python developer can abstract away the UDF and deliver data to end users.

---

// File: user-guide/transform/generic

# Generic

Walkthroughs of operations with common datasets and analytics tools.


import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: user-guide/transform/geospatial/zonal_stats

# Zonal Statistics: A Comprehensive Guide

Zonal statistics calculates aggregate statistics (e.g., mean, sum, maximum) of the pixel values of a raster that fall within areas defined by a `Polygon` dataset. The method has diverse applications such as determining vegetation health in an agricultural field, assessing surface water availability, or approximating building heights from DSM rasters.

<iframe src="/img/deckgl_vector_zstats.html"  height="400px" width="100%" scrolling="no"></iframe>

_Building `Polygon` features colored based on DSM aggregate values._

## Applications

- Height approximation: Estimate the height of buildings from DSM
- Crop yield estimation: Determine vegetation health within crop polygons using NDVI rasters
- Water resource management: Assess surface water availability in watersheds using NDWI rasters
- Forestry: Track changes in forest cover over time within administrative boundaries

## Implementing zonal statistics

The analysis involves first determining the pixels within a polygon and then calculating their aggregate statistics, such as their count, mean, sum, or maximum.

### Implementation steps

To perform this operation, first load a raster and a vector dataset for spatially overlapping areas, then perform zonal aggregations as shown below using the `xarray` and `GeoPandas` libraries.

1. Load overlapping the raster and vector data
2. For the extent of each `Polygon`, clip the raster to create a mask array
3. Perform an aggregation operation (e.g., mean, sum, max) on each masked array
4. Return the results in a `GeoDataFrame`, where each row corresponds to a `Polygon` and the columns contain the value for the aggregate zonal statistics

### Example UDF

This example UDF loads the building footprint table `gdf` and DSM raster `arr` from S3 only for a section defined by `bounds`, which corresponds to a map tile. It then calculates the zonal statistics and returns the results as the `GeoDataFrame` `gdf_zonal`. This process enables Fused Workbench to perform the calculation dynamically as users scroll and zoom in on the map.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds=None, min_zoom=15, table="s3://fused-asset/infra/building_msft_us/"
):
    dsm_to_tile = fused.load("https://github.com/fusedio/udfs/tree/91845c4/public/DSM_JAXA_Example").utils.dsm_to_tile
    utils = fused.load("https://github.com/fusedio/udfs/tree/2a76b6a/public/common/").utils

    gdf = utils.table_to_tile(bounds, table, min_zoom)
    arr = dsm_to_tile(bounds, z_levels=[4, 6, 9, 11], verbose=False)
    gdf_zonal = utils.geom_stats(gdf, arr)
    return gdf_zonal
```

The UDF would return a `GeoDataFrame` like the following, with an aggregate `stats` column for each input `Polygon`:

```plaintext
                                                geometry      stats  count
660    POLYGON ((-122.39806 37.76221, -122.39806 37.7...      22.00      5
661    POLYGON ((-122.39881 37.76219, -122.39876 37.7...      21.99    193
1452   POLYGON ((-122.39619 37.76326, -122.39613 37.7...      13.50      2
1458   POLYGON ((-122.39774 37.76327, -122.39776 37.7...      11.00     28
3033   POLYGON ((-122.39680 37.76362, -122.39701 37.7...      10.20      5
...
```

## Conclusion

While zonal statistics is a valuable tool, it's often just the starting point for more complex analyses. Combining zonal statistics with other techniques such timeseries analysis or machine learning approaches can provide even richer insights into your data.

## Demo app [beta]

import Iframe from "@site/src/components/Iframe";
import ZONAL_STATS_CODE from "@site/src/app-iframe/python/zonal-stats.py";

<div style={{marginTop: '2rem'}}>
<Iframe id="iframe-1" code={ZONAL_STATS_CODE} />
</div>

---

// File: user-guide/transform/geospatial

# Geospatial

Walkthroughs of common operations on geospatial data.



import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: workbench/udf-catalog

The UDF Catalog offers a searchable collection of [User Defined Functions (UDFs)](/core-concepts/why/) that can be imported into the [UDF Builder](/workbench/udf-builder/) for editing. It facilitates sharing and discovering UDFs within teams and the broader Fused community.

Data teams frequently encounter silos when looking to share and reuse code snippets & data across different projects. Traditional notebooks pose challenges in version control, sharing individual utility functions, managing dependencies, and reproducing results in new environments. Additionally, assets often end up disconnected from the code that generated them. UDFs on the other hand encapsulate modules, outputs, and code, which addresses issuess related to reproducibility and lineage.


- Create a [new UDF](/workbench/udf-catalog/#new-udf)
- [Search](/workbench/udf-catalog/#search) the catalog across [UDF categories](/workbench/udf-catalog/#udf-categories)
- View the detailed [profile](/workbench/udf-catalog/#udf-profile) for each UDF
- Add any UDF from its [Github URL](/workbench/udf-catalog/#add-from-github-url)




<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/udf_catalog_edit2.mp4" width="100%" />



## New UDF

Create a new UDF by clicking the top-right "New UDF" button. This opens a UDF with a base template.

![UDF Catalog](/img/udfcatalog_august.png)


## Search

Open the UDF Catalog by Clicking "Add UDF". Search UDFs by name, sort them, and toggle between gallery and list views. Click UDF cards to view their profiles or add them to the UDF Builder.

![Add UDF Button](/img/addudfbutton.png)


### UDF categories

- **Public UDFs:** Verified and accessible to all users
- **Community UDFs:** Shared by the community and accessible to all users
- **Team UDFs:** Shared privately within a team in a [GitHub](/core-concepts/content-management/git/) repo
- **Saved UDFs:** Private UDFs in the user's account


## Add from GitHub URL

If [GitHub integration is enabled](/core-concepts/content-management/git/#enabling-github-integration) on your Workbench, you can paste the link of any UDF from GitHub to open it. This allows you to open a UDF from any branch or revision from GitHub.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/import_udf_from_github_edit.mp4" width="100%" />


## UDF profile

Select a UDF to view its profile which includes the [`README.md` description](/workbench/udf-builder/code-editor/#description), last update date, author, link to its code on GitHub, and code preview.

## Contribute to Fused

Fused welcomes your skills and enthusiasm in support of the geospatial community!

There are numerous opportunities to get involved, from contributing code to engaging the community on [Discord](https://discord.com/invite/BxS5wMzdRk), [LinkedIn](https://www.linkedin.com/company/fusedio/), and other social media platforms.

### Where to start?

UDFs can be easily re-used, so before you build something new or share yours with the community, check the [UDF Catalog](https://www.fused.io/workbench/catalog) to see if someone has already built something you might benefit from! Here are a few examples of Public & Community UDFs:

- [Opening Overture Building Datasets](https://www.fused.io/workbench/catalog/Overture_Maps_Example-77970462-8c4d-44dd-87ad-3520f5d7bd83) to browse the latest building dataset from Overture's open data on [Source Coop](https://source.coop/repositories/fused/overture/description)
- [Exploring Sentinel 2 satellite imagery](https://www.fused.io/workbench/catalog/Sentinel_Tile_Example-a6b7839c-4886-4ea7-b168-c5c763c02c18) by using [Microsoft's Planetary Computer](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) or [AWS Open Data Program](https://registry.opendata.aws/sentinel-2/)
- [Computing height for buildings in the US using the ALOS 30m Digital Surface Model](https://www.fused.io/workbench/catalog/DSM_Zonal_Stats-0c801e56-d0c4-47e8-a5ce-90d37703bdb7)

### Publish a UDF to a GitHub repository

Once you write a UDF, you can use the Push to GitHub button in Fused Workbench to publish or update your UDF in a GitHub repository. You will be able to select the repository (e.g., `fusedudf`, `community`, or `public`) and automatically create a pull request on GitHub. Read how to [enable the GitHub integration](/core-concepts/content-management/git/#enabling-github-integration).

To add a UDF to the community repository, select community from the dropdown before creating a pull request.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/push_udf_to_github_edit.mp4" width="100%" />

:::info

Note that once the UDF is committed to the main branch of the repo, you can expect to wait up to 3 minutes for it to deploy and appear in the UDF Catalog.

:::

---

// File: workbench/file-explorer

The File Explorer provides a streamlined interface to browse, preview, and open files in cloud object storage and the [mounted disk](/core-concepts/content-management/file-system/#mntcache-disk). Fused supports Amazon S3 and Google Cloud Storage, with more integrations coming soon.

When working with data, it can be time-consuming to track down datasets, request access, download gigabytes of data, and write boilerplate code to read files. The File Explorer simplifies this process by enabling users to easily browse any object storage bucket, visualize file contents without writing code, and quickly create [User Defined Functions (UDFs)](/core-concepts/why/) in the [UDF Builder](/workbench/udf-builder/) with [templates](/workbench/file-explorer/#template-udfs) for specific file types.

- [Browse](/workbench/file-explorer/#browse) object storage buckets and list their files
- Quickly [preview](/workbench/file-explorer/#preview) files
- [Create](/workbench/file-explorer/#create-udf) a new UDF from a [template](/workbench/file-explorer/#template-udfs) to open the file
- [Connect](/workbench/file-explorer/#connect-your-own-bucket) an S3 or GCS bucket

import ReactPlayer from 'react-player'

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/file_explorer_intro.mp4" width="100%" />

## Browse

Browse directories and files in buckets. Use the search bar and filter options to quickly locate specific files, and "Favorite" files for quick access. To explore a bucket, find it in the "Favorites" dropdown or paste its path. To connect private buckets, contact Fused.

import FEsearch from '/img/workbench/file-explorer/file_explorer_search.png';

<div style={{ textAlign: 'center' }}>
  <img src={FEsearch} alt="File" style={{ width: '80%' }} />
</div>

## Preview

The Explorer displays a bucket's directories and objects as folders. Each listed file shows metadata such as file size and path, along with utilities to download or delete the file, copy its path, generate a [signed URL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html), and [create a UDF](/workbench/file-explorer/#create-udf) to open it.

Click on a file to preview its content. If the file has a spatial component, it will be displayed on the [map](/workbench/udf-builder/map), allowing you to zoom and pan to explore the rendered data. For images or other file types, Fused will make a best-effort to render and display the content.

import FEpreview from '/img/workbench/file-explorer/file_explorer_preview.png';

<div style={{ textAlign: 'center' }}>
  <img src={FEpreview} alt="File" style={{ width: '80%' }} />
</div>

## Create UDF

Create Fused UDFs using templates for common file types. Double-click on a file to create a new templated UDF that reads the file, or find additional readers in the file's kebab menu. [Parquet tables](https://www.upsolver.com/blog/apache-parquet-why-use) show an "Open Table" button to open them at the directory level.

import FEcreateudf from '/img/workbench/file-explorer/file_explorer_createudf.png';

<div style={{ textAlign: 'center' }}>
  <img src={FEcreateudf} alt="File" style={{ width: '80%' }} />
</div>

## Template UDFs

Template UDFs are available for common file types (like `CSV`, `Parquet`) and tools (like `DuckDB` and `GeoPandas`). See the latest template UDFs in the [UDFs repo](https://github.com/fusedio/udfs/tree/main/files).

Supported file types for vector tables include `parquet`, `JSON`, `CSV`, `excel`, `zip`, and `KML`. For raster files `GeoTIFF` and `NetCDF` are supported. If you need a file type that isn't supported, request it on the Fused [Discord channel](https://discord.com/invite/BxS5wMzdRk) or [contribute](/workbench/udf-catalog/#contribute-to-fused) a template to the community.

## Connect your own bucket

<Tag color="#3399ff">Enterprise</Tag> _This feature is accessible to organizations with a Fused Enterprise subscription._

Connect S3 or GCS buckets to access their files interactively from within the File Explorer UI and programmatically from within UDFs.

Contact Fused to set an S3 or GCS bucket on the File Explorer for all users in your organization. Alternatively, set a bucket as a "favorite" so it appears in the File Explorer for your account only.

For buckets that are not publicly accessible, follow these steps to set up the necessary permissions.

### Amazon S3

import Tag from '@site/src/components/Tag'

Set the policy below on your bucket, replacing `YOUR_BUCKET_NAME` with its name. Fused will provide `YOUR_ENV_NAME`.

import Details from '@theme/MDXComponents/Details';

<Details open={false}>


```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Allow object access by Fused fused account",
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::926411091187:role/rt-production-YOUR_ENV_NAME",
                    "arn:aws:iam::926411091187:role/ec2_job_task_role-v2-production-YOUR_ENV_NAME",
                    "arn:aws:iam::926411091187:role/fused_server_role_prod_us_west_2"
                ]
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObjectAttributes",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::YOUR_BUCKET_NAME/*",
                "arn:aws:s3:::YOUR_BUCKET_NAME"
            ]
        }
    ]
}
```

</Details>

Alternatively, use this [Fused app](https://www.fused.io/workbench#app/s/i/fa_2yQFVcbSYR1vW4Aa8zL1HC) to automatically structure the policy for you.

The bucket must enable the following CORS settings to allow uploading files from Fused.

<Details open={false}>

```json
[
    {
        "AllowedHeaders": [
            "range",
            "content-type",
            "content-length"
        ],
        "AllowedMethods": [
            "GET",
            "HEAD",
            "PUT",
            "POST"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": [
            "content-range"
        ],
        "MaxAgeSeconds": 0
    }
]
```
</Details>

### Google Cloud Storage (GCS)


To connect a Google Cloud Storage bucket to your Fused environment, you'll need to follow these steps:

1. Create a Service Account in GCS
Set up a Google Cloud service account with permissions to read, write, and list from the GCS bucket. See the Google Cloud documentation for instructions to:
- [Create a Service Account](https://cloud.google.com/iam/docs/service-accounts-create)
- [Set permissions for the Service Account](https://cloud.google.com/iam/docs/manage-access-service-accounts)

2. Download the JSON Key File
Download the JSON Key file associated with the Service Account. This file contains credentials that Fused will use to access the GCS bucket.

3. Set the JSON Key as a Secret
Set the JSON Key as a secret in the [secrets management UI](/workbench/account/#secrets-management). The secret must be named `gcs_fused`.

Once configured, you'll be able to interact with GCS buckets associated with the Service Account from within Fused.

Note that this only grants access to the File Explorer. To give a UDF access to the bucket, you can set `GS_ACCESS_KEY_ID` and `GS_SECRET_ACCESS_KEY` as secrets and, for example, set them as environment variables in the UDF.

```python showLineNumbers
import os

os.environ["GS_ACCESS_KEY_ID"] = fused.secrets["GS_ACCESS_KEY_ID"]
os.environ["GS_SECRET_ACCESS_KEY"] = fused.secrets["GS_SECRET_ACCESS_KEY"]
```

---

// File: workbench/account

On the Account page, you can edit your Username, view basic account information, and check your usage in the Usage section. You can also access the Server, Auth, and Instance tabs to view authentication settings and instance details.





 <ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/account_page_edit.mp4" width="100%" />




## Kernel

Selecting a kernel in Workbench sets the environment for your session, providing access to Python libraries at runtime.

## Secrets management

Store and manage secrets that are securely encrypted in the backend.

Add and edit snippets, and copy snippets to access them. Use the following syntax to retrieve a snippet from your UDF:

```python showLineNumbers
import fused
fused.secrets["my_secret"]
```

You can also list all available secrets with:

```python showLineNumbers
dir(fused.secrets)
```


## Environment variables
Environment variables in Fused are pre-configured by the Fused team. These variables can be accessed in the usual way through Python's os module:

```python showLineNumbers
import os
os.environ["ENV_VAR_NAME"]
```

---

// File: workbench/preferences

You can access the Preferences page from Workbench by clicking on Preferences in the bottom-left corner.

This page contains general user preferences, experimental features, theme selection, data settings, and other customization options.

import preferences from '/img/workbench/preferences/new_preferences.png';

<div style={{ textAlign: 'center' }}>
  <img src={preferences} alt="File" style={{ width: '90%' }} />
</div>

---

// File: workbench/app-builder/app-overview

# App Builder Overview

The App Builder is an IDE to transform [User Defined Functions (UDFs)](/core-concepts/why/) into interactive, shareable apps.

Data scientists often need to make analytics interactive and accessible to broader audiences. However, building traditional React apps with maps and widgets can be impractical, especially considering prototypes might be discarded. Additionally, frontend frameworks are not well-suited for transforming data or handling large datasets.


With this in mind, the App Builder enables users to build and run apps with [serverless Streamlit](https://github.com/whitphx/stlite), an open source framework to deliver dynamic data apps with just a few lines of Python. These are some of its capabilities to keep in mind:

{/* Read more about the synergy between Streamlit and Fused [here](https://blog.fused.io/2021/11/09/streamlit-fused/). */}

- [Build apps](/workbench/app-builder/app-overview/#build-apps)
- Install [dependencies](/workbench/app-builder/app-overview/#dependencies)
- [Troubleshoot](/workbench/app-builder/app-overview/#troubleshoot)
- [Call UDFs](/workbench/app-builder/app-overview/#call-udfs) and [cache](/workbench/app-builder/app-overview/#caching) responses
- [Share](/workbench/app-builder/app-overview/#share) live apps

import ReactPlayer from 'react-player'

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/app_builder_edit.mp4" width="100%" />



## Build apps

You build apps by writing Python in the code editor, shown below. As you write code you'll notice the app automatically reruns as code and widgets change (configurable in the [preferences](/workbench/preferences)).

You may add [input widgets](https://docs.streamlit.io/develop/api-reference/widgets) that interact with UDFs, display data with [data](https://docs.streamlit.io/develop/api-reference/data) and [text](https://docs.streamlit.io/develop/api-reference/text) elements, and structure the app with [layout components](https://docs.streamlit.io/develop/api-reference/layout).

import Iframe from "@site/src/components/Iframe";
import CODE from "@site/src/app-iframe/python/basic.py";

<div style={{marginTop: '2rem'}}>
<Iframe
  id="iframe-1"
  code={CODE}
  requirements={[
    "/pyarrow/pyarrow-17.0.0-cp312-cp312-pyodide_2024_0_wasm32.whl",
    "micropip",
    "pyodide-unix-timezones", // needed by pyarrow
    "requests",
    "yarl",
    "pydeck",
  ]}
  height="400px"
/>
</div>

\
Try running the code snippets below to acquaint yourself with the App Builder.


```python showLineNumbers
import streamlit as st

st.write("Hello, *Fused!* :rocket:")
```






## Dependencies

To set Python packages for your app, only packages compatible with [Pyodide](https://pyodide.org/en/stable/) are supported. Please get in touch if you need help with a specific package.


You may also choose to install dependencies at runtime to reduce start-up time. Use [micropip](https://pypi.org/project/micropip/) to install packages at runtime. 


```python showLineNumbers
import micropip
await micropip.install(["geopandas", "mercantile"])
```



## Write UDFs

You may define UDFs in the App Builder's code editor and invoke them with [`fused.run`](/workbench/app-builder/#with-fusedrun-beta). This snippet creates a UDF that returns a `DataFrame` with a column of zeros with a length determined by a slider widget.

```python showLineNumbers
import fused
import streamlit as st

count = st.slider("Count", 1, 10, 4)

@fused.udf
def udf(count: int = 1):
    import pandas as pd
    return pd.DataFrame({'values': [0] * count})

df = fused.run(udf, count=count)
st.write(df)
```

You may also run the UDF on a remote worker by setting `engine='remote'` in the `fused.run` call.

```python showLineNumbers
df = fused.run(udf, count=count, engine='remote')
```

## Call UDFs

Apps may call UDFs and load their output into memory. This enables them to run resource-intensive operations and use libraries unsupported by Pyodide. These snippets illustrate a few ways to call UDFs.

### With `fused.run` (beta)

Call a UDF by its shared token with `fused.run` and pass parameters from a [slider](https://docs.streamlit.io/develop/api-reference/widgets/st.slider).

```python showLineNumbers
import fused
import streamlit as st

threshold = st.slider("Count filter", 0, 400, 25)
df = fused.run('UDF_DuckDB_H3_SF', count=threshold)
```


### HTTP endpoints

Call UDF [HTTP endpoints](/core-concepts/run-udfs/run-small-udfs/#http-requests) with the [requests library](https://pypi.org/project/requests/) and pass parameters from a [dropdown selectbox](https://docs.streamlit.io/develop/api-reference/widgets/st.selectbox).

```python showLineNumbers
import streamlit as st
import requests

city = st.selectbox("Select city", ("Boston", "Paris", "New York"))
url = f"https://www.fused.io/server/v1/realtime-shared/fsh_2wEv0k8Xu2grl4vTVRlGVk/run/file?dtype_out_vector=geojson&city={city}"
response = requests.get(url)
st.json(response.json())
```

Render the raster response of UDFs as images.

```python showLineNumbers
import streamlit as st

st.image('https://www.fused.io/server/v1/realtime-shared/fsh_7Yuq2R1Ru1x5hgEEfNDF5t/run/tiles/11/583/787?dtype_out_raster=png')
```

## Caching

It can be helpful to cache the response of UDF calls. To cache a function in Streamlit, decorate it with [`@st.cache_data`](https://docs.streamlit.io/develop/concepts/architecture/caching).

```python showLineNumbers
import fused
import streamlit as st

@st.cache_data
def cached_output():
    return fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9')

df = cached_output()

st.write(fused.run('fsh_1uQkWaPFfB2O7Qy1zzOHS9'))
```


## Share

The App Builder settings menu includes options to generate a URL to share the app or embed it with an `<iframe>`.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/app_builder_share_edit.mp4" width="100%" />


### Shareable links
The app is saved to Fused and referenced by a token, such as `https://www.fused.io/app/fsh_1fFx0CeTHgPVf3SABvrqBz`.



## Troubleshoot

Click "Rerun" on the top-right menu of the App view in case things aren't working as expected.

import Img from '/static/img/app-builder/rerun.png';

<div style={{textAlign: 'center'}}>
<img src={Img} alt="Rerun App" />
</div>

---

// File: workbench/app-builder/add-a-map

For many geospatial applications you will want to add a map to your Fused App, especially if your UDF returns a [Map `Tile`](/core-concepts/filetile/#tile).

This section shows a few examples of how you can do that. While we do recommend you use [`pydeck`](/workbench/app-builder/app-map/#pydeck) (the Python implementation of [deck.gl](https://deck.gl/)) for its versatility, you can use other options like [`folium`](/workbench/app-builder/app-map/#folium)

:::note

    Note that you need to install dependencies with `micropip` inside your Fused app. More on this [here](/workbench/app-builder/app-overview/#dependencies).
:::


## Pydeck

Create a [pydeck](https://deckgl.readthedocs.io/en/latest/layer.html) `TileLayer` that plots a simple GeoDataFrame.

You can find this app [right here](https://www.fused.io/workbench/apps#app/s/i/fa_3qtkNGx3cLhrvd9h9FjDRz) and test it for yourself!

Here's what this would look like:

import ImgStdout from '@site/static/img/app-builder/app-builder-simple-pydeck.png';

<div style={{textAlign: 'center'}}>
<img src={ImgStdout} alt="File" style={{width: 700}} />
</div>

{/* Needs some space here */}
\

```python showLineNumbers
# installing pydeck & geopandas inside Fused app
import micropip
await micropip.install(['pydeck', 'geopandas'])

import fused
import geopandas as gpd
import pydeck as pdk
import streamlit as st

st.write("# Hello World! üëã")
st.write("Here's a simple example of a Fused app plotting NYC metro stations")

DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
gdf = gpd.read_file(DATASET)
# We buffer the points to make them more visible on our map
gdf.geometry = gdf.geometry.buffer(0.001)

# Creating an empty pydeck element
deck = st.empty()

# Initiating pydeck element with view over NYC
view_state = pdk.ViewState(
    latitude=40.73,
    longitude=-73.96,
    zoom=10,
    pitch=0
)

# Creating a GeoJSON layer with our GeoDataFrame
geojson_layer = pdk.Layer(
    'GeoJsonLayer',
    gdf,
)

updated_deck = pdk.Deck(
    layers=[geojson_layer],
    initial_view_state=view_state,
    map_style='mapbox://styles/mapbox/light-v9'
)

deck.pydeck_chart(updated_deck)
```

Read more about how to use PyDeck on their [official documentation](https://deckgl.readthedocs.io/en/latest/index.html)

:::note
    This example shows how to plot a `GeoDataFrame` directly, but you could swap this out for a UDF that returns a `GeoDataFrame` too:

    ```python showLineNumbers
    # DATASET = 'https://raw.githubusercontent.com/python-visualization/folium-example-data/main/subway_stations.geojson'
    # gdf = gpd.read_file(DATASET)
    # highlight-next-line
    gdf = fused.run("YOUR_UDF_RETURNING_A_GDF")
    ```

    Read more about `fused.run` in the [dedicated section](/core-concepts/run-udfs/run-small-udfs/#fusedrun)
:::

## Folium

Create a [streamlit-folium](https://folium.streamlit.app/) `TileLayer` that calls a UDF HTTP endpoint.

```python showLineNumbers
import folium
from streamlit_folium import st_folium

m = folium.Map(location=[22.5, -115], zoom_start=4)
url_raster = 'https://www.fused.io/server/v1/realtime-shared/fsh_3QYQiMYzgyV18rUBdrOEpO/run/tiles/{z}/{x}/{y}?dtype_out_raster=png'
folium.raster_layers.TileLayer(tiles=url_raster, attr='fu', interactive=True,).add_to(m)
st_folium(m)
```

---

// File: workbench/app-builder/app-builder

Learn everything there is about Fused Apps


import DocCardList from '@theme/DocCardList';

<DocCardList className="DocCardList--no-description"/>

---

// File: workbench/udf-builder/udf-builder

The UDF Builder is an IDE to write [User Defined Functions (UDFs)](/core-concepts/why/) and visualize their output.

Data analysis is inherently iterative. It involves loading data, writing code, executing the code, and visualizing results. As developers write and debug their code in the UDF Builder, Fused gives them immediate feedback by automatically re-running the UDF and showing the output. Developers can write UDFs in the Code Editor, preview the output on the Map, and style the visualization. To help debug, the Results pane displays logs and errors generated by the UDF.



- Write UDFs in the [Code Editor](/workbench/udf-builder/code-editor/)
- Manage UDFs as map layers in the [Navigation pane](/workbench/udf-builder/navigation/)
- Preview the UDF's output on the [Map](/workbench/udf-builder/map/) and edit the [Layer Styling](/workbench/udf-builder/styling/) its visualization
- View the UDF's logs in the [Results](/workbench/udf-builder/results/) pane

import ReactPlayer from 'react-player'

<ReactPlayer playsinline={true} className="video__player" playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_udfbuilderv.mp4" width="100%" />

---

// File: workbench/udf-builder/collections

A Collection is a way for users to organize their UDFs into different projects. This allows you to use Workbench for completely different, unrelated projects and organise your UDFs in a cleaner way. 

Currently, all collections are private, meaning only you can see and manage them.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/collections_edit.mp4" width="100%" />

We are actively working on expanding Collection features!

:::note
Collection is still in early access so you need to enable it under "Preferences -> Enable UDF Collections [Beta]" to access it
:::

---

// File: workbench/udf-builder/code-editor

The Code Editor is where developers write UDFs using standard Python libraries and installed [dependencies](/core-concepts/run-udfs/dependencies/). The [Editor](/workbench/udf-builder/code-editor/#editor) tab contain functionality to organize code, create [HTTP endpoints](/core-concepts/run-udfs/run-small-udfs/#http-requests), and configure the UDF.

import CodeEdi from '@site/static/img/workbench/udf-builder/code_editor.png';

<div style={{textAlign: 'center'}}>
<img src={CodeEdi} alt="File" style={{width: '100%'}} />
</div>

## Editor

The editor contains the UDF's [function declaration](/core-concepts/write/#function-declaration). Whenever code is updated, Fused automatically runs the function named `udf` that is decorated with [`@fused.udf`](/core-concepts/write/#fusedudf-decorator) and returns the output. Other UDFs declared in the editor are ignored unless referenced by the main `udf` function.

### Debug

The code editor highlights errors in the code and shows error logs to help debug.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/debug_code_editor.mp4" width="100%" />

### Save a UDFs

UDFs show an asterisk (`*`) next to their name when changes have been made since the last save. Clicking the "Save" icon saves the present state of the UDF under your account's UDFs.

If the "Save" icon appears greyed out, it means you're viewing a read-only version of the UDF. Make a copy to create a new version than can be modified and saved.

import SaveIcon from '@site/static/img/save_icon.png';

<div style={{textAlign: 'center'}}>
<img src={SaveIcon} alt="File" style={{width: '100%'}} />
</div>


## Utils Module

A Fused UDF can import Python objects from its accompanying [utils Module](/core-concepts/write/#utils-module), defined in the Utils Tab's code editor. You can import functions from it in your UDF with `from utils import my_function`.

Here is an example in the Public [Overture_Maps_example](https://github.com/fusedio/udfs/tree/d9f419dadf6267efc79f0ea2092a4dda0e22cb97/public/Overture_Maps_Example) UDF:

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds,
    release: str = "2025-01-22-0",
):
    from utils import get_overture

    gdf = get_overture(
        bbox=bounds,
        release=release,
    )
    return gdf
```

import ImgUtils from '@site/static/img/workbench/udf-builder/utils.png';

<div style={{textAlign: 'center'}}>
<img src={ImgUtils} alt="File" style={{width: '80%', height: '800px'}} />
</div>

### Auto, Tile, and File

On UDF Builder, UDFs can explicitly be set to run as [Tile or File](/core-concepts/filetile/) - or autoselect between the two if the `bounds` object is [typed](/core-concepts/filetile/#bounds-object-types).

import ImageFiletile from '@site/static/img/workbench/udf-builder/file_tile.png';

<div style={{textAlign: 'center'}}>
<img src={ImageFiletile} alt="File" style={{width: '75%'}} />
</div>

---

// File: workbench/udf-builder/viz-styling

# Layer Styling: Visualization Tab


The UDF builder displays data from the UDF into the map view. You can change the visual representation of a UDF's output is configured under the ["Visualization"](/workbench/udf-builder/code-editor/#visualize) tab:

import ReactPlayer from 'react-player';

<ReactPlayer className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/visualization_open_edit.mp4" width="100%" />



## Basics of DeckGL

The Visualization tab in Fused is built on top of [DeckGL](https://deck.gl/docs), a powerful JavaScript framework designed for large-scale data visualizations. DeckGL enables rendering of complex geospatial data using layers, providing flexibility to work with diverse data formats and visualization styles.

### Key Concepts in DeckGL

- **Layer Types**: DeckGL supports various layer types, such as `GeoJsonLayer`, `TileLayer`, and `BitmapLayer`, which are used to render vector, raster, and tiled data respectively.
- **Viewport Optimization**: Layers like `TileLayer` ensure that only the data visible in the current viewport is rendered, optimizing performance for large datasets.
- **Custom Properties**: DeckGL layers can be styled dynamically using properties like `getFillColor`, `getLineColor`, and `getElevation`. These properties can be hardcoded or derived from data attributes.

### Fused-Specific Enhancements

Fused extends DeckGL with custom sub-layers tailored for specific UDF outputs:
- **`rasterLayer`**: Used for raster-based visualizations (e.g., PNG outputs).
- **`vectorLayer`**: Used for vector-based visualizations (e.g., GeoDataFrame outputs).

```JavaScript
{
"tileLayer": {
  "@@type": "TileLayer",
  ...
  },
"rasterLayer": {
  "@@type": "BitmapLayer",
  ...
  },
"vectorLayer": {
  "@@type": "GeoJsonLayer",
  ...
  }
}
```

### Using `hasProp`

The `hasProp` function is a utility for conditional styling based on the presence of specific properties in the data. It allows dynamic adjustments to visualization attributes like color and elevation. Here's an example:

```JavaScript
{
"getFillColor": {
  "@@function": "hasProp",
  "property": "metric",
  "present": "@@=[255, (1 - properties.metric/500) * 255, 0]",
  "absent":[^220][^255][^100]
  },
"getElevation": {
  "@@function": "hasProp",
  "property": "metric",
  "present": "@@=properties.metric",
  "absent": 1
  }
}
```

This ensures that styling adapts dynamically based on whether the specified property (`metric`) exists in the dataset.

For more details on DeckGL layers and their properties, refer to [DeckGL documentation](https://deck.gl/docs/api-reference/layers/geojson-layer#properties).

#### Default map view

The Default map view option is now available under Visualization tab.

It can be set automatically to match the current map view.

import Imgdefaultviewstate from '/img/workbench/layer-styling/default_map_view.png';

<div style={{textAlign: 'center'}}>
<img src={Imgdefaultviewstate} alt="File" style={{width: 800}} />
</div>

You'll notice a few differences to the Editor:
- The Visualization tab isn't written in Python, rather this is a JSON file
- There are a few defaults namely `TileLayer`, `rasterLayer` and `vectorLayer`
- "Surprise Me" button. Try it out for yourself, see what happens! (you can always `Ctrl + Z` to go back if you don't like it)

import ImgVizTOverview from '/img/workbench/layer-styling/surprise_me.png';

<div style={{textAlign: 'center'}}>
<img src={ImgVizTOverview} alt="File" style={{}} />
</div>

You can explore this example right here for yourself. Click on the "UDF Builder" icon on the left to open the code editor:

<iframe
  id="udf_overture_example"
  src="https://www.fused.io/public/UDF_Overture_Maps_Example"
  style={{ width: '100%', height: '500px', border: 'none' }}
  title="Overture Maps Example"
/>

## Basics of Visualization Tab

{/* This might not actually be the case. It is only a `TileLayer` if UDF output is Tile. But what about file? */}

The Visualization tab is built on top of [DeckGL](https://deck.gl/docs), a JavaScript front-end framework build for large dataset visualizations.

Fused works with on a either a File or Tile basis (read more about this [here](/core-concepts/filetile/)). The styling will differ for each:
- `Tile` -> We're leveraging DeckGL's [`TileLayer`](https://deck.gl/docs/api-reference/geo-layers/tile-layer) in the [Map view](/workbench/udf-builder/map/) as a basis allowing us to render only data that is in the viewport at any given moment.
- `File` -> All of the output data is in a single file, so the `TileLayer` part is ignored and the sub-layers `vectorLayer` or `rasterLayer` are used directly:

We have created 2 Fused-specific sub-layers:
- `rasterLayer` for all raster-based visualisations (if your UDF returns a PNG for example)
- `vectorLayer` for all vector-based visualisations (if your UDF returns a `GeoDataFrame` for example)

Under the hood Fused will use whichever sublayer fits your UDF output, but keep in mind that both are defined in each UDF:

```JavaScript
// psuedo-code overview of Visualization tab parameters
{
  "tileLayer": {
    "@@type": "TileLayer",
    ...
  },
  // This is a Fused-specific sublayer for all raster outputs
  "rasterLayer": {
    "@@type": "BitmapLayer"
    ...
  },
  // This is a Fused-specific sublayer for all vector outputs
  "vectorLayer": {
    "@@type": "GeoJsonLayer",
    ...
  }
}
```

Depending on what your UDF returns, you can use different layer types (this is the current supported list):

- Vector [`H3HexagonLayers`](/workbench/udf-builder/styling/#vector-h3hexagonlayer) for UDFs returning a JSON with a column containing H3 indices
- Vector [`GeoJsonLayer`](/workbench/udf-builder/styling/#vector-geojsonlayer) for UDFs returning a `GeoDataFrame` (or any `DataFrame` with a geometry column)
- Raster [`BitmapLayer`](/workbench/udf-builder/styling/#raster-bitmaplayer) for UDFs returning an array

Their visualization styles can be configured with [DeckGL properties](https://deck.gl/docs/api-reference/layers/geojson-layer#properties).

:::tip
  You can hold `Cmd` on MacOS or `Ctrl` on Windows / Linux to tilt the map view.

  You can try it out in the map right below this in the "Vector `H3HexagonLayer`" section üëá
:::

## Vector `H3HexagonLayer`

{/* Not yet quite clear on what H3 return is used for? *ANY* JSON return? */}

At the moment, any `pd.DataFrame` will be rendered using the `vectorLayer` config. If the returned `DataFrame` does not have a `geometry` column and instead has a column with an H3 index you can use `H3HexagonLayer` to display those as hexagons.

In this case, the config column `getHexagon` should be set with the name of the `DataFrame` column of H3 indices. The rendered hexagons can be styled by setting values from a different column in `getFillColor` & `getElevation`.

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 19,
      "tileSize": 256,
      "pickable": true
    },
    "vectorLayer": {
      // highlight-next-line
      "@@type": "H3HexagonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "extruded": false,
      "opacity": 1,
      "coverage": 0.9,
      "lineWidthMinPixels": 5,
      // This assumes your UDF returns a DataFrame with a column called 'hex' containing all the H3 indices
      // highlight-next-line
      "getHexagon": "@@=properties.hex",
      "getLineColor": {
        "@@function": "hasProp",
        "property": "metric",
        "present": "@@=[(1 - properties.metric/500) * 255, 0, 255]",
        "absent": [200, 200, 200]
      },
      // highlight-next-line
      "getFillColor": {
        "@@function": "hasProp",
        "property": "metric",
        "present": "@@=[255, (1 - properties.metric/500) * 255, 0]",
        "absent": [220, 255, 100]
      },
      // highlight-next-line
      "getElevation": {
        "@@function": "hasProp",
        "property": "metric",
        "present": "@@=properties.metric",
        "absent": 1
      },
      "elevationScale": 10
    }
  }
  ```
</details>

<iframe
  id="h3_demo"
  loading="lazy"
  src="/img/deckgl_h3.html"
  height="400px"
  width="100%"
  scrolling="no"
></iframe>

## Vector `GeoJsonLayer`

The visualization of the output of a UDF that returns a `DataFrame` or `GeoDataFrame` can be configured dynamically based on column values. Attributes of the `vectorLayer` can be set to use either hardcoded values or column values, such as:
- Line color (`getLineColor`) and line width (`getLineWidth`)
- Elevation (`getElevation`) with `extruded` set to true
- `lineWidthUnits` helps maintain visual consistency across zoom levels when set to `pixels`


import SurpriseMe from '/img/workbench/layer-styling/surprise_me.png';

<div style={{textAlign: 'center'}}>
<img src={SurpriseMe} alt="File" style={{}} />
</div>

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 15,
      "tileSize": 256,
      "pickable": true
    },
    "vectorLayer": {
      // highlight-next-line
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "extruded": true,
      "getElevation": "@@=properties.stats*1",
      "lineWidthMinPixels": 1,
      "getLineWidth": "@@=properties.stats*10",
      // highlight-next-line
      "getLineColor": {
        "@@function": "hasProp",
        "property": "stats",
        "present": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]",
        "absent": [255, 0, 255]
      },
      "getFillColor": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]"
    }
  }
  ```
</details>

### Color styling

There are 4 ways to set the color for the stroke (`getLineColor`) and fill (`getFillColor`) of a `GeoJsonLayer`. These examples show how to set it for the fill with `getFillColor`, and the same syntax applies for the stroke with `getLineColor`. They all modify the visualization config for this UDF.

```python showLineNumbers
@fused.udf
def udf(
    bounds: fused.types.Bounds = None,
    table_path: str = "s3://fused-asset/infra/building_msft_us/",
):
    import numpy as np
    import random
    utils = fused.load("https://github.com/fusedio/udfs/tree/eda5aec/public/common/").utils
    bounds = utils.bounds_to_gdf(bounds)

    # Load data
    gdf=utils.table_to_tile(bounds, table=table_path)

    # Assign random numbers
    gdf['value'] = np.random.randint(0,10, len(gdf))

    # Assign random classes
    categories = ['residential', 'commercial', 'health', 'public']
    gdf['class'] = [random.choice(categories) for _ in range(len(gdf))]

    print(gdf)
    return gdf
```

#### With a single hardcoded color

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 19,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    },
    "vectorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "lineWidthMinPixels": 1,
      "pointRadiusMinPixels": 1,
      "getFillColor": [20,200,200,100]
    }
  }
  ```
</details>

import ImgSingleColor from '@site/static/img/viz_single.png';

<div style={{ textAlign: 'center' }}>
<img src={ImgSingleColor} alt="File" style={{ width: '80%'}} />
</div>

#### Based on a property value


<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 15,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    },
    "vectorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "extruded": true,
      // highlight-next-line
      "getElevation": "@@=properties.stats*1",
      "lineWidthMinPixels": 1,
      "getFillColor": "@@=[properties.value*50, properties.value*30, properties.value*2]"
    }
  }
  ```
</details>

import ImgProperty from '@site/static/img/viz_property.png';

<div style={{ textAlign: 'center' }}>
<img src={ImgProperty} alt="File" style={{ width: '80%'}} />
</div>


Alternatively, to support a default color when a value is absent.

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 15,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    },
    "vectorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "extruded": true,
      "getElevation": "@@=properties.stats*1",
      "lineWidthMinPixels": 1,
      "getFillColor": {
        "@@function": "hasProp",
        "property": "value",
        "present": "@@=[properties.value*50, properties.value*3, properties.value*2]",
        "absent": [
          255,
          0,
          255
        ]
      }
    }
  }
  ```
</details>

#### Using `colorCategories`

To set the color with [colorCategories](https://deck.gl/docs/api-reference/carto/styles#colorcategories), use the `attr` property to specify the table column for the values, and the  `colors` property to define the desired [color palette](https://carto.com/carto-colors/).

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 19,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    },
    "vectorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "lineWidthMinPixels": 1,
      "pointRadiusMinPixels": 1,
      "getFillColor": {
        "@@function": "colorCategories",
        "attr": "class",
        "domain": [
          "residential",
          "commercial",
          "health",
          "public"
        ],
        "colors": "Bold"
      }
    }
  }
  ```
</details>

import ImgCategory from '@site/static/img/viz_category.png';

<div style={{ textAlign: 'center' }}>
<img src={ImgCategory} alt="File" style={{ width: '80%'}} />
</div>

:::warning
Note that unexpected behaviors may arise if too many domains are used.
:::

#### Using `colorContinuous`

To set the color with [colorContinuous](https://deck.gl/docs/api-reference/carto/styles#color-continuous), use the `attr` property to specify the table column for the values, and the `colors` property to define the desired [color palette](https://carto.com/carto-colors/).

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
  "tileLayer": {
    "@@type": "TileLayer",
    "minZoom": 0,
    "maxZoom": 19,
    "tileSize": 256,
    "pickable": true
  },
  "hexLayer": {
    "@@type": "H3HexagonLayer",
    "stroked": true,
    "filled": true,
    "pickable": true,
    "extruded": true,
    "opacity": 1,
    "coverage": 0.9,
    "lineWidthMinPixels": 5,
    "getHexagon": "@@=properties.hex",
    "getLineColor": {
      "@@function": "hasProp",
      "property": "count",
      "present": "@@=[(1 - properties.count/500) * 255, 0, 255]",
      "absent": [
        200,
        200,
        200
      ]
    },
   "getFillColor": {
      "@@function": "colorContinuous",
      "attr": "count",
      "domain": [
        0,
        1000
      ],
      "steps": 15,
      "colors": "SunsetDark",
      "nullColor": [
        184,
        184,
        184
      ]
    },
    "getElevation": {
      "@@function": "hasProp",
      "property": "count",
      "present": "@@=properties.count",
      "absent": 1
    },
    "elevationScale": 10
  }
}
  ```
</details>

<div style={{ textAlign: 'center' }}>
  <img src="/img/workbench/layer-styling/color_continuous.png" alt="Color Continuous" style={{ width: '80%' }} />
</div>


## Raster `BitmapLayer`

Raster layers can be set to display a tooltip on hover by setting the `pickable` property to `true`. See [DeckGL documentation](https://deck.gl/docs/api-reference/layers/bitmap-layer#pixel-picking).

import Crops from '/img/workbench/layer-styling/crops.png';

<div style={{textAlign: 'center'}}>
<img src={Crops} alt="File" style={{}} />
</div>

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "TileLayer",
      "minZoom": 0,
      "maxZoom": 19,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    }
  }
  ```
</details>

:::tip
The transparency of raster images can be set in two ways:

1. In RGB images, the color black (0,0,0) is automatically set to full transparency.

If a 4-channel array is passed, i.e. RGBA, the value of the 4th channel is the transparency.

:::

## Custom `loadingLayer` and `errorLayer`

When `tileLayer` has `"@@type": "DebugTileLayer"` set, `loadingLayer` and `errorLayer` can be configured to show the user that the UDF is still processing or that an error occurred. This is helpful for debugging.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/debug_layer2.mp4" width="100%" />

<details>
  <summary>Expand to see Visualise code</summary>
  ```json
  {
    "tileLayer": {
      "@@type": "DebugTileLayer",
      "minZoom": 0,
      "maxZoom": 15,
      "tileSize": 256,
      "pickable": true
    },
    "rasterLayer": {
      "@@type": "BitmapLayer",
      "pickable": true
    },
    "vectorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "extruded": true,
      "getElevation": "@@=properties.stats*1",
      "lineWidthMinPixels": 1,
      "getLineColor": {
        "@@function": "hasProp",
        "property": "stats",
        "present": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]",
        "absent": [
          255,
          0,
          255
        ]
      },
      "getFillColor": "@@=[properties.stats*5, properties.stats*3, properties.stats*2]"
    },
    "loadingLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": false,
      "pickable": true,
      "lineWidthMinPixels": 10,
      "getLineColor": [
        25,
        55,
        0,
        255
      ],
      "getFillColor": [
        5,
        20,
        255,
        40
      ]
    },
    "errorLayer": {
      "@@type": "GeoJsonLayer",
      "stroked": true,
      "filled": true,
      "pickable": true,
      "lineWidthMinPixels": 10,
      "getLineColor": [
        255,
        255,
        0,
        255
      ],
      "getFillColor": [
        255,
        20,
        255,
        40
      ]
    }
  }
  ```
</details>

---

// File: workbench/udf-builder/map

As developers edit UDFs in the [Code Editor](/workbench/udf-builder/code-editor/) and explore data, they can receive immediate visual feedback on how the code's transformations affect the data.

## Geospatial data

Fused will render `gpd.GeoDataFrame`, `gpd.GeoSeries`, and `shapely geometry` UDF outputs as geometries on the map if their CRS is `EPSG:4326`. If the CRS differs, Fused will make a best-effort to project and render the geometries correctly.

To render array (raster) objects on the map, they must be `uint8` and define their spatial extent. Objects like `xarray.DataArray` already contain spatial metadata. The spatial extent of arrays without spatial metadata, like `numpy.ndarray`, can be specified with a geometry object or an array bounds as `[xmin, ymin, xmax, ymax]`. If the bounds are not present, they default to `(-180, -90, 180, 90)`.

```python showLineNumbers
return np.array([[‚Ä¶], [‚Ä¶]]), [xmin, ymin, xmax, ymax]
```

For UDFs that return [map Tiles](/core-concepts/filetile/#tile), Fused runs the UDF for only the Tiles in the viewport. This enables efficient analysis on a fraction of a dataset.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/map_overture.mp4" width="100%" />

### Map controls

The map can be panned by dragging the viewport, zoomed in and out, and rotated with `CMD` + Click + drag.

The top of the map has controls to interact with the viewport. These include an address search bar, a [basemap](https://docs.mapbox.com/api/maps/styles/) selector, a screenshot button, a fullscreen toggle, and a dropdown to freeze, resume, or reset UDF execution.


You can change the basemap by setting it in the map style settings, located at the top right of the UDF Builder map. Currently, light, dark, satellite, and blank basemaps are supported.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/mapcontrols_edit.mp4" width="100%" />

## Debug

Clicking a rendered feature enters debug mode. Pressing "Escape" or clicking "Clear debug selection" at the bottom of the map exits debug mode.

When data renders successfully on the map, clicking or hovering on it shows attributes for selected pixels or geometries. When data doesn't render, clicking errored tiles shows error messages and debugging information. Additional debugging information can be found in the [Results](/workbench/udf-builder/results/) pane.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={true} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/debugclick.mp4" width="50%" />

---

// File: workbench/udf-builder/results

Exploring and analyzing data involves scanning logs, previewing intermediary outputs, and debugging errors.

The Results section of the UDF Builder dynamically displays information related to the UDF execution on the [toolbar](/workbench/udf-builder/results/#toolbar) and across the [Stdout](/workbench/udf-builder/results/#stdout), [Selected object](/workbench/udf-builder/results/#selected-object), and [Request details](/workbench/udf-builder/results/#request-details) tabs.


### Toolbar

The toolbar at the top of the Results pane shows the latitude and longitude of the viewport center, average UDF execution time, and a button to exit the map [Debug mode](/workbench/udf-builder/map/#debug).



## Stdout

The Stdout tab shows the `stdout` of the UDF's execution logs and the `stderr` trace if execution raises an error. These logs are ephemeral - they are passed to the frontend for debugging but are not persisted anywhere.

import ImgStdout from '@site/static/img/workbench_stdout.png';

<div style={{textAlign: 'center'}}>
<img src={ImgStdout} alt="File" style={{width: 800}} />
</div>


## Selected object

The "Selected object" tab shows a GeoJSON of the feature selected on the map.

import ImgSelectedobject from '@site/static/img/workbench_selectedobject.png';

<div style={{textAlign: 'center'}}>
<img src={ImgSelectedobject} alt="File" style={{width: 800}} />
</div>


## Request details

When a UDF runs as [Tile](/core-concepts/filetile/#tile), this last tab shows a GeoJSON with the geometry and properties for the `bounds` object the UDF was called with.

import Imgrequestdetails from '@site/static/img/workbench_requestdetails.png';

<div style={{textAlign: 'center'}}>
<img src={Imgrequestdetails} alt="File" style={{width: 800}} />
</div>

---

// File: workbench/udf-builder/navigation

The Navigation pane on the left side of the UDF Builder displays the UDFs available to the user. UDFs are listed as layers. Their order corresponds to their stacking order on the map. Each layer includes icons to delete, reorder, and toggle the visibility of UDFs. The selected layer becomes the "active" layer. Its code appears in the [Code Editor](/workbench/udf-builder/code-editor/) and its output in the [Results](/workbench/udf-builder/results/) pane.

## Zoom to Layer

"Zoom to layer" allows you to quickly zoom the map to the location of the relevant UDF layer. You would use it when you want to focus the map on a specific area or features defined by the layer, especially when the UDF has a spatial component or a default view state.
UDFs that return an object with a spatial component or have a [default view state](/workbench/udf-builder/code-editor/#default-view-state) show a "zoom to layer". Clicking it zooms the map to the relevant location.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/zoom_to_layer.mp4" width="100%" />

## Layer Visibility

The layer toggle feature allows users to show or hide specific visualizations on the map. These layers provide enhanced map insights, making spatial data easier to understand and interpret.

<ReactPlayer playsinline={true} className="video__player" playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/layers.mp4" width="100%" />

## UDF Options

The UDF list includes several interactive options to manage and organize functions efficiently.

### Drag and Drop

Users can reorder UDFs by dragging them up and down within the list. To do this, they need to hover over the UDF name to see the drag handle.

### Close/Delete

If the UDF is saved, a "Close UDF" button is available to exit the editor. If the UDF is unsaved and still being edited, a "Delete UDF" button replaces it, allowing users to discard the changes.

import udfOpt from '/img/workbench/udf-builder/udf_options2.png';

<div style={{ textAlign: 'center' }}>
  <img src={udfOpt} alt="File" style={{ width: '75%' }} />
</div>

## Share

The Share button in Workbench provides access to all options to [share](/workbench/udf-builder/code-editor/#share-snippets) the UDF.


### Share snippets

UDFs saved in the UDF Builder can be called with [HTTP endpoints](/core-concepts/run-udfs/run-small-udfs/#http-requests) using [the public UDF name](/core-concepts/run-udfs/run-small-udfs/#public-udf-name) and/or [tokens](/core-concepts/run-udfs/run-small-udfs/#token).

The "Share" section shows snippets to run the UDF using a public token. This allows any application to invoke the UDF without authentication - including `cURL` calls, [Lonboard](/user-guide/out/lonboard/), [Leaflet](/user-guide/out/leaflet/), [Mapbox](/user-guide/out/mapbox/), [Google Sheets](/user-guide/out/googlesheets/), [DuckDB](/user-guide/out/duckdb/), the Fused [App Builder](/workbench/app-builder/), and Python applications with [`fused.run`](/core-concepts/run-udfs/).

import ImgShare from '@site/static/img/workbench/udf-builder/new_share.png';

<div style={{textAlign: 'center'}}>
<img src={ImgShare} alt="File" style={{width: '100%'}} />
</div>

### Private snippets

The "Snippets" section shows snippets that can only be called by services authenticated with a private token. These include `Links` and `Python snippets`.

import ImageSnippets from '@site/static/img/workbench/new_snippets.png';

<div style={{textAlign: 'center'}}>
<img src={ImageSnippets} alt="File" style={{width: '100%'}} />
</div>

### Metadata

#### Image preview

UDFs in the [UDF Catalog](/workbench/udf-catalog/) show a preview thumbnail. The image can be set in the "Image preview" field by uploading the image.

import Imgsetimage from '@site/static/img/workbench/udf-builder/img_preview.png';

<div style={{textAlign: 'center'}}>
<img src={Imgsetimage} alt="File" style={{width: 800}} />
</div>

#### Tags

UDF tags can be set to help with discoverability in the [UDF Catalog](/workbench/udf-catalog/).

import Imgtags from '@site/static/img/workbench/udf-builder/tags.png';

<div style={{textAlign: 'center'}}>
<img src={Imgtags} alt="File" style={{width: 800}} />
</div>

#### Description

UDFs can be documented using [Markdown](https://www.markdownguide.org/basic-syntax/) with a brief description of their purpose, code, and associated datasets. The description appears in the UDF profile and `README.md` file.

## Toolbar

The toolbar at the top of the code editor includes buttons to duplicate, download, close and delete a UDF, as well as view its history and push it to GitHub.

### GitHub

Organizations with the [GitHub Integration](/core-concepts/content-management/git/#connect-your-github-repository) enabled can push UDFs to a GitHub repository as a [Pull Request](https://docs.github.com/en/pull-requests) or restore a prior version of a UDF from the commit history.

### Download

Clicking "Download" downloads a `.zip` file with the UDF code, module, and configuration.

import ImageGitDropdown from '@site/static/img/core-concepts/github_dropdown.png';

<div style={{textAlign: 'center'}}>
<img src={ImageGitDropdown} alt="File" style={{width: '80%'}} />
</div>

### Default parameter values

UDFs by default run with the [parameters](/core-concepts/write/#typed-parameters) specified in their [function declaration](/core-concepts/write/#function-declaration). Predefined default parameter values take precedence and appearing within the Parameters section on the left side of the screen, under the selected UDF's options.

<ReactPlayer playsinline={true} className="video__player" loop={true} playing={false} muted={true} controls height="100%" url="https://fused-magic.s3.us-west-2.amazonaws.com/workbench-walkthrough-videos/docs_rewrite/default_parameters.mp4" width="100%" />